NOVEMBER/DECEMBER 2016

 
  

ie

WEB SCIENCE

Editor: Elena Simperl, University of Southampton, e.simperl@soton.ac.uk

  

Data Mining and Automated
Discrimination: A Mixed
Legal/Technical Perspective

Laura Carmichael and Sophie Stalla-Bourdillon, University of Southampton
Steffen Staab, University of Koblenz-Landau and University of Southampton

IVI any industries are taking advantage of data

analytics and a wealth of accessible (per-
sonal) data on the Web to bring about various auto-
mated, socially sensitive decisions. These decisions

are socially sensitive in the sense that they are likely
to have major impacts on the individuals subject to
them (for example, being refused a mortgage).

The move toward more complex forms of socially
sensitive decision making (for example, through ma-
chine learning) has various benefits, including the
greater speed in which decisions are made, the abil-
ity to process multiple decisions at the same time,
and the potential for an unprecedented amount of
data to enrich the decision-making process. For in-
stance, Kreditech Group (www.kreditech.com), a
German technology company and financial services
provider, openly uses machine learning for con-
sumer credit scoring.' Its big data credit scoring al-
gorithm processes about 20,000 constantly chang-
ing data points, including a person’s online shop-
ping and browsing behavior.

Despite the benefits of such automated decision
making (for example, greater personalization and
predictive analysis), how do we as IT professionals
know whether such automated socially sensitive
decisions are fair and fit for purpose? In other
words, how can we be certain that that the use of
data analytics for socially sensitive decision making
does not lead to unjust discrimination against
certain groups of individuals because of their age,
ethnicity, gender, or sexual orientation?

Automated Discrimination

Automated discrimination refers to instances in
which unfair treatment of certain groups of indi-
viduals occurs as a direct result of data analytics.

For instance, it can manifest as unfair pricing and
marketing of products and services.” This is also
known as “weblining.”?

Although most providers do not want their al-
gorithms to be the source of discrimination, au-
tomated discrimination could occur unintention-
ally and through proxies. A proxy is defined as a
nonprotected attribute used in lieu of a protected
characteristic; for example, a neighborhood could
be used as a proxy for ethnicity. For instance,
in the 1980s, a computer program designed to
screen applicants to St. George’s Hospital Medical
School was found to have automatically (although
unknowingly) discriminated against female and
minority applicants.4 The key problem is that
data-mining algorithms could inherit (historic) bi-
ases and prejudices; the patterns they uncover can
merely reflect current inequalities.*

Discovery and Prevention
The discovery and prevention of automated dis-
crimination is not without difficulty. Although
data-mining algorithms are well-known and un-
derstood, their actual results (that is, classifying
an individual or predicting its net value for the
company—for example, as realized in a neural
network) might be too intricate even for technical
experts to understand. In theory, we could scruti-
nize every aspect of such a predictor, but in prac-
tice, we often would not understand its internal
workings. Thus, we often are bound to assess only
the (un)fairness of its treatments from how it be-
haves with regard to actual individuals.
Furthermore, it is important that any allegation of
automated discrimination is thoroughly scrutinized.
In most cases, a simple statistical check is not
sufficient. For instance, a trend in anumber of datasets

1541-1672/16/$33.00 © 2016 IEEE

Published by the IEEE Computer Society

51
52

can be reversed when the datasets are
combined—that is, Simpson’s paradox.
For an example of Simpson’s par-
adox, consider efforts by PJ. Bickel
and colleagues to analyze whether
the decision to select applicants to
the University of California, Berke-
ley, in 1973 was influenced by gen-
der. Bickel and colleagues started
with the “simplest approach”—to ag-
gregate the data for the entire cam-
pus. In total, about 44 percent of the
male applicants were admitted to the
university, whereas about 35 percent
of the female applicants were admit-
ted.¢ At first glance, it appeared there
had been some form of discrimina-
tion based on gender—however, this
was later found to be misleading. The
initial statistical analysis did not con-
sider how the entry process to differ-
ent departments varied. Some depart-
ments attracted fewer female appli-
cants than others—for example, only
2 percent of applicants to mechanical
engineering were female, but overall
acceptance rates to mechanical engi-
neering were higher than for other de-
partments. Disaggregating the data,
the researchers examined the data of
each department individually. In fact,
the majority of departments showed
a “small but statistically significant
bias” in favor of female applicants.®

State of the Art:
Discrimination Discovery
Since 2008,’ the data-mining com-
munity has directly responded to
the challenges posed by automated
discrimination through the emerg-
ing area of discrimination-aware
data mining (DADM). DADM is fo-
cused on the discovery of unfair dis-
criminatory practices and outcomes,
which are concealed within datasets
of historical decisions.’-? A principal
DADM approach centers on the ex-
traction and analysis of discrimina-
tory classification rules.”

A significant proportion of the lit-
erature highlights the importance of
“legally grounded rules” and “legally
protected groups” as part of DADM.
For instance, DADM research al-
ready recognizes a need to use a legal
definition of discrimination.’ Despite
the recognition of legal factors, there
does not appear to be a proposed
DADM model, tool, or framework
that sufficiently addresses the com-
plexities of a specific legal framework
(for example, the UK) and its particu-
lar jurisdictional constraints.

Hence, the AI community can-
not confront the challenges posed by
automated discrimination in isola-
tion. A greater legal understanding is
therefore crucial to enrich the exist-
ing DADM research and ensure that
those directly responsible for socially
sensitive decision-making algorithms
remain legally compliant. There is
therefore an opportunity for techni-
cal and legal experts to come together
and address the challenges of auto-
mated discrimination discovery and
prevention.

Interdisciplinarity: Unlawful
Discrimination
The legal framework for automated
discrimination is difficult for IT pro-
fessionals to navigate. It is not only
multijurisdictional, but it also spans
various legal areas, including equal-
ity and data protection laws. For in-
stance, two recent US reports high-
light how automated discrimination
is being dealt with under the US legal
framework.!°!! Notably, Article 14 of
the European Convention on Human
Rights—the reach of which extends
far beyond the 28 member states of
the EU and remains relevant even af-
ter Brexit—prohibits discrimination.
While Article 21 of the Charter of
Fundamental Rights of the EU also
prohibits discrimination, equality law
continues to be largely regulated on a

www.computer.org/intelligent

national scale in the EU. For instance,
in the UK, Section 4 of the Equality
Act of 2010 provides nine categories
of protected characteristics: age, dis-
ability, gender reassignment, marriage
and civil partnership, pregnancy and
maternity, race, religion or belief, sex,
and sexual orientation. However, this
list is restrictive, because data-mining
algorithms could use other sensitive
attributes beyond this list that could
cause unfair treatment to certain
groups of individuals. For instance,
there is no category protecting socio-
economic status (although France, for
example, has recently added poverty
as a protected characteristic!*). More-
over, pursuant to the UK Equality
Act of 2010, data-mining algorithms
could indirectly discriminate where
they use proxies (background data
such as neighborhood) for particular
legally protected characteristics (for
example, ethnicity).

As we mentioned earlier, equal-
ity law is only one aspect of the legal
framework for automated discrimi-
nation. The recently adopted Euro-
pean General Data Protection Regu-
lation (GDPR) 2016/679 provides a
unified approach across the 28 mem-
ber states of the EU. Similar to the
protected characteristics outlined
by the Equality Act, Article 9 of the
GDPR outlines several “special cate-
gories of personal data” whose pro-
cessing is prohibited—subject to 10
exceptions. Note that Article 9 in-
cludes some categories of data that
are not listed as protected charac-
teristics by Section 4 of the Equality
Act—those are biometric and genetic
data. However, it does not explic-
itly mention some of these protected
characteristics, such as age, gender
reassignment, and marriage and civil
partnership.

Furthermore, pursuant to Articles
13(2)(£) and 14(2)(g) of the GDPR, data
subjects have the right to be informed

IEEE INTELLIGENT SYSTEMS
about the logic involved in auto-
mated decision making. However,
where data-mining algorithms are
concerned, will data controllers be
able to fully explain this logic? Al-
though technical methods are avail-
able (for example, Erik Strumbelj and
Igor Kononenko examine a sensitivity
analysis-based method for explain-
ing prediction models!3), they mostly
say, “This feature’—for example, in-
come—“weighs x, and as a result you
have been assigned to tariff-A as op-
posed to tariff-B.” This statement is
neither logically crisp nor completely
true. The statement suggests a linear
regression, but most data-mining al-
gorithms do not constitute linear re-
gression. Thus, this explanation is
only an approximation of what is go-
ing on underneath. Furthermore, can
data subjects be certain that such logi-
cal explanations will be expressed in
terms that the layperson understands?

Finally, it is unclear to what ex-
tent personal data reused by decision
makers must be accurate. The data
subject has a right to rectification un-
der Article 16 of the GDPR—that is,
the data controller must remedy in-
accurate data about that individual
without delay. However, in the recent
past, different legal approaches have
been taken to data accuracy. In the
case of Smeaton v. Equifax, the UK’s
Court of Appeal held that personal
data reused by credit reference agen-
cies did not have to be absolutely ac-
curate under the UK Data Protection
Act 1998, although agencies should
take reasonable steps to ensure that
it is up to date. However, the land-
mark “right to be forgotten” ruling
(taken by the Court of Justice of the
European Union in the case of Google
Spain v. AEPD, which centered on
auction notice of a repossessed home)
gave rise to a duty to process accurate
and timely data. Thus, it will be in-
teresting to see how Article 16 is in-

NOVEMBER/DECEMBER 2016

terpreted in the coming years, and
how the legal position on personal
data accuracy unfolds across sectors.

Accountability and
Transparency
To reiterate, because the legal frame-
work is multifaceted and difficult to
navigate, a dialogue between com-
puter scientists and lawyers is there-
fore crucial for the development of a
robust legal-technical approach to
the prevention and discovery of au-
tomated discrimination. A key chal-
lenge for policy makers and regula-
tors is how data-mining algorithms
can be made more accountable, both
legally and technically, to the people
they are profiling. Greater transpar-
ency could be achieved by placing a
limited duty of disclosure on those
responsible for such automated de-
cision making. This may involve the
release of (redacted) de-identified
input and output data and a dis-
crimination impact assessment, in
addition to data-protection obliga-
tions (in particular, Article 35 of
the GDPR). However, at the same
time, it must be considered whether
it is possible to formulate such a duty
without jeopardizing intellectual
property rights. The French Digital
Republic Bill of 2016, for example,
requires public sector bodies of more
than 50 employees to make publicly
available, in an open and easily re-
usable format, the rules defining the
main algorithmic processing used in
the accomplishment of their missions
when such processing forms the basis
of individual decisions.'¢
Furthermore, the capture of robust
provenance information that covers
organizational practices, processes,
and principles pertaining to auto-
mated discrimination will be vital, not
only for transparency and account-
ability but to ensure data accuracy
and uphold the right to rectification.

www.computer.org/intelligent

Challenges for Al and

the Law

Although DADM offers an excellent
body of knowledge to build on, sev-
eral challenges remain for the AI and
legal communities to jointly confront.
The principal challenge is the devel-
opment of interdisciplinary tools that
allow for the targeted use of data
analytics and data mining to un-
cover discrimination and sufficiently
address the diversity of enforcement-
related issues. Greater interdisciplin-
ary understanding is also required of
the relationship between existing so-
ciocultural, legal, and technical safe-
guards that aim to minimize unfair
treatment.

Therefore, there is a pressing need
for a legal-technical argumentation
framework that helps decision makers
assess the fairness of their black-box
decision-making systems by provid-
ing arguments for and against alle-
gations of automated discrimination.
This framework must draw together
existing machine learning algorithms
that discover and prevent unfair treat-
ment (such as DADM) as well as con-
sider legal norm compliance, which
spans a wide range of pertinent legal
measures (for example, equality, data
protection, and consumer protection
laws).

As part of the development of this
framework, it would be useful to ex-
amine the effectiveness of different
quantitative and qualitative methods.
We can do this in several ways. First,
look for correlations within all input
data: What input and output data cor-
relate with legally protected character-
istics and special categories of personal
data? Is there potential for proxies?

Second, consider the “comparative
individual” counterargument. For in-
stance, in the context of automated
decision making within the insurance
industry, are people with the same
or very similar risk category scores

53
54

placed on the same tariffs? If men and
women of the same profession with
the same risk assessment pay the same,
there is no discrimination. However,
one counterexample is probably not
enough to disprove discrimination.
Third, apply the test of reasonable-
ness: Is there a justification for this
discrimination? For instance, under
Section 13 of the Equality Act of 2010,
age discrimination can be justified on
the grounds that the discrimination is
for the purposes of a legitimate aim.
An example of a legitimate aim might
be where an applicant for a firefighter
job is asked to undergo a fitness test—
it is more likely that a younger person
will pass.!7 However, legitimate aims
are assessed on a case-by-case basis.
Fourth, assess internal processes
and procedures—for instance, to
what extent are those directly respon-
sible for a data-mining algorithm
transparent and accountable? Has
there been a discrimination impact
assessment? Is there a transparency
report?!® What provenance informa-
tion is available? This latter question
is particularly important because the
data-mining process might be work-
ing well, but a “wrong” selection of
data in the overall decision-making
process could lead to problematic re-
sults—even if the test of the data min-
ing would not show negative effects.
Finally, use rule-discovery algorithms
to discover rules within the data-
mining algorithm. However, remember
that rule-discovery algorithms have
limitations—that is, multiple rules can
give the same or similar approximations
with different discriminatory outcomes.

Promoting Fair Treatment

Automated discrimination is just one of
many issues that must be addressed in
the overall effort to promote fair treat-
ment. Although working toward the le-
gal-technical argumentation framework
briefly outlined in this article is no pan-

acea, it could potentially help those di-
rectly involved with the design, develop-
ment, and use of data analytics to better
safeguard data subjects from discrimi-
nation. Furthermore, greater account-
ability and transparency could better in-
form data subjects about how our digi-
tal footprints are (mis)used and about
our associated rights (for example, un-
der equality and data protection laws).

A. the digital age matures, we be-
come more connected (for example,
through the Internet of Things), our
digital footprints continue to expand,
and more socially sensitive decisions
are generated through data analytics.
Automated discrimination only has the
potential to increase. We need to recog-
nize that advanced forms of automated
socially sensitive decision-making sys-
tems have the potential to discriminate
just as their nonautomated counter-
parts have done in the past.*

Acknowledgments

We thank the Web Science Institute (WSI) for
supporting Part I of our research project—
Observing and Recommending from a Social
Web with Biases—as part of its Pamp-Priming
Projects 2015-2016, as well as Southampton
Law School for supporting Part II. For more
information about this research, you can find
our preliminary working report at http://
eprints.soton.ac.uk/id/eprint/393352.

References
1. E. Reynolds, “The Next Industrial
Revolution Is Coming—And It Will Be
Fuelled by AI,” Wired, 2016, 22 June 2016;
www.wired.co.uk/article/ai-revolution-
alexander-graubner-muller-kreditech.
2. A. Danna and O.H. Gandy Jr., “All
That Glitters Is Not Gold: Digging
Beneath the Surface of Data Mining,”
J. Business Ethics, vol. 40, no. 4, 2002,
pp. 373-386.
. M. Stepanek, “Weblining: Companies

ww

Are Using Your Personal Data to

www.computer.org/intelligent

1

1

1

1

oN

oo

©

0.

1.

2.

we

Limit Your Choices—And Force
You to Pay More for Products,”
Bloomberg Business Week, 3 Apr.
2000; www.bloomberg.com/news/
articles/2000-04-02/weblining.

.S. Lowry and G. Macpherson, “A Blot

on the Profession,” British Medical J.,
vol. 296, no. 6623, 1988, pp. 657-658.

.S. Barocas and A.D. Selbst, “Big Data’s

Disparate Impact,” California Law
Rev., vol. 104, 2016, pp. 1-62.

. PJ. Bickel, E.A. Hammel, and J.W.

O’Connell, “Sex Bias in Graduate
Admissions: Data from Berkeley,” Science,
vol. 187, no. 4175, 1975, pp. 398-404.

D. Pedreshi, S$. Ruggieri, and F. Turini,
“Discrimination-Aware Data Mining,”
Proc. 14th ACM SIGKDD Int'l Conf.
Knowledge Discovery and Data
Mining, 2008, pp. 560-568.

.S. Ruggieri, D. Pedreschi, and F. Turini,

“DCUBE: Discrimination Discovery in
Databases,” Proc. ACM SIGMOD Int’!
Conf. Management of Data, 2010,

pp. 1127-1130.

A. Romei, S. Ruggieri, and F. Turini,
“Discrimination Discovery in Scientific
Project Evaluation: A Case Study,”
Expert Systems with Applications, vol.
40, no. 15, 2013, pp. 6064-6079.

Big Data: A Tool for Inclusion or Exclu-
sion? Understanding the Issues, Federal
Trade Commission, Jan. 2016; www.ftc.
gov/system/files/documents/reports/big-
data-tool-inclusion-or-exclusion-under-
standing-issues/160106big-data-rpt.pdf.
C. Mufioz, M. Smith and D_J. Patil,

Big Data: A Report on Algorithmic
Systems, Opportunity, and Civil Rights,
May 2016; www.whitehouse.gov/sites/
default/files/microsites/ostp/2016_0504_
data_discrimination.pdf.

Loi n°2016-832 du 24 Juin 2016 Visant
a Lutter contre la Discrimination a
Raison de la Précarité Sociale, 2016 (in
French); www. legifrance.gouv.fr/eli/
1oi/2016/6/24/AFSX1514889L /jo/texte.

.E. Strumbelj and I. Kononenko,

“Explaining Prediction Models and
Individual Predictions,” Knowledge and

IEEE INTELLIGENT SYSTEMS
Information Systems, vol. 41, no. 3,
2014, pp. 647-665.

14. Smeaton v. Equifax Plc, England and
Wales Court of Appeal, Civ. 108, 2013.

15. Google Spain v. AEPD, EU European
Court of Justice, C-131/12, 3 WLR
659, 2014.

16. Projet de Loi pour une République Nu-
mérique: Procédure Accélérée Engagée
par le Gouvernement le 9 Décembre
2015, Article 4, item 802 (in French);
www.senat.fr/dossier-legislatif/
pjl15-325.html#timeline-7.

17. “Justifying Discrimination,” Citizens
Advice, 2016; www.citizensadvice.

org.uk/discrimination/what-are-the-

different-ty pes-of-discrimination/
justifying-discrimination.

18. A. Datta, S. Sen, and Y. Zick,
“Algorithmic Transparency via
Quantitative Input Influence: Theory and
Experiments with Learning Systems,”
Proc. 37th [EEE Symp. Security and
Privacy, 2016, pp. 598-617.

Laura Carmichael is a research fellow at
the University of Southampton. Contact her

at laural4carmichael@gmail.com.

Sophie Stalla-Bourdillon is an associ-
ate professor in IT law and director of the
Institute for Law and the Web at the Uni-

versity of Southampton. Contact her at
s.stallabourdillon@soton.ac.uk.

Steffen Staab is a professor for database
and information systems and head of the
Institute for Web Science and Technologies
at the University of Koblenz-Landau and
holds a chair for Web and Computer Science
at the University of Southampton. Contact

him at s.r.staab@soton.ac.uk.

 

Read your subscriptions
through the myCS pub-
lications portal at http://

myc

 

 

miycs.computer.org.

 

ADVERTISER INFORMATION

 

 

Advertising Personnel

Email: manderson@computer.org

Marian Anderson: Sr. Advertising Coordinator

Southwest, California:

Mike Hughes

Email: mikehughes@computer.org
Phone: +1 805 529 6790

Phone: +1 714 816 2139 | Fax: +1 714 821 4010

Southeast:

Heather Buonadies

Email: h.buonadies@computer.org
Phone: +1 973 304 4123

Fax: +1 973 585 7071

Sandy Brown: Sr. Business Development Mgr.
Email sbrown@computer.org
Phone: +1 714 816 2144 | Fax: +1 714 821 4010

 

Advertising Sales Representatives (display)

 

Advertising Sales Representatives (Classified Line)
Central, Northwest, Far East:
Eric Kincaid
Email: e.kincaid@computer.org
Phone: +1 214 673 3742
Fax: +1 888 886 8599

Heather Buonadies

Email: h.buonadies@computer.org
Phone: +1 973 304 4123

Fax: +1 973 585 7071

 

Northeast, Midwest, Europe, Middle East:

Ann & David Schissler

Email: a.schissler@computer.org, d.schissler@computer.org
Phone: +1 508 394 4026

Fax: +1 508 394 1707

Advertising Sales Representatives (Jobs Board)

Heather Buonadies

Email: h.buonadies@computer.org
Phone: +1 973 304 4123

Fax: +1 973 585 7071

 

 

 

NOVEMBER/DECEMBER 2016 www.computer.org/intelligent

55
