Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

PHILOSOPHICAL
TRANSACTIONS A

royalsocietypublishing.org/journal/rsta

e Check for
updates

Cite this article: McDermid JA, Jia Y, Porter Z,
Habli |. 2021 Artificial intelligence
explainability: the technical and ethical
dimensions. Phil Trans. R. Soc. A379:
20200363.
https://doi.org/10.1098/rsta.2020.0363

Review

Accepted: 5 March 2021

One contribution of 16 to a theme issue
‘Towards symbiotic autonomous systems.

Subject Areas:
artificial intelligence, software

Keywords:
explainability, machine learning, assurance

Author for correspondence:
John A. McDermid
e-mail: john.mcdermid@york.ac.uk

THE ROYAL SOCIETY

PUBLISHING

Artificial intelligence
explainability: the technical
and ethical dimensions

 

John A. McDermid, Yan Jia, Zoe Porter and
Ibrahim Habli

 

Department of Computer Science, University of York, Deramore
Lane, York YO10 5GH, UK

(®_ JAM, 0000-0003-4745-4272

In recent years, several new technical methods have
been developed to make Al-models more transparent
and interpretable. These techniques are often referred
to collectively as ‘AI explainability’ or ‘XAI’ methods.
This paper presents an overview of XAI methods,
and links them to stakeholder purposes for seeking
an explanation. Because the underlying stakeholder
purposes are broadly ethical in nature, we see this
analysis as a contribution towards bringing together
the technical and ethical dimensions of XAI. We
emphasize that use of XAI methods must be linked
to explanations of human decisions made during the
development life cycle. Situated within that wider
accountability framework, our analysis may offer a
helpful starting point for designers, safety engineers,
service providers and regulators who need to make
practical judgements about which XAI methods to
employ or to require.

This article is part of the theme issue ‘Towards
symbiotic autonomous systems’.

1. Introduction

Increasingly, artificial intelligence (AlI)—specifically,
machine learning (ML)—is being used in ‘critical’
systems. Critical systems directly affect human well-
being, life or liberty. These may be digital systems
(such as those that are used by human experts to
inform decisions regarding medical treatment or prison
sentences) or embodied autonomous systems (such as
highly automated cars or unmanned aerial vehicles).

© 2021 The Authors. Published by the Royal Society under the terms of the
Creative Commons Attribution License http://creativecommons.org/licenses/
by/4.0/, which permits unrestricted use, provided the original author and
source are credited.
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

The use of critical ML-based systems to assist or to replace the human decision-maker raises many
questions about when, and whether, we should trust them. AI explainability (CAI) methods are
part of the answer. The use of XAI methods can contribute to building assurance, or justified
confidence, in critical systems employing ML.

In this paper, we will link stakeholder purposes and the technical dimensions of explainability.
Stakeholders may seek to use XAI methods for a range of reasons, such as to assess confidence,
to inform consent, to contest decisions or to regulate the use of systems. These reasons are often
broadly ethical in nature. We argue that XAI methods are one way to help serve these purposes,
but the requirement for justification also traces the explananda back to human decisions during
design and implementation. Thus, XAI methods sit within a wider accountability ecosystem. Our
approach has similarities to [1] in focusing on the stakeholder groups who comprise the audience
for XAI methods, but our focus is more distinctly on the practical reasons for which stakeholders
seek an explanation; we also add some stakeholder classes, such as prediction-recipients, as
distinguished from end-users and the courts. Our approach also has similarities with [2] in that it
identifies variations in requirements for explanations across stakeholders, but we focus more on
external stakeholders, e.g. regulators, given our greater emphasis on safety and assurance.

The remainder of the paper is structured as follows. Section 2 introduces and contextualizes the
explainability of ML-based systems. Section 3 identifies key stakeholder classes, and it considers
the time dimension and general underlying purposes stakeholders will likely have for XAI
methods. This helps to structure an analysis of the state of the art in explainabilty; §4 surveys
widely used global and local XAI methods, categorizing the latter as either feature-importance or
example-based methods. Section 5 illustrates some of these methods in use for a clinical Decision-
Support System (DSS). The integration of the analysis of stakeholder purposes and XAI methods
then occurs in §6 which includes a table that cross-references the needs of stakeholders against
the XAI methods available. This is supported by a narrative description of three scenarios to
deepen understanding. Section 7 takes a systems engineering perspective, discussing trade-offs
between explainability and performance, and addressing the broader role of XAI methods in
safety assurance. Section 8 considers the importance of explainability in achieving and assuring
trustworthy AI and ML.

2. Explaining explainability
(a) The challenge of Al explainability

Conceptually, traditional software development follows a defined ‘life cycle’. It starts with
the definition of requirements, proceeds via design to implementation, e.g. coding, and
then the software is progressively tested as individual parts of the software are integrated to make
up the overall system. Where systems are critical, the life cycle is very rigorous. Key requirements,
e.g. for safety, are defined and refined at each stage of the development. Verification gives
assurance that the system meets its key requirements; McDermid [3] illustrates this process for
safety-critical software in aviation. Typically, where there is a formal regulatory system, standards
define what needs to be done to achieve assurance and to gain approval for deployment of the
system.

By contrast, development of ML-based systems is a highly iterative process, with a very
different life cycle and the current standards do not give a basis for assurance. The models at the
heart of ML-based systems are trained on data representative of the problem to be addressed and
then their performance is evaluated against pre-defined criteria, e.g. the number of false positives
in detecting tumorous growths in X-ray images, and refined until their performance is satisfactory.
The models have utility because they generalize beyond their initial training data. For example,
autonomous vehicles (AVs) can identify pedestrians in situations that were not present in their
training dataset, predict their trajectory and carry out manoeuvres to avoid a collision.

E9E00Z0Z 6LE ¥ 205 Y ‘SUOY Ud PYS4/leusno{/B10-BuIys)]qndAra}D0sjeKo1
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

data model «a:
data >) . — ——>} predictions
preparation development
|
y
interpretation interpretation
of data prep | of model —
6 |, |
4 explainability interpretation
visualization of predictions

stakeholders

Figure 1. Context and roles of explainability. (Online version in colour.)

There are many classes of ML, e.g. neural networks (NNs) [4], support vector machines
(SVMs) [5] and (deep) reinforcement learning (RL) [6]. NNs have sub-classes, e.g. convolutional
neural networks (CNNs) [7] and deep neural networks (DNN) [8]. Our aim here is to ‘explain
explainability’ so far as practicable without going into details of particular ML methods. For our
purpose here, we can characterize ML-based systems as being trained on large datasets to perform
classification or regression tasks. When the resulting models are used for classification purposes,
they make probabilistic predictions, e.g. a 90% probability that an image contains a tumour.

ML models are often highly complex and thus are not directly amenable to human inspection
(alternatively ‘opaque’ or ‘black boxes’). Further, the ML model structure may not match the
features humans would use in making the decisions, so interpretation would remain difficult even
if the model could be inspected. Some image analysis systems can make erroneous classifications
of objects when a small amount of noise is added which is imperceptible to a haman—but very
significant in the model because of the features that have been learnt [9].

In simple terms, XAI methods seek to provide human interpretable representations of the ML
models to help overcome these and other problems.

(b) Context and roles for explainability

We use a simple illustration, based on the ML life cycle model in [10], to ‘explain explainability’
(figure 1). This figure is intended to show that different stakeholders, e.g. users, regulators and
courts, may have different purposes in trying to understand what the ML-based system is doing.
The intent is that the information is presented to the stakeholder in a meaningful context. This, in
turn, can help to inform human decision-making, e.g. deciding whether or not to approve the use
of an ML-based system or to accept a prediction or recommendation.

The boxes shaded in gold-brown in figure 1 indicate what explanations may be needed.
XAI research in the technical community mainly focuses on explaining the system’s outputs
(predictions) and on the model. But there will also often be a need to explain the collection of
the data (box shaded in green), and preparation of the training data to show that it is balanced,
e.g. in terms of gender or race, or to show that it covers all the different sorts of road junction
found ina given country where an AV is to be used. Data preparation is a key explanandum of an
ML model, and the human decision makers should be able to explain the choice of the particular
dataset, which is the first step in the ML life cycle.

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

ML models reflect features of the problem which the system is intended to solve (and of its
solution). In practice, the ML system developer shapes the set of features represented in the model
by training it on selected data, assessing its performance (e.g. what proportion of pedestrians it
correctly recognizes—known as true positives—and those objects, such as life-size pictures of
people in an advert on the side of a bus, it mistakenly categorizes as pedestrians—known as false
positives), and iterating to improve performance. Training will seek to balance the performance
between the different criteria. This balance will be decided by the developer for a particular
system, e.g. for an AV, a high level of false positives may be acceptable to reduce false negatives,
for safety reasons. Development of ML models always involves such balances or trade-offs. What
XAI methods can do is to highlight the consequences of such trade-offs. In fact, a lot of work on
explainability, e.g. [2], is focused on developers to help them guide ML-model development but,
in this paper, we will primarily focus on other stakeholders, external to the development.

To avoid confusion with the decisions made by humans in-the-loop, the term ‘predictions’ is
used in figure 1 but these might include decisions made by an autonomous system, e.g. an AV may
decide to stop when it detects a traffic light at red. We continue with this terminology throughout
the rest of paper: all outputs of the ML-based system—whether decisions, recommendations,
predictions or classifications—will be referred to as ‘predictions’.

(c) Types of explainability method

At the first level of our analysis, we will focus on two dimensions of XAI methods:

— Local versus global—a local explanation relates to a single prediction (arising from a
single input to an ML model), whereas a global explanation seeks to explain the model as
a whole [2] thus shedding light on the range of possible predictions.

— Time—we split time for the explanations into three categories: prior—before the
prediction is made; contemporaneous—at the same time as the prediction; and post—
after the prediction is made.

In §4, we will consider feature-importance and example-based methods. We will also employ
the distinction between model-specific and model-agnostic explanation. A model-agnostic
explanation can be produced independent of the method used for developing the model, e.g.
NNs or SVMs, whereas model-specific explanations depend on the type of ML model used.

3. Stakeholders and explanations
(a) Stakeholders

There are several stakeholder groups who might require an explanation of the ML model and its
predictions. Within the scope of this paper, we identify the following classes of stakeholder, each
bearing a different relation to the system:

Prediction-recipients (e.g. mortgage applicants, offenders in custody, hospital patients). These
stakeholders do not use the ML-based system themselves (the prediction is usually mediated by
an expert user) but they are directly affected by its predictions.

End users (e.g. car drivers, on-line shoppers). These stakeholders are both direct users of the
ML-based system and are also directly affected by it. Although the end user will often be a
prediction-recipient, we exclude them from that category as they use the system directly. Even
so, end users may not always have direct visibility of individual predictions, e.g. for AVs.

Expert users (e.g. clinicians, remote pilots). These stakeholders are direct users of the ML-based
system but they are not directly affected by its predictions. They are indirectly affected since they
may be accountable (both legally and morally) for consequences of enacted predictions.

E9E00Z0Z 6LE ¥ 205 Y ‘SUOY Ud PYS4/leusno{/B10-BuIys)]qndAra}D0sjeKo1
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

Regulatory agencies (e.g. Financial Services Authority, Vehicle Certification Agency, Medical and
Healthcare Products Regulation Agency). These stakeholders are neither direct users of the ML-
based system, nor directly affected by it. They do, however, protect the interests of prediction-
recipients and end users. The regulatory ecosystem is complex and needs to adapt to ML-based
systems [11]. Even so, these bodies are responsible for system approval and deployment; they
also oversee the system’s continued (safe) use. Assessors and insurer-funded research centres, e.g.
Thatcham, often provide the expert guidance and scrutiny that underpin this regulatory activity.

Service providers (e.g. Google, Automated Driving System Entities (ADSE) [12]). These
stakeholders are the companies and legal entities who put the system forward for authorization,
and vouch for the system when it is deployed. They may be the manufacturer or the software
developer, or a joint venture between the two [12]. These stakeholders may be legally liable for
the behaviour of the ML-based system once deployed [13].

Accident and Incident Investigators (e.g. National Transportation Safety Board (NTSB), Marine
Accident Investigation Branch (MAIB), Health and Safety Executive (HSE)). These stakeholders
are responsible for analysing accidents or incidents, and for making recommendations for
avoiding such events in the future with the same system or similar systems. In some cases, e.g.
the HSE, they may also be responsible for initiating legal proceedings;

Lawyers and the Courts (e.g. Barristers, Crown Prosecution Service (CPS)). These stakeholders
are interested in determining liability for harm caused by an ML-based system. Individual
lawyers may seek compensation on behalf of a prediction-recipient or end user.

Insurers (e.g. DirectLine, Aviva). These stakeholders cover financial risk on behalf of service
providers and users. In practice, they play a useful role to ensure that safety standards are met:
they may require evidence that a service provider has met regulatory requirements, and even
impose stricter standards of their own.

(b) Purpose of explanation

Studies of explanations span the sciences, psychology, cognitive science and philosophy [14].
Researchers have noted that the term ‘explanation’ has essentially been re-purposed by the XAI
community [15]. What it means in its technical sense touches only on some dimensions of the
multi-disciplinary discourse on explanations and their functions.

Explanations provided by XAI methods are descriptive. This speaks to the transparency that
the techniques can provide. There are similarities here with scientific modelling. Both deal in
approximations that provide descriptions of phenomena or behaviour [15]. XAI methods can
also provide causal and logical explanations. They provide some understanding as to how a
prediction is generated by the ML model. This speaks to the interpretability that the techniques
can provide (hence our use of the term ‘intepretation’ in figure 1). Causality is central to accounts
of explanation in philosophy, law, psychology and cognitive science [14]. But philosophical
accounts also place an emphasis on normative explanations [16]. These are explanations that offer
good reasons for a belief, decision or action; in this way, they can justify a process or an outcome
to those affected by it. Explanations provided by XAI methods do not supply explanations in
this sense [17]. The methods may, for example, highlight which features in the data have been
assigned a larger weight by the model, which determines the effect of a feature on a prediction or
the feature’s importance in the model [18]. This assignation of weight is in turn determined by the
feature’s success in producing accurate results in the training phase. As such, reasons given for
the importance of a certain feature must refer back to human decision-making during the training
of the model. This is the wider accountability framework within which XAI methods sit. People,
and not (just) systems, are answerable for decisions made in the ML development life cycle.

A great deal of work has been done in the social sciences about people’s expectations from
explanations. Some of these, such as the finding that people prefer contrastive explanations
accompanied by an underlying causal explanation, can be met or approximated to some degree
by XAI methods [14]. Psychological studies indicate an explainer’s values inform explanation
selection and evaluation, and these choices in turn can have a significant influence on the

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

recipient’s understanding of an event; the explainer should therefore reflect carefully upon the
XAI method used and its communication to the recipient [15]. It is important that explanations
are communicated at the appropriate level of abstraction for the stakeholder [19]. The use of
visual interfaces can also improve some stakeholders’ epistemic access to an explanation [20]. But
equally, interpretations of system behaviour that are presented as more rigorous and complete
than they actually are will contribute to unjustified trust [21,22].

Below, we identify some general underlying reasons stakeholders may have for seeking the
explanations provided by XAI methods. This characterization is not intended to be understood as
homogeneous or exclusive. A single stakeholder, such as an expert user, may have more than one
underlying purpose, e.g. they may seek an explanation to determine whether a model complies
with regulation as well as an explanation to evaluate confidence in a specific prediction before
acting on it. By the same token, individuals from different stakeholder classes may have similar
purposes, e.g. to use the information to challenge a particular prediction. Thus our analysis,
particularly in §6, should be seen as indicative, not definitive or exhaustive, and it is intended
as a starting point on which to build.

Our hypothesis is that understanding these underlying purposes—which we have distilled
into the general categories below on the basis of first-hand experience working with developers,
industry and regulators—will help to inform understanding of explanation requirements, such as
whether a global or local explanation is required, which in turn can inform which XAI methods
are most appropriate in a given context. The timing of the explanation will also be relevant to
explanation requirements. Our analysis broadly aligns with those in [23,24]. We believe that
there would be merit in further empirical study to confirm the nature and importance of the
relationships between stakeholder classes and their underlying reasons for seeking explanations
of ML-models and predictions.

1. Clarity. Greater clarity of the model or its predictions is something all stakeholders, almost
by definition, require. It is a prerequisite to meet all the other purposes given below.
All dimensions of XAI methods are relevant to answering this need: global explanations
prior to deployment; local explanations contemporaneously and local explanations
retrospectively. It is also important, however, to temper this requirement with honesty
from those providing the XAI methods. They should not be taken to offer clear, or exact,
explanations when such clarity is not feasible [21].

2. Compliance. Determinations of compliance with law, regulation, or best practice is another
underlying purpose to which XAI methods may contribute. Sector-specific regulators will
have their own requirements for the approval of ML-based systems. In addition, cross-
domain Acts of Parliament apply (e.g. Data Protection Act 2018, UK GDPR, Equality
Act 2010). It has been suggested that stakeholders may rely upon XAI methods to
fulfil legal duties to provide information about the logic of specific outputs to affected
individuals [25]. These will be post-hoc local explanations. In addition, global XAI
methods could become part of the toolkit of both regulatory bodies and compliance
officers to interrogate and demonstrate the fitness of the system for purpose [26]. And
local contemporaneous XAI methods might play a role in ongoing assurance of the
model’s performance in context.

3. Confidence. Stakeholders will often want to evaluate their confidence in a prediction
before proceeding with a decision that has been informed by the prediction. Research
suggests that the provision of sufficiently detailed explanations can affect the acceptance
of algorithmic decisions by users—primarily expert-users [25]. XAI methods may be used
to serve this purpose. Global explanations may inform degrees of confidence in the range
of a model’s predictions prior to deployment. Where the ML-based systems are used by
human experts to inform decision-making, a local and contemporaneous explanation will
be required in order to decide whether to act on a specific prediction in real time.

4. Consent and Control. XAI methods may also play a role in enabling stakeholders to
better exercise their own human autonomy in relation to an ML-model [24]. Appropriate

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

explanations could enable users to give their informed consent to recommendations by
ML-based personal assistants, for example, or, in the case of an AV, to understand a
transition demand sufficiently to resume effective hands-on control of the system. This
purpose is closely related to confidence, since both ultimately concern acceptance. As
with the previous case, the explanations that best serve this purpose will likely be local
and contemporaneous.

5. Challenge. Stakeholders who seek to challenge or contest a particular prediction may
also rely in part upon XAI methods to do so. Examples of members of particular
demographic groups being adversely and unfairly affected by ML-based predictions are
legion. One particularly egregious example was the COMPAS system, which predicted
an individual’s risk of recidivism, and often incorrectly assigned high risk scores to
black defendants and low risk scores to white defendants [27]. Other examples include
bias in hiring and loan decisions [28]. Demands for fairness often lead to demands for
interpretable models [17]. XAI methods may help identify when an error has occurred, or
provide evidence to contest a prediction. For such purposes, the requirement will be for
a local explanation, after the recommendation has been made.

6. Continual improvement. Finally, XAI methods can help developers of ML-based systems,
as well as other stakeholders such as accident investigators and regulators, to ensure that
the systems are continually improved and updated. The requirement here will be for both
global and local explanations.

These different underlying reasons for seeking an explanation are broadly ethical in nature.
They relate to the obligations people and organizations owe to one another. They relate to whether
people’s reasonable expectations of fairness and respect are met by a model’s predictions. They
relate to the exercise of individual human autonomy. They relate to stakeholder assessments
of whether a model’s behaviour aligns with normative goals for the system. But necessarily,
these purposes will only be served in part by XAI methods. The methods themselves are often
approximations, and give partial and selective information. Moreover, XAI methods do not
provide normative explanations. They will need to sit within a wider justificatory discourse, in
which human organizations and decision-makers provide the reasons for the choices that led to
the models being developed as they were throughout the life cycle.

4. Explainability methods

Research shows that local XAI methods are far more common than global XAI methods for
complex ML models [2]. In this section, we first briefly discuss some relatively simple ML models
that are intrinsically interpretable and which can provide both local and global explanations.
We then focus on the more complex ML models that tend to be used in critical applications
where local XAI methods can provide valuable information. For these more complex models,
we look at both feature importance methods, which can be model-agnostic or model-specific, and
at example-based methods, which are generally model-agnostic.

(a) Intrinsically interpretable ML models

Some types of ML model are viewed as being intrinsically interpretable (explainable) due
to their simplicity, e.g. linear regression and decision trees [29]. For example, the weights of
a linear regression model can be viewed as a crude feature importance score giving global
insight into the model if the input features are at a similar scale. The feature importance for
a decision tree can be calculated based on the mean decrease of Gini impurity [30] or, as
an alternative, using permutation feature importance [31], which calculates importance based
on the decrease in the model score when a single feature value is randomly shuffled in the
dataset. These explanation methods can provide global insight into the decision tree model, and
permutation feature importance has been shown to avoid some flaws of the Gini impurity-based

E9E00Z0Z 6LE ¥ 205 Y ‘SUOY Ud PYS4/leusno{/B10-BuIys)]qndAra}D0sjeKo1 5
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

method [32] and is model-agnostic. In addition, these interpretable ML models are often used as
a surrogate to approximate other complex ML models giving insight into the more complex ML
model [33].

(b) Explainability methods for complex ML models

There are many different ways to categorize XAI methods, as outlined above. Here, we sub-divide
the methods that are useful for complex ML models into feature importance and example-
based methods. Feature importance methods can be model-agnostic or model-specific, unlike
explanations of intrinsically interpretable models that are normally model-specific. Example-
based explanations are normally model-agnostic and are important for explaining the complex
ML models used in critical applications. There are many different XAI methods in the literature.
We briefly describe some of the more widely used methods here, give an illustrative example in
§5 and show how the XAI methods map to stakeholder needs in §6.

(c) Feature importance methods

Feature importance is by far the most popular method in explainability research [34]. There
are two main sub-categories of feature importance methods. One is perturbation-based
methods. Another is gradient-based methods. Perturbation-based methods make perturbations
to individual inputs either by removing, masking or altering an input feature or set of input
features and observing the difference with the original output. This approach can be used in many
different applications, e.g. image data, tabular data or text data [35,36]. For example, in an image
classification task using CNN, perturbation was implemented by occluding different segments of
an input image and visualizing the change in the predicted probability of the classification [37].

LIME (Local Interpretable Model-Agnostic Explanations) is a popular pertubation-based
method [38]. It generates the explanation by approximating the complex ML model using an
interpretable one, e.g. a linear model, learned on perturbations of the single input sample of
interest. LIME assumes it is possible to fit an interpretable model around a single input sample
that mimics how the complex ML model behaves locally. The simple interpretable model can then
be used to explain the predictions of the more complex ML model for this single input sample.

Perturbation methods based on Shapley values from cooperative game theory are also very
popular [39]. Shapley values are a way to assign the total gain from a cooperative game to its
players guaranteeing a unique solution. In using Shapley values to explain a model prediction,
the model input features are viewed as the players and the model prediction is the gain resulting
from the cooperative game. However, it is difficult to calculate the exact Shapley values in practice
as they are exponential in the size of the model input features. Consequently, approximate
methods have been proposed, e.g. aggregation-based methods [40], Monte Carlo sampling [41]
and approaches for graph-structured data, e.g. language and image data [42].

SHAP. (SHapley Additive exPlanations) [43] is another method approximating Shapley values.
SHAP incorporates several tools, e.g. KernelISHAP and TreeSHAP [44]. KernelSHAP is a weighted
linear regression approximation of the exact Shapley value inspired by LIME and it can be used
to provide local explanations for any ML model. TreeSHAP is an efficient estimation approach
for tree-based models only, i-e. it is model-specific. The work on SHAP has defined a new class of
additive feature importance measures which unifies several existing explainability methods.

Perturbation-based methods allow a direct estimation of feature importance, but they tend to
be very slow as they perturb a single input feature or set of features each time, so as the number of
input features in the ML model grows, it can take a long time to generate the importance score for
all of the features, e.g. for image analysis [45]. Also, as complex ML models are often nonlinear, the
explanation is strongly influenced by the set of features that are chosen to be pertubated together.
In comparison, gradient-based methods have the potential to be much more efficient.

The basic gradient-based method is just to calculate the gradient of the output with respect
to the input. For example, a ‘saliency map’ is produced by calculating the gradient of the output

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

with respect to the input in an image classification task identifying pixels that have a significant
influence on the classification [46]. There are several variants of gradient-based methods. Gradient
* Input multiplies the gradient (strictly the partial derivative) by the input value to improve the
sharpness of feature importance [47]. Integrated Gradients is similar to Gradient * Input, in that
it computes the gradient of the output with respect to each input feature by integrating over a
range from a baseline to the current value of the feature, to produce an average gradient [48].
This method has a number of desirable properties associated with it. DeepLIFT (Deep Learning
Important FeaTures) [49] has been developed specifically for use with deep NNs. DeepLIFT
compares the activation of each neuron to its ‘reference activation’ and assigns an importance
score to each input according to the difference. The ‘reference activation’ is obtained through
some user-defined reference input to represent an uninformative background value, for example,
for image classification this could be a totally black image. DeepLIFT has been shown to bea good
approximation to Integrated Gradients in most situations [50].

(d) Example-based methods

Example-based methods use particular input instances to explain complex ML models, thus they
normally provide local explanations. This is motivated by the way humans reason, using similar
situations to provide explanations [51]. This is common practice, for example, in the law [52,53]
where judicial decisions are often based on precedents (known as case law). There is growing
interest in using example-based methods to explain complex ML models and some view them as a
useful complement to feature-based explanations [54]. We describe three example-based methods.

Counterfactual explanations for ML models were introduced by Wachter ef al. [55]. They use
similar situations that give different predictions from the current input instance to the ML model,
e.g. achieving a desirable outcome in healthcare. These can be used, for example, to indicate what
changes in a patient’s state or treatment are needed to allow them to be discharged from hospital.
To be used this way, it is important that the counterfactual explanations minimize the difference
between the current input features and the counterfactual examples. The kinds of metrics that
should be used to minimize the difference is an ongoing area of research [56,57].

Adversarial examples were discovered and discussed by Szegedy et al. [58]. They are small,
intentional, feature perturbations that cause an ML model to make an incorrect prediction, e.g.
to mis-classify an object in image analysis [59] or to fool reading comprehension systems in text
classification tasks [60,61]. It is different from counterfactual explanations which are typically
used when the ML model is fixed. An adversarial example in autonomous driving might be to
add noise to an image of a stop sign so that it would not be recognized by the ML model, although
it would seem unchanged to a human. Once such problems are identified, they can be used to
improve the robustness of complex ML models. Therefore, adversarial examples are generally
used during model training, rather than providing explanations like feature importance methods,
but more robust ML models can improve the quality of the feature importance results [62].

Influential instances are the inputs from the training dataset that were most influential for the
predictions of the ML model, i.e. the ML model parameters are highly influenced by these inputs.
One simple way of finding influential examples is to delete inputs from the training dataset, to
retrain the model and to assess the impact; while straightforward, this is impractical for large
datasets. Often mathematical techniques are used which do not require retraining the model [63].
Like adversarial examples, influential examples are best used during training and contribute more
to ML model robustness than providing direct explanations.

5. Explainability example

In this section, we present a concrete example, applying a feature importance method (DeepLIFT)
to a healthcare application. In Intensive Care Units (ICUs), mechanical ventilation is a complex
clinical intervention that consumes a significant proportion of ICU resources [64,65]. It is of critical
importance to determine the right time to wean the patient from mechanical support. However,

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

 

 

 

 

 

ROC curve
1.0
0.8
2
g
£ 06
» 04
s
£
0.2
—— CNN (area = 0.94)
0 ——DNN (area = 0.93)
0 0.2 0.4 0.6 0.8 1.0

false positive rate

Figure 2. ROC Curves for the example NNs. (Online version in colour.)

Table 1. Accuracy for the example NNs.

accuracy

CNN 86.5%

assessment of a patient's readiness for weaning is a complex clinical task and it is potentially
beneficial to employ ML to assist clinicians [66]. The example uses NNs based on the MIMIC-
III dataset [67] to predict readiness for weaning in the next hour. The NN models are trained
on data for 1839 patient admissions, the NN architecture and hyperparameters are tuned using
a validation dataset of 229 patient admissions, and performance is evaluated on 231 patient
admissions. For more detail, see [54] which also shows the use of counterfactual explanations.

The performance of an ML model is often assessed in terms of accuracy of the prediction and
the area under the receiver operating characteristics curve (AUC-ROC). For a ‘random’ model the
AUC-ROC would be 0.5 and for a perfect model it would be 1. The example compares both CNN
and DNN. Based on performance, both are promising, achieving around 87% accuracy and 0.93-—
0.94 AUC-ROC (figure 2 and table 1). Based on performance, there seems to be little to choose
between them. However, we then used DeepLIFT to determine feature importance for the two
NNs, see figure 3 where longer bars mean that the features are of greater importance. Note that
the sign indicates positive and negative influences on the outcome and zero means that the feature
is of little importance. This shows significant differences between the two models.

First, DeepLIFT reveals that the CNN shows ethnicity, gender and age all have an importance
near zero, whereas the DNN shows age and gender as having higher importance. Such
information should be of critical importance to regulators determining which models to approve.
It would be over-simplistic to infer from this information that the CNN is less biased than the
DNN, since making models formally ‘blind’ to protected characteristics rarely removes the risk
of bias, and may even have the opposite effect, depending on context [68]. But making the feature
importance visible to healthcare regulators enables them to ask the right questions about the
potential negative impact of proposed ML-based applications on certain demographic groups.

Second, the feature importance for the CNN is more consistent with clinical knowledge.
Some features, e.g. the Richardson-RAS scale [69], which show how alert patients are, have high
importance in both NNs. However, the CNN places greater importance on a number of patient
conditions such as tidal volume (depth of breaths) and on features of the treatment, e.g. the mode
of operation of the ventilator, which would typically be considered by a clinician. This use of

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

 

 

 

 

 

 

 

 

(a) (b)
inspired O, fraction Es SpO, es
ventilator mode | inspired O, fraction —EEEEEE==z
peak insp. pressure as mean airway pressure ee a
PEEP set Le PEEP set ||
tidal volume (observed) SS age =
mean airway pressure | peak insp. pressure |
admit type = O, flow a
arterial O, pressure = gender a
Spo, = PH (arterial) a
blood pressure(systolic) a tidal volume (observed) |
heart rate a arterial CO, pressure i
blood pressure(mean) | blood pressure (mean) i
admission weight a heart rate 1
O, flow 1 Arteril O, pressure |
ethnicity I blood pressure (diastolic) |
gender 1 blood pressure (systolic) |
age i plateau pressure 1
PH (arterial) i respiratory rate (spont) 1
respiratory rate m ethnicity 1
respiratory rate (spont) | respiratory rate t
blood pressure (diastolic) = ventilator mode =
arterial CO, pressure zz spontaneous breathing trails |
spontaneous breathing trails | admit type mH
plateau pressure ; admission weight =
richmond-ras scale | tichmond-RAS scale |]
-0.5 04-03 -0.2 -0.1 0 -0.1 0 0.1 0.2 03

Figure 3. Comparative feature importance. (a) CNN feature importance, (6) DNN feature importance. (Online version in colour.)

explainability is an example of decision support for an expert user, but it also has a role in helping
ML model developers produce more effective results, and helping regulators determine approval.

This example shows the importance of XAI methods for complex ML models and how the use
of performance alone is not enough to assess whether or not a given ML model is an appropriate
advisory tool in a safety-critical situation. This also shows how explanations can be visualized, as
identified in figure 1, so as to be accessible to stakeholders.

6. Integration of stakeholder purposes and XAl methods

The discussion of stakeholders in §3 and the survey of XAI methods in §4 is integrated in
table 2. Here, we first explain the table structure then give a more in-depth discussion of three
combinations of stakeholder and scenario in which explanations are required, highlighting the
ethical considerations for each; we also revisit the example from the previous section showing
how it relates to the table.

Table 2 identifies the stakeholders and the purpose of explanations in a given scenario,
e.g. confidence and compliance for a regulator engaged in system approval. Timing for the
explanations is identified, and the last three rows correspond to the boxes shaded in gold-brown
in figure 1 and identify whether explanations should be local or global, with an illustration of
candidate XAI methods presented in the last two rows (methods for visualizing data are out
of the scope of this paper). The table is intended to be illustrative, not exhaustive. To keep the
table compact, investigation covers a number of cases, not just physical accidents. For example, a
prediction-recipient might make an immediate challenge to a decision (shown in the right-most
column) but the Courts, and a lawyer operating on the recipient’s behalf, might be interested in
global as well as local explanations, e.g. to see if an ML model displayed systematic bias.

Prediction-recipients, being the directly affected individuals, should always be the core focus
of ethics and safety. We also include here directly affected individuals who are excluded from
prediction systems. An important ethical consideration for prediction-recipients in domains such
as criminal justice and retail banking is that they are not subject to an unfairly discriminatory

E9E00Z0Z 6LE ¥ 205 Y ‘SUOY Ud PYS4/leusno{/B10-BuIys)]qndAra}D0sjeKo1
royalsocietypublishing.org/journal/rsta

Phil. Trans. R. Soc. A 379: 20200363

 

(sjapow

aiqeyaidsayul

‘L4lTdaaq
“AVHS|ausay) [230]

‘eu
220]

UOISDap se dW} owes

abuayey>

woddns uolsiap

qualdida/ uolaIpaid

Puce

(suoneuejdxa

jenyoejayuno>

‘s;apow
alqeyaidsaqul) [90]

(sjapow

aiqeyaidsayul

‘aoueLodui| aumeay
uoneynwuad) jeqojb

jeqo|6 ‘je20|

Uo|sPap se aU!) Wes

abuayeyp
‘Jouyuo> pue
yuasuod ‘aauapyuod

woddns uolsiap

Jasn Yadxa

(sjapow

aiqeyaidsayul

‘L4lTdaaq
“AVHS|ausay) [230]

UO!sDap se aw!) aes

jouyuo>
pue quasuop ‘abuayjey>

OSN DDIAIOS

Jasn pus

eu
(sanueysul
jeuanyul
‘sajdwiexa
jeuesuaape ‘sjapow
ayqeyaiduayul) jeqojb
leqoj6
uawdodap-aid
(quawianosduu
snonulyuod)

‘soueljdwo>
‘aouapyuod

uawAojdap waysks

Japiaoid aniasas

(suoneuejdxa
jenyoejayuno>
“AVHS|auday) [230]
(d¥HS294,
‘suoieue|dxa
jenyoejayuno>
‘soueyodui| aumeay
uoneynwuad) jeqojb

jeqo|6 ‘je20|

quappul-jsod

quawanoidul
snonuljuo>
‘Dueljdwiod ‘Aue
quap|ul
Jo quappoe ayebiysaAul

loyebysaaul yuappre

eu
(sanueysul
jenuanyut
‘sajdwiexa
jeuesuaape ‘sjapow

ajqeyaiduayul) jeqojb

eqoy6

uawdodap-aid

apueljdwod ‘aauapyuo>

Jeaoudde waysks
Joyeinba
Toa ieyy

“SPouJau Je|IWIS Gulsn ‘sjuaidpal Uolspap Woy sabualjeyp ayebysaaul osje Aew syNo7/ssaAMe7 pue (syuApIoul) sabeyno, adiAsas ayebiysaaul ACW JaPIAOld aIAIISy

Ayiqeureydxa uol21}paid

Ayjiqeurejdxa japow

 

eurejdxa eyep

suoneuedxa jo bulwi

uoneuesdxa jo asodind

oueuars
siap|oyayeys
EIT cYAUORTUELITT

 

“SOLUADS PUR S/ap|oyayeys JUa/ayIp JO} sjUaWadnbay Ayjiqeule|dxa jo suofeaysn

TTOT JOQUIGAON /O UO /S1O'SurlYyst[qndAyoroosTeAor//:sdyyy WioI pepeopumod

 

“Teqel
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

prediction. There are many examples of ML-based models that have been shown to reinforce bias
against individuals on the grounds of race and gender [70], as well as postcode or socio-economic
status [71]. While the primary explananda here are the human decisions that went into creating
the dataset (figure 1), XAI methods will also be required to help to determine whether the ML
model reflects or exacerbates bias [72]. As discussed above, feature importance methods can help
to determine this. It should be noted that automated predictions about individuals constitute
personal data in the case that the individual is identifiable [26]. They therefore fall within the
remit of data protection law. This gives rise to requirements for a post-hoc local explanation, should
the individual seek to challenge an automated prediction that has been made about them. This is
illustrated in the rightmost column in table 2, showing the relevance of interpretable models to
give contemporaneous explanations, but note also the possible need for post-event explanations
in support of legal processes.

Expert-users, such as radiographers and oncologists, are the individuals for whom most
advisory ML-based systems are designed. The objective is for such stakeholders to be able to
determine confidence in order to make an informed decision about whether to accept and act on the
prediction, thus to exercise consent and control. Itis presently unclear whether insufficient scrutiny
of an ML-generated prediction would constitute a legal breach of a doctor’s duty of care; however,
they may also want to challenge or query predictions. But clinicians clearly have a strong moral
duty to ensure their patient’s well-being and safety [73]. XAI methods can help them to fulfil that
duty. Clinicians make diagnostic decisions by considering and weighing a set of features, data
points and clinical markers [74]; see also the example in §5. The clinician may therefore find
value in example-based explanations, such as counterfactual explanations, particularly where
their immediate apprehension of the patient, and insights gleaned from additional information
not included in the ML model (e.g. biopsy results), indicates a different result to the prediction
provided by the system [73].

Service-providers have several explanatory requirements, given their need to comply with
regulation, ensure the safety of systems, meet the requirements of end users, and to provide
explanations to investigators post-hoc. This gives rise to a comprehensive range of explanations
required, both global explanations prior to deployment for purposes of confidence, compliance and
local explanations post-hoc to support continuous development. The middle column in table 2 shows
the role of XAI methods in the pre-deployment case for the service provider; the column for
investigation also covers service providers seeking to understand unintended behaviour and to
improve the system.

In addition, the example in §5 is a DSS, and it reflects the requirements for an expert user. In
particular, the example shows feature importance methods used to provide model explainability,
in support of Confidence.

7. Discussion

The study of XAI methods is normally conducted from a purely technical perspective. But when,
and what type of, explanations are needed, and by which stakeholders, is also often an ethical
question. What we have sought to do here is to bring the two dimensions together. The intent
is that the information in the form illustrated in table 2 can be used to identify candidate XAI
methods, although we acknowledge that the coverage in the table is not exhaustive. Further, this
does not produce a ‘unique’ solution and, for example, both LIME and DeepLIFT could be used
to provide local explanations for an accident investigation where deep NNs have been used. The
development of both ML and XAI methods is proceeding apace so it is unlikely that the choice of
XAI methods will be codified in the near future, if indeed that is possible, but it is hoped that an
analysis in the form of table 2 will help inform method choice.

Some have argued that only interpretable models should be used for critical decisions [22], and
table 2 would seem to support this view. We take a rather broader view that there are trade-offs to
be made between performance and explainability. For example, if it is essential to understand the
model, then the choice of ML methods might be limited, primarily to interpretable models and

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

these have the benefit of providing contemporaneous explanations. But these are less powerful
than many other ML methods, and might not perform well in terms of mission objectives. Further,
algorithms such as DeepLIFT are fast — the examples shown in figure 3a,b took around 1-2 min
to produce on a modest computer—so these might be useful where a slight delay in producing
explanations was acceptable. This suggests that more explicit recognition should be given to XAI
methods when choosing the ML methods to use where assurance is a key factor. For example, with
AVs, while in theory contemporaneous explanations might be of value, in practice it is doubtful
whether or not they would be useful to drivers—and using more powerful ML methods that still
permit the use of XAI methods to support incident analysis might be justifiable.

It is generally accepted that verification and validation (V&V) of ML is challenging, and there
is no widely agreed ‘best’ way to undertake V&V. Work on building an assurance model for V&V
has produced ‘desiderata’ for assurance through the ML life cycle, using a model which inspired
the structure of figure 1 [10]. It seems unlikely that assurance processes for ML will reach a level
of rigour equivalent to the prevailing standards for ‘conventional software’. Thus, it is possible
that explainability will, in time, come to have a greater role in assurance. One of the reasons for
believing this is that autonomy, in essence, transfers decision-making from humans to machines—
one way to gain confidence that this has been done satisfactorily is to expose the nature of the
decision-making process—and this is what explainability seeks to do.

Finally, assurance cases, in the form of structured arguments supported by evidence [75], play
a significant role in communicating why it is believed that a system is safe to deploy, particularly
in safety-critical industries. The more complex and novel the system and its context are, the more
important the role of the assurance case is in informing the risk-based deployment decision. There
is an increasing interest in the use of assurance cases to justify the safety of ML-based systems,
particularly for automotive [76] and healthcare applications [77]. The notion of explainability,
particularly pre-deployment, could form a key part of an ML assurance case used to explain
and justify, e.g. to regulators, key decisions about the choice of the ML model and quality and
suitability of the data sets. Post-deployment, local XAI methods could help to implement a highly
dynamic assurance case [78,79] where the critical predictions made by an ML-based system could
be used to update the assumptions about, and confidence in, the system deployed compared with
the assessment made pre-deployment.

8. Conclusion

ML-based systems are already being used in situations that can have an effect on human well-
being, life and liberty. This, combined with the fact that they move decision-making away
from humans, makes it an assurance imperative to provide evidence that this transference is
appropriate, responsible and safe. Explanations of ML-models and ML-generated predictions can
be provided as part of this evidence. But they sit within a wider accountability framework, in
which human decision-makers are still required to give the normative reasons or justifications
(which XAI methods cannot provide) for the ML-models. Our analysis of stakeholder needs
and the contrast with the capabilities of XAI methods gives, we believe, a starting point for
understanding how to employ explainability in an assurance role. Assurance of ML-based
systems deployed in living environments has an ethical dimension. This is reflected in the
underlying ethical nature of the reasons—to inform consent, to challenge an unfair prediction,
to assess confidence before implementing a decision that, if wrong, could harm the recipient—
for which stakeholders might require visibility of an ML-model or an explanation of one of its
predictions. We hope that this paper will help shift the balance of work on XAI methods from
a largely technical one to a wider consideration of the role of explainability in assurance and
achieving evidence-based acceptance of ML.

Data accessibility. https: / /github.com/Yanjiayork/mechanical_ventilator.
Authors’ contributions. JA.Mc.D. conceived the approach presented in the paper, and developed the explainability
model and tables. YJ. analysed the ML methods and produced the example. Z.P. and J.A.Mc.D. produced

E9E00Z0Z 6LE ¥ 205 Y ‘SUOY Ud PYS4/leusno{/B10-BuIys)]qndAra}D0sjeKo1
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

the analysis of stakeholder groups, of explanation purposes, and the discussion of their integration with
explainability methods. IH. contributed the assurance perspective. All authors read and approved the
manuscript. Z.P. would also like to thank Paul Noordhof for a helpful discussion.

Competing interests. The authors declare that they have no competing interests.

Funding. This work was supported by the Assuring Autonomy International Programme, funded by the
Lloyd’s Register Foundation and the University of York.

References

1.

10.

11.

12.

13.

14.

15.

16.
17.

18.

19.

20.

Arrieta AB et al. 2020 Explainable Artificial Intelligence (XAT): concepts, taxonomies,
opportunities and challenges toward responsible AI Inf. Fusion 58, 82-115.
(doi:10.1016 /j.inffus.2019.12.012)

. Bhatt U et al. 2020 Explainable machine learning in deployment. In Proc. of the 2020 Conf. on

Fairness, Accountability, and Transparency, pp. 648-657.

. McDermid JA. 2010 Safety Critical Software. In Encyclopedia of Aerospace Engineering (eds

R Blockley, W Shyy). https: / /doi.org/10.1002 /9780470686652.eae506.

. Zhang GP. 2000 Neural networks for classification: a survey. [EEE Trans. Syst. Man Cybern.

Part C (Appl. Rev.) 30, 451-462. (doi:10.1109/5326.897072)

. Wang G. 2008 A survey on training algorithms for support vector machine classifiers. In

2008 Fourth Int. Conf. on Networked Computing and Advanced Information Management, vol. 1,
Gyeongju, South Korea, 2-4 September 2008, pp. 123-128. New York, NY: IEEE.

. Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA. 2017 Deep reinforcement

learning: a brief survey. IEEE Signal Process Mag. 34, 26-38. (doi:10.1109/MSP.2017.
2743240)

Rawat W, Wang Z. 2017 Deep convolutional neural networks for image classification: a
comprehensive review. Neural Comput. 29, 2352-2449. (doi:10.1162/neco_a_00990)

. Sze V, Chen Y, Yang T, Emer JS. 2017 Efficient processing of deep neural networks: a tutorial

and survey. Proc. [EEE 105, 2295-2329. (doi:10.1109/JPROC.2017.2761740)

. Marcus G, Davis E. 2019 Rebooting AI: building artificial intelligence we can trust. London, UK:

Vintage.

Ashmore R, Calinescu R, Paterson C. 2019 Assuring the machine learning lifecycle: desiderata,
methods, and challenges. http://arxiv.org /abs /190504223.

Centre for Data Ethics and Innovation. Barometer Report. https://assets.publishing.
service.gov.uk/government/uploads/system/ uploads /attachment_data/ file /894170/CDEI
_AI_Barometer.pdf. 2020.

Law Commission/The Scottish Law Commission. Automated Vehicles: Consultation Paper 2
on Passenger Services and Public Transport A joint consultation paper. 2019.

Burton S, Habli I, Lawton T, McDermid J, Morgan P, Porter Z. 2020 Mind the gaps: assuring
the safety of autonomous systems from an engineering, ethical, and legal perspective. Artif.
Intell. 279, 103201. (doi:10.1016/j.artint.2019.103201)

Miller T. 2019 Explanation in artificial intelligence: insights from the social sciences. Artif.
Intell. 267, 1-38. (doi:10.1016/j.artint.2018.07.007)

Mittelstadt B, Russell C, Wachter S. 2019 Explaining explanations in AI. In Proc. of the Conf.
on fairness, accountability, and transparency, Atlanta, GA, 29-31 January 2019, pp. 279-288.
New York, NY: ACM.

Raz J. 2011 From normativity to responsibility. Oxford, UK: Oxford University Press.

Lipton ZC. 2018 The mythos of model interpretability: in machine learning, the
concept of interpretability is both important and slippery. Queue 16, 31-57. (doi:10.1145/
3236386.3241340)

Biran O, McKeown K. 2014 Justification narratives for individual classifications. In Proc. of the
AutoML workshop at ICML, vol. 2014, pp. 1-7.

Ward FR, Habli I. 2020 An assurance case pattern for the interpretability of machine learning
in safety-critical systems. In Int. Conf. on Computer Safety, Reliability, and Security, Lisbon,
Portugal, 15-18 September 2020, pp. 395-407. Berlin, Germany: Springer.

Tsamados A, Aggarwal N, Cowls J, Morley J, Roberts H, Taddeo M, Floridi L. 2021 The ethics
of algorithms: key problems and solutions. AI & SOCIETY 1-16.

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

21.

23.

24.

25.

26.

27.

28.
29.

30.

31.
32.

33.
34.

35.

36.

37.

38.

39.

40.

41.

43.

45.

Brundage M et al. 2020 Toward trustworthy AI development: mechanisms for supporting
verifiable claims. http: //arxiv.org/abs/200407213.

. Rudin C. 2019 Stop explaining black box machine learning models for high stakes

decisions and use interpretable models instead. Nat. Mach. Intell. 1, 206-215.
(doi:10.1038 /s42256-019-0048-x)

Weller A. 2019 Transparency: motivations and challenges. In Explainable AI: interpreting,
explaining and visualizing deep learning (eds W Samek, G Montavon, A Vedaldi, LK Hansen,
K-R Miiller), pp. 23-40. Berlin, Germany: Springer.

Langer M, Oster D, Speith T, Hermanns H, Kastner L, Schmidt E, Sesing A, Baum K. 2021
What do we want from Explainable Artificial Intelligence (XAI)? A stakeholder perspective on
XAI and a conceptual model guiding interdisciplinary XAI research. Artif. Intel. 296, 103473.
(doi:10.1016 /j.artint.2021.103473)

Binns R, Van Kleek M, Veale M, Lyngs U, Zhao J, Shadbolt N. 2018 ‘It’s Reducing a Human
Being to a Percentage’ perceptions of justice in algorithmic decisions. In Proc. of the 2018
Chi Conf. on human factors in computing systems, Montreal, Canada, 21-26 April 2018, pp. 1-14.
New York, NY: ACM.

Information Commissioners Office & Alan Turing Institute. 2020 Explaining Decisions
made with AI. See https:/ /ico.org.uk /for-organisations / guide-to-data-protection/key-data-
protection-themes/explaining-decisions-made-with-artificial-intelligence/.

Freeman K. 2016 Algorithmic injustice: how the Wisconsin Supreme Court failed to protect
due process rights in State v. Loomis. North Carolina. J. Law Technol. 18, 75.

Barocas S, Selbst AD. 2016 Big data’s disparate impact. Calif. L. Rev. 104, 671.

Hastie T, Tibshirani R, Friedman J. 2009 The elements of statistical learning. Berlin, Germany:
Springer.

Louppe G. 2014 Understanding random forests: from theory to practice. http: / /arxiv.org /
abs /14077502.

Breiman L. 2001 Random forests. Mach. Learn. 45, 5-32. (doi:10.1023 / A:1010933404324)

Parr T, Turgutlu K, Csiszar C, Howard J. 2018 Beware default random forest importances.
March 26, 2018.

Molnar C. 2020 Interpretable machine learning. See Lulu.com.

Gilpin LH, Bau D, Yuan BZ, Bajwa A, Specter M, Kagal L. 2018 Explaining explanations: an
overview of interpretability of machine learning. In 2018 IEEE 5th Int. Conf. on data science and
advanced analytics (DSAA), Turin, Italy, 1-3 October 2018, pp. 80-89. New York, NY: IEEE.
Montano J, Palmer A. 2003 Numeric sensitivity analysis applied to feedforward neural
networks. Neural Comput. Appl. 12, 119-125. (doi:10.1007/s00521-003-0377-9)

Liang B, Li H, Su M, Bian P, Li X, Shi W. 2017 Deep text classification can be fooled. http: //
arxiv.org /abs /170408006.

Zeiler MD, Fergus R. 2014 Visualizing and understanding convolutional networks. In
European Conf. on computer vision, Ziirich, Switzerland, 6-12 September 2014, pp. 818-833. Berlin,
Germany: Springer.

Ribeiro MT, Singh S, Guestrin C. 2016 ‘Why should I trust you?’ Explaining the predictions
of any classifier. In Proc. of the 22nd ACM SIGKDD Int. Conf. on knowledge discovery and data
mining, San Francisco, CA, 13-17 August 2016, pp. 1135-1144. New York, NY: ACM.

Shapley LS. 1953 A value for n-person games. Contrib. Theory Games 2, 307-317.

Bhatt U, Ravikumar P, Moura JM. 2019 Towards aggregating weighted feature attributions.
http:/ /arxiv.org/abs/190110040.

Strumbelj E, Kononenko L. 2014 Explaining prediction models and individual predictions with
feature contributions. Knowl. Inf. Syst. 41, 647-665. (doi:10.1007/s10115-013-0679-x)

. Chen J, Song L, Wainwright MJ, Jordan MI. 2018 L-shapley and c-shapley: efficient model

interpretation for structured data. http: / /arxiv.org /abs/180802610.

Lundberg SM, Lee SI. 2017 A unified approach to interpreting model predictions. In Advances
in neural information processing systems (eds U von Luxburg, I Guyon, S Bengio, H Wallach,
R Fergus), pp. 4765-4774.

. Lundberg SM et al. 2020 From local explanations to global understanding with explainable AI

for trees. Nat. Mach. Intell. 2, 2522-5839. (doi:10.1038 /s42256-019-0138-9)
Zintgraf LM, Cohen TS, Adel T, Welling M. 2017 Visualizing deep neural network decisions:
prediction difference analysis. http: / /arxiv.org /abs /170204595.

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

46.

47.

48.

49.

50.

51.

52.

53.
54.

55.

56.

57.

58.

59.

60.

61.

62.

63.

65.

66.

67.

68.

69.

70.

Simonyan K, Vedaldi A, Zisserman A. 2013 Deep inside convolutional networks: visualising
image classification models and saliency maps. http: / /arxiv.org/abs/13126034.

Shrikumar A, Greenside P, Shcherbina A, Kundaje A. 2016 Not just a black box:
learning important features through propagating activation differences. http: //arxiv.org/
abs /160501713.

Sundararajan M, Taly A, Yan Q. 2017 Axiomatic attribution for deep networks. http: / /arxiv.
org /abs/170301365.

Shrikumar A, Greenside P, Kundaje A. 2017 Learning important features through propagating
activation differences. http: / /arxiv.org /abs/170402685.

Ancona M, Ceolini E, Oztireli C, Gross M. 2017 Towards better understanding of gradient-
based attribution methods for deep neural networks. http: / /arxiv.org /abs /171106104.
Aamodt A, Plaza E. 1994 Case-based reasoning: foundational issues, methodological
variations, and system approaches. AI Commun. 7, 39-59. (doi:10.3233 / AIC-1994-7104)
Kolodner JL. 1992 An introduction to case-based reasoning. Artif. Intell. Rev. 6, 3-34.
(doi:10.1007 / BF00155578)

Richter MM, Weber RO. 2016 Case-based reasoning. Berlin, Germany: Springer.

Jia Y, Kaul C, Lawton T, Murray-Smith R, Habli I. 2020 Prediction of weaning from mechanical
ventilation using convolutional neural networks. Artif. Intell. Med. 117, 102087.

Wachter 5S, Mittelstadt B, Russell C. 2017 Counterfactual explanations without opening
the black box: automated decisions and the GDPR. Harv. JL & Tech. 31, 841.
(doi:10.2139 /ssrn.3063289)

Mothilal RK, Sharma A, Tan C. 2020 Explaining machine learning classifiers through
diverse counterfactual explanations. In Proc. of the 2020 Conf. on Fairness, Accountability, and
Transparency, Barcelona, Spain, 27-30 January 2020, pp. 607-617.

Sharma S, Henderson J, Ghosh J. 2019 Certifai: Counterfactual explanations for robustness,
transparency, interpretability, and fairness of artificial intelligence models. http: / /arxiv.org/
abs /190507857.

Szegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow I, Fergus R. 2013 Intriguing
properties of neural networks. http: / /arxiv.org/abs /13126199.

Xie C, Tan M, Gong B, Wang J, Yuille AL, Le QV. 2020 Adversarial examples improve
image recognition. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition,
Seattle, WA, 13-19 June 2020, pp. 819-828. New York, NY: IEEE.

Jia R, Liang P. 2017 Adversarial examples for evaluating reading comprehension systems.
http: / /arxiv.org/abs/170707328.

Sato M, Suzuki J, Shindo H, Matsumoto Y. 2018 Interpretable adversarial perturbation in input
embedding space for text. http: //arxiv.org/abs/180502917.

Etmann C, Lunz S, Maass P, Schénlieb CB. 2019 On the connection between adversarial
robustness and saliency map interpretability. http: / /arxiv.org/abs/190504172.

Koh PW, Liang P. 2017 Understanding black-box predictions via influence functions. http://
arxiv.org /abs/170304730.

. Wunsch H, Wagner J, Herlim M, Chong D, Kramer A, Halpern SD. 2013 ICU occupancy

and mechanical ventilator use in the United States. Crit. Care Med. 41, 2712-2719.
(doi:10.1097 /CCM.0b013e318298a139)

Ambrosino N, Gabbrielli L. 2010 The difficult-to-wean patient. Expert Rev. Respiratory Med. 4,
685-692. (doi:10.1586 /ers.10.58)

Kuo HJ, Chiu HW, Lee CN, Chen TT, Chang CC, Bien MY. 2015 Improvement in the
prediction of ventilator weaning outcomes by an artificial neural network in a medical ICU.
Respir. Care 60, 1560-1569. (doi:10.4187 /respcare.03648)

Johnson AE et al. 2016 MIMIC-III, a freely accessible critical care database. Sci. Data 3, 1-9.
(doi:10.1038 /sdata.2016.35)

Kroll JA, Barocas S, Felten EW, Reidenberg JR, Robinson DG, Yu H. 2016 Accountable
algorithms. U. Pa. L. Rev. 165, 633.

Sessler CN, Gosnell MS, Grap MJ, Brophy GM, O’Neal PV, Keane KA, Tesoro EP, Elswick RK.
2002 The Richmond Agitation—-Sedation Scale: validity and reliability in adult intensive care
unit patients. Am. J. Respir. Crit. Care Med. 166, 1338-1344. (doi:10.1164/recm.2107138)
Mehrabi N, Morstatter F, Saxena N, Lerman K, Galstyan A. 2019 A survey on bias and fairness
in machine learning. http:/ /arxiv.org/abs/190809635.

E9E00Z0Z 6LE ¥ 205 Y ‘SUOY Ud PYS4/leusno{/B10-BuIys)]qndAra}D0sjeKo1 A
Downloaded from https://royalsocietypublishing.org/ on 07 November 2022

71.

72.

73.

74.

75.

76.

78.

79.

Eubanks VA. 2018 Automating inequality: how high-tech tools profile, police, and punish the poor.
New York, NY: St. Martin’s Press.

Centre for Data Ethics and Innovation. Review into Bias in Algorithmic Decision-Making;
2020. See https:/ /assets.publishing.service.gov.uk/government/uploads/system/uploads /
attachment_data/file/939109/CDEI_review_into_bias_in_algorithmic_decision-making.pdf.
Habli I, Lawton T, Porter Z. 2020 Artificial intelligence in health care: accountability and
safety. Bull. World Health Organ. 98, 251. (doi:10.2471 /BLT.19.237487)

Sullivan E. 2019 Understanding from machine learning models. Br. J. Philos. Sci.
(doi:10.1093 /bjps/axz035)

Kelly TP. Arguing safety: a systematic approach to managing safety cases. PhD thesis,
University of York.

Burton $, Gauerhof L, Heinzemann C. 2017 Making the case for safety of machine learning in
highly automated driving. In Int. Conf. on Computer Safety, Reliability, and Security, Trento, Italy,
12-15 September 2017, pp. 5-16. Berlin, Germany: Springer.

. Picardi C, Hawkins R, Paterson C, Habli I. 2019 A pattern for arguing the assurance of

machine learning in medical diagnosis systems. In Int. Conf. on Computer Safety, Reliability,
and Security, pp. 165-179. Berlin, Germany: Springer.

Denney E, Pai G, Habli I. 2015 Dynamic safety cases for through-life safety assurance. In 2015
IEEE/ACM 37th IEEE Int. Conf. on Software Engineering, Florence, Italy, 16-24 May 2015, vol. 2,
pp. 587-590. New York, NY: IEEE.

Asaadi E, Denney E, Menzies J, Pai GJ, Petroff D. 2020 Dynamic assurance cases: a pathway
to trusted autonomy. Computer 53, 35-46. (doi:10.1109/MC.2020.3022030)

C9E00202 “6LE Y 205 y SUD] jIYd PIS4/leusnol/B10-BuIyst|qndAra.sosjekou
The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)

Does Explainable Artificial Intelligence Improve Human Decision-Making?

Yasmeen Alufaisan,! Laura R. Marusich, 7? Jonathan Z. Bakdash,> Yan Zhou,‘ Murat
Kantarcioglu +

' EXPEC Computer Center at Saudi Aramco
Dhahran 31311, Saudi Arabia
2 U.S. Army Combat Capabilities Development Command Army Research Laboratory South
at the University of Texas at Arlington
3 U.S. Army Combat Capabilities Development Command Army Research Laboratory South
at the University of Texas at Dallas
* University of Texas at Dallas
Richardson, TX 75080
yasmeen.alufaisan@ aramco.com, {laura.m.cooper20.civ, jonathan.z.bakdash.civ}@mail.mil,
{yan.zhou2,muratk} @utdallas.edu

Abstract

Explainable AI provides insights to users into the why for
model predictions, offering potential for users to better un-
derstand and trust a model, and to recognize and correct AI
predictions that are incorrect. Prior research on human and
explainable Al interactions has

typically focused on measures such as interpretability, trust,
and usability of the explanation. There are mixed findings
whether explainable AI can improve actual human decision-
making and the ability to identify the problems with the un-
derlying model. Using real datasets, we compare objective
human decision accuracy without AI (control), with an AI
prediction (no explanation), and AI prediction with explana-
tion. We find providing any kind of AI prediction tends to
improve user decision accuracy, but no conclusive evidence
that explainable AI has a meaningful impact. Moreover, we
observed the strongest predictor for human decision accuracy
was AI accuracy and that users were somewhat able to detect
when the AI was correct vs. incorrect, but this was not sig-
nificantly affected by including an explanation. Our results
indicate that, at least in some situations, the why information
provided in explainable AI may not enhance user decision-
making, and further research may be needed to understand
how to integrate explainable Al into real systems.

Introduction

Explainable AI is touted as the key for users to “under-
stand, appropriately trust, and effectively manage... [AI
systems])” (Gunning 2017) with parallel goals of achiev-
ing fairness, accountability, and transparency (Sokol 2019).
There are a multitude of reasons for explainable AI, but there
is little empirical research for its impact on human decision-
making (Miller 2019; Adadi and Berrada 2018). Prior be-
havioral research on explainable AI has primarily focused
on human understanding/interpretability, trust, and usabil-
ity for different types of explanations (Doshi-Velez and Kim

Copyright © 2021, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

6618

2017; Hoffman et al. 2018; Ribeiro, Singh, and Guestrin
2016, 2018; Lage et al. 2019).

To fully achieve fairness and accountability, explainable
AI should lead to better human decisions. Earlier research
demonstrated that explainable AI can be understood by peo-
ple (Ribeiro, Singh, and Guestrin 2018). Ideally, the com-
bination of humans and machines will perform better than
either alone (Adadi and Berrada 2018), such as computer-
assisted chess (Cummings 2014), but this combination may
not necessarily improve the overall accuracy of AI systems.
While (causal) explanation and prediction share common-
alities, they are not interchangeable concepts (Adadi and
Berrada 2018: Shmueli et al. 2010; Edwards and Veale
2018). Consequently, a ”good” explanation, interpretable
model predictions, may not be sufficient for improving
actual human decisions (Adadi and Berrada 2018; Miller
2019) because of heuristics and biases in human decision-
making (Kahneman 2011). Therefore, it is important to
demonstrate whether, and what types of, explainable AI can
improve the decision-making performance of humans us-
ing that AI, relative to performance using the predictions of
black box” AI with no explanations and for human making
decisions with no AI prediction.

In this work, we empirically investigate whether explain-
able AI improves human decision-making using a two-
choice classification experiment with real-world data. Us-
ing human subject experiments, we compared three differ-
ent settings where a user needs to make decision 1) No AT
prediction (Control), 2) AI predictions but no explanation,
and 3) AI predictions with explanations. Our results indi-
cate that, while providing the AI predictions tends to help
users, the why information provided in explainable AI does
not specifically enhance user decision-making.

Background and Related Work

Using Doshi-Velez and Kim’s (2017) framework for inter-
pretable machine learning, our current work focuses on: real
humans, simplified tasks. Because our objective is on eval-
uating decision-making, we do not compare different types
of explanations and instead used one of the best available
explanations: anchor LIME (Ribeiro, Singh, and Guestrin
2018). We use real tasks here, although our tasks involve
relatively simple decisions with two possible choices. Addi-
tionally, we use lay individuals rather than experts. Below,
we discuss prior work that is related to our experimental ap-
proach.

Explainable AI/Machine Learning

While machine learning models largely remain opaque and
their decisions are difficult to explain, there is an urgent need
for machine learning systems that can “explain” its reason-
ing. For example, European Union regulation requires “right
to explanation” for any algorithms that make decisions sig-
nificantly impacting users with user-level predictors (Par-
liament and Council of the European Union 2016). In re-
sponse to the lack of consensus on the definition and eval-
uation of interpretability in machine learning, Doshi- Velez
and Kim (2017) propose a taxonomy for the evaluation
of interpretability focusing on the synergy among human,
application, and functionality. They contrast interpretability
with reliability and fairness, and discuss scenarios in which
interpretability is needed. To unmask the incomprehensible
reasoning made by these machine learning/AI models, re-
searchers developed explainable models that are built on top
of the machine learning model to explain their decisions.
The most common forms of explainable models that pro-
vide explanations for the decisions made by machine learn-
ing models are feature-based and rule-based models. The
feature-based models resemble feature selection where the
model outputs the top features that explain the machine
learning prediction and their associated weights (Datta, Sen,
and Zick 2016; Ribeiro, Singh, and Guestrin 2016). The
tule-based models provide simple if-then-else rules to ex-
plain predictions (Ribeiro, Singh, and Guestrin 2018; Alu-
faisan et al. 2017). It has been shown that rule-based models
provide higher human precision when compared to feature-
based models (Ribeiro, Singh, and Guestrin 2018).

Lou et al. (2012) investigate the generalized additive mod-
els (GAMs) that combine single-feature models through a
linear function. GAMs are more accurate than simple lin-
ear models, and can be easily interpreted by users. Their
empirical study suggests that a shallow bagged-tree with
gradient boosting is the best method on low to medium di-
mensional datasets. Anchor LIME is an example of the cur-
rent state-of-the-art explainable rule-based model (Ribeiro,
Singh, and Guestrin 2018). It is a model-agnostic system
that can explain predictions generated by any machine learn-
ing model with high precision. The model provides rules,
referred to as anchors, to explain the prediction for each
instance. A rule is an anchor if it sufficiently explains the
prediction locally such that any changes to the rest of the
features, features not included in the anchor, do not effect
the prediction. Anchors can be found in two different ap-
proaches: bottom-up approach and beam search. Wang et al.
(2017) present a machine learning algorithm that produces
Bayesian rule sets (BRS) comprised of short rules in the dis-
junctive normal form. They develop two probabilistic mod-

6619

els with prior parameters that allow the user to specify a
desired size and shape and balance between accuracy and
interpretability. They apply two priors—beta-binomials and
Poisson distribution—to constrain the rule generation pro-
cess and provide theoretical bounds for reducing computa-
tion by iteratively pruning the search space. In our experi-
ments, we use anchor LIME to provide explanations for all
our experimental evaluation due to the high human precision
of anchor LIME as reported in Ribeiro, Singh, and Guestrin
(2018).

Human Decision-Making and Human Experiments
with Explainable AI

A common reason for providing explanation is to improve
human predictions or decisions (Keil 2006). People are not
necessarily rational (i.c., maximizing an expected utility
function). Instead, decisions are often driven by heuristics
and biases (Kahneman 2011). Also, providing more infor-
mation, even if relevant, does not necessarily lead people
to making better decisions (Gigerenzer and Brighton 2009).
Bounded rationality in human decision-making using satis-
fying with constraints (Gigerenzer and Brighton 2009) is an
alternative theory to heuristics and biases (Kahneman 2011).
Regardless of the theoretical account for human decision-
making, people, which can include experts (Dawes, Faust,
and Meehl 1989), generally do not make fully optimal deci-
sions.

At a minimum, explainable AI should not be detrimen-
tal to human decision-making. The literature on decision
aids (a computational recommendation or prediction, typ-
ically without an explicit explanation) has mixed findings
for human performance. Sometimes these aids are benefi-
cial for human decision-making, whereas at other times they
have negative effects on decisions (Kleinmuntz and Schkade
1993; Skitka, Mosier, and Burdick 1999). These mixed find-
ings may be attributable to absence of explanations; this
can be investigated through human experiments testing AT
predictions with explanations compared with AI predictions
alone.

Most prior human experiments with explainable AI have
concentrated on interpretability, trust, and subjective mea-
sures of usability, such as preferences and satisfaction, with
work on decision-making performance remaining somewhat
limited (Miller 2019; Adadi and Berrada 2018). Earlier
results suggest explainable AI can increase interpretabil-
ity (e.g. Ribeiro, Singh, and Guestrin 2018), trust (e.g.
Lakkaraju and Bastani 2020; Ribeiro, Singh, and Guestrin
2016; Selvaraju et al. 2017), and usability (e.g. Ribeiro,
Singh, and Guestrin 2018) to varying degrees, but this does
not necessarily translate to better performance on real-world
decisions about the underlying data, such as whether to ac-
tually use the AI’s prediction, whether the AI has made an
error, and the role of explanations. In fact, recent work has
shown that subjective measures commonly assessed (e.g.,
preference and trust) do not predict actual human perfor-
mance (Buginca et al. 2020; Zhang, Liao, and Bellamy
2020); similarly, performance on common proxy tasks such
as predicting the AI’s decision also may not be indicative of
actual decision-making performance (Bucinca et al. 2020).
These findings highlight the need for more study of the im-
pact of AI explanation on objective human performance, not
just proxy or subjective measures.

In the limited studies that do examine the effect of ex-
planation on human decision-making performance, there are
mixed findings about whether the explanation provides an
additional benefit over AI prediction alone. For example,
some researchers found that human performance was better
when an AI prediction was accompanied by explanation than
performance with the prediction alone (Bucinca et al. 2020;
Lai and Tan 2019) However, other studies did not show any
additional benefit of explanation over AI prediction alone
(Green and Chen 2019), with some even showing evidence
of worse performance with explanation (Poursabzi-Sangdeh
et al. 2018; Zhang, Liao, and Bellamy 2020).

The two papers finding a benefit for explanations con-
sisted of a task in which users made decisions about the
fat content in pictures of food (Bucinca et al. 2020) and
judgments about whether text from hotel reviews were gen-
uine or deceptive (Lai and Tan 2019). They also both used
a simple binary choice as the decision-making task. In con-
trast, the work finding no improvement in decision accuracy
with explainable AI used datasets that comprised variables
and outcomes, including probabilistic assessments for risks
with recidivism and loan outcomes (Green and Chen 2019),
decisions about real estate valuations (Poursabzi-Sangdeh
et al. 2018), and predictions about income (Zhang, Liao,
and Bellamy 2020). In addition, instead of simple binary
choices, these studies used prediction of values along a con-
tinuum (Green and Chen 2019; Poursabzi-Sangdeh et al.
2018), and binary choice with the option to switch after see-
ing the model prediction (Zhang, Liao, and Bellamy 2020).

Besides dataset and task differences, there are two other
distinctions among these papers. Only a single paper as-
sessed decision-making under time pressure (Zhang, Liao,
and Bellamy 2020) and only two papers informed users if
their decisions were correct or incorrect (Green and Chen
2019; Zhang, Liao, and Bellamy 2020). Our study de-
sign uses datasets of multiple variables and outcomes and
provides correct/incorrect feedback, but also uses a very
straightforward binary choice task. This combination could
potentially resolve the disparity in results from the studies
above.

Methods

In this section, we first describe the two datasets used in our
experiments. We then provide the details of our experimental
design and hypotheses, participant recruitment, and general
demographics of our sample.

Dataset

To conduct our experiments, we choose two different
datasets that have been heavily used in prior research that
tries to understand algorithmic fairness and accountability
issues. For example, the COMPAS dataset has been used to
detect potential biases in criminal justice system (Angwin
et al. 2016). The Census income dataset, which has been
used to test many machine learning techniques, involves pre-

6620

dictions of individuals’ income status. This has been associ-
ated with potential biases in making decisions such as access
to credit and job opportunities.

We choose these datasets primarily because they both in-
volve real-world contexts that are understandable and en-
gaging for human participants. Further, the two datasets dif-
fer widely in number of features and in the overall accuracy
classifiers can achieve in their predictions. This allows us
to explore the effects of these differences on human perfor-
mance; in addition, it ensures that our findings are not lim-
ited only to a specific dataset. We briefly discuss each dataset
in more detail below.

COMPAS stands for Correctional Offender Management
Profiling for Alternative Sanctions (Angwin et al. 2016). It is
a scoring system used to assign risk scores to criminal defen-
dants to determine their likelihood of becoming a recidivist.
The data has 6,479 instances and 7 features. These features
are: gender, age, race, priors count, and charge degree risk
score, and whether the defendants re-offended in two years
or not. We let the binary re-offending feature be our class.

Census income (CI) data contains information used to
predict individuals’ income (Dua and Graff 2017). It has
32,561 instances and 14 features. These features are: age,
workclass, education, marital status, occupation, relation-
ship, race, sex, capital gain, capital loss, hours per week,
and country. The class value is low income (less or equal to
50K) or high income (greater than 50K). We preprocessed
the dataset to allow equal class distribution !.

Experimental Design

Prior results demonstrating people interpret, trust, and pre-
fer explainable AI, suggesting it will improve the accuracy
of human decisions. Hence, our primary hypotheses are that
explainable AI would aid human decision-making. The hy-
potheses (H.) are as follows:

H. 1 Explainable AI enhances decision-making process
compared to only an AI prediction (without explanation)
and a control condition with no AI.

[H. 1.a] A participant performs above chance in pre-
diction tasks.

H. 2 A participant’s decision accuracy is positively associ-
ated with AI accuracy.

H. 3 Average participant’s decision accuracy does not out-
perform AI accuracy.

H.4 Participants outperform AI accuracy more often with
explainable AI over AI prediction alone.

H.5 Participants follow explainable AI recommendation
more often than AI only recommendation.

H. 6 Explainable AI increases participants’ decision confi-
dence.

H.7 A participant’s decision confidence is positively corre-
lated with the accuracy of his/her decision.

'The CI dataset is from 1994. We adjusted for inflation by using
a present value of 88k. From 1994 to January 2020 (when the ex-
periment was run) inflation in the U.S. was 76.45%: https://www.
wolframalpha.com/input/?i=inflation+from+ 1994+to+jan+2020
 

Age @

Greater_than_45 | Felony

 

 

Charge Degree (7) | Race @)

Caucasian | Low

Risk Score @) | Sex@ | Number of Priors @)

Female | 1

 

 

—= Control
—Al
— Al Explanation

 

|

Will Not Re-offend

Al Predicts: Will NOT Re-offend

Al Explanation:
* race: Caucasian, ano

e risk_score: Low, ano

* priors_count: priors_count is between 0.00 and 1.00

Choose whether or not you think this person will commit another crime within 2 years.

Will Re-offend

Figure 1: Example from the study demonstrating the information appearing in the three AI conditions for a trial from the

COMPAS dataset condition.

To investigate these hypotheses, we used a 2 (Dataset:
Census and COMPAS) x 3 (AI condition: Control, AI, and
AI with Explanation) between-participants experimental de-
sign. The three AI conditions were:

e Control: Participants were provided with no prediction or
information from the AI.

e AI: Participants were provided with only an AI prediction.

e AI with Explanation: Participants received an AI predic-
tion, as well as an explanation of the prediction using an-
chor LIME (Ribeiro, Singh, and Guestrin 2018).

To achieve more than 80% statistical power to detect a
medium effect size for this design, we planned for a sample
size of N = 300 (50 per condition).

In all conditions, each trial consists of a description of an
individual and a two-alternative forced choice for the classi-
fication of that individual. Each choice was correct on 50%
of the trials, thus chance performance for human decision-
making accuracy was 50%. Additionally, an AI prediction
and/or explanation may appear, depending on the AI con-
dition (see Figure 1). After a decision is made, participants
are asked to enter their confidence in that choice, on a Likert
scale of 1 (No Confidence) to 5 (Full Confidence). Feed-
back is then displayed, indicating whether or not the previ-
ous choice was correct.

We compared the prediction accuracy of Logistic Regres-
sion, Multi-layer Perceptron Neural Network with two lay-
ers of 50 units each, Random Forest, Support Vector Ma-
chine (SVM) with rbf kernel and selected the best classifier
for each dataset. We chose a Multi-layer Perceptron Neu-
ral Network for Census income data where it resulted in
an overall accuracy of 82% and SVM with rbf kernel for
COMPAS data with an overall accuracy of 68%. Census
income accuracy closely matches the accuracy reported in
the literature (Dua and Graff 2017; Alufaisan, Kantarcioglu,
and Zhou 2016) and COMPAS accuracy matches the re-
sults published by ProPublica (Angwin et al. 2016). We split

6621

the data to 60% for training and 40% for testing to allow
enough instances for the explanations generated using an-
chor LIME (Ribeiro, Singh, and Guestrin 2018).

In our behavioral experiment, 50 instances were randomly
sampled without replacement for each participant. Thus, AI
accuracy was experimentally manipulated for participants
(Census: mean AI accuracy = 83.85%, sd = 3.67%; COM-
PAS: mean AI accuracy = 69.18%, sd = 4.65%). Because
of the sample size and large number of repeated trials per
participant, there was no meaningful difference in mean AIT
accuracy for participants in the AI condition vs. those in the
AI explanation condition (p = 0.90).

Participant Recruitment and Procedure

We developed the experiment using jsPsych (De Leeuw
2015), and hosted it on the Volunteer Science platform (Rad-
ford et al. 2016) *. Participants were recruited using Ama-
zon Mechanical Turk (AMT) and were compensated $4.00
each. We collected data from 50 participants in each of the
six experimental conditions, for a total of 300 participants
(57.67% male). Most participants were 18 to 44 years old
(80.67%). This research was approved as exempt (19-176)
by the Army Research Laboratory’s Institutional Review
Board.

Participants read and agreed to a consent form, then re-
ceived instructions on the task, specific to the experimental
condition they were assigned to. They completed 10 practice
trials, followed by 50 test trials and a brief questionnaire as-
sessing general demographic information and comments on
strategies used during the task. The median time to complete
the practice and test trials was 18 minutes.

Results and Discussion

In this section we analyze and describe the effects of dataset,
Al condition, and AI accuracy on the participants’ decision-

*https://volunteerscience.com/
Participant Accuracy
80

70

Dataset

® Census
-® COMPAS

60

f-—-t—~

Al
Condition

Mean Participant Accuracy (%)

50

 

Control Al Explanation

Figure 2: Mean participant accuracy in each AI and dataset
condition. Error bars represent 95% confidence intervals.

making accuracy, ability to outperform the AI, adherence to
AI recommendations, confidence ratings, and reaction time.

Participant Decision-Making Accuracy

We compared participants’ mean accuracy in the experiment
across conditions using a 2 (Dataset) x 3 (AD factorial Anal-
ysis of Variance (ANOVA) (see Figure 2). We found signif-
icant main effects, with a small effect size for AI condition
(F(2,294) = 8.19,p < 0.001, n? = 0.04) and a nearly
large effect for dataset condition (F'(1, 294) = 46.51,p <
0.001, 7? = 0.12). In addition, there was a significant in-
teraction with a small effect size (F'(2,294) = 8.38,p <
0.001, 7? = 0.05), indicating that the effect of AI condition
depended on the dataset. Specifically, the large effect for in-
creased accuracy with AI was driven by the Census dataset.

Contrary to H. 1, explainable AI did not substantially im-
prove decision-making accuracy over AI alone. We followed
up on significant ANOVA effects by performing pairwise
comparisons using Tukey’s Honestly Significant Difference.
These post-hoc tests indicated that participants who viewed
the Census dataset showed improved accuracy over control
when given an AI prediction (p < 0.01) and higher accuracy
with AI explanation versus control (p < 0.001), but there
was no statistically significant difference in participant accu-
racy for AI compared to AI explanation (p = 0.28). Whereas
the COMPAS dataset had no significant differences in par-
ticipant accuracy across pairwise comparisons for the three
Al conditions (ps > 0.75). Also, the mean participant accu-
racy for the COMPAS control condition (mean = 63.7%, sd
= 9.24%) was comparable to participant accuracy for prior
decision-making research using the same dataset (mean =
62.8%, sd = 4.8%) (Dressel and Farid 2018).

There was strong evidence supporting H. 1.a, the vast ma-
jority of participants had mean accuracy exceeding guessing
(50% accuracy). The overall participant accuracy across all
conditions was 65.65% (sd = 10.92%), with 90% (or 270 out
of 300) participants performing above chance on the classi-
fication task. This indicates that the task was challenging but
feasible for almost all participants.

6622

Mean Al and Participant Accuracy

vr

 

Al

Al Explanation l
r

~
a

° Dataset
2 *
680 , ‘, “® Census
< “© COMPAS

a
a

  

Mean Participant Accuracy (%)

p
So
e

 

 

60 70 80 90 60 70

Mean Al Accuracy (%)

80 90

Figure 3: Mean AI accuracy (per participant) and mean par-
ticipant accuracy by AI and AI Explanation and the two
datasets. The shaded areas represent 95% confidence inter-
vals.

AI Accuracy and Participant Decision-Making
Accuracy

We also evaluated the effect of the randomly varied AI ac-
curacy for each participant on their decision-making accu-
racy. We used linear regression to analyze this relationship,
specifying participant accuracy as the dependent variable
and the following as independent variables: mean AI accu-
racy (per participant), AI condition, and dataset condition,
see Figure 3. Regressions are represented by the solid lines
with the shaded areas representing 95% confidence inter-
vals. The control condition is not included in the analysis
or figure, because the accuracy of the AI is not relevant if
no AI prediction is presented to the participant. The over-
all regression model was significant with a large effect size,
F (4,195) = 21.23,p < 0.001, Reajnsted = 0.29. Consis-
tent with H. 2, there was a large main effect for AI accu-
racy (3 = 0.70,p < 0.001, R? = 0.28). Also, there was a
small AI accuracy and dataset interaction (8 = —0.07,p <
0.01, R? = 0.03), reflecting the same interaction depicted
in Figure 2. There were no significant regression differences
for dataset or AI versus AI Explanation, ps > 0.60; there
was no significant effect of dataset because it largely drove
Al accuracy. Note it is not just that participants perform bet-
ter with the higher mean AI accuracy of the Census dataset,
both datasets had large positive relationships with partici-
pant accuracy and corresponding mean AI accuracy shown
in Figure 3.

Outperforming the AI Accuracy An interesting ques-
tion is whether the combination of AI and human decision-
making can outperform either alone. The previous analy-
ses showed that the addition of AI prediction information
improved human performance over controls with humans
alone. We also evaluated how often the human decision-
making accuracy outperformed the accuracy of the cor-
responding mean AI prediction accuracy, which was ex-
perimentally manipulated. Although most participants per-
 

 

 

 

| Dataset/Condition AI AT Explanation
Census 0 1
COMPAS 10 3

 

 

 

Table 1: Number of participants with decision accuracy ex-
ceeding their mean AI accuracy.

Following Al Recommendations

 

 

90
xt
2
= 80
6
S Dataset
C @® Census
> -® Compas
c
5 70
2
Oo
a

60

Al Correct Al Incorrect
Condition

Figure 4: Mean proportion of participant choices match-
ing AI prediction as a function of whether the AI cor-
rect/incorrect and the dataset condition. Error bars repre-
sent 95% confidence intervals. To simplify this figure, re-
sults were collapsed for the AI and AI Explanation condi-
tions which did not have a significant main effect, p = 0.62.

formed well above chance, only a relatively small number
of participants had decision accuracy exceeding their mean
AI prediction (7% or 14 out of 200). This result largely sup-
ports H. 3 and also shown above in Figure 3 where each dot
represents an individual; and dots above the black dashed
line show the participants that outperformed their mean AI
prediction. The black dashed line shows equivalent perfor-
mance for mean AI accuracy and mean participant accuracy.

Adherence to AI Model Predictions Participants fol-
lowed the AI predictions more often when the AI was correct
versus when the AI was incorrect, indicating some recogni-
tion of when the AI makes bad predictions (see Figure 4,
F(1,196) = 36.15,p < 0.001,72 = 0.16). This was con-
sistent with participant sensitivity to AI recommendations,
evidence for H. 2. Also, participants were better able to rec-
ognize correct vs. incorrect AI predictions when they were
in the Census condition, demonstrated in the significant in-
teraction between AI correctness and dataset, F(1,196) =
9.01,p < 0.01, 72 = 0.04. None of the remaining ANOVA
results were significant, ps > 0.16. Thus, there was no ev-
idence for higher adherence to recommendations with ex-
plainable AI, which rejected H. 5.

6623

Confidence Ratings
4.00

o
x
a

Dataset

® Census
-—® COMPAS

Mean Confidence Rating
o
a
a

o
N
o

3.00

 

Control Al Explanation

Al
Al Condition

Figure 5: Mean participant confidence ratings in each AI
condition. Error bars represent 95% confidence intervals.

Confidence Ratings

We found that AI (without and with explanation) resulted
in slightly increased mean confidence. There was a small
effect of AI condition on mean confidence (see Figure 5,
F (2,294) = 3.58,p = 0.03, 7? = 0.02). Post hoc tests
indicated participants had significantly lower mean confi-
dence in the control condition than AI, p < 0.03, but there
were no statistical differences for other pairwise compar-
isons, ps > (0.25. This contradicted H. 6, and there was
no evidence of a confidence increase with explanations. In
addition, there was no evidence for a main effect of dataset
condition or interaction, ps > 0.84.

Confirming H. 7, we found a positive relationship for
accuracy and confidence rating within individuals indicat-
ing that participants’ confidence ratings were fairly well-
calibrated with their actual decision accuracy. We calculated
each participant’s mean accuracy at each confidence rating
they used, and then conducted a repeated measures corre-
lation (Bakdash and Marusich 2017) (rpm = 0.48,p <
0.001).

Additional Results

We also assessed reaction time and summarize self-reported
decision-making strategies. These results are exploratory,
there were no specific hypotheses. There was no significant
main effect of AI condition on participants’ reaction time
(F(2, 294) = 2.13, p = 0.12, 7° = 0.01). There was only
a main effect of dataset condition (F'(1, 294) = 28.52,p <
0.001, 72 = 0.09), where participants took an average of
1600 ms longer in the Census condition than the COMPAS
condition (see Figure 6). This effect was most likely due to
the Census dataset having more variables for each instance
than the COMPAS dataset, and thus requiring more reading
time on each trial. The addition of an explanation did not
meaningfully increase reaction time over an AI prediction
only.

Subjective measures, such as self-reported strategies and
measures of usability, often diverge from objective measures
of human performance (Andre and Wickens 1995; Nisbett
and Wilson 1977) such as actual decisions (Bucinca et al.
Reaction Time
8000

(ms)

6000

ime

Dataset

® Census
= Compas

4000

Mean Reaction Ti

2000

Control Al Explanation

Al
Al Condition

Figure 6: Mean reaction time in each AI and dataset condi-
tion. Error bars represent 95% confidence intervals.

2020). Participants self-reported varying strategies to make
their decisions, yet there was a clear benefit for AI predic-
tion (without and with explanation). In the AI and AI expla-
nation conditions: n = 80 indicated using the data without
mentioning AI, n = 39 reported using a combination of the
data and the AI, and only n = 16 said they primarily used,
trusted, or followed the AI. Despite limited self-reported
use of the AI in the two relevant conditions, decision ac-
curacy was higher with AI (Figure 2), strongly associated
with AI accuracy (Figure 3), and there was some sensitivity
to whether the AI was followed when it was correct versus
incorrect (Figure 4). Nearly 80% of user comments could be
coded, blank and nonsense responses could not be coded.

Discussion

Our results show providing an AI prediction enhances hu-
man decision accuracy, but in opposition to the hypotheses
adding an explanation positively impact decisions and in-
crease the ability to outperform the AI. This finding is in
line with some previous studies that also found no added
benefit for explanation over AI prediction alone (Green and
Chen 2019; Poursabzi-Sangdeh et al. 2018; Zhang, Liao, and
Bellamy 2020). This suggests that it was not the simplicity
of the decision-making task that accounts for the opposite
findings in Bucinca et al. (2020) and Lai and Tan (2019),
since the current study also uses a relatively straightforward
binary decision. Rather, it may be the case that tasks with
highly intuitive datasets are required for explanation to im-
prove performance. Future work may address this question
directly.

One possible explanation for findings of no added bene-
fit of explanation is that providing more information, even if
task-relevant, does not necessarily improve human decision-
making accuracy (Gigerenzer and Brighton 2009; Goldstein
and Gigerenzer 2002; Nadav-Greenberg and Joslyn 2009).
This phenomenon is attributed to cognitive limitations and
people using near-optimal strategies, and corresponds with
Poursabzi-Sangdeh et al.’s (2018) findings that explanation
caused information overload, reducing people’s ability to de-
tect AI mistakes. However, their study and most other papers
did not use the speeded response paradigm we used here,

6624

suggesting this was not solely attributable to participants re-
sponding as quickly as possible.

The lack of a significant, practically-relevant effect for
explainable AI was not due to lack of statistical power or
ceiling performance - nearly all participants consistently
performed above chance, but well below perfect accuracy.
These findings also illustrate the need to compare decision-
making with explainable AI to other conditions including no
Al and AI prediction without explanation. If we did not have
an AI only (decision aid) condition a reasonable but flawed
inference would have been that explainable AI enhances de-
cisions.

Limitations The present findings have limited generaliz-
ability to decision-making with other datasets and explain-
able AI techniques. For example, the effectiveness of ex-
planations, or lack thereof, for human decision-making may
depend on a variety of factors: the specific explanation tech-
nique, the properties of the dataset, and the task itself (such
as probabilistic predictions vs two choice decisions). Nev-
ertheless, this paper demonstrates that one cannot assume
explainable AI will necessary improve human decisions and
the need to evaluate objective measures, of human perfor-
mance, in explainable AI.

Conclusions and Future Work

Much of the existing research on explainable AI focuses on
the usability, trust, and interpretability of the explanation.
In this paper, we fill in the research blank by investigat-
ing whether explainable AI can improve human decision-
making. We design a behavioral experiment in which each
participant recruited using Amazon Mechanical Turk is
asked to complete 50 test trials in one of six experimental
conditions. Our experiment is conducted on two real datasets
to compare human decision with an AI prediction and an AT
with explanation. Our experimental results demonstrate that
AI predictions alone can generally improve human decision
accuracy, while the advantage of explainable AI is not con-
clusive. We also show that users tend to follow AI predic-
tions more often when the AI predictions are accurate. In
addition, AI with or without explanation can increase the
confidence of human users which, on average, was well-
calibrated to user decision accuracy.

In the future, we plan to investigate whether explainable
AI can help improve fairness, safety, and ethics by increas-
ing the transparency of AI models. Human decision-making
is a key outcome measure, but is certainly not the only goal
for explainable AI. We also plan to explore the difference
of distributions in the error space between human decision
and AI predictions, especially at decision boundaries. Also,
whether human-machine collaboration is feasible through
interactions in closed feedback loops. We will also expand
our datasets to include other data format such as images.

Acknowledgments

The views and conclusions contained in this document are
those of the authors and should not be interpreted as rep-
resenting the official policies, either expressed or implied,
of the U.S. Army Combat Capabilities Development Com-
mand Army Research Laboratory or the U.S. Government.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Government purposes notwithstanding
any copyright notation. M.K. and Y.Z. were supported by
ARL under grant W911NF-17-1-0356. We thank Jason Rad-
ford for help with implementing the experiment on the Vol-
unteer Science platform and Katelyn Morris for indepen-
dently coding the comments. We also thank Dan Cassenti for
input on the paper, and Jessica Schultheis and Alan Breacher
for editing the paper.

Ethical Impact

For many important critical decisions, depending on the AI
model prediction may not be enough. Furthermore, many re-
cent regulations such as GDPR (Parliament and Council of
the European Union 2016) allow potential audit of AI pre-
diction by a human. Therefore, it is critical to understand
whether the explanations provided by explainable AI meth-
ods improve the overall prediction accuracy, and help human
decision makers to detect errors. Our results indicate that al-
though the existence of an AI model may improve human
decision-making, the explanations provided may not auto-
matically improve the accuracy. We believe that our results
could help ignite the needed research to explore how to bet-
ter integrate explanations, AI models and human operators
to have better outcomes compared AI models or humans
alone.

Another consideration is the data itself, models created
using systematically biased data will simply mirror and rein-
force patterns inherent to the data. For example, in the COM-
PAS dataset a high risk score for re-offending has far lower
accuracy for black defendants than white ones (Angwin
et al. 2016). Furthermore, multiple AI systems for predicting
criminal activity have relied on dirty” data with racial bias
due to flawed polices and procedures (Richardson, Schultz,
and Crawford 2019). One solution may be to combine ap-
proaches for fair machine learning (Corbett-Davies and Goel
2018) with explainable AI, while considering dataset prop-
erties such as its provenance.

References

Adadi, A.; and Berrada, M. 2018. Peeking inside the black-
box: A survey on Explainable Artificial Intelligence (XAI).
IEEE Access 6: 52138-52160.

Alufaisan, Y.; Kantarcioglu, M.; and Zhou, Y. 2016. De-
tecting Discrimination in a Black-Box Classifier. In In Pro-
ceedings of the 2016 IEEE 2nd International Conference on
Collaboration and Internet Computing (CIC). TEEE.

Alufaisan, Y.; Zhou, Y.; Kantarcioglu, M.; and Thuraising-
ham, B. 2017. From Myths to Norms: Demystifying Data
Mining Models with Instance-Based Transparency. In In
Proceedings of the 2017 [EEE 3rd International Conference
on Collaboration and Internet Computing (CIC). IEEE.

Andre, A. D.; and Wickens, C. D. 1995. When users want
what’s not best for them. Ergonomics in design 3(4): 10-14.

6625

Angwin, J.; Larson, J.; Mattu, S.; and Kirchner, L. 2016.
Machine Bias. https://www.propublica.org/article/machine-
bias-risk-assessments-in-criminal- sentencing.

Bakdash, J. Z.; and Marusich, L. R. 2017. Repeated mea-
sures correlation. Frontiers in psychology 8: 1-13. doi:
10.3389/fpsyg.2017.00456.

Buginca, Z.; Lin, P.; Gajos, K. Z.; and Glassman, E. L. 2020.
Proxy Tasks and Subjective Measures Can Be Misleading
in Evaluating Explainable AI Systems. In Proceedings of
the 25th International Conference on Intelligent User Inter-
faces, TUI’20, 454-464. doi:10.1145/3377325.3377498.

Corbett-Davies, S.; and Goel, S. 2018. The measure and
mismeasure of fairness: A critical review of fair machine
learning. arXiv preprint arXiv: 1808.00023 .

Cummings, M. M. 2014. Man versus machine or man+ ma-
chine? [EEE Intelligent Systems 29(5): 62-69.

Datta, A.; Sen, S.; and Zick, Y. 2016. Algorithmic Trans-
parency via Quantitative Input Influence: Theory and Ex-
periments with Learning Systems. In In Proceedings of the
2016 IEEE Symposium on Security and Privacy (SP), 598-
617.

Dawes, R. M.; Faust, D.; and Meehl, P. E. 1989. Clinical
versus actuarial judgment. Science 243(4899): 1668-1674.

De Leeuw, J. R. 2015. jsPsych: A JavaScript library for cre-
ating behavioral experiments in a Web browser. Behavior
research methods 47(1): 1-12.

Doshi-Velez, F; and Kim, B. 2017. Towards a rigorous
science of interpretable machine learning. arXiv preprint
arXiv: 1702.08608 .

Dressel, J.; and Farid, H. 2018. The accuracy, fairness,
and limits of predicting recidivism. Science advances 4(1):
eaao5580.

Dua, D.; and Graff, C. 2017. UCI Machine Learning
Repository. URL http://archive.ics.uci.edu/ml. Accessed on
05.2020.

Edwards, L.; and Veale, M. 2018. Enslaving the algorithm:
From a “Right to an Explanation” to a “Right to Better De-
cisions”? IEEE Security & Privacy 16(3): 46-54.

Gigerenzer, G.; and Brighton, H. 2009. Homo heuristicus:
Why biased minds make better inferences. Topics in cogni-
tive science 1(1): 107-143.

Goldstein, D. G.; and Gigerenzer, G. 2002. Models of eco-
logical rationality: the recognition heuristic. Psychological
review 109(1): 75.

Green, B.; and Chen, Y. 2019. The Principles and Limits
of Algorithm-in-the-Loop Decision Making. Proc. ACM
Hum.-Comput. Interact. 3(CSCW). doi:10.1145/3359152.
URL https://doi.org/10.1145/3359152.

Gunning, D. 2017. Explainable artificial intelli-
gence (xai). URL https://www.darpa.mil/attachments/
XAIProgramUpdate.pdf. Accessed on 05.2020.

Hoffman, R. R.; Mueller, S. T.; Klein, G.; and Litman, J.
2018. Metrics for explainable ai: Challenges and prospects.
arXiv preprint arXiv: 1812.04608 .
Kahneman, D. 2011. Thinking, fast and slow. Macmillan.

Keil, F C. 2006. Explanation and understanding. Annu. Rev.
Psychol. 57: 227-254.

Kleinmuntz, D. N.; and Schkade, D. A. 1993. Informa-
tion displays and decision processes. Psychological Science
4(4): 221-227.

Lage, I.; Chen, E.; He, J.; Narayanan, M.; Kim, B.; Ger-
shman, S.; and Doshi-Velez, F 2019. An evaluation of
the human-interpretability of explanation. arXiv preprint
arXiv: 1902.00006 .

Lai, V.; and Tan, C. 2019. On human predictions with expla-
nations and predictions of machine learning models: A case
study on deception detection. In Proceedings of the Confer-
ence on Fairness, Accountability, and Transparency, 29-38.

Lakkaraju, H.; and Bastani, O. 2020. “How do I fool you?”:
Manipulating User Trust via Misleading Black Box Expla-
nations. In Proceedings of the AAAI/ACM Conference on
Al, Ethics, and Society, 79-85. ACM. doi:10.1145/3375627.
3375833.

Lou, Y.; Caruana, R.; and Gehrke, J. 2012. Intelligible Mod-
els for Classification and Regression. In KDD’/2, Beijing,
China. ACM.

Miller, T. 2019. Explanation in artificial intelligence: In-
sights from the social sciences. Artificial Intelligence 267:
1-38.

Nadav-Greenberg, L.; and Joslyn, S. L. 2009. Uncertainty
forecasts improve decision making among nonexperts. Jour-
nal of Cognitive Engineering and Decision Making 3(3):
209-227.

Nisbett, R. E.; and Wilson, T. D. 1977. Telling more than we
can know: verbal reports on mental processes. Psychological
review 84(3): 231.

Parliament and Council of the European Union. 2016. Reg-
ulation (EU) 2016/679 of the European Parliament and of
the Council of 27 April 2016 on the Protection of Natu-
tal Persons with Regard to the Processing of Personal Data
and on the Free Movement of such Data, and Repealing
Directive 95/46/ EC (General Data Protection Regulation).
https://eur-lex.europa.eu/eli/reg/2016/679/oj.

Poursabzi-Sangdeh, F.; Goldstein, D. G.; Hofman, J. M.;
Vaughan, J. W.; and Wallach, H. 2018. Manipulat-
ing and measuring model interpretability. arXiv preprint
arXiv: 1802.07810 .

Radford, J.; Pilny, A.; Reichelmann, A.; Keegan, B.; Welles,
B. F; Hoye, J.; Ognyanova, K.; Meleis, W.; and Lazer, D.
2016. Volunteer science: An online laboratory for experi-

ments in social psychology. Social Psychology Quarterly
79(4): 376-396.

Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ” Why
should i trust you?” Explaining the predictions of any clas-
sifier. In Proceedings of the 22nd ACM SIGKDD interna-
tional conference on knowledge discovery and data mining,
1135-1144.

6626

Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2018. Anchors:
High-precision model-agnostic explanations. In Thirty-
Second AAAI Conference on Artificial Intelligence.

Richardson, R.; Schultz, J. M.; and Crawford, K. 2019. Dirty
data, bad predictions: How civil rights violations impact po-
lice data, predictive policing systems, and justice. NYUL
Rev. Online 94: 15.

Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;
Parikh, D.; and Batra, D. 2017. Grad-CAM: Visual Explana-
tions from Deep Networks via Gradient-Based Localization.
In 2017 IEEE International Conference on Computer Vision
(ICCV), 618-626.

Shmueli, G.; et al. 2010. To explain or to predict? Statistical
science 25(3): 289-310.

Skitka, L. J.; Mosier, K. L.; and Burdick, M. 1999. Does
automation bias decision-making? International Journal of
Human-Computer Studies 51(5): 991-1006.

Sokol, K. 2019. Fairness, Accountability and Transparency
in Artificial Intelligence: A Case Study of Logical Predictive
Models. In Proceedings of the 2019 AAAIACM Conference
on Al, Ethics, and Society, 541-542.

Wang, T.; Rudin, C.; Doshi-Velez, F; Liu, Y.; Klampfi,
E.; and MacNeille, P. 2017. A Bayesian Framework for
Learning Rule Sets for Interpretable Classification. Jour-
nal of Machine Learning Research 18(70): 1-37. URL
http://jmlr.org/papers/v 18/16-003 html.

Zhang, Y.; Liao, Q. V.; and Bellamy, R. K. E. 2020. Effect
of Confidence and Explanation on Accuracy and Trust Cal-
ibration in AI-Assisted Decision Making. In Proceedings
of the 2020 Conference on Fairness, Accountability, and
Transparency, FAT* ’20, 295-305. doi:10.1145/335 1095.
3372852.
® Check for updates

Received: 22 November 2019 Revised: 18 September 2020 Accepted: 21 September 2020

 

DOT: 10.1002/widm.1391

eS) WIRE
OVERVIEW @ DATA MENG AND KNOWLEDGE DISCOVERY WI LEY

A historical perspective of explainable Artificial
Intelligence

Roberto Confalonieri‘® | Ludovik Coba’ | Benedikt Wagner? | Tarek R. Besold’ ©

‘Faculty of Computer Science, Free

University of Bozen-Bolzano, Bozen- Abstract
Bolzano, Italy Explainability in Artificial Intelligence (AI) has been revived as a topic of
*Research Centre for Machine Learning, active research by the need of conveying safety and trust to users in the

City University, London, UK “how” and “why” of automated decision-making in different applications

3. :

Neurocat GmbH, Berlin, German: : : : :
y such as autonomous driving, medical diagnosis, or banking and finance.

Correspondence While explainability in AI has recently received significant attention, the ori-

Roberto Confalonieri, Faculty of gins of this line of work go back several decades to when AI systems were

Computer Science, Free University of . . .
mainly developed as (knowledge-based) expert systems. Since then, the defini-

Bozen-Bolzano, Dominikanerplatz
3, Bozen-Bolzano 1-39100, Italy. tion, understanding, and implementation of explainability have been picked
Email: roberto.confalonieri@unibz.it up in several lines of research work, namely, expert systems, machine learn-
ing, recommender systems, and in approaches to neural-symbolic learning
and reasoning, mostly happening during different periods of AI history. In
this article, we present a historical perspective of Explainable Artificial Intelli-
gence. We discuss how explainability was mainly conceived in the past, how
it is understood in the present and, how it might be understood in the future.
We conclude the article by proposing criteria for explanations that we believe
will play a crucial role in the development of human-understandable explain-
able systems.

This article is categorized under:
Fundamental Concepts of Data and Knowledge > Explainable AI
Technologies > Artificial Intelligence

KEYWORDS

explainable AI, explainable recommender systems, interpretable machine learning, neural-
symbolic reasoning

 

1 | INTRODUCTION

As of 2020, explainability has been identified as a key factor for adoption of AI systems in a wide range of contexts
(Doshi-Velez & Kim, 2017; Lipton, 2018; Ribeiro, Singh, & Guestrin, 2016a). Discussion accompanying the increasingly
common deployment of intelligent systems in application domains such as autonomous vehicles and transportation,
medical diagnosis, or insurance and financial services have shown that when decisions are taken or suggested by

This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided
the original work is properly cited.
© 2020 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.

 

WIREs Data Mining Knowl Discov. 2021;11:e1391. wires.wiley.com/dmkd 1 of 21
https://doi.org/10.1002/widm.1391

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
2 of 21 WI LEY_ @ WIREs CONFALONIERI er at.

DATA MINING AND KNOWLEDGE DISCOVERY

automated systems, it is essential for practical, social, and—with increasing frequency—legal reasons that an explana-
tion can be provided to users, developers, and regulators.

As a case in point, the European Union's General Data Protection Regulation (GDPR) stipulates a right to obtain
“meaningful information about the logic involved”—commonly interpreted as a “right to an explanation”’—for con-
sumers affected by an automatic decision (Parliament and Council of the European Union, 2016).?

The reasons for equipping intelligent systems with explanatory capabilities are not limited to issues of user rights
and of technology acceptance, though. Explainability is also required by designers and developers to enhance system
robustness and to enable diagnostics to prevent bias, unfairness, and discrimination, as well as to increase trust by all
users in why and how decisions are made.

Being able to provide an explanation of why a certain decision was made, has thus become a desirable property of
intelligent systems (Doran, Schulz, & Besold, 2017). Explanations should help users in understanding the model of the
system, in order to maintain it, and to use it effectively; they should also assist the user when debugging the model to
prevent and rectify incorrect conclusions. In addition, explanations can serve educational purposes and be helpful for
people in discovering and understanding novel concepts in an application domain. Finally, explanations are related to
users’ trust and persuasion, they should convey a sense of actionability, and convince users that the system's decisions
are the most convenient for them.

Notwithstanding, there is no clear agreement about what an explanation is, nor what a good explanation entails. Its
manifestations have been studied across different incarnation of AI systems and disciplines. The first notions of
explainability in Artificial Intelligence had subsided together with that in expert systems after the mid-1980s
(Buchanan & Shortliffe, 1984; Wick & Thompson, 1992), and have been brought back into the focus by recent successes
in machine learning technology (Guidotti et al., 2018), for both autonomous (Nunes & Jannach, 2017) and human-in-
the-loop systems (Holzinger, 2016; Holzinger, Plass, et al., 2019), with applications in recommender systems
(Tintarev & Masthof, 2015), and approaches of neural-symbolic learning and reasoning (Garcez et al., 2015).

In this article, we look at the literature of Explainable Artificial Intelligence (XAI) from a historical perspective of
traditional approaches as well as approaches currently being developed. The relevant literature is vast, and this article
does not aim to be a complete overview of the XAI literature. For each of the perspectives, the reader can find more
comprehensive literature reviews in machine learning and Deep Learning (Arrieta et al., 2020; Fernandez, Herrera,
Cordon, Jose del Jesus, & Marcelloni, 2019; Guidotti et al., 2018; Mueller, Hoffman, Clancey, Emrey, & Klein, 2019),
recommender systems (Nunes & Jannach, 2017; Tintarev & Masthof, 2015), and Neural-Symbolic Approaches (Garcez
et al., 2015). The aim of the article is rather to provide an overview and discuss how different notions of explainability
(resp. format of explanations) have been conceived, and to provide several examples.

The main contributions of this article are:

* To provide an overview of XAI, and how it is understood in expert systems, machine learning, recommender systems,
and neural-symbolic learning and reasoning approaches.
* To provide the reader with a wide range of references, (s)he can use to gain a deeper understanding in the topic of XAI.

The article is organized as follows. In Section 2, we give an overview of the different notions of explainability that
are subsequently addressed from different perspectives throughout the article. Section 3 describes two notions of expla-
nations prominently represented in the expert system literature, namely explanations as line of reasoning and as
problem-solving activities. In Section 4, we present how the notion of explanation is commonly understood in machine
learning, as well as a few examples of such explanations. Section 5 discusses how explanations are conceptualized in
the context of recommender systems. Section 6 identifies the increasingly popular perspective of Neural-Symbolic
Learning and Reasoning as promising approach to explainability in AI systems. Section 7 provides a critical discussion
and comparison of the different notions of explainability mentioned throughout the article, and introduces general
desiderata for explainability and a set of challenges for the development of human-understandable explainable AI sys-
tems. Section 8 concludes the article.

2 | WHATIS A (GOOD) EXPLANATION?

Defining what an explanation is remains a still open research question. In particular, determining the criteria for a good
explanation as of today is an active debate in various fields, including cognitive science, computer science, psychology,

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
 

CONFALONIERI - 3 of 21
— gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY. °

and philosophy (Confalonieri et al., 2019; Guidotti et al., 2018; Hoffman, Mueller, Klein, & Litman, 2018; Lipton, 2018;
Lombrozo, 2016; Miller, 2019).

Miller (2019) articulates the link between discussion in the social sciences and explainability in AI, providing an in-
depth survey on research on explanations in philosophy, psychology, and cognitive science. Three major findings were
highlighted. First, explanations are counterfactual, and humans tend to understand why a certain event happened
instead of some other events. Second, explanations are selective and focus on one or two possible causes—instead of all
possible causes—for a decision or recommendation; that is, explanations should not overwhelm the user with too much
information. Third, explanations are a social conversation and interaction for the purpose of transferring knowledge,
implying that the explainer must be able to leverage the mental model of the explainee while engaging in the explana-
tion process. While according to Miller (2019) these three points are key properties when building useful explanations,
the different notions of explainability prevalent in XAI only recently started to take them into account.

Psychology researchers have studied and defined properties of explanations that are human-oriented. For instance,
Lombrozo (2016) suggested that one needs to differentiate between distinct possible goals for explainability, while
highlighting why and how human explanatory cognition provides crucial constraints for the design of XAI systems.
Hilton (1990) pointed out that explanations imply social interactions, and that for machine-generated explanations, it is
essential to associate semantic information with an explanation (or its components) for effective knowledge transmis-
sion to human users. Kulesza et al. (2013) investigated the relationship between certain properties of generated explana-
tions and the fidelity of users’ mental models, finding that completeness ultimately appears to be more important than
soundness, and that oversimplification is detrimental to users’ trust in an explanation.

Work in computer science hitherto focused to the most part on the mechanistic aspects of how explanations are
generated (Guidotti et al., 2018). This includes not only approaches in machine learning and recommender systems,
but also in knowledge-based systems. The types of explanations these systems are able to create—and, consequently,
their properties—mainly depend on the type of reasoning employed in the system, namely, symbolic, subsymbolic, or
hybrid.?

Symbolic reasoning systems draw conclusions or explain why a certain hypothesis holds based on a knowledge
base—usually encoded as a set of production or symbolic rules—and an inference mechanism, such as deduction,
abduction, or analogical reasoning (Doyle, Tsymbal, & Cunningham, 2003; Lacave & Diez, 2004; Mitchell, Keller, &
Kedar-Cabelli, 1986). Explanations in these systems consist of either descriptions coupled to the reasoning trace of the
system, or descriptions more coupled to the story behind the decision-making process of the system (Buchanan &
Shortliffe, 1984; Wick & Thompson, 1992). In either case, metrics and desirable properties for these explanations are,
for instance, accuracy, adaptability, and comprehensibility. While these explanations are typically meant to be a precise
reconstruction of the system behavior, they also should be adaptable to match different user profiles. Indeed, lay users
might be more interested in a less accurate but more understandable explanation, whereas expert users might prefer
more technical and precise explanation formats.

Subsymbolic (or connectionist) reasoning systems are, generally speaking, those that rely on machine learning
models in which representations are in most cases distributed and processing occurs simultaneously in multiple parallel
channels. Unfortunately, these properties frequently bring about a certain black-box nature of the corresponding
models. As a consequence, explanations in these systems often take the form of interpretable models that approximate
or try to mimic the behavior of the black-box (Andrews, Diederich, & Tickle, 1995; Guidotti et al., 2018). An interpret-
able model allows users to understand how decisions are made by means of local or global post-hoc explanations
(Guidotti et al., 2018). Such interpretable models are typically evaluated using metrics such as accuracy and fidelity.
These metrics measure to what extent an interpretable model is able to maintain competitive levels of accuracy with
respect to the original black-box model, and to what extent the model is able to accurately imitate a black-box predictor
respectively. Additional metrics targetting the notion of causability of explanations have recently been introduced by
(Holzinger, Langs, et al., 2019; Holzinger, Carrington, & Miiller, 2020). Causability refers to the extent to which an
explanation achieves a certain level of causal understanding in a specified context of use and is measured in terms of
effectiveness, efficiency, satisfaction related to causal understanding and its transparency for a user. As an additional
class of explanation approaches predominantly for black-box models, methods providing explanations based on coun-
terfactuals (i.e., hypothetical input examples that show how a different decision or prediction could have been obtained)
recently also moved into the focus of active research (see e.g., Mothilal, Sharma, & Tan, 2020).

A particular category of sub-symbolic reasoning systems are recommender systems. There is no clear consensus in
the recommender systems literature on what makes for a good explanation (Nilashi, Jannach, & bin Ibrahim, O.,
Esfahani, M. D.,, & Ahmadi, H., 2016; Nunes & Jannach, 2017; Tintarev & Masthof, 2015). In fact, an explanation on

asuOdt'] SUOLULLOD eATEaID e[qeor{dde oy) q poUlard axe so[otUE YO {eSN Jo se[Ns Joy Arwagr] OULU Ao|1A4 UO (SUOTIPUOD-PUL-sULIOYWOD’€e[LM KrwAgtoUT|UO//:SdMY) SUOTIPUDD PUe SULIT, ay 22g “[ZZOZ/LL/LO] UO KBAgry OUTTUD AOL AY “TaeIS] UBAYIOD Aq [EEL UEPIAYZOQT ‘OL/LOp/MOD Kom AreaqiaurTUO'soMLANy/'SdNY WOLy PapEO[UMOK] ‘| “1ZOZ “S6LPTPEL
4of 21 WI LEY_ @ WIREs CONFALONIERI er at.

DATA MINING AND KNOWLEDGE DISCOVERY

the recommendation can have different goals, and impact decision-makers differently (Coba, Rook, et al., 2019). For
example, a tailored explanation can persuade or help a user in finding an item more efficiently (Tintarev &
Masthof, 2015). When implementing an explanation, a usual approach is to first determine its objective. For instance,
stakeholders might be interested in delivering persuasive explanations, since they increase the probability of acceptance
or purchase of a recommended item (Nunes & Jannach, 2017). Trustworthiness is another desired property of an expla-
nation, since users tend to return to and reuse systems that they trust (L. Chen & Pu, 2005). Moreover, efficient, effective,
and satisfying explanations help the users in deciding fast and making good decisions and increase the ease of use,
respectively (Tintarev & Masthof, 2015). Transparency fosters the understandability for the user of the underlying logic
of the advice-giving systems, and scrutability allows the user to tell that the system is wrong. These properties are often
correlated. For instance, transparent explanations should also be comprehensible, and are known to convey trust. For a
detailed discussion about the relationships between characteristics we refer the reader to (Balog & Radlinski, 2020).

Hybrid or neural-symbolic systems are those systems that combine symbolic and sub-symbolic reasoning (Garcez
et al., 2015). The sub-symbolic system is able to build predictive models using connectionist machine learning and
processing large amounts of data, while the symbolic system is equipped with a rich representation of domain knowl-
edge and can be used for higher-level, structured reasoning. These symbolic elements are used by the system to explain
the decisions made by the sub-symbolic components. Also here, accuracy and fidelity are, once more, important metrics
to measure the performance of an interpretable model; whereas consistency and comprehensibility are desirable proper-
ties of the produced explanations from the explainee’s point of view. The domain knowledge can serve as basis for
common-sense reasoning, and supports knowledge abstraction, refinement, and injection (Confalonieri, Eppe,
Schorlemmer, Kutz, & Pen*aloza, R.,, & Plaza, E., 2018; Lehmann & Hitzler, 2010). As such, the system has not only
the capability to create explanations for the sub-symbolic parts, but also to change the explanations’ level of accuracy
and technicality depending on the user profile. Furthermore, the system can refine the extracted knowledge, and inject
it back to the sub-symbolic system to improve its performance (Garcez, Broda, & Gabbay, 2001).

3 | EXPLANATIONS IN EXPERT SYSTEMS

Expert or knowledge-based systems are software systems augmented by expert or domain knowledge. They are consid-
ered as one of the first instantiations of AI systems. They were developed to support humans in making decisions in sev-
eral domains (Doyle et al., 2003; Lacave & Diez, 2004; Mitchell et al., 1986; Wick & Thompson, 1992).

An expert system consists of a knowledge base encoding the domain knowledge, usually modeled as a set of produc-
tion rules, a rule interpreter or reasoner that makes use of the knowledge base, and an interface through which the user
can query the system for knowledge.

In the literature on expert systems, explanations are mainly understood in one of two ways: an explanation as a line
of reasoning, or as a problem-solving activity.

3.1 | Explanations as lines of reasoning

Seeing an explanation as a line of reasoning means mainly understanding it as a trace of the way that production or
inference rules are used by the system to make a certain decision. While this kind of explanation mainly accommodates
the need of knowledge engineers to understand whether the system's reasoning is technically sound, it (or slight vari-
ants of it) can also be provided as an explanation to domain experts (Buchanan & Shortliffe, 1984; Mitchell et al., 1986).

The most famous instantiation of a system that was able to provide this kind of explanation is MYCIN (Buchanan &
Shortliffe, 1984). MYCIN is a rule-based system with consultation capabilities developed in the 1970s, created with the
aim to provide doctors with diagnostic and therapeutic advice about patients with an infection. MYCIN’s expertise con-
sists of a static knowledge base containing domain specific knowledge of an expert, as well as factual knowledge about
the particular problem under consideration.

The domain or expert knowledge is modeled by means of production rules (see Table 1), which are used to provide
diagnosis solutions to specific cases. That is, the user provides some knowledge about a specific patient as input, and
the system uses this knowledge to instantiate rules and to make the diagnosis corresponding to the specific case.

The explanation capability in MYCIN consists of a general question answering module and a reasoning-status
checker. The former answers simple English language questions concerning the system's decision in a consultation, or

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI - 5 of 21
— gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY. °

 

TABLE1 Example ofa MYCIN PREMISE: (AND (SAME CNTXT GRAM GRAMNEG)
rule, in both its logical internal form
. . (SAME CNTXT MORPH ROD)
and English translation
(SAME CNTXT AIR ANAEROBIC))
ACTION: (CONCLUDE CNTXT IDENTITY BACTEROIDES TALLY .6)
IF: (1) The gram stain of the organism is gramneg,

(2) The morphology of the organism is rod, and
(3) The aerobicity of the organism is anaerobic
THEN: There is suggestive evidence (.6) that

The identity of the organism is bacteroides

about the system's general knowledge. The latter provides explanations about the line of reasoning followed by the
system.

The question answering module accepts a set of predefined questions that allow an expert user to query the dynamic
knowledge or rationale with respect to the rules, parameters, values, and contexts used in a specific consultation. On
the other hand, the reasoning status checker allows the user to go deeper into the sequence of rules that are used. The
question answering module accepts two basic explanation commands: a why command, by which the user can ascend
the reasoning chain and explore higher-goals: and a how command, by which the user can descend the chain of infer-
ences exploring how a goal was achieved.

Although the provisioning of explanations as lines of reasoning of why certain decisions were “logically” made
improves the interpretability of expert systems, humans, when asked to account for complex reasoning, tend to also
reconstruct a story that describes the problem-solving behind the decision. That is, they might reconstruct an explana-
tion that fits their level of knowledge and expertise. For instance, a lay user will not benefit much from a very technical
explanation, compared to a domain expert or a knowledge engineer. This is what motivated the reconceptualization of
explanations as a problem-solving activity by itself as we will discuss in the next section.

3.2 | Explanations as a problem-solving activity

Conceiving of explanations as problem-solving activities means not only re-constructing the line of reasoning of the sys-
tem, but also taking into account different levels of abstraction. These could range from very technical to more explana-
tory explanation formats accommodating different user profiles.

The adaptability of explanations to different types of users can be achieved by de-coupling the explanation capability
from the main reasoning functionality, and by focusing the explanation on the problem-solving knowledge used to
solve a certain task (Hassling, Clancey, & Runnels, 1984; Wick & Thompson, 1992). An example of an expert system
exhibiting this adaptability is Rex (Wick & Thompson, 1992).

Rex was designed to provide explanations of how an expert system moves from the data of a particular case to a final
conclusion (a line of explanation) by building a “story” as an abstract of the expert systems reasoning. Rex was an inde-
pendent component from the expert system used, provided that an interface as well as two knowledge bases existed: a
knowledge specification and explanatory knowledge. The former acted as an interface between the knowledge of the
expert system and the knowledge of the explanation system, and it covered the problem-solving expertise used to solve
problems within the domain. The latter was knowledge used to create an explanation.

The explanation model of Rex is shown in Figure 1. The model takes a set of reasoning cues, and a set of constraints
as input. The reasoning cues consist of knowledge used and inferred by the expert system during the resolution of a cer-
tain case. This knowledge is filtered by a set of problem constraints that decide which of these reasoning cues are avail-
able to the explanation system. The selected reasoning cues are then mapped to the knowledge specification of the
domain, the screener. The knowledge specification (spec) is the common ground between the expert system and the
explanatory system and it is a high-level representation of the domain. It allows the explanation system to abstract from
the procedural details of the expert system. The knowledge specification consists of transitions between hypotheses,
where any transition requires the satisfaction of some goals and the existence of some reasoning cues. At this step, only
some of the transitions might be enabled, thus only some hypotheses can be inferred, and become available to the

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
 

6 of 21 WI LEY_— qt CONFALONIERT er at.

es WIRE Ree AND KNOWLEDGE DISCOVERY

 
     
 

 

 

 

 

 

 
  
   

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

CUE uplift
Expert Reasoning Cues Value : true
v Type : direct
Name : the high uplift pressures acting on the dam
Nickname : uplift pressures
Valuename : = *cue*
s s Problem
pee ereener Constraints HYPOTHESIS erosion
Value true
Name : the erosion of soil from under the dam
Nickname : erosion
Solution : Line of Valuename : *hypothesis*
Constraints Explainer Explanation
GOAL det-cause
Name : determine causal relationships
Nickname : determine causes
CUE SCRIPT c erosion-to-sliding
Explanatory Story
Knowledge Teller Uses (<drainage> <uplift> <sliding>)
Supports :  <erosion>
Achieves :  det-cause
Bottoms : (<drainage> <uplift> <sliding>)
Verbalizer Story Veconstraint : (and <drainage> <uplift> <sliding>)
Text :  <erosion> would cause <broken-pipe>
resulting in <drainage> thereby creating
<sliding> and eventually <uplift>
, GOAL SCRIPT causal
Explanation End-User Holds : (<det-cause>)

Text =: (simply <det-cause>)

FIGURE 1 Explanation capability as a problem-solving activity (left) and example of explanatory knowledge (right) (Wick &
Thompson, 1992)

I attempted to find the cause of an excessive load on a concrete dam. Based on the FIGURE 2 Example ofa line of explanation in Rex
broken pipes in the foundation, the sliding of the dam, the uplift pressures, and the (Wick & Thompson, 1992)

slow drainage, I was able to make an initial hypothesis. In studying causal relations,

IT found that the erosion of soil from under the dam would cause broken pipes,

resulting in slow drainage, thereby creating increased uplift pressures and eventually

sliding of the dam downstream. This led me to conclude erosion was the cause of the

excessive load.

explanatory knowledge. The explainer can finally build an explanation line by taking into account the knowledge speci-
fication and the explanatory knowledge.

The explanatory knowledge is a key component of the explanation process (see Figure 1). It models cues, goals, and
hypotheses. Transitions among these elements are modeled through scripts. Scripts are represented using a frame-based
language. The explainer tries to find an explanation “plan” using only transitions whose hypotheses can be proven. The
search of the explanation plan is carried out backward from the final conclusion until reaching the empty hypothesis.
Each state in the explanation plan corresponds to an explanation that uses cues and a hypothesis as data, establishes
other cues and a hypothesis as conclusions, and traverses certain edges in the knowledge specification.

Once an explanation is found, the story-teller organizes it into a consistent flow from data to conclusions. Then, it
presents the explanation as a story according to a grammar that models the memory structure built during human
story-understanding. The basic idea is to extract the information concerning the structure of each hypothesis transition
from the line of explanation. Each transition is formatted as a story-tree with a setting, theme, plot, and resolution. The
story-tree is then converted to textual description by the verbalizer that fills in a template with the problem description,
goal description, movement description, and the conclusion of the expert system. A line of explanation in Rex looks like
the explanation shown in Figure 2.

4 | EXPLANATIONS IN MACHINE LEARNING

While some machine learning models can be considered interpretable by design, namely decision trees, decision rules,
and decision tables,* the majority of machine learning models work as black-boxes. Given an input, a black-box returns

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI - 7 of 21
— gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY. °

the result of a decision task (classification, prediction, recommendation, etc.), but it does not reveal sufficient details
about its internal behavior, resulting in an opaque decision model. For this reason, explainability in machine learning
is formulated as the problem of finding an interpretable model that approximates the black-box model as much as pos-
sible, typically seeking high fidelity.

The literature about explainable or interpretable machine learning is vast. A recent survey on interpretable machine
learning methods and techniques can be found in (Guidotti et al., 2018). There, a classification of explanation models is
proposed:

 

* Global methods: The extraction of an explainable counterpart from a black-box model aims at providing an overall
approximation of the behavior of the black-box, such that all decisions made by the latter can be tracked in terms of
interpretable mechanisms, for example, (Craven & Shaolin, 1995; Frost & Hinton, 2017).

* Local methods: Explanations are built for the decisions made by a black-box model over specific outcomes/instances

of a dataset. In this sense, interpretable local models are considered a local approximation of how the black-box

works. This kind of explanations can vary greatly depending on the instance considered, for example (Kim, Rodin, &

Shah, 2014; Ribeiro, Singh, & Guestrin, 2016b; Ribeiro, Singh, & Guestrin, 2018).

Introspective methods: Explanations are built by relating inputs to outputs of a black-box model. For instance, expla-

nations can consist of saliency masks for Deep Neural Network models in image classification (such as Convolutional

Neural Networks [CNNs]), for example (Hendricks et al., 2016; Park et al., 2016; Same, Wigand, & Miiller, 2019) or

groups of input-output tokens that are causally related, for example, (Alvarez-Melisa & Jackova, 2017).

In the following, we present some global and local explanation methods: PDPs (partial dependence plots), LIME
(local interpretable model-agnostic explanations; Ribeiro et al., 2016b), and SHAP (Shapley Additive explanations;
Lundberg & Lee, 2017). Furthermore, we dedicate a section to counterfactual explanations (Mothilal et al., 2020;
Watcher, Mittelstadt, & Russell, 2018).

4.1 | Global explanations

The goal of extracting explanations via an interpretable global model is to automatically generate general representa-
tions of the black-box model and its relationship to features of the dataset is has been trained on. One possible strategy
is to generate symbolic representations of all decisions made by the complex model and represent it in a directly inter-
pretable way. An example of this is the extraction of decision trees, for example (Craven & Shavlik, 1995; Frosst &
Hinton, 2017), and decision rules from a trained neural network, for example (Odense & Garcez, 2017; Zhou, Jiang, &
Chen, 2003), or the extraction of feature importance vectors, for example, (Lou, Caruana, & Gehrke, 2012; Lou,
Caruana, Gehrke, & Hooker, 2013), from noninterpretable models.* In some other cases, the interpretable model is a
refinement of previous models, which were used to build the black box, such as in the case of Knowledge Neural Net-
works (Towell & Shavlik, 1993).

A different example can be found in PDPs Friedman (2000), which compute the effect of various variables in the
predicted outcome of a machine learning model. This effect can be linear (as in linear regression) or more complex.
PDP works by marginalizing the machine learning model output over the distribution of features so that the function
shows the relationship between the features one is interested in, and the predicted outcome. PDP works well when one
wants to explain two or three features (since it generates 2-D and 3-D plots) and when the features are uncorrelated. In
other cases, Accumulated Local Effect plots are used. They work with the conditional instead of the marginal distribu-
tion (Apley & Zhu, 2016). Figure 3 shows an example of these explanations.

4.2 | Local explanations

In local explanation methods, the individual predictions of a black-box model can be approximated by generating local
surrogate models that are intrinsically interpretable.

This strategy has been implemented for instance in LIME; Ribeiro et al., 2016b). The LIME approach exploits the
fact that the trained black-box model can be queried multiple times about the predictions of particular instances. By
perturbing the data used for training, LIME generates a new dataset after feeding the black-box model with perturbed

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
8 of 21 WI LEY_ @ WIREs CONFALONIERI er at.

DATA MINING AND KNOWLEDGE DISCOVERY

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

5500 $5005
50. 500 500
5000 5000 5000 4
”
o
2
3
3 0 0 Le
& 4500 4500+
5 4500 2
ae“. =
— 50: 500 509
= 4000 4000 5
os
2
2
3
2
a 0 a ) ~
co 2500 2500 | -100: 1000 1000
3000 3000 = +1500 ~1500 ~1500
oO 10 20 30 oO 25 50 75 100 oO 10 20 30 o 10 20 30 oO 25 50 75 100 10 20 30
Temperature Humidity Wind speed Temperature Humidity Wind speed

 

FIGURE 3 | Explanations as partial dependence plots—PDPs (left) and Accumulated Local Effect—ALE (right) showing how
temperature, humidity, and wind speed affect the predicted number of rented bicycles on a given day (Molnar, 2019). Due to correlation
between temperature and humidity, the PDP shows a smaller decrease in predicted number of bikes for high temperature or high humidity
compared to the ALE plots. The example shows that when features of a machine learning model are correlated, PDPs are not very accurate
and cannot be trusted (Apley & Zhu, 2016)

Predicted value negative positive

6.21 < RM <= 6.64
942 EE) 46.57 14

(min) 22.77 (max)

Feature Value

    

 

[('6.21 < RM <= 6.64", -1.4447236495087157),

(“TAX <= 283.25', 0.7417794586320381),

("0.45 < NOX <= 0.54", 0.7290587475071078),
("6.91 < LSTAT <= 11.38', 0.647686973013182),
ise : ('17.00 < PTRATIO <= 19.10', 0.2490023133440792) ]

[TAX <= 283.25
7

PTRATIO

 

FIGURE 4 Local explanation extracted through LIME in the Boston dataset (Harrison & Rubinfeld, 1978). The dataset contains
information collected by the U.S Census Service concerning housing in the area of Boston, Massachusetts. On the left, the median value of
owner-occupied homes in $1000's (the predicted value), is explained using a linear regression model using 5 over 14 features (RM, average
number of rooms per dwelling; TAX, full-value property-tax rate per $10; 000; NOX, nitric oxides concentration; LSTAT, % lower status of
the population; PTRATIO, pupil-teacher ratio by town). On the right, the local explanation in the form of a linear regression using the
mentioned features can be appreciated

data and creates a new interpretable model from the predictions made over the new dataset. The local surrogate model
is weighted by the proximity of the perturbed instances to the original ones such that it has a high local fidelity.

Methods like LIME generate explanations by creating surrogate models that are interpretable and have a low num-
ber of features in order to keep the complexity of the interpretable model low. Figure 4 shows an example of a local
explanation extracted by LIME. In the example, the predicted variable is explained using a linear regression.

However, the sampling method used to train the interpretable model is not applicable to situations in which feature
spaces are high dimensional or when black-box model decision boundaries are complex. In these scenarios, more fea-
tures have to be taken into account in order to increase local fidelity, to the detriment of interpretability. An extension
of the method, which uses rules instead of surrogate models, has recently been proposed by the authors of LIME. The
method, called ANCHOR (Ribeiro et al., 2018), uses the same perturbation space as LIME and constructs explanations
by adapting their coverage to the model structure. In this regard, explanations have a well-defined boundary in terms of
their faithfulness to the black-box model.

4.3 | Counterfactual explanations

A counterfactual explanation provides “what-if” information in terms of which alterations of the input features could
change the output of a predictive model. A counterfactual explanation is then defined as the smallest change to the

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI - 9 of 21
— gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY. °

“world” (as captured by the input data) that can be made to obtain a desired outcome (Wachter, Mittelstadt, &
Russell, 2018); for example, You were denied a loan because your annual income was £30,000. If your income had been
£45,000, you would have been offered a loan. In this scenario, the borrower receives information regarding why the loan
was denied but also will be informed as to what she should do in order to change this outcome.

One of the issues that has to be addressed when generating counterfactuals is that some features might not be
changeable (e.g., a person's gender, race, or birth-place). Hence it becomes crucial to present counterfactuals that are
indeed actionable in the application domain. An actionable example, thus, refers to what can concretely be done next
in order to change the outcome of a given decision. For a counterfactual to be actionable it has to meet four properties:
proximity, obeying user constraints, sparsity, and causal constraints (Mothilal et al., 2020). Furthermore, presenting
users with a set of diversified examples (i.e., a range of suggested actions) can help them shed light on how the system
works, and can ease the adoption of these changes.

Unlike explanation methods that depend on approximating the classifier’s decision boundary (Ribeiro et al., 2016a),
counterfactual explanations have the advantage that they are more human understandable (Mothilal et al., 2020), and
that they are always truthful with respect to the underlying model by giving direct outputs of the algorithm (Wachter,
Mittelstadt, & Russell, 2018). These properties might prove to be particularly useful in the context of explainability and
the GDPR.

An approach to generating counterfactuals was proposed in (Wachter, Mittelstadt, & Russell, 2018). Soon after, the
importance of diversity also in counterfactuals was acknowledged by (Russell, 2019), who correspondingly proposed a
method to generate diversified counterfactuals for linear models. More recently, Mothilal et al. (2020) proposed Diverse
Counterfactual Explanations (DiCE), a novel-model agnostic approach for generating counterfactual examples that are
both actionable and diverse.® An example of counterfactual explanations using DiCE can be seen in Figure 5.

 

5 | EXPLANATIONS IN RECOMMENDER SYSTEMS

Recommender systems make use of a large variety of models as back-end engines to serve customized recommenda-
tions to users. Such models can be based on Collaborative Filtering, which include Matrix Factorisation (MF; Koren,
Bell, & Volinsky, 2009) and all its variants, for example, singular value decomposition (SVD; Nati & Jaakkola, 2003) or
nonnegative matrix factorisation (NMF; Lee & Seung, 1999), Nearest Neighbors, and methods based on embeddings
such as Deep Learning (Wang, He, Feng, Nie, & Chua, 2018) or Knowledge-based Embeddings for Recommendation
(Zhang, Ai, Chen, & Wang, 2018).

Explanations in recommender systems is a popular topic and has received considerable attention in recent years
(Nunes & Jannach, 2017; Tintarev & Masthof, 2015). Most of the corresponding work aims to answer the question of
why a particular recommendation has been served. This answer can take into account many different aspects used by
the recommendation algorithm, such as past interactions characteristics, or contextual information, for example, loca-
tion of the user, his or her social context, or the time the recommendation is provided.

Query instance (original outcome : @)

age workclass education marital_status occupation race gender hours_per_week income

 

Q 22.0 Private HS-grad Single Service White Female 45.0 0.009411

Diverse Counterfactual set (new outcome : 1)

 

age workclass education marital_status occupation race gender hours_per_week income
0 57.0 Private Doctorate Single White-Collar White Female 45.0 0.724
1 36.0 Private Prof-school Married Service White Female 37.0 0.869
2 22.0 Self-Employed Doctorate Married Service White Female 45.0 0.755
3 43.0 Private HS-grad Married White-Collar White Female 63.0 0.822

FIGURE 5_ Example of counterfactual explanations with DiCE (Mothilal et al., 2020). In this example, a neural network was trained to
predict the income of a person based on the above eight features (age, work-class, etc.). The first table represents the original query, where
the model computed a negative outcome. The second table represents the counterfactual examples

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
10 of 21 WI LEY_— G) WIREs CONFALONIERT er at.

DATA MINING AND KNOWLEDGE DISCOVERY

Explainable recommendation systems can be broadly classified into two categories: model-based and post-hoc. The
first tackle the mechanistic part of the recommendation, aiming at explaining the way the algorithm proposes a particu-
lar recommended item, while the latter analyze the output of a trained recommender in order to infer an explanation
for all (recent) recommendations served.

The following subsections provide a brief overview on current state-of-the-art explainable recommender models,
and present some forms of explanations meant to increase persuasiveness, effectiveness, efficiency, user satisfaction,
and efficiency in platforms that serve recommendations.

5.1 | Explainable recommender system models

One of the most widespread methods which recommendation engines are based on is MF. Factorisation models rely on
latent representations of users and items so as to predict either the item(s) with the highest chance to be interacted
with, or the rating of an item given by a user. Problems arise when trying to explain the latent factors that contribute to
the prediction: the exact meaning of each factor is generally unknown and therefore more information about user inter-
ests and item characteristics is required. Explicit factor models (EFM; Zhang, 2015) take into account information pro-
vided by the user about features of items that she might be interested in (through reviews and explicit feedback) and
map them to the latent factors used in the (matrix or tensor) factorization part. Tensor factorization is an extension of
EFM's, where the cube user-item-features is used to predict ratings with embedded explanations in terms of features
(X. Chen, Qin, Zhang, & Xu, 2016). Implicit feedback for explaining a recommendation has also been proposed by
means of neighborhood-based explanations: in these models a recommended item comes with an explanation of the
style “x% similar users viewed this item,” which can be extracted thanks to an explainability regularizer that forces user
and item latent vectors to be close if x% of users have interacted with the same item.

There are other approaches to explaining recommendations, which are based on the use of external knowledge of
items in order to provide personalized explanations on new recommendations. Knowledge-based explanations for rec-
ommender systems (Catherine, Mazaitis, Eskénazi, & Cohen, 2017) make use of knowledge graphs that relate item
properties and users’ behavior in terms of their past interactions with items. With such graphs, different paths can con-
nect a particular user to a particular item (i.e., the graph relationships) in the form of links (either views, purchases, or
category), the building blocks of the provided explanations. On a similar note, if user-item relationships are represented
as graphs, graph theory can provide insights about how users behave in terms of their interests on different items. For
instance, Heckel and Vlachos (2016) proposed a method to compute coclustering to find similar users in terms of their
interests and similar items in terms of their properties using an user-item bipartite graph. Explanations can then be
retrieved by using shared information between users, considering the purchase/interaction behavior of similar users on
recommended items as the core of the explanation.

A different approach to explaining the performance of a recommender engine is to consider it as a black-box that
can be probed so as to extract statistical features of recommendations (Peake & Wang, 2018). Explanations can then
highlight what percentage of the users have behaved similarly and therefore can provide the confidence on the recom-
mendation to be effective. Besides, the black-box can be approximated by an interpretable version of the recommender
engine, for example, association rules or similarity-based models, that can preserve high accuracy while being intrinsi-
cally interpretable (Singh & Anand, 2018).

Finally, there recently has been a surge in the number of deep learning-based recommender models deployed in rec-
ommender engines (He et al., 2017). Many deep learning techniques, such as CNNs; Seo, Huang, Yang, & Liu, 2017) or
Recurrent Neural Networks and Long-Short Term Memory networks (RNN-LSTM) (Hidasi & Karatzoglou, 2018) are used
to implement different recommendation strategies, such as sequential recommendations (LSTMs) or context-aware rec-
ommendations using user reviews. Attention-based methods are used to highlight the importance of words used in user
reviews of past interactions in order to provide explanations about new recommendations. These algorithms use natural
language generation in the explanations that can also take into account visual features of the items of interest.

5.2 | Explanation styles in recommender systems

Herlocker, Konstan, and Riedl (2000) compared a large number of different styles of explanations and found that rating
histograms generally were users’ preferred mechanism for rendering the data behind the recommendations transparent.

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI . ja 11 of 21
— GD WIRES yew WILEY 2!

DATA MINING AND KNOWLEDGE DISC

Supporting these results, these visual explanations of user styles have proven to be popular in many studies ever since
(Bilgic & Mooney, 2005; Cosley, Lam, Albert, Konstan, & Riedl, 2003). Recently, a study using the visual rating histo-
gram paradigm specifically identified user-based explanations and high mean rating values as the most popular styles
(Kouki, Schaffer, Pujara, O'Donovan, & Getoor, 2017).

Friedrich and Zanker (2011) proposed a taxonomy to classify different approaches to generate explanations for rec-
ommendations. Among the types of explanations in their taxonomy, there are collaborative explanations. These are
explanations that justify recommendations based on the amount as well as the concrete values of ratings that derive
from similar users, where similarity is typically determined based on similar behavior and preference expressions dur-
ing past interactions.

The explanation taxonomy proposed by (Papadimitriou, Symeonidis, & Manolopoulos, 2012) extends this classifica-
tion by making a distinction based on the three fundamental concepts used for explaining recommendations, which are
users, items, and item features. They can be used to denote the following explanation styles:

* User Style, which provides explanations based on similar users,
» Item Style, which is based on choices made by users on similar items, and
» Feature Style, which explains the recommendation based on item features (content).

Please note, that any combination of the aforementioned styles is then categorized as a multi-dimensional hybrid
explanation style.

For the User Style, several collaborative filtering recommender systems, such as the one used by Amazon in their
online stores, adopted the following style of justification: “Customers who bought item X also bought items Y, Z, ....”
This is called User style (Bilgic & Mooney, 2005) as it is based on users performing similar actions like buying or rating
items (see also Figure 6). Regarding the Item style of explanation, justifications are of the form: “Item Y is rec-
ommended because you highly rated or bought item X, Z, ....” Thus, the system depicts those items that is, X, Z, ..., that
mostly influenced the recommendation of item Y. Bilgic and Mooney (2005) claimed that the Item style is preferable
over the User style, because it allows users to accurately formulate their true opinion about an item. In case of Feature
style explanations, the description of items is exploited to determine a match between a current recommended item
and observed user interests. For instance, restaurants may be described by features such as location, cuisine, and cost.
If a user has demonstrated a preference for Chinese cuisine and Chinese restaurants are recommended, then explana-
tions will note the Chinese cuisine or the restaurants’ cost aspects. As part of the work in Coba, Zanker, Rook, &
Symeonidis (2018), the authors tested users’ preference for different explanation styles in a study. They found that User
Style explanations were the most preferred. In later studies, they also provided evidence that perception of explanations
relates to personality characteristics, and they proposed model-based approaches to further personalize explanations
(Coba, Rook, et al., 2019; Coba, Symeonidis, et al., 2019).

6 | EXPLANATIONS IN NEURAL-SYMBOLIC LEARNING AND REASONING

Neural-Symbolic Learning and Reasoning seeks to integrate principles from neural network learning with logical rea-
soning (Garcez et al., 2015). Although neural networks and symbolic systems are frequently painted as two irreconcil-
able paradigms, the differences actually are more subtle and less fundamental than frequently presumed.

Symbolic systems operate on the symbolic level where reasoning is performed over abstract, discrete entities follow-
ing logical rules. A common goal of work on symbolic systems is to model (certain aspects of) common-sense reasoning,
for example, the kind of reasoning humans do in their everyday lives, which is considered to automatically allow for

 

 

Rating Number of Neighbours

a 0

as as 0

oe ark 0
FIGURE 6 Example of an explanation interface visualizing a User style st stestigylty 10
explanation using the explainability power of nearest neighbors for a target user (Coba, stestertesterts 23

 

Symeonidis, et al., 2019)

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
12 of 21 WI LEY_— G) WIREs CONFALONIERT er at.

DATA MINING AND KNOWLEDGE DISCOVERY

better explainability. Neural networks, on the other hand, operate in the sub-symbolic (or connectionist) level. Individ-
ual neurons do not necessarily represent a readily recognizable concept, or any discrete concept at all. Instead, they
often model statistical regularities present in the training dataset, imbuing the system with statistical predictive capabil-
ities rather than allowing it to perform sound abstract reasoning. As discussed by Besold, Garcez, Bader, et al. (2017),
the integration between both levels could, therefore, bridge low-level information processing such as frequently
encountered in perception and pattern recognition with reasoning and explanation on a higher, more cognitive level of
abstraction.

Achieving this integration promises a range of benefits such as representations, which are abstract, reusable, and
general-purpose. Having these readily available could directly allow to tackle some of the pressing issues with current
deep learning practices. While the data efficiency and sample complexity of deep learning systems tend to be very com-
putationally demanding and data-heavy, symbolic approaches are less difficult in that aspect. Furthermore, deep learn-
ing approaches often do not generalize well out of the sample distribution and prove to be a limited foundation for
transfer learning, whereas symbolic representation can help to overcome these limitations. Last and most importantly
in this context, deep learning systems lack transparency while symbolic approaches can be designed in such a way as to
follow a humanly comprehensible decision-making process (see, e.g., Garcez et al., 2019; Muggleton, Schmid, Zeller,
Tamaddoni-Nezhad, & Besold, 2018).

6.1 | The neural-symbolic integration cycle

Figure 7 illustrates the general idea underlying neural-symbolic approaches. On one side, there is a symbolic system,
both writable and readable by human experts. On the other side, we have a neural network capable of taking full
advantage of connectionist training methods. The iterative loop between both sides allows for the embedding of sym-
bolic (expert) knowledge into the sub-symbolic model as well as for the extraction of learned and refined knowledge
from the connectionist model, which can drive the data-based modification and fine-tuning of predefined rules
(see e.g., Besold, Garcez, Stenning, et al., 2017).

This cycle already hints at the four main pillars of neural-symbolic systems: representation, extraction, reasoning,
and learning. Knowledge representation provides the mapping between the integrated symbolism and connectionism.
The different forms of representations can be divided into rule-based, formula-based, and embeddings. As previously
mentioned, the aim is to extract symbolic knowledge given a trained neural network for explaining and reasoning aims.
There have also been efforts at integrating neural-symbolic systems into the immediate process of learning. Inductive
Logic Programming (ILP), for example, develops a logic program directly from examples (Franca, Zaverucha, &
Garcez, 2014). In addition to this, learning with logical constraints generally has shown to be beneficial for improving
the data efficiency (Garnelo & Shanahan, 2019). These constraints can, for example, be integrated as a logic network
module on top of a regular neural network. As a consequence, models can further learn relations in-between the inner
abstractions as well as guiding the model to explain its prediction. Reasoning is another essential goal of neural-
symbolic systems. Successful integration aim to perform symbolic reasoning on the knowledge learned during the train-
ing phase (Garcez et al., 2001).

Complementing Deep Learning systems by integrating symbolic representations such as Knowledge Graphs can
serve as a lingua franca between humans and AI systems. Sarker, Xie, Doran, Raymer, and Hitzler (2017) propose that
methods for explanations should be seen as interactive systems. The authors present a method that enables active

Neural Network

 
 
  

   

Data
Background

knowledge

OUR |
Consolidation enteritis

Trained Network

FIGURE 7 _ Illustration of the
neural-symbolic cycle

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI . ja 13 of 21
— GD WIRES ow -WILEY_L 2!

DATA MINING AND KNOWLEDGE DISC

monitoring for classifiers where humans can act on given explanations. These interactive approaches become increas-
ingly relevant as they provide not only extracted information, but also the ability to act on this information. Against
that backdrop symbolic representation are taken to be vital as enablers of human-intelligible explanations. Similarly,
Futia and Vetrd (2020) state that hybrid methods will further allow for explanations targeted at nonexperts based on
querying and reasoning mechanisms, which are at the core of the integrated semantic components.

6.2 | Explanations via knowledge extraction

Staying close to the neural-symbolic cycle, most traditional approaches to explainability in neural-symbolic systems
aim to generate a set of symbolic rules that approximates the behavior of a sub-symbolic model.

The task of generating—usually by via some form of learning—these rules is known as knowledge extraction
(Towell & Shavlik, 1993). The extraction process seeks to optimize for different metrics and criteria, namely, accuracy,
fidelity, consistency, and comprehensibility. On the one hand, accuracy (i.e., a measure for the performance of the rules
on the original test sets) and fidelity metrics (i.e., a measure for the ability of the rules to replicate the behavior of the
original sub-symbolic model) relate to performance dimensions of the extracted interpretable model. On the other
hand, consistency and comprehensibility are related to the consumer of the rules: rules should be precisely representing
the underlying model, but should also be easy to understand and use. This usually requires a trade-off between consis-
tency and comprehensibility.

The extracted rules can then be used to revise and consolidate available background knowledge (often taking the
form of domain knowledge). This background knowledge can be used not only to provide meaningful semantics for the
explanations—facilitating, in this way, human-machine interactions—but can also be injected back into the sub-
symbolic model itself in order to improve its performance (Ziegler et al., 2017).

Returning to the task of knowledge extraction, two main approaches are commonly considered: one of the decom-
positional, the other one pedagogical. Algorithms falling of the first type extract rules directly from the structure and
weights of the sub-symbolic model. This is usually achieved by first extracting rules that approximate the behavior of
each connectionist unit. Then, these unit-level rules are aggregated to form the composite rule base of the neural net-
work as a whole (Andrews et al., 1995). To extract rules in such a way, these methods need access to the internal layers
of the sub-symbolic model. Often this access cannot be obtained (e.g., due to intellectual property considerations), but
one might still need and want to be able to extract explanations. Algorithms belonging to the class of pedagogical
approaches overcome this limitation. They treat the sub-symbolic model as an “oracle,” and extract information from
input-output pairings. A prime example for a pedagogical approach to knowledge extraction is Trepan (Craven &
Shavlik, 1995).

Trepan is a tree induction algorithm that recursively extracts decision trees from statistical classifiers, originally
intended in particular for use with feed-forward neural networks (but as the original classifier is treated like a
generic oracle within the algorithm, Trepan can be considered in principle agnostic to the type of sub-symbolic
model at hand). Craven and Shavlik (1995)s approach can be seen as an extension of the ID2-of-3 algorithm
(Murphy & Pazzani, 1991), a method for building decision trees from data based on “m-of-n” rules—that is, m out
of n specified conditions must be true to send an example down a particular branch. These tests are usually built by
a greedy search algorithm that starts from the single feature that maximizes information gain, and iteratively adds
features to the test until information gain is no longer improved by doing so. Trepan combines this with the idea of
using a trained machine learning classifier as oracle, in its original version targeting multi-layer perceptrons (MLPs).
At each splitting step, the oracle’s predicted labels are used instead of the known real labels from the input dataset.
Figure 8 shows an example of a Trepan tree extracted from a trained MLP. The use of the classifier as oracle serves
two purposes: first, it helps to avoid overfitting to outliers in the training data. Second, and more importantly, it
helps to build deeper trees.

While Trepan extracts trees from sub-symbolic models by approximating the models to an arbitrarily close degree
without having direct access to their architecture and units, there is still the problem of assessing to what extent the
extracted trees are human-understandable. Recent work measured human understandability of decision trees using
syntactic and cognitive metrics (Huysmans et al., 2011; Piltaver, Lustrek, Gams, & Martinéi¢-Ipsic, 2016). Building on
these, Confalonieri, Weyde, et al. (2020) also showed how human understandability of surrogate decision trees can be
enhanced by using and integrating domain knowledge, for example, in the form of ontologies, in the decision tree
extraction.

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
14 of 21 WI LEY_— @ WIREs CONFALONIERT er at.

DATA MINING AND KNOWLEDGE DISCOVERY

 

FIGURE 8 | Trepan tree extracted from a trained
neural network predicting diabetes risk based on the
Glucose >= 129.993790 Pima Indians dataset (Smith, Everhart, Dickson,

BloodPressure 0.40277 Knowler, & Johannes, 1988)
Pregnancies >= 6.377451

alse Ne:
1 of

: : Insulin >= 25.884817
ao Low T2 Diab Risk > NS ee |

1 of

 

 

 

 

 

 

 

BMI < 0.189945

fo alse

 

 

 

 

2 of

SkinThickness < 21.272927 High T2 Diab Risk
DiabetesPedigreeFunction < 0.875887
False

Low T2 Diab Risk High T2 Diab Risk

7 | DISCUSSION

 

 

 

      

The historical overview provided in this article hints at a categorization of explanations. This categorization relies on
the reasoning characteristics of the underlying decision system, namely, symbolic, sub-symbolic, and hybrid.

Expert systems were one of the first realizations of applied AI, where the aim was to build systems able to aid
humans in decision-making activities in very specific domains. Making these systems operative required a knowledge
acquisition effort in which domain knowledge had to be formally specified. This knowledge formalization was essential
to develop intelligent systems able to reason, draw new conclusions, and to generate explanations. Explanations in
these systems consisted of either descriptions coupled to the reasoning trace of the system, or descriptions decoupled
from the reasoning itself, but more focused on the story behind the decision-making process itself. Since knowledge in
expert systems in most cases aimed at modeling (some aspects of) common-sense reasoning, explanations generated by
these systems were usually human-understandable. Nonetheless, acquiring and modeling domain knowledge is a com-
plex task, and it is subject to human interpretation and the point of view that the modeler decides to capture.

Machine learning was introduced to alleviate this knowledge acquisition problem. Machine learning algorithms are
indeed capable of identifying data patterns from (in most cases) large amounts of data, but this often happens at the
price of creating black-box models. An explanation in these systems is mainly understood as an interpretable model
that approximates the behavior of the underlying black-box. Explanations of this type allow users to understand why a
certain conclusion or recommendation is made, by means of local, global, introspective, or counterfactual explanations.
Whereas these explanations seek to maximize metrics such as accuracy (i.e., the performance of the extracted interpret-
able model on the test sets), fidelity (i.e., the ability of the extracted interpretable model to replicate the behavior of the
black-box model), they also have to be understandable by human users. Clearly, accuracy and understandability often
compete with each other, and a reasonable trade-off must be found. For instance, a very technical and precise explana-
tion (e.g., in equation form) may be appropriate for a data scientist, but not for a lay person, who prefers possibly a less
accurate but more comprehensible representation format of the explanation.

Most explainability methods nowadays are not powerful enough to give guarantees about truthfulness and closeness
of the explanation with respect to the underlying model. Most metrics currently in place are lacking a reliable way of
expressing this uncertainty. For instance, the measured fidelity is supposed to be a satisfactory proxy of closeness of the
representation to the underlying model. However, this metric is limited in its capacity and capability to find

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI . ja 15 of 21
— GD WIRES ow -WILEY_L 82!

DATA MINING AND KNOWLEDGE DISC

semantically meaningful representations that allow for transparent reasoning, as it is solely optimizing for resemblance
of the explained model.

Aspects of understandability of explanations for lay users has for a long time been overlooked. As also pointed
out in (Bhatt et al., 2020), the majority of deployments do not focus on the end-users, who are affected by the
model, but rather on machine learning engineers, who use explainability to debug the model itself. In practice, there
is a gap between explainability and the goal of transparency, since explanations primarily serve “internal” stake-
holders rather than “external” ones. To bridge this gap, explanations need to be human-understandable and adapt-
able to different stakeholders (Ribera & Lapedriza, 2019). Trustworthy systems need to target explanations for
different types of user, taking into account their different goals, and providing relevant and selected (customized)
information to them. This requires an approach to explainable AI that starts from a user-centered perspective.
Related to this, guidelines behind Responsible AI establishing that fairness, accountability and privacy (especially
related to data fusion) should be considered when implementing AI models in real environments have been dis-
cussed in (Arrieta et al., 2020).

Finally, while explainability has been addressed in some form or another since the mid-1980, its general under-
standing and definition(s) are still under discussion. In particular, proposing a set of global desiderata for explanations
appears to be challenging, since these properties often depend on the application domain. Notwithstanding, we con-
clude our discussion by pointing out some desiderata that, we believe, should be taken into account for the develop-
ment of XAI systems, particularly putting the user at the heart of the entire explainability enterprise:

* Causal: Knowing what relationship there is between input and output, or between input features can foster human-
understandable explanations. However, causal explanations are largely lacking in the machine learning literature,
with only few exceptions such as (Chattopadhyay, Manupriya, Sarkar, & Balasubramanian, 2019). A related problem
is then how to measure the causal understanding of an explanation (causability) (Holzinger, Langs, et al., 2019).
While this is always possible for explanations of human statements, as the explanation is per-se related to a human
model, measuring the causal understanding of an explanation of a machine statement has to be based on a causal
model, which is not the case for most machine learning algorithms (Holzinger et al., 2020).

* Counterfactual: Reviewed empirical evidence indicates that humans psychologically prefer counterfactual or contras-
tive explanations (Miller, 2019). For instance, people do not ask why event P happened, but rather why event P hap-
pened instead of some event Q. It is thus important to provide explanations that are both contrastive and direct.
Some preliminary steps have been taken in this direction, for example, (Mothilal et al., 2020). Issues related to the
diversity and proximity of counterfactuals arise in designing counterfactual explanations.

» Social: Interactive transfer of knowledge is required in which information is tailored according to the recipient's
background and level of expertise. Explanations can be conceived of as involving one or more explainers and
explainees engaging in information transfer through dialogue, visual representation, or other means (Hilton, 1990).
Conversational or argumentative processes can enhance user's inspection of explanations, and increasing user's trust
in the system.
Selective: Explanations do not always need to be complex representations of the real world. They should be epistemi-
cally relevant for the explainee. The informational content of explanations has to be selected according to the user's
background and needs, as humans do not expect the complete cause of an event. Clearly, this depends on the stake-
holders’ profiles. For instance, explaining a medical diagnosis to a doctor requires a level of technicality, which, pre-
sumably, is not necessary for most lay users.
* Transparent: Explanations should help the explainee in understanding the underlying logic of the decision system,
and possibly identifying that the system is wrong. Nonetheless, explanations can sometimes be used to learn about
the model or the training data. Therefore, a trade-off between transparency and privacy must be found when generat-
ing explanations. Generally, methods to address these concerns will have to be developed for training a differentially
private model that is able generate local and global explanations. Harder, Bauer, and Park (2020) is an example of
methods of this kind.

Semantic: If explanations are symbolically grounded—by means of ontologies, conceptual networks, or knowledge

graphs—they can support common-sense reasoning. Formal representation and reasoning can in turn enact various

forms of knowledge manipulation, such as abstraction and refinement (Confalonieri et al., 2018; Confalonieri,

Galliani, et al., 2020; Keet, 2007; Lehmann & Hitzler, 2010; Troquard et al., 2018). These forms of manipulation can

play an important role when one wants to develop a system able to provide personalized explanations matching dif-

ferent stakeholder profiles.

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
16 of 21 WI LEY_— G) WIREs CONFALONIERT er at.

DATA MINING AND KNOWLEDGE DISCOVERY

* Interactive: Explanations should be interactive, allowing the explainee to revise and consolidate some previous back-
ground knowledge. The background knowledge can be used not only to provide meaningful semantics for the expla-
nations, facilitating, in this way, human-machine knowledge interactions, but also injected back to the underlying
model to improve its performances (e.g., Kulesza, Burnett, Wong, & Stumpf, 2015).

More generally, if one cares about finding ways of successful communication between humans and AI systems, esta-
blishing a common ground of inherent logic from the ground up appears reasonable. This common ground can be facilitated
by the modularity that integrates perception at the sub-symbolic level and reasoning at the symbolic level. Recent advance-
ments in AI demonstrate robust solutions for many perception tasks. However, to enforce some understanding of the model
at a fundamental level, logical integration using symbolic representations will play an important role in the future.

8 | CONCLUSION

We reviewed the literature on explainability in AI, and provided a historical overview of how the notion of explanation
has been conceived from traditional to more recent perspectives, namely in the context of expert systems, of machine
learning, of recommender systems, and of neural-symbolic learning and reasoning.

The main goal of this article was not to provide a comprehensive review of the literature on XAI, which can be
found in, for example, (Andrews et al., 1995; Arrieta et al., 2020; Fernandez et al., 2019; Guidotti et al., 2018; Mueller
et al., 2019; Nunes & Jannach, 2017; Tintarev & Masthof, 2015). We aimed, instead, at describing different notions of
explanations, examples thereof, as well as properties, and metrics used to evaluate explanations. The article, thus, con-
tains a wide range of references that the reader can use to “navigate” through different notions of explanations, and
gain a deeper understanding of the topic of explainable AI.

In providing this historical overview, we analyzed the different notions of explanation to understand what makes
for a good explanation. While we are unable to provide a single answer, one conclusion that can be drawn is that for
explanations to be human-understandable, they need to be user-centric explanations. To this end, we proposed some
desiderata for explanations, that, in our opinion, are crucial for the development of human-understandable explana-
tions, and, in general, of explainable intelligent systems.

ACKNOWLEDGMENTS

The authors want to thank Daniel Malagarriga for many valuable discussions regarding topics covered in this article. A
significant part of the work has been carried out at Alpha Health, Telefénica Innovacién Alpha, Barcelona, Spain. The
authors thank the Department of Innovation, Research and University of the Autonomous Province of Bozen/Bolzano
for covering the Open Access publication costs.

CONFLICT OF INTEREST
The authors have declared no conflict of interest for this article.

AUTHOR CONTRIBUTIONS

Roberto Confalonieri: Conceptualization; investigation; project administration; writing-original draft; writing-review
and editing. Ludovik Coba: Investigation; writing-original draft; writing-review and editing. Benedikt Wagner:
Investigation; writing-original draft; writing-review and editing. Tarek Richard Besold: Project administration;
writing-review and editing.

ORCID
Roberto Confalonieri © https://orcid.org/0000-0003-0936-2123
Tarek R. Besold © https://orcid.org/0000-0002-8002-0049

ENDNOTES

' The right to explanation refers to the right of end-users and, more generally, service consumers, to ask for explana-
tions of why a certain decision was reached by an AI system, such as in the case of loan allowance by a bank, recom-
mendations, and medical diagnosis. For a different point of view on this, please refer to Wachter, Mittelstadt, and
Floridi (2017).

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI - 17 of 21
— gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY. °

? Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing of personal data and on
the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) [2016] OJ
1119/1.

3 The categorization of explanations based on the type of system (sub-symbolic, symbolic, and hybrid) also relates to
Michie (1988)'s criteria for machine learning: weak, strong and ultra-strong. Michie’s aim was to provide operational
criteria for various qualities of machine learning that include not only predictive performance but also comprehensi-
bility of learned knowledge. His weak criterion identifies the case in which the machine learner produces improved
predictive performance with increasing amounts of data. The strong criterion additionally requires the learning sys-
tem to provide its hypotheses in symbolic form. Last, the ultra-strong criterion extends the strong criterion by requir-
ing the learner to teach the hypothesis to a human, whose performance is consequently increased to a level beyond
that of the human studying the training data alone.

 

* A different problem is then to decide how much these models are human understandable; see for example Huysmans,
Dejaeger, Mues, Vanthienen, and Baesens (2011) for a comparison of the comprehensibility of decision tables, trees,
and rules.

° In Section 6, we will have a closer look at Trepan (Craven & Shavlik, 1995) as a concrete example. Trepan is a global
explanation method that extracts decision trees from neural networks. The discussion has been relegated to Section 6
as Trepan can also be considered a neural-symbolic approach.

° Here, novelty and diversity are concepts that relate to (serendipitous) information discovery which have been studied,
among others, in the fields of information search and recommender systems (Clarke et al., 2008; Vargas &
Castells, 2011).

RELATED WIREs ARTICLES
Causability and explainability of artificial intelligence in medicine

 

REFERENCES

Alvarez-Melis, D., & Jaakkola, T. S. (2017). A causal framework for explaining the predictions of black-box sequence-to-sequence models.
CoRR, abs/1707.01943.

Andrews, R., Diederich, J., & Tickle, A. B. (1995). Survey and critique of techniques for extracting rules from trained artificial neural net-
works. Knowledge-Based Systems, 8(6), 373-389.

Apley, D. W., & Zhu, J. (2016). Visualizing the effects of predictor variables in black box supervised learning models. CoRR, abs/1612.08468.

Arrieta, A. B., Rodriguez, N. D., Ser, J. D., Bennetot, A., Tabik, S., Barbado, A., ... Herrera, F. (2020). Explainable artificial intelligence (XAD:
Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82-115 Retrieved from https://doi.
org/10.1016/j.inffus.2019.12.012

Balog, K., & Radlinski, F. (2020). Measuring Recommendation Explanation Quality: The Conflicting Goals of Explanations. Proceedings of the
43rd International ACM Sigir Conference on Research and Development in Information Retrieval.

Besold, T. R., Garcez, A. d., Stenning, K., van der Torre, L., & van Lambalgen, M. (2017). Reasoning in non-probabilistic uncertainty: Logic
programming and neural-symbolic computing as examples. Minds and Machines, 27(1), 37-77.

Besold, T. R., Garcez, A. S., Bader, S., Bowman, H., Domingos, P. M., Hitzler, P., .. . Zaverucha, G. (2017). Neural-symbolic learning and rea-
soning: A survey and interpretation. CoRR, abs/1711.03902.

Bhatt, U., Xiang, A., Sharma, S., Weller, A., Taly, A., Jia, Y.,... Eckersley, P. (2020). Explainable Machine Learning in Deployment. Proceed-
ings of the 2020 Conference on Fairness, Accountability, and Transparency. pp. 648-657. Association for Computing Machinery:
New York, NY. Retrieved from https://doi.org/10.1145/3351095.3375624

Bilgic, M., & Mooney, R. J. (2005). Explaining Recommendations: Satisfaction vs. Promotion. Proceedings of Beyond Personalization 2005: A
Workshop on the Next Stage of Recommender Systems Research at the 2005 International Conference on Intelligent user Interfaces.
pp. 13-18.

Buchanan, B. G., & Shortliffe, E. H. (1984). Rule based expert systems: The MYCIN experiments of the Stanford heuristic programming project,
Boston: Addison-Wesley Longman Publishing Co., Inc.

Catherine, R., Mazaitis, K., Eskénazi, M., & Cohen, W. W. (2017). Explainable Entity-based Recommendations with Knowledge Graphs. Pro-
ceedings of the Poster Track of the 11th ACM Conference on Recommender Systems (RecSys 2017).

Chattopadhyay, A., Manupriya, P., Sarkar, A., & Balasubramanian, V. N. (2019). Neural Network Attributions: A Causal Perspective. In
K. Chaudhuri and R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning. Vol. 97, pp. 981-990.
Long Beach, CA: PMLR. Retrieved from http://proceedings.mlr. press/v97/chattopadhyay19a.html

Chen, L., & Pu, P. (2005). Trust Building in Recommender Agents. Proceedings of the Workshop on Web Personalization, Recommender Sys-
tems and Intelligent User Interfaces at the 2nd International Conference on e-Business and Telecommunication Networks. pp. 135-145.

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
18 of 21 WI LEY_— G) WIREs CONFALONIERT er at.

DATA MINING AND KNOWLEDGE DISCOVERY

Chen, X., Qin, Z., Zhang, Y., & Xu, T. (2016). Learning to Rank Features for Recommendation Over Muitiple Categories. Proceedings of the
39th International ACM Sigir Conference on Research and Development in Information Retrieval. pp. 305-314. ACM: New York, NY.

Clarke, C. L. A., Kolla, M., Cormack, G. V., Vechtomova, O., Ashkan, A., Biittcher, S., & MacKinnon, I. (2008). Novelty and Diversity in Infor-
mation Retrieval Evaluation. Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval—SIGIR 08, 659. doi: https://doi.org/10.1145/1390334.1390446

Coba, L., Rook, L., Zanker, M., & Symeonidis, P. (2019). Decision Making Strategies Differ in the Presence of Collaborative Explanations. Pro-
ceedings of the 24th International Conference on Intelligent User Interfaces—IUI19. pp. 291-302. New York, NY: ACM Press. doi:
https://doi.org/10.1145/3301275.3302304

Coba, L., Symeonidis, P., & Zanker, M. (2019). Personalised novel and explainable matrix factorisation. Data Knowledge Engineering, 122,
142-158.

Coba, L., Zanker, M., Rook, L., & Symeonidis, P. (2018). Exploring Users' Perception of Collaborative Explanation Styles. 2018 IEEE 20th Con-
ference on Business Informatics (CBI), pp. 70-78. Retrieved from http://arxiv.org/abs/1805.00977. doi: https://doi.org/10.1109/CBI.2018.
00017

Confalonieri, R., Besold, T. R., Weyde, T., Creel, K., Lombrozo, T., Mueller, S. T., & Shafto, P. (2019). What Makes a Good Explanation? Cog-
nitive Dimensions of Explaining Intelligent Machines. In A. K. Goel, C. M. Seifert, & C. Freksa (Eds.), Proceedings of the 41th Annual
Meeting of the Cognitive Science Society, CogSci 2019: Creativity + Cognition + Computation. pp. 25-26. Montreal, Canada. cog-
nitivesciencesociety.org. Retrieved from https://mindmodeling.org/cogsci2019/papers/0013/index.html

Confalonieri, R., Eppe, M., Schorlemmer, M., Kutz, O., & Pen“aloza, R., & Plaza, E. (2018). Upward refinement operators for conceptual
blending in the description logic €¥**. Annals of Mathematics and Artificial Intelligence, 82(1), 69-99.

Confalonieri, R., Galliani, P., Kutz, O., Porello, D., Righetti, G., & Troquard, N. (2020). Towards Even More Irresistible Axiom Weakening.
S. Borgwardt & T. Meyer (Eds.). Proceedings of the 33rd International Workshop on Description Logics (DL 2020) Colocated with the
17th International Conference on Principles of Knowledge Representation and Reasoning (KR 2020). Vol. 2663. Rhodes, Greece. CEUR-
WS.org. Retrieved from http://ceur-ws.org/Vol-2663/paper-8.pdf

Confalonieri, R., Weyde, T., Besold, T. R., & del Prado Martin, F. M. (2020). Trepan Reloaded: A Knowledge-driven Approach to Explaining
Black-box Models. Proceedings of the 24th European Conference on Artificial Intelligence. Vol. 325, pp. 2457-2464). IOS Press. doi:
https://doi.org/10.3233/FAIA200378

Cosley, D., Lam, S. K., Albert, I, Konstan, J. A., & Riedl, J. (2003). Is Seeing Believing? How Recommender System Interfaces Affect Users' Opin-
ions. Proceedings of the Conference on Human Factors in Computing Systems (CHI'03). Vol. 5, pp. 585-592.

Craven, M. W., & Shavlik, J. W. (1995). Extracting tree-structured representations of trained networks. In Neural Information Processing Sys-
tems (pp. 24-30). Cambridge, MA: MIT Press.

Doran, D., Schulz, S., & Besold, T. R. (2017). What Does Explainable AI Really Mean? A New Conceptualization of Perspectives. Proceedings of
the 1st International Workshop on Comprehensibility and Explanation in AI and ML Colocated with AI*IA 2017 (Vol. 2071). Available
from CEUR-WS.org.

Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. CoRR, abs/1702.08608.

Doyle, D., Tsymbal, A., & Cunningham, P. (2003). .A review of explanation and explanation in case-based reasoning (Technical Report). Dub-
lin: Trinity College Dublin, Department of Computer Science.

Fernandez, A., Herrera, F., Cordon, O., Jose del Jesus, M., & Marcelloni, F. (2019). Evolutionary fuzzy Systems for Explainable Artificial
Intelligence: Why, when, what for, and where to? Computational Intelligence Magazine, 14(1), 69-81. https://doi.org/10.1109/MCI.2018.
2881645

Franga, M. V., Zaverucha, G., & Garcez, A. S. (2014). Fast relational learning using bottom clause Propositionalization with artificial neural
networks. Machine Learning, 94(1), 81-104.

Friedman, J. H. (2000). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189-1232.

Friedrich, G., & Zanker, M. (2011). A taxonomy for generating explanations in recommender systems. AI Magazine, 32(3), 90. Retrieved from
https://aaai.org/ojs/index.php/aimagazine/article/view/2365. https://doi.org/10.1609/aimag.v32i3.2365

Frosst, N., & Hinton, G. E. (2017). Distilling a Neural Network Into a Soft Decision Tree. Proceedings of the First International Workshop on
Comprehensibility and Explanation in AI and ML 2017 colocated with 16th International Conference of the Italian Association for Arti-
ficial Intelligence (AI*IA 2017). CEUR Workshop Proceedings. Vol. 2071.

Futia, G., & Vetrd, A. (2020). On the integration of knowledge graphs into deep learning models for a more comprehensible AI—Three chal-
lenges for future research. Information, 11(2), 122. Retrieved from https://doi.org/10.3390/info11020122. https://doi.org/10.3390/
info11020122

Garcez, A. S., Besold, T. R., De Raedt, L., Foldiak, P., Hitzler, P., Icard, T., ... Silver, D. L. (2015). Neural-symbolic Learning and Reasoning:
Contributions and Challenges. AAAI Spring Symposium—Technical Report.

Garcez, A. S., Broda, K., & Gabbay, D. M. (2001). Symbolic knowledge extraction from trained neural networks: A sound approach. Artificial
Intelligence, 125(1-2), 155-207.

Garcez, A. S., Gori, M., Lamb, L. C., Serafini, L., Spranger, M., & Tran, S. N. (2019). Neural-symbolic computing: An effective methodology
for principled integration of machine learning and reasoning. CoRR, abs/1905.06088.

Garnelo, M., & Shanahan, M. (2019). Reconciling deep learning with symbolic artificial intelligence: Representing objects and relations. Cur-
rent Opinion in Behavioral Sciences, 29, 17-23.

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI - 19 of 21
— gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY. °

Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A survey of methods for explaining black box
models. ACM Computing Surveys, 51(5), 1-42.

Harder, F., Bauer, M., & Park, M. (2020). Interpretable and Differentially Private Predictions. The Thirty-fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAT
Symposium on Educational Advances in Artificial Intelligence, EAAI 2020. pp. 4083-4090. New York, NY: AAAI Press. Retrieved from
https://aaai.org/ojs/index.php/AAAT/article/view/5827

Harrison, D., & Rubinfeld, D. L. (1978). Hedonic housing prices and the demand for clean air. Journal of Environmental Economics and Man-
agement, 5(1), 81-102.

Hasling, D. W., Clancey, W. J., & Rennels, G. (1984). Strategic explanations for a diagnostic consultation system. International Journal of
Man-Machine Studies, 20(1), 3-19.

He, X., Liao, L., Zhang, H., Nie, L., Hu, X., & Chua, T.-S. (2017). Neural Collaborative Filtering. Proceedings of the 26th International Confer-
ence on World Wide Web. pp. 173-182.

Heckel, R., & Vlachos, M. (2016). Interpretable recommendations via overlapping co-clusters. CoRR, abs/1604.02071.

Hendricks, L. A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., & Darrell, T. (2016). Generating visual explanations. In B. Leibe, J. Matas,
N. Sebe, & M. Welling (Eds.), Computer vision-ECCV 2016 (pp. 3-19). Cham: Springer International Publishing.

Herlocker, J. L., Konstan, J. A., & Riedl, J. (2000). Explaining Collaborative Filtering Recommendations. Proceedings of the 2000 ACM Confer-
ence on Computer Supported Cooperative Work—CSCW'00. pp. 241-250.

Hidasi, B., & Karatzoglou, A. (2018). Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. Proceedings of the
27th ACM International Conference on Information and Knowledge Management. pp. 843-852. New York, NY: ACM.

Hilton, D. J. (1990). Conversational processes and causal explanation. Psychological Bulletin, 107, 65-81.

Hoffman, R. R., Mueller, S. T., Klein, G., & Litman, J. (2018). Metrics for explainable AI: Challenges and prospects. CoRR, abs/1812.04608.

Holzinger, A. (2016). Interactive machine learning for health informatics: When do we need the human-in-the-loop? Brain Informatics, 3,
119-131. Retrieved from http://www.springer.com/computer/ai/journal/40708. https://doi.org/10.1007/s40708-016-0042-6

Holzinger, A., Carrington, A., & Miiller, H. (2020). Measuring the quality of explanations: The system causability scale (SCS). KI—Kiinstliche
Intelligenz (German Journal of Artificial intelligence), 34, 193-198. https://doi-org/10.1007/s13218-020-00636-z

Holzinger, A., Langs, G., Denk, H., Zatloukal, K., & Miiller, H. (2019). Causability and explainability of artificial intelligence in medicine.
WIREs Data Mining and Knowledge Discovery, 9(4), €1312 Retrieved from https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312

Holzinger, A., Plass, M., Kickmeier-Rust, M., Holzinger, K., Crisan, G. C., Pintea, C. M., & Palade, V. (2019). Interactive machine learning:
Experimental evidence for the human in the algorithmic loop. Applied Intelligence, 49(7), 2401-2414. https://doi.org/10.1007/s10489-018-
1361-5

Huysmans, J., Dejaeger, K., Mues, C., Vanthienen, J., & Baesens, B. (2011). An empirical evaluation of the comprehensibility of decision
table, tree and rule based predictive models. Decision Support Systems, 51(1), 141-154.

Keet, C. M. (2007). Enhancing Comprehension of Ontologies and Conceptual Models Through Abstractions. Proceedings of the 10th Congress
of the Italian Association for Artificial Intelligence (ai*ia 2007). pp. 813-821.

Kim, B., Rudin, C., & Shah, J. (2014). The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification.
Proceedings of the 27th International Conference on Neural Information Processing Systems. Vol. 2. pp. 1952-1960. Cambridge, MA:
MIT Press.

Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, 42(8), 42-49.

Kouki, P., Schaffer, J., Pujara, J., O'Donovan, J., & Getoor, L. (2017). User Preferences for Hybrid Explanations. Proceedings of the Eleventh
ACM Conference on Recommender Systems—RecSys'17. pp. 84-88.

Kulesza, T., Burnett, M., Wong, W.-K., & Stumpf, S. (2015). Principles of Explanatory Debugging to Personalize Interactive Machine Learning.
Proceedings of the 20th International Conference on Intelligent User Interfaces. pp. 126-137. New York, NY: Association for Computing
Machinery. https://doi-org/10.1145/2678025.2701399

Kulesza, T., Stumpf, S., Burnett, M., Yang, S., Kwan, I., & Wong, W.-K. (2013). Too Much, Too Little, or Just Right? Ways Explanations Impact
End Users' Mental Models. 2013 TEEE Symposium on Visual Languages and Human Centric Computing. pp. 3-10.

Lacave, C., & Diez, F. J. (2004). A review of explanation methods for heuristic expert systems. The Knowledge Engineering Review, 19(2),
133-146. Retrieved from. https://doi.org/10.1017/S0269888904000190

Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by nonnegative matrix factorization. Nature, 401, 788-791.

Lehmann, J., & Hitzler, P. (2010). Concept learning in description logics using refinement operators. Machine Learning, 78(1-2), 203-250.

Lipton, Z. C. (2018). The mythos of model interpretability. Queue, 16(3), 30:31-30:57.

Lombrozo, T. (2016). Explanatory preferences shape learning and inference. Trends in Cognitive Sciences, 20(10), 748-759.

Lou, Y., Caruana, R., & Gehrke, J. (2012). Intelligible Models for Classification and Regression. Proceedings of the 18th ACM KDD.
pp. 150-158. ACM.

Lou, Y., Caruana, R., Gehrke, J., & Hooker, G. (2013). Accurate Intelligible Models with Pairwise Interactions. Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 623-631. ACM.

Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. In I. Guyon, et al. (Eds.), Advances in neural infor-
mation processing systems (Vol. 30, pp. 4765-4774). Red Hook, NY: Curran Associates, Inc.

Michie, D. (1988). Machine Learning in the Next Five Years. Proceedings of the 3rd European Conference on European Working Session on
Learning. pp. 107-122. Marshfield, MA: Pitman Publishing, Inc.

 

asuOdt'] SUOLULLOD eATEaID e[qeor{dde oy) q poUlard axe so[otUE YO {eSN Jo se[Ns Joy Arwagr] OULU Ao|1A4 UO (SUOTIPUOD-PUL-sULIOYWOD’€e[LM KrwAgtoUT|UO//:SdMY) SUOTIPUDD PUe SULIT, ay 22g “[ZZOZ/LL/LO] UO KBAgry OUTTUD AOL AY “TaeIS] UBAYIOD Aq [EEL UEPIAYZOQT ‘OL/LOp/MOD Kom AreaqiaurTUO'soMLANy/'SdNY WOLy PapEO[UMOK] ‘| “1ZOZ “S6LPTPEL
20 of 21 WI LEY_— G) WIREs CONFALONIERT er at.

DATA MINING AND KNOWLEDGE DISCOVERY

Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267, 1-38. Retrieved from
http:/ www.sciencedirect.com/science/article/pii/S0004370218305988. https://doi.org/10.1016/j.artint.2018.07.007

Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1(1),
47-80 Retrieved from http://dx.doi.org/10.1023/A:1022691120807

Molnar, C. (2019). Interpretable machine learning. Retrieved from https://christophm.github.io/interpretable-ml-book/

Mothilal, R. K., Sharma, A., & Tan, C. (2020). Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations. FAT*
2020—Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. pp. 607-617. New York, NY: Association for
Computing Machinery. doi: https://doi.org/10.1145/3351095.3372850

Mueller, S. T., Hoffman, R. R., Clancey, W. J., Emrey, A., & Klein, G. (2019). Explanation in human-AlI systems: A literature meta-review,
synopsis of key ideas and publications, and bibliography for explainable AI. CoRR, abs/1902.01876. Retrieved from http://arxiv.org/abs/
1902.01876

Muggleton, S. H., Schmid, U., Zeller, C., Tamaddoni-Nezhad, A., & Besold, T. (2018). Ultra-strong machine learning: Comprehensibility of
programs learned with ilp. Machine Learning, 107(7), 1119-1140.

Murphy, P. M., & Pazzani, M. J. (1991). ID2-of-3: Constructive Induction of M-of-N Concepts for Discriminators in Decision Trees. Machine
Learning Proceedings 1991.

Nati, N. S., & Jaakkola, T. (2003). Weighted Low-rank Approximations. 20th International Conference on Machine Learning. pp. 720-727.
AAAT Press.

Nilashi, M., Jannach, D., & bin Ibrahim, O., Esfahani, M. D., & Ahmadi, H. (2016). Recommendation quality, transparency, and website
quality for trust-building in recommendation agents. Electronic Commerce Research and Applications, 19, 70-84.

Nunes, I., & Jannach, D. (2017). A systematic review and taxonomy of explanations in decision support and recommender systems. User
Modeling and User-Adapted Interaction, 27(3-5), 393-444. Retrieved from http://link.springer.com/10.1007/ s11257-017-9195-Ohttp://
1s13-www.cs.tu-dortmund.de/homepage/publications/jannach/Journal UMUAI 2017 2.pdf. https://doi-org/10.1007/s11257-017-9195-0

Odense, S., & Garcez, A. S. (2017). Extracting m of n rules from restricted boltzmann machines. In A. Lintas, S. Rovetta, P. F. Verschure, &
A. E. Villa (Eds.), Artificial neural networks and machine learning—ICANN 2017 (pp. 120-127). Cham: Springer International
Publishing.

Papadimitriou, A., Symeonidis, P., & Manolopoulos, Y. (2012). A genralized taxonomy of explanations styles for traditional and social recom-
mender systems. Data Mining and Knowledge Discovery, 24(3), 555-583. Retrieved from https://link.springer.com/content/pdf/10.
10072Fs10618-011-0215-0.pdf. https://doi-org/10.1007/s10618-011-0215-0

Park, D. H., Hendricks, L. A., Akata, Z., Schiele, B., Darrell, T., & Rohrbach, M. (2016). Attentive explanations: Justifying decisions and
pointing to the evidence. CoRR, abs/1612.04757.

Parliament and Council of the European Union. (2016). General data protection regulation.

Peake, G., & Wang, J. (2018. Explanation mining: Post hoc interpretability of latent factor models for recommendation systems. Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 2060-2069. New York, NY: Association
for Computing Machinery. Retrieved from http://dl.acm.org/doi/10.1145/3219819.3220072. doi: https://doi-org/10.1145/3219819.3220072

Piltaver, R., LuStrek, M., Gams, M., & Martinéi¢-Ipsi¢, S. (2016). What makes classification trees comprehensible? Expert Systems with Appli-
cations, 62 (C, 333-346.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016a). Model-agnostic interpretability of machine learning. CoRR, abs/1606.05386.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016b). Why Should I Trust You?: Explaining the Predictions of Any Classifier. Proceedings of the
22nd International Conference on Knowledge Discovery and Data Mining. pp. 1135-1144. ACM.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2018). Anchors: High-precision model-agnostic explanations. In AAAI (pp. 1527-1535). New
Orleans, Louisiana: AAAI Press.

Ribera, M., & Lapedriza, A. (2019). Can We Do Better Explanations? A Proposal of User-Centered Explainable AI. Joint Proceedings of the
ACM IUI 2019 Workshops Colocated with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI 2019). Vol. 2327.
CEUR-WS.org.

Russell, C. (2019). Efficient Search for Diverse Coherent Explanations. Fat* 2019—Proceedings of the 2019 Conference on Fairness, Account-
ability, and Transparency. pp. 20-28. doi: https://doi.org/10.1145/3287560.3287569

Samek, W., Montavon, G., Vedaldi, A., Hansen, L. K., & Miiller, K.-R. (2019). Explainabie artificial intelligence: Understanding, visualizing
and interpreting deep learning models, Basel, Switzerland: Springer International Publishing.

Sarker, M. K., Xie, N., Doran, D., Raymer, M., & Hitzler, P. (2017). Explaining Trained Neural Networks with Semantic Web Technologies: First
Steps. Ceur Workshop Proceedings.

Seo, S., Huang, J., Yang, H., & Liu, Y. (2017). Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review
Rating Prediction. Proceedings of the Eleventh ACM Conference on Recommender Systems. pp. 297-305. New York, NY: ACM.

Singh, J., & Anand, A. (2018). Posthoc interpretability of learning to rank models using secondary training data. arXiv:1806.11330.

Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C., & Johannes, R. S. (1988). Using the ADAP Learning Algorithm to Forecast the
Onset of Diabetes Mellitus. Proceedings—Annual Symposium on Computer Applications in Medical Care.

Tintarev, N., & Masthof, J. (2015). Explaining recommendations: Design and evaluation. In Recommender systems handbook (pp. 217-253).
Boston, MA: Springer. https://doi-org/10.1007/978-1-4899-7637-6

Towell, G. G., & Shavlik, J. W. (1993). Extracting refined rules from knowledge-based neural networks. Machine Learning, 13(1), 71-101.

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
CONFALONIERI - 21 of 21
— gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY. °

Troquard, N., Confalonieri, R., Galliani, P., Pefialoza, R., Porello, D., & Kutz, O. (2018). Repairing Ontologies via Axiom Weakening. In S. A.
Mcllraith & K. Q. Weinberger (Eds.), Proceedings of the Thirty-second AAAI Conference on Artificial Intelligence, (AAAI-18),
pp. 1981-1988. New Orleans, Louisiana. AAAI Press. Retrieved from https://www.aaai.org/ocs/index.php/AAAI/AAAT18/paper/view/
17189

Vargas, S., & Castells, P. (2011). Rank and Relevance in Novelty and Diversity Metrics for Recommender Systems. Proceedings of the Fifth ACM
Conference on Recommender Systems—RECSYS'11. p. 109. New York, NY: ACM Press. doi: https://doi.org/10.1145/2043932.2043955

Wachter, S., Mittelstadt, B., & Floridi, L. (2017). Why a right to explanation of automated decision-making does not exist in the general data
protection regulation. International Data Privacy Law, 7(2), 76-99. https://doi.org/10.1093/idpl/ipx005

Wachter, S., Mittelstadt, B., & Russell, C. (2018). Counterfactual explanations without opening the black box: Automated decisions and the
GDPR. Harvard Journal of Law & Technology, 31(2), 841-887.

Wang, X., He, X., Feng, F., Nie, L., & Chua, T. (2018). TEM: Tree-Enhanced Embedding Model for Explainable Recommendation. Proceedings
of the 2018 World Wide Web Conference. pp. 1543-1552.

Wick, M. R., & Thompson, W. B. (1992, March). Reconstructive expert system explanation. Artificial Intelligence, 54(1-2), 33-70.

Zhang, Y. (2015). Incorporating Phrase-level Sentiment Analysis on Textual Reviews for Personalized Recommendation. Proceedings of the
Eighth ACM International Conference on Web Search and Data Mining. pp. 435-440. New York, NY: ACM.

Zhang, Y., Ai, Q., Chen, X., & Wang, P. (2018). Learning over knowledge-base embeddings for recommendation. CoRR, abs/1803.06540.

Zhou, Z.-H., Jiang, Y., & Chen, S.-F. (2003). Extracting symbolic rules from trained neural network ensembles. AI Communications, 16
(1), 3-15.

Ziegler, K., Caelen, O., Garchery, M., Granitzer, M., He-Guelton, L., Jurgovsky, J., .. . Zwicklbauer, S. (2017). Injecting Semantic Background
Knowledge into Neural Networks Using Graph Embeddings. 2017 IEEE 26th International Conference on Enabling Technologies: Infra-
structure for Collaborative Enterprises (WETICE). pp. 200-205.

 

How to cite this article: Confalonieri R, Coba L, Wagner B, Besold TR. A historical perspective of explainable
Artificial Intelligence. WIREs Data Mining Knowl Discov. 2021;11:e1391. https://doi.org/10.1002/widm.1391

 

asuaary] SUOLLULOS) aANeaID o[gEoT{dde Wp fq pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULIAUKOS’ o[LMAreIqifoUT|UO//:sdnY) SUOTTPUCD PUE SULIET, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD Aart A “ToeUST eUEIYDOD Aq [GET UEPLN/ZOOTOL/Loppuios oli Areiqrauruo'sourmyy'sdny Wioxy popeotumog ‘| “1ZOT ‘S6LPZPET
EXPLAINABLE ARTIFICIAL INTELLIGENCE —
THE NEW FRONTIER IN LEGAL INFORMATICS

Bermhard Waltl / Roland Vogl

Research Associate, Technical University of Munich, Department of Informatics,
Software Engineering for Business Information Systems

Boltzmannstrafe 3, 85748 Garching bei Minchen, DE

b.waltl@tum.de; http://wwwmatthes.in.tum.de

Executive Director of CodeX — the Stanford Center for Legal Informatics, and
Executive Director of the Stanford Program in Law, Science and Technology
Crown Quadrangle, 559 Nathan Abbott Way, Stanford, CA 94305-8610, USA
rvogl@law.stanford.edu; http://codex.stanford.edu/

Keywords: Explainable artificial intelligence, next-generation decision support systems, AI governance,
interpretation, explanation, algorithmic decision-making

Abstract: In recent years, mainstream media coverage on artificial intelligence (Al) has exploded Major
Al breakthroughs in winning complex games, such as chess and Go, in autonomous mobility,
and many other fields show the rapid advances of the technology. Al is touching more and more
areas of human life, andis making decisions that humans frequently find difficult to understand.
This article explores the increasingly important topic of «explainable Al» and addresses the
questions why we need to build systems that can explain their decisions and how should we
build them. Specifically, the article adds three additional dimensions to capture transparency
to underscore the tremendous importance of explainability as a property inherent to machine
learning algorithms. It highlights that explainability can be an additional feature and dimen-
sion along which machine learning algorithms can be categorized. The article proposes to view
explainability as an intrinsic property of an Al system as opposed to some external function or
subsequent auditing process. More generally speaking, this article contributes to legal infor-
matics discourse surrounding the so-called «third wave of Al» which leverages the strengths
of manually designed knowledge, statistical inference, and supervised machine learning tech-
niques.

1. Introduction

Artificial Intelligence (AI) covers a broad range of concepts and terms, and its essence is hard to define.
In 1978 BELLMAN defined AI as the «[The automation of] activities that we associate with human thinking,
activities such as decision-making, problem solving, learning ...» (BELLMAN, 1978). While this is a very
broad and abstract definition, it is still valid today. Russe. and Norvic differentiated a variety of concurrent
and complementary definitions, which are organized along the categories of thinking like a human, thinking
rationally, acting like a human, acting rationally (RussELL & Norvie, 2009). The sheer explosion of Al systems
and Algorithmic Decision-Making (ADM) affecting our daily-lives can be traced back to the following three
developments:

1. Data that are used especially for training and evaluation of machine learning based systems are more
easily available in digital form

2. High performance computing infrastructure

3. Efficient algorithms that can automate increasingly complex tasks
Bernhard Walt! / Roland Vogl

 

Section 2 of this article will provide a discussion of the meaning of explainability and transparency. These
concepts are then woven into our discussion of ADM in Section 3. We will show that explainability of ADM
has to be viewed as an intrinsic property of an ADM system, much as the system’s performance is an intrinsic
property of the system, with commonly accepted metrics for evaluation (see Sections 3.2. and 3.3.). Finally,
the paper will briefly discuss explainability along the lifecycle of an ADM procedure, which goes beyond the
application of a machine learning classifier including also the data acquisition and pre-processing phases (see
Section 4).

1.1. AlTin the Legal Domain and the Expanding Use of Algorithmic Decision Making

The idea of formalizing decision-making processes so that they can be automated by algorithms has been an
appealing idea for many legal scholars and practitioners for a long time. The field of legal informatics has
concemed itself with the many questions surrounding uses of AI in legal settings since its early days. The
International Association for Artificial Intelligence and Law and other AI communities have substantially in-
creased the understanding of possibilities, challenges, and limitations of AI applications in the legal field.
According recent scholarship in the field, we can generally distinguish the following different of AI reasoning
approaches: Deductive reasoning (legal expert systems, classical logic programming, etc.), Case-based reason-
ing (induction of rules based on prior cases and precedents, etc.), Abductive reasoning (semantic entailment,
finding simple and likely explanations, etc.), Defeasible reasoning (non-monotonic logics, and argumenta-
tion, etc.), Probabilistic reasoning (fuzzy logic, reasoning on indeterminate and vague terms, etc.), Reasoning
on ontologies (formal knowledge representations, semantic web, OWL, etc.), Statistical reasoning including
machine learning approaches (un-/supervised ML, etc.), Advanced machine learning (active, interactive, and
reinforcement learning, etc.) (AsHLEY, 2017; BENCH-CaPon ET AL., 2012; Russet, & Norvic, 2009). In
recent years, Al research based on statistics, machine learning and data mining has exploded. Among other
uses, these techniques are specifically leveraged for predictive analytics purposes. Companies in a number
of different industries, such as advertising, financial services, insurance, telecommunication, logistics, health
care are using predictive models to gain strategic advantage over their competition. Software-supported fore-
casting of legal decisions (WALTL, BonczeK, SCEPANKOvA, LANDTHALER, & Matrues, 2017), and numerous
other applications of artificial intelligence to predict legal outcomes have been launched in recent years around
the world (VocL, 2017).

Already, in the heydays of legal expert systems in the 1990s, HERBERT FIEDLER observed that there are almost
mythological expectations when it comes to algorithmic decision-making (Frepier, 1990). For the field of
legal expert systems, he discussed six distinct expectations that people have with regard to ADM, including
the following two pertaining to the concept of explainability:

1. The ability to easily understand and follow knowledge representation, and
2. The explainability and transparency of decisions.

Today, little attention is given to the possibilities of using pre-defined rules based on deductive and ontological
reasoning techniques that are the main techniques underlying legal expert systems as a means for explaining
automated decision-making processes. Current research on ADM is more focused on the techniques of the
machine learning field, rather than the more static and human engineered decision structures of legal expert
systems. Nevertheless, it is worth noting that handcrafted reasoning tools, such as the Oracle Policy Au-
tomation tool!, are used by organizations all around the world to represent decision structures and to enable
automated (legal) reasoning. But, as Fiedler points out, even the decisions of those hand-crafted systems are
hard to understand and explain. In light of that, and in light of the prevalence of the opaquer machine-learning
based automated decision-making systems, more attention to the actual algorithmic processing underlying an

1 https://www.oracle.com/applications/oracle-policy-automation/index.html (all websites last accessed on 12 January 2018).
Explainable Artificial Intelligence — the New Frontier in Legal Informatics

 

automated decision is warranted. Consequently, we are focusing our attention on providing a more nuanced
view of explanation representations.

1.2. The Need for More Algorithmic Transparency

At their core, Al-based systems implement highly formalized processes while avoiding ambiguities and un-
certainties. However, the complexity of these systems and their internal structure makes their inner workings
difficult to understand even for experts. This is troubling as more and more advanced AI systems are making
decisions that affect our daily lives, including decisions on our credit score, on recruiting, health care related
decisions, transportation (including autonomous driving), logistics, and many more areas. In light of a grow-
ing list of examples of AI systems that produced unfair or discriminatory outcomes, public discourse about
AI has recently tured from euphoria to concern (e.g., Cary O’ NEILL, 2016). These cases have made clear
that we can no longer view ADM systems as mysterious black-boxes, and in order to earn a human’s trust,
the functioning of ADM systems will have to be understandable. It should be noted that explainable artificial
intelligence does not mean the same as having systems that prevent discrimination in algorithmic decision
making. Instead it represents a new property of machine learning techniques. One could of course argue that
explanations are required to determine whether discrimination occurred. However, answering that question
also requires an analysis of attributes and properties of the term «discrimination».

1.3. The Opacity of Algorithmic-Decision-Making Software

From an academic computer science perspective, the currently increased public awareness around artificial
intelligence on one hand has the potential to promote innovation and new research in the field. On the other
hand, the danger of heightened expectations is of course that if expectations are not met, we may again enter
a period of reduced interest and investment in artificial intelligence, or in other word another «AI winter»?
might be on the horizon.

We believe that part of the media frenzy around AI is due to the opaque and «black-box»-like nature of Al
methods and procedures, including methods that are mathematically complex and encapsulated in «ready-
to-use» software packages such as MLib of Apache Spark?, Scikit-leam*, or Deepleamning4j>. These are
commonly used to train, test, and evaluate the performance ofa machine learning classifier that is subsequently
used to predict the outcome of a given task, such as classification of images, categorization of text, the next
move in a chess game, or the likelihood of winning a tral.

These software modules operate in a way that suggests even to the human expert user that something magical
is happening once a classifier is trained and it can by itself produce seemingly intelligent answers. As long as
these software components are viewed as black-boxes with easy to use interfaces, the sentiment that humanly
unexplainable things are happening inside the box is perpetuated. However, because machine learning and
other forms of artificial intelligence rely on the application of mathematical insights and processes, such as
optimization operations, or Naive Bayes probabilities, their internal procedures are in fact fully deterministic®
and can at least in principle be reproduced by humans. However, because many of these machine learning
algorithms deal with high-dimensional data and a representation of the features that are not commonly under
stood by humans, the mere inspection of the classifier might not provide answers in in the form that humans
would expect. The insights that one can gain by analyzing the machine learning classifier — by for example

Not too long ago the AI winter was a consequence of the highly raised expectations that could not be met by technology. AI winter
is a metaphor and describes a period of reduced funding and interest in AI. The period includes a lack of trust in AI technology due
to broken promises and only partially fulfilled expectations.

https://spark.apache. org/mllib/.

http://scikit-learn.org/stable/.

https://deeplearning4j.org/.

Unless no random variables are used within the classification process.

nw ew
Bernhard Walt! / Roland Vogl

 

looking at the internal states representing a combination of the classifier type and the training data — are ina
form is not suitable to be interpreted by humans. This in turn begs the question as to how these internal states
can be represented in a way that humans can still understand and interpret the internal structure of artificially
intelligent systems. Finding a way that allows us to interpret the internal structure of Al systems, we believe,
will contribute to a broader acceptance and a better understanding of the potential and limitations of AI and
algorithmic decision making.

In recent years, we have witnessed the birth of the subfield of interpretable machine-learning. We are already

seeing important advances in the field. Jung et al., for example, proposed simple rules for bail decisions’.

8 Legislators are also working

Corpett-Davigs ET AL. recently discussed the fairness of such algorithms’
towards building legal frameworks that can help prevent algorithms from creating undesired results, such as
discrimination”. Latest discussions in politics and societal pressure even caused legislators to examine the
potentials of governing ADM in the field of discrimination. This can, for example, be observed in Germany,

the ministry of justice announced a project on a feasibility study on governing ADM”,

1.4. The Role of Explanations within a Complex Process

There are commonly adapted frameworks to describe the phases involved in creating a system, which can
decide autonomously. Figure 2 shows a process, which consists of five subsequent phases. This represents a
model process for illustration purposes, which does not contain any iterations and feedback loops. In practi-
cal applications the process is typically more complex (Waltl, Landthaler, et al., 2017). During each phase,
activities are automatically performed and intermediate results are produced that influence the overall ADM.

 

 

 

 

 

pata Pre Transfor- Training & Post-
acauieition processing nation . application of Process cess ng
. ‘election o: nterpretation
(ata cleansing) features) Al model evaluation)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 1: ADM is a complex process of at least five different phases (FavyaAp, PIATETSKY-SHAPIRO, &
Smytu, 1996).

However, when it comes to explaining the ADM, it is not sufficient to look at the classification result on one
instance only. In order to fully understand an automated decision, the whole process needs to be considered.
Otherwise the explanation might not be representative and might not reflect all input factors and parameters
which led to a particular decision (see Section 3.2.).

https://hbr.org/2017/04/creating-simple-rules-for-complex-decisions.

https://www.nytimes.com/201 7/12/20/upshot/alg orithms-bail-criminal-justice-system.html.

9 This is strongly emphasized within Article 21 and 22 of the General Data Protection Regulation (GDPR, Regulation (EU)
2016/679) have been interpreted to provide for a «right to explanation» with regard to algorithmic decision-making that affects
an individual. «The data subject shall have the right not to be subject to a decision based solely on automated processing, includ-
ing profiling, [...].» and «[...] obtain an explanation of the decision reached after such assessment and to challenge the decision.»
There is an ongoing debate around the meaning of this rule and to this date there does not seem to be an agreement on the potential
consequences of this rule, and how it should be implemented. Some commentators call Article 21 «toothless» and raise important
questions regarding this new right (WacuTer, Mitrecstapt, & Fioripi, 2017).

10 Feasibility study on discrimination in ADM, to be published in 2018 by Gesellschaft far Informatik.
Explainable Artificial Intelligence — the New Frontier in Legal Informatics

 

2. A Framework for Explanation and Interpretation
2.1. Trying to Capture the Essence of Explainability

A possible definition for explanation in the context of ADM could be: «A formal and unambiguous descriptive
representation of the output of a classifier based on the current and prior input represented as parameters.» This
definition consists of four main parts:

Formal and unambiguous representation
. Descriptive nature of the representation
. Output of a classifier with regard to a decision

BwNe

. Current and prior input of the classifier

While satisfying from technical point of view, it is questionable whether this definition actually creates more
clarity for a lay person. Even the behaviour of neural networks can be «explained» with mathematical formulas
including operations on (very large) matrices in a high-dimensional space. We could theoretically create an
unambiguous representation of these systems’ decisions, and it would be valid. However, just as clarity around
certain legal concepts can be destroyed by excessive legalese in a contract for example, if most people are
unable to comprehend these technical representations of an algorithm’s inner workings, human subjects of
ADM would not be reassured and it would do little to further the goals of the explainable artificial intelligence
field.

Ina recent article DosHI-VELEz ET AL. stated on the governance of explanations that «when demanding expla-
nation from humans, what we typically want to know is how and whether certain input factors affected the final
decision or outcome» (DosHI-VELEZ ET AL., 2017). Unfortunately, this definition and finding does not provide
much helpful guidance for those who design and implement the algorithms, mostly software developers and
computer scientists. A more differentiated perspective on ADM is required.

GuNNING provided us with the following set of basic questions that help to assess ADM (GunwNine, 2017).
The questions can be considered to as guidelines to provide more structure to the development of ADM system
and improve their intrinsic explainability:

. Why did that output happen?

. Why not some other output?

. For which cases does the machine produce a reliable output?

. Can you provide a confidence score for the machine’s output?

. Under which circumstances, i.e. state and input, can the machine’s output be trusted?
. Which parameters effect the output most (negatively and positively)?

SDwfswWN

. What can be done to correct an error?

Based on these questions, algorithms can be built to provide explanations for why specific instances or entire
classes that were trained for a specific task (in case of supervised machine learning tasks) were classified in the
way they were. On a superficial level, these kinds of explanations provide insights on the reasoning process.
However, they do not provide answers with regard to other important considerations, such as whether the
algorithm may have discriminated against the particular subject of an automated decision. The classification
process is in itself just the application of pre-trained machine learning models (see Section 3), which are based
on applied mathematics, in other words deterministic and value-free processes. Moral or legal questions — such
as whether someone has been discriminated against — should in our opinion not be answered by the algorithm.
Answering those questions requires operationalized methods that allow for objective evaluation based on clear
conditions. For example, if a decision is significantly influenced by one feature or attributes that are part of
individual identity, such as age or sex, this would allow for objective evaluation of an algorithmic decisions
Bernhard Walt! / Roland Vogl

 

discriminatory outcomes. But the ADM process in practice covers more than just the application of a machine
learning algorithm, which has been discussed in greater detail in Section 1.4.

Technological considerations on the intrinsic properties of ADM can be used to develop a framework for
interpretations and explanations of Al systems. A recent work on the interpretation of machine learning models
implies the following useful framework by Lipton (2016):

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

. aa: Decomposa- Algorithmic Textual ee Local
Simulatability bility Transparency descriptions Visualizations explanations Examples
Transparency Interpretability
Explainability

 

 

 

Figure 2: Taxonomy of explainability in the field of ADM.

Explainability is divided along functional and descriptive categories and covers the two main fields trans-
parency and interpretability. Transparency is sub-divided into simulatability (What-if-analysis), decompos-
ability along the whole process of ADM (see Section 3), and algorithmic transparency (see Section 2.5.).

Interpretability is divided into four sub-categories:

1. provision of textual descriptions of how a model behaves and how specific features contribute to a
classification,

2. visualizations as dense representations of features and their contexts,

3. local explanations providing information about the impact of one particular feature, and finally, and

4. examples that illustrate the internal structure of a trained model.

In this context, transparency means the process of making a decision-making process visible with all the phases
and interactions between components of the algorithms. Complementary to that, interpretability summarize
every effort that is made by humans or machines to provide descriptive information, i.e. visualizations, local
explanations, that enable humans to understand the decisions made by ADM.

2.2. Expanding Comparability Dimensions of Machine Learning Algorithms for ADM
Machine learning algorithms, which are powering many of the artificial intelligence systems at issue here, can
be analyzed along different parameters and dimensions. Korstantis proposed a comparison chart which has
become widely accepted (Kots1antis, 2007). Table 1 below shows the dimensions that Korstantis proposed.
We adapted the table to the research question of this article. The columns list six common machine learning
algorithms, and the rows describe the desirable property of the specific machine learning algorithm.
Explainable Artificial Intelligence — the New Frontier in Legal Informatics

 

 

 

 

 

Decision Trees Neural Naive Bayes | kNN Deductive
Networks SVM logic
based!!
Accuracy ee eee ° ee eee oe
Speed of learning coe . eoee coos ° oe
Speed of wee. wees wees ° wees wees
classification

 

Tolerance w.rt.
input

 

Transparency of the
process

 

Transparency of the
model

 

Transparency theof
classification

 

 

 

 

 

 

 

 

 

Table 1: Extension of comparison of common machine learning algorithms based on Kortstantis (2007).

Table 1 shows that the main features of interest to end-users are the overall accuracy of an algorithm and

the speed of learning or classification. Of course, the overall effort required to train a classifier is also very

important measure, especially in practical usage scenarios. We propose to add the three following rows that
represent the explainability and transparency of ADM dimensions:

— Transparency of the process: The ability of explain refers to the entire process of ADM, which consists
of different steps that are all based on individual design decisions. So, to what degree can an ADM
process be made transparent and explainable? This refers mainly to the structure of the classification
process (see Section 4). Developing a machine leaning classifier does not only require the training of
a machine learning classifier. It also requires that parameterization of the algorithm, data collection,
pre-processing, ete. Throughout the parameterization process many parameters have to be set. For
example, decision trees, including random forests, can be parameterized to have a maximal depth. This
is known as «pruning», which helps explain the impact of a particular parameter on the trained model.
In addition, human biases can influence the data collection which in tum can influence the training and
the classification of the model!?.

— Transparency of the model: To what degree can the model underlying a trained classifier be made
transparent and be communicated clearly to humans? In most implementations of machine learning
classifiers, the internal representation of the trained model is stored very monolithically, which does
not facilitate deeper understanding of the trained model. Classifiers, such as decision trees or deductive
logic based systems, represent their decision structures in rules that can be analyzed and interpreted by
humans. More elaborate classifiers such as Naive Bayes or SVMs! rely on a more elaborate mathe-
matical representation, mainly optimizing of probabilities or a so-called kernel function. These internal
representations require much more effort to understand and interpretation. Finally, neural networks tend

Technically deductive logic based approaches are mostly implemented with rule-based systems and do belong to the class of ma-

chine learning based approaches.

An excellent observation has been made by Goopman & FLAxmAN (2016): «Machine learning depends upon data that has been
collected from society, and to the extent that society contains inequality, exclusion or other traces of discrimination, so too will the
data.»

Support Vector Machine.
Bernhard Walt! / Roland Vogl

 

to have a very complex internal structure, including large amount of so-called «hidden layers,» that make
the underlying decision-making process very hard to interpret and comprehend. For every classifier, the
reasoning structure is highly rational, albeit in a very formal mathematical sense, which frequently does
correspond with what engineers and users would deem acceptable clarity.

— Transparency of the classification: To which degree can a classified instance of a trained classifier be
made transparent and be communicated? In addition to the transparency of the underlying knowledge
on which a model is trained, the transparency of the classification used plays an important role. Given a
concrete classification that lead to a specific algorithmic decision, to what degree can the classifier itself
make the factors transparent that lead to the specific decision? What were the features that contributed
to a specific decision and what was the concrete weight of their contribution? This should also take into
account whether a feature contributed directly or indirectly (by influencing other features) to the overall
classification. This should not only complement the transparency of the model but should also have an
impact on future research and systems engineering. There may be instances where the transparency of
the model may not be important at all, while the reproducibility of a decision can be critical.

The addition of three additional dimensions to capture transparency aims to underscore the tremendous impor
tance of explainability as a property inherent to machine learning algorithms. It highlights that explainability
can be an additional feature and dimension along which machine learning algorithms can be categorized. The
next section will extend the concept of transparency as an inherent property and establish it as a concept that
research and industry alike should take into account as a critical aspect in developing new algorithms.

2.3. Explainability as an Intrinsic Property of Machine Learning Algorithms

Every machine learning algorithm has different properties that can be used as a means to categorize and com-
pare them against each other. We have proposed transparency of machine algorithms as an additional impor
tant property. In our framework explainability is not treated as an optional characteristic or additional external
property that can be «plugged-in» to machine learning classifiers like adding a new component or applica-
tion to a set of given functions and capabilities. Instead it is an intrinsic property of every ADM that can be
improved and enhanced just as other (performance) attributes can be improved.

Machine leaming

 

Neural networks (perceptrons, CNN, RNN, etc.)

 

Statistical model (SVM, etc.)

 

 

 

Ensemble methods _}
—

Decision trees

 

 

[ Graphical models (bayesian networks, markov model, etc.)

 

 

»
aoa eo 5 5 Explainability
Figure 3: Explainability of machine learning classifiers as desirable property based on GUNNING (2017).

GUNNING suggested explainability as a desirable property that does not have to negatively affect the perfor
mance of a machine learning classifiers (GUNNING, 2017). The main idea is illustrated in Figure 3. The figure
shows different classes of machine learning algorithms. Although the sets of classifiers are stacked in the fig-
ure, this is not meant to create the impression that they are building on each other or that one would be superior
Explainable Artificial Intelligence — the New Frontier in Legal Informatics

 

to another. One could compare these classifiers along a variety of dimensions such as speed of learning or the
time needed for the classification of one instance. The figure maps five different categories of machine learn-
ing techniques into a two-dimensional space, illustrating the relationship between explainability and accuracy
for each category. The order of the different categories is random and is not meant to invite conclusions about
whether the accuracy of a particular category of classifiers is superior to the accuracy of another. This results
from the fact that the performance of a particular algorithm heavily depends on the concrete classification task,
as well as the training data, and the selected features. GuNNING’s figure primarily represents a visual effort to
express the overarching objective that increasing the explainability of machine learning classifiers should be
accomplished without decreasing the accuracy or the computational performance of the classifier. However,
most current techniques lack the ability to explain how a particular decision was made. Instead they are mainly
focused on performance of the classifiers/algorithms in terms of accuracy and computability. We concur with
Gunnine’s conclusion implied in Figure 3 in that different classifier categories allow for different levels of
explainability features and components.

3. Conclusion

As the above analysis shows, furnishing machine learning classifiers and ADM processes in general with
functionality that increases their explainability is a non-trivial task. In order to contribute to a more nuanced
discourse on the promises and pitfalls of Al, we attempted provide a constructive differentiation and termino-
logical clarification regarding explainable Al. We conclude that explanations of decisions made by AI systems
are possible. We differentiated three levels of transparency, specifically transparency on the process, on the
model, and the instance.

Artificial intelligence is based on deterministic procedures and mathematical operations. Although it can
be observed, that machines outperform humans in many tasks, their decisions are rational and follow strict
mathematical, i.e. rational, structures. We need to put more emphasis on making these structures transparent.
This would not only allow us to increase their explainability, but it also allows us to optimize these systems
and understand the boundaries of artificial intelligence in greater depth. We are already witnessing a trend
towards the use of explanatory classification systems in the field of computer vision.!4 We argue that increasing
explainability of machine learning techniques more generally, will not only increase the acceptance of ADM
based on machine learning (see Section 1), but it will also allow system engineers to improve the classification
mechanisms and the algorithmic decision making itself. The increasing demands by regulators to provide
secure and transparent systems for consumers, will make the emerging field of interpretable machine learning
more and more attractive for academic research and entrepreneurial solutions.

4. References

ASHLEY, K. D. (2017). Artificial Intelligence and Legal Analytics: New Tools for Law Practice in the Digital Age. Cam-
bridge: Cambridge University Press.

BELLMAN, R. (1978). An Introduction to Artificial Intelligence: Can Computers Think?: Boyd & Fraser.

BencuH-Capon, T., ARASZKIEWICz, M., ASHLEY, K., ATKINSoN, K., Bex, F., Borces, F.,... Wyner, A. (2012). A history
of AI and Law in 50 papers: 25 years of the international conference on AI and Law. Artificial Intelligence and Law.

14 Th that field, modern classification systems for images are mainly built on so-called trained neural networks, which are viewed
as the classifiers that are in every dimension the least transparent and the most difficult to explain. Hendricks showed that even
state-of-the-art neural network classifiers can be modified to provide a visual explanation for the classification of images and their
content (HENDRICKs ET AL., 2016). A visual explanation contains both explanations for a concrete image (which correspond to
our category of transparency of classification) as well as for a class (corresponding to our category for transparency of the model).
Hendricks» article provides a good example of the technologies that can be leveraged to enhance explainability of so-called «deep
models», which are considered difficult to interpret.
Bernhard Walt! / Roland Vogl

 

Dosut-VeLez, F., Kortz, M., Bupisu, R., Bavirz, C., GERSHMAN, S., O’Brien, D.,. .. Woop, A. (2017). Accountability
of AI Under the Law: The Role of Explanation. arXiv preprint arXiv: 1711.01134.

Fayyab, U., PIaTeTsKy-SHAPIRO, G., & Smytu, P. (1996). From data mining to knowledge discovery in databases. AJ
magazine, 17(3), 37.

Frepter, H. (1990). Entmythologisierung von Expertensystemen In H. Bonin (Ed.), Einfiihrung in die Thematik fiir Recht
und offentliche Verwaltung. Heidelberg: Decker & Miiller- Verlag.

Goopman, B., & FLaxman, S. (2016). European Union regulations on algorithmic decision-making and a «right to ex-
planation». arXiv preprint arXiv: 1606.08813.

Guwnina, D. (2017). Explainable Artificial Intelligence (XAL)

Henpricks, L. A., Akata, Z., RoHRBACH, M., Donauug, J., ScuieLe, B., & Darrett, T. (2016). Generating Visual
Explanations. Paper presented at the ECCV.

Korsiantis, S. B. (2007). Supervised Machine Learning: A Review of Classification Techniques. Paper presented at the
Proceedings of the 2007 conference on Emerging Artificial Intelligence Applications in Computer Engineering.

Lirton, Z. C. (2016). The mythos of model interpretability. a-Xiv preprint arXiv: 1606.03490.
RussEL, S., & Norvie, P. (2009). Artificial Intelligence: A modern approach.
Voat, R. (2017). In M. Hartung, M.-M. Bues, & G. Halbleib (Eds.), Digitalisierung des Rechismarkts. C. H. Beck.

Wacuter, S., Mirrecstapt, B., & Froript, L. (2017). Why a right to explanation of automated decision-making does not
exist in the general data protection regulation. Jnternational Data Privacy Law, 7(2), 76-99.

Watt , B., Bonczex, G., ScepaNKova, E., LANDTHALER, J., & MatTHEs, F. (2017). Predicting the Outcome of Appeal
Decisions in Germany’s Tax Law. 89-99. doi: 10.1007/978-3-319-64322-9 8

Watt , B., LANDTHALER, J., SCEPANKOVA, E., MaTTHEs, F., GEIGER, T., STOCKER, C., & SCHNEIDER, C. (2017). Automated
Extraction of Semantic Information from German Legal Documents. Jusfetter IT, Conference Proceedings IRIS 2017.
IEEE Access:

Multidiscipinary Rapid Review £ Open Access Journal

 

Received August 5, 2018, accepted September 4, 2018, date of publication September 17, 2018, date of current version October 12, 2018.

Digital Object Identifier 10.1109/ACCESS.2018.2870052

Peeking Inside the Black-Box: A Survey on
Explainable Artificial Intelligence (XAl)

AMINA ADADI® AND MOHAMMED BERRADA

Computer and Interdisciplinary Physics Laboratory, Sidi Mohammed Ben Abdellah University, Fez 30050, Morocco

Corresponding author: Amina Adadi (amina.adadi@ gmail.com)

ABSTRACT At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread
adoption of artificial intelligence (AD) in our daily life, which contributes to accelerating the shift towards a
more algorithmic society. However, even with such unprecedented advancements, a key impediment to the
use of Al-based systems is that they often lack transparency. Indeed, the black-box nature of these systems
allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on
explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of
Al-based systems. It is recognized as the sine qua non for AI to continue making steady progress without
disruption. This survey provides an entry point for interested researchers and practitioners to learn key
aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature,
we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present

major research trajectories.

INDEX TERMS Explainable artificial intelligence, interpretable machine learning, black-box models.

L_ INTRODUCTION

A. CONTEXT

Nowadays, Artificial Intelligence (AI) is democratized in
our everyday life. To put this phenomenon into numbers,
International Data Corporation (IDC) forecasts that global
investment on AI will grow from 12 billion U.S. dollars
in 2017 to 52.2 billion U.S. dollars by 2021 [1]. Meanwhile,
the statistics portal Statista, expects that revenues from the
AI market worldwide will grow from 480 billion U.S. dollars
in 2017 to 2.59 trillion U.S. dollars by 2021 [2]. Gartner
identifies AI as an inescapable technology among the Gartner
Top 10 Strategic Technology Trends for 2018. Along with
immersive experiences, digital twins, event-thinking and con-
tinuous adaptive security, they are shaping the next generation
of digital business models and ecosystems [3]. Consequently,
the proliferation of AI is having a significant impact on
society. Indeed, AI has already become ubiquitous and we
have become accustomed about AI making decisions for us
in our daily life, from product and movie recommendations
on Netflix and Amazon to friend suggestions on Facebook
and tailored advertisements on Google search result pages.
However, in life-changing decisions such as disease diagno-
sis, it is important to know the reasons behind such a critical
decision. Here, the crucial need for explaining AI outcomes
becomes fully apparent.

Problematically, though they appear powerful in terms of
results and predictions, AI algorithms suffer from opacity,
that it is difficult to get insight into their internal mecha-
nism of work, especially Machine Learning (ML) algorithms.
Which further compound the problem, because entrusting
important decisions to a system that cannot explain itself
presents obvious dangers.

To address this issue, Explainable Artificial Intelligence
(XAD proposes to make a shift towards more transparent AI.
It aims to create a suite of techniques that produce more
explainable models whilst maintaining high performance
levels.

B. XAI’S LANDSCAPE DYNAMIC

XAI has been gaining increasing attention recently. The
growing dynamic around this field has been reflected in
several scientific events. Examples of annual international
conference series dedicated exclusively to the topic include
Fairness, Accountability, and Transparency (FAT-ML) work-
shop at KDD 2014-2018 [4] and ICML Workshop on Human
Interpretability in Machine Learning (WHI) 2016-2018 [5].
The topic has also become the key concern in panel dis-
cussions at specific sessions in major conferences such as
NIPS 2016 Workshop on Interpretable ML for Complex Sys-
tems [6], ICAI 2017 and IJCAIV/ECAI 2018 Workshops on

2169-3536 © 2018 IEEE. Translations and content mining are permitted for academic research only.

52138 Personal use is also permitted, but republication/redistribution requires IEEE permission.

VOLUME 6, 2018

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

Explainable Artificial Intelligence [7], XCI 2017 on Explain-
able Computational Intelligence [8] and IJCNN 2017 on
Explainability of Learning Machines [9]. This year (2018) is
flourishing by a wide range of dedicated workshops to the
topic: CD-MAKE 2018 Workshop on Explainable Artificial
Intelligence [10], ICAPS 2018 Workshop on EXplainable AI
Planning [11], HRI 2018 Workshop on Explainable Robotic
Systems [12], ACM Intelligent User Interfaces (IUD 2018
workshop on Explainable Smart Systems (EXSS 2018) [13],
IPMU 2018 on Advances on Explainable Artificial Intelli-
gence [14], and finally ICCBR 2018 organize XCBR: the
First Workshop On Case-Based Reasoning For The Expla-
nation Of Intelligent Systems [15].

A high-level analysis of XAI’s landscape leads to identify
the key players and influencers behind this intense dynamic.
Indeed, two of the most prominent actors pursuing XAI
research are: (i) a group of academics operating under the
acronym FAT* [4] and (ii) civilian and military researchers
funded by the Defense Advanced Research Projects Agency
(DARPA) [16].

FAT* academics (meaning fairness, accountability, and
transparency in multiple artificial intelligence, machine
learning, computer science, legal, social science, and
policy applications) are primarily focused on promoting
and enabling explainability and fairness in algorithmic
decision-making systems with social and commercial impact.
With over than 500 participants and more than 70 papers,
FAT* conference, which held its fifth annual event in Febru-
ary 2018, brings together annually researchers and practition-
ers interested in fairness, accountability, and transparency in
socio-technical systems.

The other group, DARPA, launched its XAI program
in 2017 with the aim of developing new techniques capable of
making intelligent systems explainable, the program includes
11 projects and will continue running until 2021. DARPA
funded researchers seem primarily interested in increasing
explainability in sophisticated pattern recognition models
needed for security applications. Even though DARPA is
funded by the US Department of Defense, the program
involves researchers drawn from various academic institu-
tions and diverse corporate teams.

Increasing interest in XAI has also been observed in the
industrial community. Companies on the cutting edge of
contributing to make AI more explainable include H2O.ai
with its driverless AI product [17], Microsoft with its next
generation of Azure: Azure ML Workbench.! Kyndi with its
XAI platform for government, financial services, and health-
care,2 and FICO with its Credit Risk Models. To push the
state of XAI even further, FICO is running the Explainable
Machine Learning Challenge (xML challenge) [18]. The goal
of this challenge is to identify new approaches for creating
machine learning based AI models with both high accuracy

! https://azure .microsoft.com/en-us/services/machine-learning-services/
2 https://kyndi.com
3http://www.fico.com

VOLUME 6, 2018

and explainability. On the other hand, Cognilytica has exam-
ined in its “AI Positioning Matrix” (CAPM) the market of
AI products. It proposed a chart where XAI technologies
are arguably identified as high-sophisticated implementations
beyond the threshold of the actual technology [19].

C. CONTRIBUTION AND ORGANIZATION

Motivated by the preceding concerns and observations, this
article collects and shares findings from a comprehensive and
in-depth survey on XAI. In fact, by considering XAI as a
field in terms of its research, we propose to step back for a
holistic view of the present state-of-the-art advancements in
this research field, in order to chart a path toward promising
and suitable directions for future research. Unlike studies,
that focus on specific dimensions of explainability, this work
advocates the multidisciplinary nature of the studied field and
introduces the major aspects and domains of explainability
from different perspectives. A key claim of this paper is that
the issue of explaining Al-powered systems is scientifically
interesting and increasingly important, hence the necessity of
providing a firm basis from the lens of literature to ground
further discussion. The aim is to help interested researchers
to quickly and effectively grasp important facets of the topic
by having a clear idea about key aspects and related body of
research.

In this sense, we make three contributions:

« We propose a comprehensive background regarding
the main concepts, motivations, and implications of
enabling explainability in intelligent systems.

« Based ona literature analysis of 381 papers, we provide
an organized overview of the existing XAI approaches.

« We identify and discuss future research opportunities
and potential trends in this field.

Accordingly, the remainder of the survey is organized
as follows. Section II presents a preliminary background.
Section III surveys the latest developments in the XAI field
and organizes surveyed approaches according to four per-
spectives. Section IV discusses research directions and open
problems that we gathered and distilled from the literature
survey. Finally, Section V concludes this survey.

ll. BACKGROUND

A. UNDERSTANDING XAI: A CONTEXTUAL DEFINITION
XAI is a research field that aims to make AI systems results
more understandable to humans.

The term was first coined in 2004 by Van Lent et al. [20],
to describe the ability of their system to explain the behavior
of Al-controlled entities in simulation games application.

While the term is relatively new, the problem of explain-
ability has existed since the mid-1970s when researchers
studied explanation for expert systems [21]. However,
the pace of progress towards resolving such problem has
slowed down as AI reached an inflection point with the spec-
tacular advances in ML. Since then the focus of AI research
has shifted towards implementing models and algorithms

52139
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

Interest over time for Explainable Artificial Intelligence term

FIGURE 1. Google trends result for research interest of < Explainable Artificial Intelligence > term.

that emphases predictive power while the ability to explain
decision processes has taken a back seat.

Recently, XAI topic has received renewed attention from
academia and practitioners. Figure 1 illustrates the remark-
able resurgence of XAI term research interest using google
trends. The re-emergence of this research topic is the direct
result of the unstoppable penetration of AI/ML across indus-
tries and its crucial impact in critical decision-making pro-
cesses, without being able to provide detailed information
about the chain of reasoning that leads to certain decisions,
recommendations, predictions or actions made by it. There-
fore, the social, ethical and legal pressure calls for new AI
techniques that are capable of making decisions explainable
and understandable.

Technically, there is no standard and generally accepted
definition of explainable AI. Actually, XAI term tends to refer
to the movement, initiatives, and efforts made in response to
AI transparency and trust concerns, more than to a formal
technical concept. Thus, to put some clarification around
this trend, we quote some XAI definitions as seen by those
who are calling for it. According to DARPA [16], XAI aims
to “produce more explainable models, while maintaining a
high level of learning performance (prediction accuracy); and
enable human users to understand, appropriately, trust, and
effectively manage the emerging generation of artificially
intelligent partners’’.

The goal of enabling explainability in ML, as stated by
FAT* [4], “‘is to ensure that algorithmic decisions as well as
any data driving those decisions can be explained to end-users
and other stakeholders in non-technical terms”’.

FICO [19], the organizer of xML Challenge, see XAI as
“an innovation towards opening up the black-box of ML”
and as ‘“‘a challenge to create models and techniques that both
accurate and provide good trustworthy explanation that will
satisfies customers’ needs”’.

For an interested researcher, we believe that it is very
important, to gain a deep understanding of the XAI concept,
beyond colloquial definitions, primary goals and shallow

52140

facts. For this, we propose to explore the big picture of the
key concepts shaping the XAI landscape.

While performing our scan of literature, which will be
detailed in the next section, we conducted a linguistic search
to identify and record relevant terms across research commu-
nities that strongly relate to the concept of XAI. The goal of
this analysis is to gain insights into how research communities
approach explainability and to detect the main concepts that
contribute to define this notion. As a result, the word cloud
shown in Figure 2 provides an intuitive grasp of the XAI’s
scope and allows drawing the big picture of this research field
by highlighting the important related concepts. Important
terms are ordered according to the frequency of its appearance
as keywords in the surveyed papers, and this after filtering
technical terms like deep learning, decision tree, sensitive
analysis etc.

Understandable Al
Comprehensible Al
Accurate Al/ML

Transparent Al Black box

Interpretable ML

a _ Asi Data science
Cognitive science Intelligible ML

Responsable Al

Interactive Al Explainable Al
Ethics

FIGURE 2. XAI word cloud.

In Table 1, we cast insights on a sample of relevant and
common related XAI concepts that we believe help to define
contextually the studied field.

As detailed in Table 1, XAI is not a monolithic concept,
it reflects several distinct related notions. Explainability is
closely related to the concept of interpretability: interpretable

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

TABLE 1. Key related concepts of XAI.

 

Term

Description

 

Interpretable Machine Learning

An interpretable system is a system where a user cannot only see but also study and
understand how inputs are mathematically mapped to outputs [22]. This term is favored over
“explainable” in the ML context where it refers to the capability of understanding the work
logic in ML algorithms.

Researchers often use the two terms “interpretability” and “explainability” synonymously
[23], [24]. Even though there is an acknowledgment for the need of a clear taxonomy [25].
Other authors use other terms such as understandability [26] or comprehensibility [27] to
refer to the same issue, while some industrials [28] prefer the term intelligible AI.

 

Black-box problem

In science, computing, and engineering, the terms black box, gray box and white box are
used with reference to different levels of closure of the component internal essence [29]. In
particular, a black box component does not disclose anything about its internal design,
structure and implementation, whereas its opposite side, a white box component is
completely exposed to its user. In between, there may exist different levels of grey box
components depending upon how much details are available. Commercially, The “black
box” concept has been exploited by technological enterprises, usually in their efforts to
protect intellectual property and maintain competitiveness.

In AI, the difficulty for the system to provide a suitable explanation for how it arrived at an
answer is referred to as “‘the black-box problem.”

 

Responsible Artificial
Intelligence

Responsible AI is an AI that takes into account societal values, moral and ethical
considerations.

Responsible AI has three main pillars: Accountability, Responsibility, and Transparency.
Together, these considerations form the A.R.T. principles for AI [30]:

Accountability refers to the need to explain and justify one’s decisions and actions to its
partners, users and others with whom the system interacts.

Responsibility refers to the role of people themselves and to the capability of AI systems to
answer for one’s decision and identify errors or unexpected results.

Transparency refers to the need to describe, inspect and reproduce the mechanisms through
which AI systems make decisions and learns to adapt to its environment, and to the
governance of the data used created.

Other initiatives focus on some additional considerations such as fairness and ethics in
defining Responsible AI [4].

 

Accurate Artificial Intelligence

Al’s accuracy is a performance metric that refers to the number of correct predictions made
by the model (typically a ML model) over all kinds of predictions made [31].

 

Data science

AI models usually require getting a training and testing set of Data. Data science is a field
that unifies statistics, data analysis, machine learning and their related methods in order to
understand and analyze actual phenomena with data [32].

 

Social science

Explanation is, first and foremost, a form of social interaction. The general discipline of
social science is concerned with society and the relationships among individuals within a
society [33]. Some interesting social sciences theories include causality, systematic
cognitive biases, contrastive explanation, and argumentation.

 

Third-wave AI

Driven by a contextual adaptation, new researches are shaping the so-called third wave AI
(also called Artificial Intelligence 3.0) where AI systems construct explanatory models for
classes of real-world phenomena (XAJ), learn and reason as they encounter new tasks and
situations (Continuous learning) and can establish natural communication with human
(Interactive AI, Human-machine symbiosis, Brain-Computer Interface) [34].

 

Artificial General Intelligence
(AGI)

 

 

AGI was a primary goal of the initial AI field, it is the intelligence of a machine that could
successfully perform any intellectual task that a human being can, AGI is also referred to as
"strong AI", "full AI" or as the ability of a machine to perform "general intelligent
action"[35].

AI research that study machines that can perform actions superior to human intellect are
known as: Artificial Superintelligence (ASD[36].

 

 

systems are explainable if their operations can be under-
stood by human. We note that even though “Explainable”
is a keyword in the XAI appellation, in ML community

VOLUME 6, 2018

the term “‘interpretable’’ is more used than ‘“Explainable”’.
Figure 3 confirms this observation, it shows trends regarding
the use of the two terms in both scientific and public settings.

52141
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

ntific community Use in public setting

 

ble Al/ ML

FIGURE 3. Google trends result for comparing the use of “Explainable”
and “Interpretable” according to the context.

Furthermore, it should be noted that none of the afore-
mentioned variation terms (understandable, comprehensible,
intelligible...) is enough specific to enable formalization.
They implicitly depend on the user’s expertise, preferences
and other contextual variables.

XATis centered on the challenge of demystifying the black
boxes, it also implies Responsible AI as it can helps to pro-
duce transparent models. This should happen without affect-
ing the AI models accuracy, thus in AI in general and in ML
specifically, often a tradeoff must be made between accuracy
and interpretability. An obvious link with data science field
arises as accuracy is closely tied to the quality and the quantity
of the training data.

Rarely in literature, we come across the term “‘social sci-
ence”’ or it derivative, yet explanation is a form of social
interaction and clearly, it has psychological, cognitive and
philosophical projections. Based on the conducted analysis,
ideas from social science and human behavior are not suffi-
ciently visible in this field.

Finally, XAI is a part of a new generation of AI technolo-
gies called the third wave AI, one of the objectives of this
ambition “‘wave’’ is to precisely generate algorithms than
can explain themselves. Ultimately, all this culminates in the
quest for reaching human intelligence level, a goal known as
AGI.

Based on this terms analysis, we built a unified and
structured view of the main concepts related to XAI field

  
    
 

    
   

  
 

Human
Science

Data

Trade-off Science

 

Explainable Artificial Intelligence

interpretable, Comprehensible
Understandable, Intelligible Al{ML

Accurate Artificial Intelligence

 

 

 

(illustrated in Figure 4). We believe that aiming holism in
approaching XAI concept, helps researchers to quickly be
initiated about the topic and its context. Moreover, knowing
the main keywords used in the field and the variation of

terms that are relatively referring to the same concepts,
represent a helpful prerequisite to conduct a relevant and
fruitful research.

B. USING XAI: THE NEED AND THE APPLICATION
OPPORTUNITIES

1) THE NEED FOR XAl

For commercial benefits, for ethics concerns or for regulatory
considerations, XAI is essential if users are to understand,
appropriately trust, and effectively manage AI results. Based
on the explored literature, the need for explaining AI systems
may stem from (at least) four reasons, although it may appear
that there is an overlap between these four reasons, from
our standpoint, they capture the different motivations for
explainability.

a: EXPLAIN TO JUSTIFY

The past several years have seen multiple controversies over
AI/ML enabled systems yielding biased or discriminatory
results [37], [38]. That implies an increasing need for expla-
nations to ensure that AI based decisions were not made
erroneously. When we talk about an explanation for a deci-
sion, we generally mean the need for reasons or justifications
for that particular outcome, rather than a description of the
inner workings or the logic of reasoning behind the decision-
making process in general.

Using XAI systems provides the required information to
justify results, particularly when unexpected decisions are
made. It also ensures that there is an auditable and provable
way to defend algorithmic decisions as being fair and ethical,
which leads to building trust.

Furthermore, henceforth AI needs to provide justifications
in order to be in compliance with legislation, for instance the
“right to explanation’’, which is a regulation included in the

 

 

 

Human- Machine
Collaboration

 

 

Continuous learning

BAB PAIYL Ie

  

 

Contributes to

 

 

Responsible Artificial intelligence

Accountable, Transparent,
Fairness, Intelligible, Ethics

 

 

FIGURE 4. A schematic view of XAI related concepts.

52142

 

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

General Data Protection Regulation (GDPR) that comes into
effect across the EU on May 25, 2018 [39].

b: EXPLAIN TO CONTROL

Explainability is not just important for justifying decisions.
It can also help prevent things from going wrong. Indeed,
understanding more about system behavior provides greater
visibility over unknown vulnerabilities and flaws, and helps
to rapidly identify and correct errors in low criticality situa-
tions (debugging). Thus enabling an enhanced control.

c: EXPLAIN TO IMPROVE

Another reason for building explainable models is the need to
continuously improve them. A model that can be explained
and understood is one that can be more easily improved.
Because users know why the system produced specific out-
puts, they will also know how to make it smarter. Thus, XAT
could be the foundation for ongoing iteration and improve-
ment between human and machine.

d: EXPLAIN TO DISCOVER

Asking for explanations is a helpful tool to learn new facts,
to gather information and thus to gain knowledge. Only
explainable systems can be useful for that. For example,
given that AlphaGo Zero [40] can excel at the game of Go
much better than human players, it would be desirable that
the machine can explain its learned strategy (knowledge) to
us. So It will come as no surprise if, in future, XAI models
taught us about new and hidden laws in biology, chemistry
and physics.

We conclude that explainability is a powerful tool for
justifying AI based decisions. It can help to verify predictions,
for improving models, and for gaining new insights into the
problem at hand. Which leads towards more trustworthy AI
systems (Figure 5).

 
  
    

Explain to Explain to
justify control \

 

 

Explain to
discover

Explain to
improve

 
   
 

 

 

FIGURE 5. Reasons for XAl.

Even though academics and practitioners approve on the
importance of XAI, not everyone agrees that there is a

VOLUME 6, 2018

pressing need for greater interpretability in AI systems.
At this regard, the value of XAI was called into question
recently by Google research director Norvig [41], who noted
that humans are not very good at explaining their decisions
either, and claimed that the credibility of an AI system results
could be gauged simply by observing its outputs over time.
The AI powerhouse company researcher has stressed, indeed,
an important point. Certainly, explainability is an essential
property; however, it is not always a necessity. In fact, requir-
ing every AI system to explain every decision could result
in less efficient systems, forced design choices, and a bias
towards explainable, but less capable and versatile outcomes.
Furthermore, making AI systems explainable is undoubtedly
expensive; they require considerable resources both in the
development of the AI system and in the way it is interrogated
in practice. Thus, it is important to think about why and
when explanations are useful. The need for explainability
depends on: (a) The degree of functional opacity caused by
the complexity of AI algorithms: if it is low, no high level
of interpretability is required. (b) The degree of resistance
of the application domain to errors. If it has high resis-
tance, unexpected errors are acceptable. For an AI system
for targeted advertising, for example, a relatively low level of
interpretability could suffice, as the consequences of it going
wrong are negligible. On the other hand, the interpretability
for an Al-based diagnosis system would be significantly
higher. Any errors could not only harm the patient but also
deter adoption of such systems. Therefore, any domain where
the cost of making a wrong prediction is very high present a
potential application domain of XAI approaches.

2) XA APPLICATION DOMAINS

Interestingly, XAI can bring significant benefit to a large
range of domains relying on AI systems. Herein we explore
some potential domains where there is a need for a research
work on explainable models.

a: TRANSPORTATION
Automated vehicles hold the promise for decreasing traf-
fic deaths and providing enhanced mobility but also pose
challenges in addressing the explainability of AI decisions.
Autonomous vehicles have to make split-second decisions
based on how they classify the objects in the scene in front of
them. If a self-driving car suddenly acts abnormally because
of some misclassification problem. The consequence can be
dangerous. This is not a possibility this is already happening,
recently, a self-driving Uber killed a woman in Arizona.
It was the first known fatality involving a fully autonomous
vehicle. The information reported anonymous sources who
claimed the car’s software registered an object in front of
the vehicle, but treated it in the same way it would a plastic
bag or tumbleweed carried on the wind [42]. Only an explain-
able system can clarify the ambiguous circumstances of such
situation and eventually prevent it from happening.
Transportation is a potential application domain of the
XAI. Works towards explaining self- driving vehicle behavior

52143
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

has been already started [43], [44], but there is a long way
to go.

6b: HEALTHCARE

Medical diagnosis model is responsible for human life. How
can we be confident enough to treat a patient as instructed by
a black-box model?

In the mid-1990s, an artificial neural network (ANN)
was trained to predict which pneumonia patients should be
admitted to hospitals and which treated as outpatients. Ini-
tial findings indicated neural nets were far more accurate
than classical statistical methods. However, after an exten-
sive test, it turned out that the neural net had inferred that
pneumonia patients with asthma have a lower risk of dying,
and should not be admitted. Medically, this is counterin-
tuitive however, it reflected a real pattern in the training
data—asthma patients with pneumonia usually were admit-
ted not only to the hospital but directly to the ICU, treated
aggressively, and survived [37]. It was then decided to aban-
don the AI system because it was too dangerous to use it
clinically. Only by interpreting the model, we can discover
such a crucial problem and avoid it. Recently, researchers
have conducted preliminary work aiming to make clinical
AlJ-based systems explainable [37], [45]-[47]. The increas-
ing number of these works confirms the challenge and
the interest of applying XAI approaches on the healthcare
domain.

c: LEGAL

In criminal justice, AI has the potential to better assess risks
for recidivism and reduce costs associated with both crime
and incarceration. However, when using a criminal decision
model to predict the risk of recidivism at the court, we have
to make sure the model behaves in an equitable, honest
and nondiscriminatory manner. In Loomis v. Wisconsin [48],
the case challenged the use of proprietary, closed source risk
assessment software in sentencing Mr. Loomis to prison. The
case alleged that the software ‘‘Correctional Offender Man-
agement Profiling for Alternative Sanctions: COMPAS” [49]
violates the process rights by taking gender and race into
account. The algorithms used were considered trade secrets
and the causal audit process was not clearly known to the
Judge.

Transparency of how a decision is made is a neces-
sity in this critical domain, yet very few works are made
towards making automated decision making in legal system
explainable [49]-[51].

d: FINANCE

In financial services, benefits of using AI tools include
improvements related to wealth-management activities,
access to investment advice, and customer service. However,
these tools also pose questions around data security and fair
lending. Indeed, the financial industry is highly regulated and
loan issuers are required by law to make fair decisions. Thus,
one significant challenge of using AI-based systems, in credit

52144

scores and models, is that it’s harder to provide the needed
“reason code” to borrowers — the explanation of why they
were denied credit. Especially when the basis for denial is the
output from an opaque ML algorithm. Some credit bureaus
agency such as Equifax and Experian are working on promis-
ing research projects to generate automated reason codes and
make AI credit-based score decisions more explainable and
auditor friendly [52].

e: MILITARY

Originally, the current famous XAI’s initiative is made by
military researchers [16], and the growing visibility of XAI
topic is due largely to the call for research and the solicitation
of DAPRA Projects.

Unsurprisingly, AI in the military arena also suffers from
the AI explainability problem. In a report from MIT Tech-
nology review, Knight [53] delves into the challenges of
relying on autonomous systems for military operations. As in
the healthcare domain, this often involves life and death
decisions, which again leads to similar types of ethical and
legal dilemmas. The academic AI research community is
well represented in this application domain with the DAPRA
Ambitious XAI program, along with some research initiatives
that studies explainability in this domain [54].

The line of work made in each of the discussed domains
confirms the need of XAI. However, such works are only in
their infancy, hearty research effort is yet to be done.

Moreover, XAI can find an interesting application in others
domains like cybersecurity, education, entertainment, gov-
ernment, image recognition etc. An interesting chart of poten-
tial harms from automated decision-making was presented by
Future of Privacy Forum [55], it depicts the various spheres of
life where automated decision-making can cause injury and
where providing automated explanation can turn them to a
trustful processes, this includes employment, insurance and
social benefit, housing and differential pricing of goods and
services.

C. ENABLING XAI: THE TECHNICAL CHALLENGE

Clearly, the awareness and demand for explainability are
growing in various domains, hence the question as “why the
use of XAI is not systematic?’ or more simply ‘“‘why is not
everyone using XAI?’’.

In fact, bringing interpretability to AI systems is a very
challenging technical issue. Explainability of intelligent sys-
tems has run the gamut from traditional expert systems, which
are totally explainable but inflexible and hard to use, to Deep
Neural Networks (DNN), which are effective but virtually
impossible to see inside.

If we look back at the expert systems of the 80’s, we had
what we would consider a scrutable system: an inference
engine leveraged a knowledge base to make assertions that
it could explain using the chain of reasoning that led to
the assertion [56]. Explanation capabilities are frequently
the most significant benefit provided by an expert system,
but these systems were completely built on subject matter

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

expertise and while powerful, were somewhat inflexible.
Moreover, though significant progress was made on explain-
ability during this period, with solid principles established,
however the “explainability’’ problem was not considered to
have been completely solved [57].

Modem machine learning algorithms go to the opposite
end of the spectrum, yielding systems capable of working
purely from observations and creating their own represen-
tations of the world on which to base their predictions.
Nevertheless, the complexity that bestows the extraordinary
predictive abilities on ML algorithms also makes the results
the algorithms produce hard to understand. Indeed, ML algo-
rithms are difficult to interpret, because of their structure and
the way they are working. ML algorithms intrinsically con-
sider high-degree interactions between input features, which
make disaggregating such functions into human understand-
able form difficult. We take as an example the DNN, the most
successful contemporary ML model. DNN has a generic
multi-layer nonlinear structure consisting of many hidden
layers and numerous number of neurons per layer, such archi-
tecture helps to produce high-level prediction through multi-
ple levels of linear transformations and non-linear activations.
While a single linear transformation may be interpreted by
looking at the weights from the input features to each of the
output classes, multiple layers with non-linear interactions at
every layer imply disentangling a super complicated nested
structure which is a difficult task and potentially even a
questionable one [58].

Another perception of ML interpretability’s technical chal-
lenge is explained by Hall and Gill [59] in their introduction
to Machine Learning Interpretability book. They present the
mathematical problems in interpretable ML so-called “the
multiplicity of good models’’ [60]. As mentioned before,
given the complicated structure of ML models, for the same
set of input variables and prediction targets, complex machine
learning algorithms can produce multiple accurate models by
taking very similar but not the same internal pathway in the
network, so details of explanations can also change across
multiple accurate models. This systematic instability makes
automated generated explanations difficult.

Arguing that AI/ML interpretability is a challenging issue,
does not mean that all AI/ML techniques have the same
level of opacity. Indeed, there are algorithms that are more
interpretable than others are, and there is often a tradeoff
between accuracy and interpretability: the most accurate
AI/ML models usually are not very explainable (for example,
deep neural nets, boosted trees, random forests, and support
vector machines), and the most interpretable models usually
are less accurate (for example, linear or logistic regression).
Rather than being a static tradeoff, accuracy along with inter-
pretability is a dynamic target that ongoing researches try to
reach, as it will be discussed in Section IIIB.

Furthermore, beyond the technical challenge, given the
goal, the nature and the implications of XAI, progress towards
overcoming this challenge, can only be achieved through
interdisciplinary collaboration, where expertise and theories

VOLUME 6, 2018

from different research fields are combined and methods and
techniques are developed from multiple perspectives to move
research forward. From our standpoint, enabling technolo-
gies and methods for XAI potentially belong to four basic
research areas: (i) Data science: AI/ML algorithms are data
hungry, they need more data to produce better predictions and
decisions. The backward path that targets to produce better
explanation and justification eventually also depends on that
data. Data science is then a core element in the explainabil-
ity process. (ii) Artificial Intelligence/Machine Learning:
to generate explanation we need a computational process,
we claim that using AI/ML as a computational process to
explain AI/ML is an interesting work trail. (ii) Human
science:to produce artificial explanations, it is worth first to
models how humans explain decisions and behavior to each
other [33]. Therefore, approaching theories from human sci-
ence can lead to innovative explainable models. (V) Human
Computer Interaction (HCD): the user’ understanding and
trust of the system partly depends on the way he interacts
with the machine. Given HCI’s [61] core interest in technol-
ogy that entails understanding and better empowering users,
techniques from this research field can help in developing
transparent systems.

Focusing on the What, the Why, the Where and the How,
we tried to propose an extensive background regarding XAI
by defining the concept of XAI, exposing the motivation
behind its reemergence, identifying the segments of the mar-
ket where the results are promising, and finally presenting
some potential research areas that could potentially contribute
to overcome the technical challenge related to XAI systems.
The next section aims to capture researchers’ attention on the
growing research body of XAI through a literature survey.

Ti, REVIEW

A. RELATED SURVEYS

Despite the fact that the volume of research in interpretable
and explainable AI is quickly expanding, a holistic survey
and a systematic classification of these research works are
missing. Indeed, according to the literature, there are few
review papers in this field.

Two inescapable position papers are [25] and [62] that
try to formalize the concept of explainability. The former
attempted to provide a taxonomy of both the desiderata and
methods in interpretability research. Lipton’s work is not
a survey in itself but it provides a solid discussion about
what might constitute interpretability through the lens of the
literature.

The survey of Doshi-Velez and Kim tried to define tax-
onomies and best practices for interpretability as a “rigorous
science’. The main contribution of this paper is a taxonomy
of interpretability evaluation. In doing so, the authors shifted
the focus on only one dimension of expandability: its mea-
surement.

In their survey, Abdul er ai. [61] analyzed a sizable liter-
ature of explainable research based on 289 core papers and

52145
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

120

BK
oS
oO

80

NUMBER OF ARTICLES
nn
o

40
20
0
se s gv Ss Si se S So S oe 2 & S

FIGURE 6. Surveyed articles by year (2004-2018).

12 412 citing papers and built a citation network. However,
this work focus mainly on setting an HCI research agenda in
explainability.

A recent survey by Guidotti et al. [63] reviewed methods
for explaining black-box models at a large scale including
data mining as well as machine learning. They presented a
detailed taxonomy of explainability methods according to the
type of problem faced. Even though the survey considered
holism in terms of models (it discusses all black-box models),
it emphasized only interpretability’s mechanisms, ignoring
by this other explainability dimensions such as evaluation.
Hence, the detailed technical overview of surveyed methods
makes it hard to get a quick understanding of the explanation
methods space.

Finally, Dosilovic er al. [64] proposed a general overview
of the topic in their conference paper. They presented the
advances on explainability in machine learning models under
the supervised learning paradigm, with a particular focus on
DNN.

In contrast of existing surveys that focus on particular
aspects of explainability, our survey provides a comprehen-
sive and an organized overview of XAI research contributions
from different perspectives. We target holism and clarity in
exploring and exposing explainable approaches space.

B. A HOLISTIC SURVEY

We conducted an extensive literature review by examin-
ing relevant papers from six major academic databases:
SCOPUS, IEEExplore, ACM Digital Library, Google
Scholar, Citeseer Library and ScienceDirect, in addition of
preprints posted on arXiv. A keyword based search was
used to select papers, it consists of searching for index
keywords based on common variations in literature of
the terms “‘intelligible’’, “interpretable”, “transparency”,
“black box’”’,‘‘understandable”’,“‘comprehensible” and

52146

YEAR OF PUBLICATION

“explainable” and its union with a set of terms related to
Al including ‘Artificial Intelligence’’, “Intelligent system’’
and “Machine learning”’, or terms referring to ML algorithms
such as: “deep learning”, “classifier”, ‘decision tree’’.
As we are mainly interested in recent advances in this field,
the research was restricted to articles published between
2004 and 2018. The gathered papers were then scanned
based on the titles, abstracts and keywords to determine
relevant articles for further analysis. The list of selected
papers was largely completed afterwards, by using a back-
ward and forward snowballing strategy that consists of using
the reference list of the selected papers and the citations to
these papers to identify additional papers [65]. The final list
of papers includes 381 papers. The publication timeline of
these papers is shown in Figure 6, it illustrates the recent
exponential increase of papers in this field.

Next, we will present a brief pointer to the relevant works
by approaching them from different perspectives.

In a broad stroke, we describe the XAI space along
four main axes, each one spanning its own spectrum and
together they shape holistically XAI research landscape.
Indeed, the aim is to present a comprehensive and holistic
analysis of the state of art of this field by projecting works
on four complementary axes. We do not intend to enumerate
all the surveyed papers. However, we try to meet two criteria.
The papers included in our discussion (a) are deemed to be
a significant work (received high citation level) and (b) have
good coverage of the corresponding axis.

AXIS I. XAI METHODS TAXONOMY:

EXPLAINABILITY STRATEGIES

In the quest to make AI system explainable, several expla-
nation methods and strategies have been proposed in rela-
tively short period, especially for ML algorithms. In this axis,
we propose an overview of existing interpretability methods.

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

The majority of works discuss explainability in ML algo-
rithms and thus, it is “interpretability” term that will usually
be used.

Based on the conducted survey of the literature, we arrive
to classify the methods according to three criteria: (1) the com-
plexity of interpretability, (ii) the scoop of interpretability,
and (iii) the level of dependency from the used ML model.

We note that, since explainability in AI is still an emerging
field, the classes of methods belonging to the proposed taxon-
omy are neither mutually exclusive nor exhaustive. However,
this can be a good yardstick to compare and contrast across
multiple methods.

In the following subsections, we will describe the main fea-
tures of each class and give examples from current research.

A. COMPLEXITY RELATED METHODS

The complexity of a machine-learning model is directly
related to its interpretability. Generally, the more complex
the model, the more difficult it is to interpret and explain.
Thus, the most straightforward way to get to interpretable
AI/ML would be to design an algorithm that is inherently and
intrinsically interpretable. Many papers support this classic
approach, to name a few:

Letham er al. [66] proposed a model called Bayesian Rule
Lists (BRL) based on decision tree, the authors claimed that
preliminary interpretable models provide a concise and con-
vincing capabilities to gain domain experts trust.

Caruana er al. [37] described an application of a learning
method based on generalized additive models to the pneu-
monia problem. They proved the intelligibility of their model
through case studies on real medical data.

Xu et al. [67] introduced an attention based model that
automatically learns to describe the content of images. They
showed through visualization how the model is able to inter-
pret the results.

Ustun and Rudin [68] presented a sparse linear models
for creating a data-driven scoring systems called SLIM,
the results of this work highlight the interpretability capabil-
ity of the proposed system to provide users with qualitative
understanding due to their high level of sparsity and small
integer coefficients.

A common challenge, which hinders the usability of this
class of methods, is the tradeoff between interpretability and
accuracy [69]. As noted by Breiman [70] ‘accuracy generally
requires more complex prediction methods ...[and] simple
and interpretable functions do not make the most accurate
predictors”. In a sense, intrinsic interpretable models come
at a cost of accuracy.

An alternative approach to interpretability in machine
learning is to construct a high complex uninterpretable
black-box model with high accuracy and subsequently use a
separate set of techniques to perform, what we could define as
a reverse engineering process to provide the needed explana-
tions without altering or even knowing the inner works of the
original model. This class of methods offers then a post-hoc
explanation [25]. Though it could be significantly complex

VOLUME 6, 2018

and costly, most recent works done in XAI field belong to
post-hoc class, it includes natural language explanations [71],
visualizations of learned models [72], and explanations by
example [73]. More approaches will be presented in detail
in the next subsections.

In light of this, we can conclude that the overall utility value
of interpretability depends on the nature of the prediction
task. As long as the model is accurate for the task, and
uses a reasonably restricted number of internal components,
intrinsic interpretable models are sufficient. If otherwise the
prediction target involved complex and highly accurate mod-
els, considering post-hoc interpretation models is necessary.

It should also be noted that in literature there is a group of
intrinsic methods for complex uninterpretable models. These
methods aim to modify the internal structure of a complex
black-box model that are not primarily interpretable - typ-
ically a DNN- to mitigated their opacity and thus improve
their interpretability [74]. The used methods may either be
components that add additional capabilities, components that
belong to the model architecture [75], [76], e.g. as part of
the loss function [77], or as part of the architecture structure,
in terms of operations between layers [78], [79].

B. SCOOP RELATED METHODS

Interpretability implies understanding an automated model,
this supports two variations according to the scoop of inter-
pretability: understanding the entire model behavior or under-
standing a single prediction. In the studied literature,
contributions are made in both directions. Accordingly,
we distinguish between two subclasses: (i) Global inter-
pretability and (ii) Local interpretability.

1) GLOBAL INTERPRETABILITY

Global interpretability facilitates the understanding of the
whole logic of a model and follows the entire reasoning
leading to all the different possible outcomes. This class
of methods is helpful when ML models are crucial to
inform population level decisions, such as drugs consumption
trends or a climatic change [80]. In such cases, a global effect
estimate would be more helpful than many explanations for
all the possible idiosyncrasies.

Works that propose globally interpretable models include
the aforementioned additive models for predicting pneumo-
nia risk [37] and rule sets generated from sparse Bayesian
generative model [66]. However, these models are usually
specifically structured thus limited in predictability to pre-
serve interpretability.

Yang et al. [80] proposed a global model interpretation
via recursive partitioning called (GIRP) to build a global
interpretation tree for a wide range of ML models based
on their local explanations. In their experiments, the authors
highlighted that their method can discover whether a partic-
ular ML model is behaving in a reasonable way or overfit to
some unreasonable pattern.

Valenzuela-Escarcega et al. [81] proposed a supervised
approach for information extraction, which provides a global,

52147
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

deterministic interpretation. This work supports the idea that
representation learning can be successfully combined with
traditional, pattern-based bootstrapping yielding models that
are interpretable.

Nguyen et al. [82] proposed an approach based on acti-
vation maximization—synthesizing the preferred inputs for
neurons in neural networks—via a learned prior in the form
of a deep generator network to produce a global interpretable
model for image recognition. Activation maximization tech-
nique was previously used by Erhan et al. [83].

Even though a multitude of techniques is used in litera-
ture to enable global interpretability. Arguably, global model
interpretability is hard to achieve in practice, especially for
models that exceed a handful of parameters. Analogically to
human, who focus effort on only part of the model in order
to comprehend the whole of it, local interpretability can be
more readily applicable.

2) LOCAL INTERPRETABILITY

Explaining the reasons for a specific decision or single pre-
diction means that interpretability is occurring locally. This
scoop of interpretability is used to generate an individual
explanation, generally, to justify why the model made a spe-
cific decision for an instance. Several explored papers pro-
pose local explanation methods. We provide next an overview
of the explanation methods examined in reviewed papers.

Ribeiro et al. [84] proposed LIME for Local Interpretable
Model-Agnostic Explanation. This model can approximate
a black-box model locally in the neighborhood of any pre-
diction of interest. Newer, related, and highly anticipated
work from the creators of LIME, called anchors [85], extends
LIME using decision rules. In the same vein, leave-one-
covariate-out (LOCO) [86] is another popular technique for
generating local explanation models that offer local variable
importance measures.

Another attempt to produce local explanations is made
by Baehrens er al. [87]. In this work, the authors presented
a method capable of explaining the local decision taken
by arbitrary nonlinear classification algorithms, using the
local gradients that characterize how a data point has to
be moved to change its predicted label. By following this
line of work, we find a set of works using similar methods
for image classification models [88]-[91]. Actually, it is a
common approach to understanding the decisions of image
classification systems by finding regions of an image that
were particularly influential to the final classification. Also
called sensitivity maps, saliency maps, or pixel attribution
maps [92], these approaches use occlusion techniques or cal-
culations with gradients to assign an “‘importance’’ value to
individual pixels which are meant to reflect their influence on
the final classification.

Based on the decomposition of a model’s predictions on
individual contributions of each feature, Robnik-Sikonja and
Kononenko [93] proposed to explain the model prediction
for one instance by measuring the difference between the
original prediction and the one made with omitting a set

52148

of features. Recent works that use decomposition to explain
locally include [94] and [95].

While there are several different techniques for obtaining
local-explanations [96]-[100], recent work by Lundberg and
Lee [101] have shown that there are equivalences among
these techniques. They introduced a promising newer tech-
nique with solid theoretical support called Shapely Explana-
tions that unifies local approaches.

An interesting and promising line of work is focusing on
combining the strength and the benefits of both local and
global interpretability. Examples include [102]-[104]. The
four possible combinations are: (i) The standard global model
interpretability answers how does the model make predic-
tions. (ii) Global model interpretability on a modular level
identifies how do parts of the model influence predictions.
(iii) Local interpretability for a group of predictions indicates
why did the model make specific decisions for a group of
instances. (iv) And finally, the usual local interpretability for
a single prediction used to justify why did the model make a
specific decision for an instance [105].

Another observation that should be noted is that in the
reviewed literature, local explanations is the most used meth-
ods to generate explanations in DNNs. However, even though
these approaches are developed to explain neural networks,
authors usually underline that their approaches can be poten-
tially adopted to explain any kind of model, which means they
are agnostic models. Another way to classify explanations
method that will be explored next.

C. MODEL RELATED METHODS
Another important way to classify model interpretability
techniques is whether they are model agnostic, meaning they
can be applied to any types of ML algorithms, or model
specific, meaning techniques that are applicable only for a
single type or class of algorithm.

1) MODEL-SPECIFIC INTERPRETABILITY

Model-specific interpretability methods are limited to spe-
cific model classes. Intrinsic methods are by definition
model-specific. The drawback of this practice is that when
we require a particular type of interpretation, we are limited
in terms of choice to models that provide it, potentially at
the expense of using a more predictive and representative
model. Therefore, there has been a recent surge in inter-
est in model-agnostic interpretability methods as they are
model-free.

2) MODEL-AGNOSTIC INTERPRETABILITY

Model-agnostic methods are not tied to a particular type of
ML model. In other words, this class of methods separates
prediction from explanation. Model-agnostic interpretations
are usually post-hoc, they are generally used to interpret ANN
and could be local or global interpretable models. In the inter-
est of improving interpretability AI models, a large amount of
model-agnostic methods have been developed recently using
range techniques from statistics, machine learning and data

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

science. Since the reviewed papers lie mostly in this class,
we present herein an overview of the studied works grouped
by techniques. These broadly fall into four technique types:
(i) Visualization, (ii) Knowledge extraction, (iii) Influence
methods and (iv) Example-based explanation.

a: VISUALIZATION

A natural idea to understand a ML model, especially DNN,
is to visualize its representations to explore the pattern hid-
den inside a neural unit. Unsurprisingly, a consistent body
of research investigates this way with the help of diverse
visualization techniques in order to see inside these black
boxes. Visualization techniques are essentially applied to
supervised learning models. Amongst the reviewed literature,
the popular visualization techniques are: (i) Surrogate mod-
els, (ii) Partial Dependence Plot (PDP) and (iii) Individual
Conditional Expectation (ICE).

i) SURROGATE MODELS

A surrogate model is a simple model used to explain a com-
plex model. More specifically, it is an interpretable model
(like a linear model or decision tree) which is trained on
the predictions of the original black-box model in order to
interpret the latter. However, there are almost no theoret-
ical guarantees that the simple surrogate model is highly
representative of the more complex model. The aforemen-
tioned LIME [84] approach is a prescribed method for
building local surrogate models around single observations.
Bastani et al. [106] used a surrogate model approach where
they extract a decision tree that represents model behavior.
Another remarkable work by Thiagarajan et al. [107] pro-
posed an approach for building Tree View visualizations using
a surrogate model.

ii) PARTIAL DEPENDENCE PLOT (PDP)

PDP is a graphical representation that helps visualizing the
average partial relationship between one or more input vari-
ables and the predictions of a black-box model. Works that
use PDP to understand supervised learning model include:
Green and Kern [108] who used PDPs to understand the
relationship between predictors and the conditional average
treatment effect for a voter mobilization experiment, with
the predictions being made by Bayesian Additive Regression
Trees (Chipman et al. [109]). In the ecological literature,
Elith et ai. [110], who rely on stochastic gradient boost-
ing, used PDPs to understand how different environmental
factors influence the distribution of a particular freshwater.
Berk and Bleich [51] demonstrated the advantage of using
Random Forests and the associated PDPs to accurately model
predictor-response relationships under asymmetric classifica-
tion costs that often arise in criminal justice settings. Recently
Welling et al. [111] proposed a methodology called For-
est Floor, to visualize and interpret random forest models,
the proposed techniques rely on the feature contributions
method rather than PDP. As argued by the authors the advan-
tages of Forest Floor over PDP is that interactions are not

VOLUME 6, 2018

masked by averaging. Thus, it is possible to locate interac-
tions, which are not visualized in a given projection.

iii) INDIVIDUAL CONDITIONAL EXPECTATION (ICE)

ICE plots extend PDP, whereas PD plots provide a coarse
view of a model’s workings, ICE plots reveal interac-
tions and individual differences by disaggregating the PDP
output. Recent works use ICE rather than the classi-
cal PDP. For instance, Goldstein et al. [112] introduced
ICE techniques and proved it advantage over PDP. Later
Casalicchio et al. [113] proposed a local feature importance
based approach that uses both partial importance (PI) and
individual conditional importance (ICD plots as visual tools.

b: KNOWLEDGE EXTRACTION

It is difficult to explain how ML models work, especially
when the models are based on ANN. indeed, as cited before,
multilayer feedforward networks are universal approxima-
tors. However, since learning algorithms modify cells in the
hidden layer, this may constitute interesting internal represen-
tations. The task of extracting explanations from the network
is therefore to extract, in a comprehensible form, the knowl-
edge acquired by an ANN during training and encoded as an
internal representation.

In the explored literature, several works propose methods
to extract the knowledge embedded in the ANN that mainly
rely on two techniques: (1) Rule Extraction and (ii) Model
Distillation.

i) RULE EXTRACTION

One effort to gain insight into highly complex models is the
use of rule extraction [114]-[116]. Works supporting this
technique propose approaches that provide a symbolic and
comprehensible description of the knowledge learned by the
network during its training by extracting rules that approx-
imate the decision-making process in ANN by utilizing the
input and output of the ANN. Which is, by the way, the kind
of knowledge used in traditional artificial intelligence expert
systems. The survey by Ras [74] had taken on the classifi-
cation of rule extraction strategies proposed earlier in [117]
and [118] and proposed three modes to extract rules: (a) ped-
agogical rule extraction, (b) decompositional rule extraction
and (c) eclectic Rule-Extraction.

Decompositional approaches focus on extracting rules at
the level of individual units within the trained ANN, i.e.
the view of the underlying ANN is one of transparency
(e.g. [93]-[95]). While pedagogical approaches treat the
trained ANN as a black-box i.e. the view of the underlying
ANN is opaque, the Orthogonal Search-based Rule Extrac-
tion algorithm (OSRE) from [119] is a successful pedagog-
ical methodology often applied in biomedicine. The third
type (eclectic) is a hybrid approach for rule extraction that
incorporates elements of both the decompositionnel and ped-
agogical rule-extraction techniques [120].

52149
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

ii) MODEL DISTILLATION

Another technique that falls in the knowledge extraction
category is model distillation. Distillation is a model com-
pression to transfer information (dark knowledge) from deep
networks (the ‘“‘teacher’’) to shallow networks (the “‘stu-
dent’’) [121], [122]. Model compression was originally pro-
posed to reduce the computational cost of a model at runtime
but has later been applied for interpretability.

Tan et al. [49] investigated how model distillation
can be used to distill complex models into transpar-
ent models. Che et al. [123] introduced in their paper a
knowledge-distillation approach called Interpretable Mimic
Learning, to learn interpretable phenotype features for mak-
ing robust prediction while mimicking the performance of
deep learning models. A recent work by Xu ef al. [124]
presented DarkSight, a visualization method for interpreting
the predictions of a black-box classifier on a data set in a
way inspired by the notion of dark knowledge. The pro-
posed method combines ideas from knowledge distillation,
dimension reduction, and visualization of DNN. Interested
researchers can also consult these [125]-[127] for further
details about this technique.

c: INFLUENCE METHODS

This type of techniques estimates the importance or the rel-
evance of a feature by changing the input or internal com-
ponents and recording how much the changes affect model
performance. Influence techniques are often visualized. In the
reviewed literature, there are three alternative methods to
obtain input variable’s relevance: (i) Sensitivity analysis,
(ii) Layer-wise relevance propagation and (iii) Feature
Importance.

i) SENSITIVITY ANALYSIS

Sensitivity refers to how an ANN output is influenced by its
input and/or weight perturbations [128]. It is used to verify
whether model behavior and outputs remain stable when data
is intentionally perturbed or other changes are simulated in
data. Visualizing the results of sensitivity analysis (SA) is
considered an agnostic explanation technique, since display-
ing models stability as data change over time enhance trust
in machine learning results. SA has been increasingly used in
explaining ANN in general and DNN classification of images
in particular [129] and [130]. However, it is important to note
that SA does not produce an explanation of the function value
itself, but rather a variation of it. The purpose of performing
a SA is thus usually not to actually explain the relationship
found. Instead, SA is generally used to test models for sta-
bility and trustworthiness, either as a tool to find and remove
unimportant input attributes or as a starting point for some
more powerful explanation technique (e.g. decomposition).

ii) LAYER-WISE RELEVANCE PROPAGATION (LRP)
Another technique to compute relevances was proposed
in [131] as the Layer-wise Relevance Propagation algorithm.

52150

LRP redistributes prediction function backwards, starting
from the output layer of the network and backpropagating
up to the input layer. The key property of this redistribution
process is referred to as relevance conservation. In contrast to
SA, this method explains predictions relative to the state of
maximum uncertainty, i.e. it identifies properties which are
pivotal for the prediction “rooster’’.

iif) FEATURE IMPORTANCE

Variable importance quantifies the contribution of each input
variable (feature) to the predictions of a complex ML model.
The increase of the model’s prediction error is calculated after
permuting the feature in order to measure a feature’s impor-
tance. Permuting the values of important features increases
the model error. While permuting the values of unimportant
features are ignored by the model and thus keeps model
error unchanged. Based on this technique, Fisher et al. [132]
proposed a model-agnostic version of the feature importance
called Model Class Reliance (MCR). While the aforemen-
tioned work [113] proposed a local version of the feature
importance called SFIMP for permutation-based shapley fea-
ture importance. LOCO [86] use a local feature importance as
well.

d: EXAMPLE-BASED EXPLANATION
Example-based explanation techniques select particular
instances of the dataset to explain the behavior of machine
learning models. Example-based explanations are mostly
model-agnostic because they make any ML model more inter-
pretable. The slight difference with model-agnostic methods
is that the example-based explanation methods interpret a
model by selecting instances of the dataset and not by acting
on features or transforming the model.

Based on the conducted review we identified two promis-
ing example-based interpretability techniques: (i) Prototypes
and criticisms and (ii) Counterfactuals explanations.

i) PROTOTYPES AND CRITICISMS

Prototypes are a selection of representative instances from the
data [133]-[135], thus item membership is determined by its
similarity to the prototypes which leads to overgeneralization.
To avoid this, advantage exceptions have to be shown, also
called criticisms: instances that are not well represented by
those prototypes. Kim [136] developed an unsupervised algo-
rithm for automatically finding prototypes and critics for a
dataset, called MMD-critic. When applied to unlabeled data,
it finds prototypes and critics that characterize the dataset as
a whole.

ii) COUNTERFACTUALS EXPLANATIONS

Wachter et ai. [137] presented the concept of “unconditional
counterfactual explanations” as a novel type of explanation
of automated decisions. Counterfactual explanations describe
the minimum conditions that would have led to an alternative
decision (e.g. a bank loan being approved), without the need
to describe the full logic of the algorithm. The focus here is

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

on explaining a single prediction in contrast to adversarial
examples where the emphasis is on reversing the prediction
and not explaining it [138].

The proposed classification of techniques is based on
our study of the actual literature. As the research contri-
butions in this class of methods are actively growing, new
model-agnostic techniques are regularly proposed.

Finally, it is worth to note that the main advantage of
model-agnostic methods is “flexibility” at the model, the
explanation and representation level. Nevertheless, although
model-agnostic interpretability techniques are convenient,
they often rely on surrogate models or other approximations
that can degrade the accuracy of the explanations they pro-
vide. While model-specific interpretation techniques tend to
use the model to be interpreted directly, leading to potentially
more accurate explanations.

 

 

 

 

_~ Global
(
Explainability =| ave 7
Method Scoop a \
I Can be \
J \ N\
_— a ~> Local
/ \
{ \
Intrinsic Post-hoc

By definition / \ tueauey
A \

Model-specific «— - ~~» Model-agnostic

FIGURE 7. A pseudo ontology of XAl methods taxonomy.

To sum up (Figure 7), a key distinction is drawn in current
thinking in terms of explaining the ML based AI system
between true transparency (interpretable models such as deci-
sion tree, rules and linear models) and post-hoc interpreta-
tions, additional techniques used to lighten up the darkness of
complex black-box models such as DNN, and that either by
generating local explanations for particular inputs or by glob-
ally explaining the entire model. Local explanations focus on
data and provide individual explanations, they provide trust
to model outcomes. While global explanations focus on the
model and provide an understanding of the decision process,
it connotes some sense of understanding the mechanism by
which the model works. Thus in term trustworthiness, local
explanations are more faithful than global explanations.

Unarguably, the most popular class of explainability meth-
ods is model-agnostic class, this type of methods is usually
used for ANN models. Because they are model indepen-
dent, consequently, model-agnostic techniques are compa-
rable, that is possible to compare the behavior of the same
model with different types of agnostic model techniques.

Table 2 summarizes the various explainability techniques
listed so far. Together with some good references and their
projection on the detailed methods’ taxonomy, form a useful
reference for the reader to gain knowledge about recent XAI
techniques.

VOLUME 6, 2018

AXIS 2. XAl MEASUREMENT: EVALUATING
EXPLANATIONS

“Are all models in all defined-to-be-interpretable model
classes equally interpretable?’’ This is how Doshi-Velez and
Kim [62] questioned interoperability measurement and eval-
uation issue. Indeed, despite the growing body of research
that produces interpretable ML methods, there have been
few works on evaluating these methods and quantifying their
relevance (only 5% of the studied papers focus on this issue).
This probably due to the subjective nature of explainability.
However, given the number of existing interpretability meth-
ods, the need for comparing, validating, quantifying and thus
evaluating these methods arises.

Doshi-Velez and Kim established a baseline of evaluation
approaches and proposed three major types of interpretability
evaluation: (i) application-grounded: put the explanation into
the application and let the end user (typically a domain expert)
test it. This type evaluates the quality of an explanation in the
context of its end-task, (ii) human-grounded: is about con-
ducting simplified application-grounded evaluation where
experiments are run with lay humans rather than domain
experts. This type is most appropriate when the goal is to test
more general notions of the quality of an explanation, and (iii)
functionally-grounded: this type does not involve humans,
it is most appropriate once we have a class of models or
regularizers that have already been validated, e.g. via human-
grounded experiments.

Based on Doshi-Velez’s evaluation classification, Mohseni
and Ragan [148] presented a human-grounded evaluation
benchmark for evaluating instance explanations of images
and textual data. They demonstrated that by comparing
the explanation results from classification models to the
benchmark’s annotation meta-data, it is possible to evalu-
ate the quality and appropriateness of local explanations.
Thus, they showed how human-grounded evaluation could
be used as a measure to qualify local machine-learning
explanations.

Earlier, Huysmans et al. [149] investigated decision trees,
decision tables, propositional rules, and oblique rules in order
to understand which is the most interpretable. To this end,
they performed an end-user experiment to compare them.
They found that overall decision trees and decision tables
were the most Interpretable, but that different tasks made the
tree or table more desirable.

Backhaus and Seiffert [150] suggested quantitative mea-
sures to compare ML methods in their capability to offer
interpretation. A number of machine learning methods
learned on real-world spectral data was considered for
testing.

Poursabzi-Sangdeh er al. [151] argued that quantifying
interpretability implies defining it in terms of alignment with
a set of human-interpretable concepts and proposed a general
framework called Network Dissection for quantifying the
interpretability of latent representations of ANN by identi-
fying hidden units’ semantics for any given neural net, then
aligning them with human-interpretable concepts.

52151
IEEE Access’

TABLE 2. Summary of explainability techniques.

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

o
So 3 2
g 2/2 |83
8 S 8 o¢€
. |S |e?
4 a 2 39
z 2/3 |22
z ze
Techniques
Decision trees bast [140], [141], [142], I G SP
Rule lists [66], [143], [144], [145], [146] | I G SP
LIME [84], [85], [102], [147] H L AG
Shapely explanations [101] H L AG
. [87], [88], [89], [90], [91],
Saliency map [96], [97] H L AG
Activation maximization [82], [83] H G AG
Surrogate models [106], [107], [84] H G/L AG
Partial Dependence Plot (PDP) [108], [51], [110] H G/L AG
Individual Conditional Expectation (ACD [112], [113] H L AG
. [74], [114], [115], [116],
Rule extraction [117], [118] H G/L AG
Decomposition [93], [94], [95] H L AG
tgs [49], [123], [124], [125],
Model distillation [126], [127] H G AG
Sensitive analysis [129], [130] H G/L AG
Layer-wise Relevance Propagation (LRP) [131] H G/L AG
Feature importance [113], [132], [86] H G/L AG
Prototype and criticism [133], [134], [135], [136] H G/L AG
Counterfactuals explanations [137] H L AG

 

 

 

I: Intrinsic, H: Post-hoc, G: Global, L: Local, SP: Model-specific, AG: Model-agnostic

Bau et al. [152] have a different perception of explainabil-
ity, they see it as a latent property that can be influenced by
different manipulable factors (such as the number of inputs,
the complexity of the model, or even the user interface)
and that affects different measurable outcomes (such as an
end user’s ability to trust or debug the model). They ran
in their work related to manipulation and measurement of
model interpretability, an interesting experiment which con-
sists of changing factors that are thought to make models
more or less interpretable and measuring how these changes
affect people’s decision making, they focused on two factors:
the number of input and whether the model is transpar-
ent or black-box. The finding of this experiment stipulates
that participants who are presented with a transparent and
minimum inputs model are better able to simulate the model’s
predictions. However, they do not find significant differences
in participants’ trust or prediction error.

Paul ‘s claim [153] is based on the fact that a considerable
number of methods have been proposed for improving and
evaluating the interpretability of topic models and discusses

52152

how ideas from topic modeling such as human feedback
and automated metrics could be applied to evaluating ML
interpretability.

A recent work by Gilpin et al. [154] proposed a method-
ological approach for evaluating interpretability of ML mod-
els according to a taxonomy that distinguishes three types of
explainability: emulate the processing, explain the represen-
tation and explanation-producing networks.

A common factor that directly impacts the quality of
explainability and which is approached from different view-
points in the above studies is: Human. In the next axis,
we propose to discuss in detail this factor by highlighting the
works focusing on its impact on AI explainability.

AXIS 3. XAl PERCEPTION: HUMAN IN THE LOOP

Explain and understand are two different actions, explain-
ing depends mainly on what is explained (i.e. the original
model) and how explanation is made (i.e. the interpretability
method), while understanding depends in addition of these
elements on who is receiving the explanation (i.e. explainee,

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

in other words human). To be explainable, a ML model
has to be human-understandable. This represents a challenge
for designing XAI as it implies communicating a complex
computational process to human which requires in addition
of ML expertise, HCI skills as well.

Furthermore, since explanation as a human action has long
been studied in philosophy and psychology. Thus, these fields
should be consulted in order to simulate the human explana-
tion process and take inspiration from developed models in
these fields.

Keeping human in the loop is then a determinant factor
of the overall explainability value. However, the conducted
literature review has identified the dearth of works focusing
on the human factor impact in XAI.

In this axis, we survey works that discussed the role of
human from two perspectives: (i) the first one focuses on how
to produce explanations that simulate the human cognitive
process, while (ii) the second one focuses on how to produce
human-centered explanations.

A. HUMAN-LIKE EXPLANATIONS

The work of Miller [33] is perhaps the most significant
attempt at articulating the link between human science and
XAL In his paper, Miller [33] provided an in-depth survey
on research in philosophy, psychology, and cognitive sci-
ence which study the explanation topic. The author noted
that the latter could be a valuable resource for the progress
of the field of XAI. He highlighted three major findings:
(i) Explanations are contrastive: people do not ask why event
E happened, but rather why event E happened instead of some
event F. (ii) Explanations are selective and focus on one or two
possible causes and not all causes for the recommendation.
(iii) Explanations are social conversation and interaction for
transfer of knowledge, implying that the explainer must be
able to leverage the mental model of the explainee while
engaging in the explanation process. He asserted that it is
imperative to take into account these three points if the goal
is to build a useful XAI.

A call for using social science models in XAI was made
in [155]. The authors of this paper argue that most of the exist-
ing literature on XAI methods are based on the developer’s
intuitions rather than to be focused on the intended users.
Based ona light literature survey, they demonstrate that social
science aspects are rarely undertaken in current XAI research
and present some key results from human science field that
are relevant to XAT.

By going back to the interpretability taxonomy of meth-
ods, it is worth to note at this point that post-hoc inter-
pretability techniques are analogous to the human way of
explaining decisions. As noted by Lipton [25], “To the
extent that we might consider humans to be interpretable,
it is [post-hoc] interpretability that applies”. Furthermore,
the example based explanations agnostic method [136] is
explicitly inspired by the cognitive science of human reason-
ing. Specifically, human reasoning is often prototype-based,
using representative examples as a basis for categorization

VOLUME 6, 2018

and decision-making. Similarly, Kim’s method use represen-
tative examples to explain and cluster data.

B. HUMAN-FRIENDLY EXPLANATIONS

An early work by Bauer and Baldes [156] proposed an
ontology-based interface that allows (non-expert) user to gain
a deeper insight into the knowledge represented by ML mod-
els, towards intelligible and transparent ML models.

Recently, Zhu er al. [157] noted that most existing works
focus on new explaining methods, and not on usability, practi-
cal interpretability and efficacy on real users. They introduced
a derived research area called eXplainable AI for Design-
ers (XAID) and proposed a human-centered approach for
facilitating game designers to co-create with AI/ML tech-
niques through XAID.

Tamagnini et al. [158] proposed Rivelo, a pedagogical
visual analytics interface that enables expert user of binary
classifiers by interactively exploring a set of instance-level
explanations.

Abdul et al. [61] investigated how HCI research can
help to develop practical explainable systems with effi-
cacy for the end users. The authors performed a sizable
data-driven literature analysis through which they set an HCI
research agenda in explainability. They also pointed the most
relevant works that attempt to make explanation human-
understandable through interfaces, in textual form or through
visual explanations.

Amongst agnostic methods, visualization is the most
human-centered technique, indeed this method produce bet-
ter explanations to see through the black-box, but unfortu-
nately, some techniques belonging to this method produce
visualizations that, while visually interesting, are not fully
understandable by their human viewers. In a recent work by
Hohman er al. [159] where a survey of the role of visual
analytics in deep learning is presented, the authors acknowl-
edge the importance of producing visualizations and interpre-
tations for DNN that are human understandable and expose
works that attempt to produce such visualizations.

AXIS 4. XAl ANTITHESIS: EXPLAIN OR PREDICT

So far, we have presented works that support XAI from dif-
ferent perspectives. Off the beaten path, and before presenting
our synthesized ideas, we propose in this axis to expose works
that challenge typical approaches, adjust intuitive beliefs and
conjecture previous findings regarding XAI. Seeking to be
holistic, the aim here is to propose a structured proposal that
respects the triad thesis, antithesis and synthesis.

In their work “Are Explanations Always Important?’
Bunt er al. [160] raised questions as to the importance of, and
consequently anticipated usage of, explanation techniques
within systems that support users in making low-cost deci-
sions. Based on their studies, they found that generally these
opaque intelligent systems are positively perceived despite
the lack of meaningful or accessible explanation. They noted:
“While some users were interested in accessing more infor-
mation, the dominant responses were that the applications

52153
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

were sufficiently transparent, or that the cost of viewing an
explanation would outweigh the benefit”. XAI is thus not
yet ready to penetrate the market of this kind of intelligent
systems.

“Too Much, Too Little, or Just Right?” is a work proposed
by Kulesza et al. [161] where they presented their findings
regarding how explanations impact end users’ mental models.
Interestingly, they suggest that completeness is more impor-
tant than soundness in explanation: increasing completeness
via certain information types helped user’s mental models
and, surprisingly, their perception of the cost/benefit tradeoff
of attending to the explanations. They also found that con-
trary to what the human-friendly explanation would have us
believe, oversimplification can be a problem: “when sound-
ness was very low, user experienced more mental demand and
lost trust in the explanations, thereby reducing the likelihood
that users will pay attention to such explanations at all’’.

Even though the work by Holliday et al. [162] entitled
“User Trust in Intelligent Systems: A Journey Over Time
is work”, confirmed based on experimental studies that
explanation impact trust. This work challenges the typical
approach that considered trust in intelligent systems is only
captured as a single quantitative measure at the conclusion of
a task.

Most research works on the ML interpretability agreed
and contribute towards more rigorous notion of interpretabil-
ity [62]. In contrast to this wave of thoughts, Offert [163]
suggested in his work “I know it when I see it’ that a
better understanding of the deficiencies of the intuitive notion
of interpretability is needed as well. That is we have to
“consider interpretability precisely in terms of what it is
not” in order to identify where it is impaired by intuitive
considerations.

Wang et al. [164] proposed in their work ’’ Trading Inter-
pretability for Accuracy’ an Oblique Treed Sparse Additive
Models that sacrifices a certain degree of interpretability for
accuracy in order to achieve entirely sufficient accuracy.

From a statistical standpoint, Shmueli [165] debated ‘“‘To
Explain or to Predict?’’ dilemma by giving a special emphasis
on machine learning field.

Finally, by taking machine learning as a model where
prediction is more important than explanation, Yarkoni and
Westfall [166] argued that in psychology ‘an increased focus
on prediction, rather than explanation, can ultimately lead us
to a greater understanding of behavior”. They named their
work: “Choosing Prediction Over Explanation in Psychol-
ogy: Lessons From Machine Learning”’.

IV. DISCUSSION

Due to the broad spectrum of XAI approaches, it is almost
impossible to perform an exhaustive survey of all XAI works.
It is also inconceivable and unthinkable to include all the
381 studied papers in this work, thus for synthesis and rel-
evance concerns, only a subset of works was detailed in
this survey. As mentioned before, the selection criteria were
mostly based on the popularity and impact of proposals. As a

52154

supplement, at each axis, we made sure to include fresh works
in order to give interested researchers an idea about recent
trends.

The proposed review was underpinned by a solid back-
ground that covers all aspects related to the XAI topic. In the
background section, we deliberately include non-academic
venues with significant attention. Indeed, due to the youth of
the studied domain and its rapid growth, it turns out that these
non-traditional sources are also important to review, as they
are highly influential and impactful to the field.

We conclude our survey with a compilation of the main
findings as well as interesting facts from previous studies.
In parallel, we discuss some research directions and open
problems distilled from the surveyed works.

A. TOWARDS MORE FORMALISM

XAI is a multifaceted objective that cannot be addressed
by singular disciplinary efforts. However, synergetic use of
methods from different research horizons must be done in
a soundly integrated way. In other words, for the field to
progress, it should be supported by a standalone research
community who, at this stage of advancement, should mainly
be engaged towards more formalism in terms of:

(i) Systematic definitions: depending on their background,
researchers use synonymously concepts that are semantically
different [25], and refers to the same notions by different
names (e.g. [113] and [132]). A consensus on definitions
must be done in order to enable easier transfer of results and
information.

(ii) Abstraction: given the number of research proposals,
there is a sufficient material for efforts consolidation in form
of a generic explainable framework that would guide the
production of end-to-end explainable approaches. Instead of
isolated interpretability methods that though their technical
relevance remain only fragments of the whole solution, which
is larger than a technical operation on ML algorithm out-
comes.

In this vein, abstracted explanation generation is another
potential venue. Dosilovic et al. [64] discussed in their work,
the utility of such abstraction in finding properties and gen-
erating hypotheses about data-generating processes, which
is important for future Artificial General Intelligence (AGD
systems.

(iii) Formalizing and quantifying: Guidotti et al. [63], Dhu-
randhar eral. [167], Puri et al. [168], and Varshney et al. [169]
tend to base their proposal ona detailed problem formulation
that becomes invalid as soon as the method of interpretabil-
ity or the explained model change. To further progress in this
field, it is imperative to generalize the expansibility prob-
lem formulation in a rigorous way, irrespective of changing
factors and variables. As a direct effect, this will advance
the state of art of explainability classifying, qualifying and
evaluating sub-issues.

Indeed, with the amount of the existing explainabil-
ity methods in the literature, the first area for future
work is developing formalized rigorous evaluation metrics

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

and methods. Otherwise, we risk to be forced to explain
explanation. As observed before, in literature, there is no clear
way to quantify explainability, the related line of work is just
inits infancy, which represents an opportunity and a challenge
at the same time.

B. HUMAN-MACHINE TEAMING

It is not enough to just explain the model, the user has to
understand it. However, even with an accurate explanation,
developing such an understanding could require supplemen-
tary answers for questions that users would likely have. Thus,
explainability can only happen throw interaction between
human and machine. Envisioning interactive explanation sys-
tems that support many different follow-up and drill-down
actions after presenting an initial explanation to the user, is a
potential research path to pursue in order to advance the XAI
field.

This two-way partnership motivates naturally the use of
HCI and human sciences disciplines. Nevertheless as dis-
cussed before, there is a lack of literature around explain-
able systems that take into account these two dimensions.
Two keen observations made respectively by Miller [33]
and Abdul er al. [61] attest to this lack: (i) ‘“‘social sci-
ences and human behavioral studies are not having enough
impact in explainable AI’’ and (ii) “the streams of research
in explainable systems and in the HCI community tend to
be relatively isolated”. The challenge is then to link the
results of HCI empirical studies with human science theo-
ries in order to drive from both of them added value into
explainability approaches and hopefully contribute to more
human-centric explainable models. Consequently, adaptive
explainable models would make their appearance, by offering
context-aware explanations that would adapt according to
their environment changes such as: the user profile (level of
expertise, domain knowledge, cultural background, interests
and preferences and other contextual variables) and the expla-
nation request setting (justification, teaching, audit . . .).

If machine human teaming is expected to spark signif-
icant research in AI explanability. In the era of Internet
of Things (loT) we should also be waiting for the emerg-
ing of another research body focusing on the machine-to-
machine explanation. Conceiving explanation for machine
consumption will drive some considerations that worth fur-
ther research. Ultimately, however, it is likely that future
explainable approaches, especially adaptive one, will need to
provide both kinds of explanation.

C. EXPLAINABILITY METHODS COMPOSITION

Work on explainability tends to advance quantitatively inter-
pretability methods reflected in a huge proliferation of inter-
pretability techniques (which by the way makes defining a
taxonomy of interpretability methods a challenging task).
Comparatively, little attention is given to approaches that
discussed the potential of combining different interpretability
methods to achieve a more powerful explanation. Indeed,
in literature we have seen how some techniques can be

VOLUME 6, 2018

used in complementary to others (e.g. sensitive analysis and
visualization), but not how to treat disparate interpretability
methods as elementary and composable building blocks that
could synergistically create new added value techniques.
We believe this is a rich, under-explored area for future
research. Hence, enabling composability in XAI can poten-
tially contribute to effectively solve optimization issue in this
field and making explainability and accuracy move in the
same direction.

Furthermore, an eventual combination could also concern
actual interpretability methods focusing on ML models and
classical solutions of explainability related to expert sys-
tems. How exactly to combine elements from both classi-
cal explainable expert systems and present interpretable ML
methods is a topic of debate, for instance, Preece [57] argues
that elements of that earlier work on expert systems offer
routes to making progress towards XAI today.

D. OTHERS EXPLAINABLE INTELLIGENT SYSTEMS

Most of the existing works in literature focus on explain-
ability in machine learning, which is just one type of AI.
However, the same issues also confront other intelligent sys-
tems. Particularly (i) explainable AI planning and (ii) explain-
able agent are beginning to gain recognition as a promising
derived field of XAI.

Planning is an important area of AI, it is used in domains
where learning is not an option. Where the planner is mostly
concerned with establishing the correctness and quality of a
given plan with respect to its own model, adding expandabil-
ity implies to translate the produced plan steps (e.g., PDDL
plans) in a human understandable form. Thus, intuitively,
explainable AI planning (XAIP) is mostly algorithm depen-
dent and serve more as a debugging system for an expert user.
The explainability opportunities that arise in AI planning
was recently explored by Fox et al. [170]. They described
some initial analysis of the issue and proposed a roadmap for
achieving an effective explainability. They based their idea
on the fact that, in contrast of ML, AI Planning is potentially
more favorably disposed to be explainable: (i) planners can
eventually be trusted, (ii) planners can allow an easy interac-
tion with humans, and (iii) planners are relatively transparent.
In fact, the main issue in enabling explainability in planners
is mainly related to the gap between planning algorithms and
human problem solving.

Other introductory works include [171] and [172], in addi-
tion of a dozen papers that was discussed very recently in a
dedicated session of the ICAPS Workshop [11].

These initial works open up a number of future directions
for XAIP, begin by a need of a full formulation of the explain-
able planning problem, other open problems include the fea-
sibility explainability in case of planning under uncertainty
and explainability of task and motion planning in robotics.
A revision of the rich AI planning literature to identify works
that could contribute to XAIP can also serve as a useful
starting point for progress in this subfield.

52155
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

In a related thread of work, researchers have looked at
the idea of explainable agents. As agents are supposed
to represent human behavior, works in this area mainly
focus on behavior explanations generation so that agents
could explain the reasons behind their actions. Significant
earlier works proposed approaches for self-explaining vir-
tual agents in scenario-based training systems [173], [174].
Recent works have investigated the possibilities of explain-
ability of advanced autonomous agents like socially interac-
tive robots [175]-[178].

E. ECONOMIC PERSPECTIVE

There are significant benefits to gain from being on the
front foot and investing in explainability today. Indeed, due
to the social and ethical pressure, XAI could turn into a
competitive differentiator and drives a real business value.
Moreover, the technical challenge of explainability involving
the tradeoff between accuracy and interpretability, affects
significantly the cost of XAI products. Curiously, in literature
economical perspective of XAT is an area of less research yet
no less important. Encouraging economic interpretations is
essential to address several issues such as explainability cost
estimation and variation, algorithm propriety, revealing trade
secrets and predicting XAI market evolution.

Amongst few works found in literature, Akyol et al. [179]
proposed a first attempt to quantitatively analyze the cost
of transparency (PoT) in ML algorithms. The work of
Igami [180] about the connections between machine learning
and econometrics, proposed the perspective of “Structural
Econometrics for Explainable AI’. He noted that “relaxing
the implicit econometric assumptions would make the results
economically interpretable’.

The discussed outlooks are by no means exhaustive, but
give a few leads for further exploration from different per-
spectives based on our compilation of existent studies in lit-
erature. We hope that the proposed directions will inspire new
research that can improve the current state of the art in XAI.

V. CONCLUSION

Matter-of-factly, XAT is a vital interdisciplinary research field
in the AI ecosystem. In the spirit of holism, we presented in
this paper a comprehensive background regarding this field.
Taking inspiration from how we assimilate and familiarize
ourselves with new topics, we focused on the Five W’s and
How (What, Who, When, Why, Where, and How) to cover all
aspects related to XAI. Moreover, in the interest of mapping
the broad landscape around XAI research, this survey has
thoroughly reviewed a portfolio of explainability approaches
and organized them from different perspectives.

Findings showed that XAI is not just a labcoat research
field, its impact is spanning in a large range of application
domains. However, we have seen evidence throughout this
work for the lack of formalism in terms of problem formula-
tion and clear unambiguous definitions. Furthermore, it has
been noted that the human’s role is not sufficiently studied
in existing explainability approaches. In essence, attention is

52156

devoted to interpreting ML models letting other promising
AI system explainability under-explored. It has then been
concluded that considerable effort will be required in the
future to tackle the challenges and open issues with XAI.

REFERENCES

[1] International Data Corporation IDC. (2018). Worldwide Semiannual

Cognitive Artificial Intelligence Systems Spending Guide. Accessed:

Jun. 6, 2018. [Online]. Available: https://www.ide.com/getdoc.jsp?

containerld=prUS43662418

Statista. (2018). Revenues From the Artificial Intelligence (AI) Market

Worldwide From 2016 to 2025. Accessed: Jun. 6, 2018. [Online]. Avail-

able: —_https://www.statista.com/statistics/6077 16/worldwide-artificial-

intelligence-market-revenues/

Gartner. (2017). Top 10 Strategic Technology Trends for 2018.

Accessed: Jun. 6, 2018. [Online]. Available: https://www.gartner.com/

doc/3811368?srcld=1-6595640781

[4] S. Barocas, S. Friedler, M. Hardt, J. Kroll, S. Venka-Tasubramanian, and

H. Wallach. The FAT-ML Workshop Series on Fairness, Accountability,
and Transparency in Machine Learning. Accessed: Jun. 6, 2018. [Online].
Available: http://www.fatml.org/

[5] B. Kim, K. R. Varshney, and A. Weller. 20/8 Workshop on Human

Interpretability in Machine Learning (WHI). [Online]. Available:

https://sites.google.com/view/whi2018/

A. G. Wilson, B. Kim, and W. Herlands. (2016). Proceedings of NIPS

2016 Workshop on Interpretable Machine Learning for Complex Systems.

[Online]. Available: https://arxiv.org/abs/1611.09139

[7] D. W. Aha, T. Darrell, M. Pazzani, D. Reid, C. Sammut, and P. Stone, in
Proc. Workshop Explainable AI (XAI) IICAI, 2017.

[8] M. P. Farina and C. Reed, in Proc. XCI, Explainable Comput. Intell.
Workshop, 2017.

[9] I. Guyon et al., in Proc. LICNN Explainability Learn. Mach., 2017.

[10] A. Chander et al., in Proc. MAKE-Explainable Al, 2018.

[11] S. Biundo, P. Langley, D. Magazzeni, and D. Smith, in Proc. ICAPS
Workshop, EXplainable AI Planning, 2018.

[12] M. Graaf, B. Malle, A. Dragan, and T. Ziemke, in Proc. HRI Workshop,
Explainable Robot. Syst., 2018.

[13] T. Komatsu and A. Said, in Proc. ACM Intell. Interfaces (IUI) Workshop,
Explainable Smart Syst. (EXSS), 2018.

[14] J.M. Alonso, C. Castiello, C. Mencar, and L. Magdalena, in Proc. IPMU,
Adv, Explainable Artif. Intell., 2018.

[15] B.D. Agudo, D. Aha, and J. R. Garcia, in Proc. ICCBR, ist Workshop
Case-Based Reasoning Explanation Intell. Syst. (XCBR), 2018.

[16] D. Gunning. Explainable artificial intelligence (KAI), Defense Advanced
Research Projects Agency (DARPA). Accessed: Jun. 6, 2018. [Online].
Available: http://www.darpa.mil/program/explainable-artificial-
intelligence

[17] P. Hall, M. Kurka, and A. Bartz. (2018). Using H2O Driverless Al,
H2O.AI. Accessed: Jun. 6, 2018. [Online]. Available: https://Awww.
h2o.ai/wp-content/uploads/201 8/01/DriverlessAIBooklet.pdf

[18] Cognilytica. (2018). Cognilytica’s AI Positioning Matrix (CAPM).
Accessed: Jun. 6, 2018. [Online]. Available: https://www.cognilytica.
com/2018/01/09/cognilyticas-ai-positioning-matrix-capm/

[19] FICO. (2018). Explainable Machine Learning Challenge. Accessed:
Jun. 6, 2018. [Online]. Available: https://community.fico.com/s/
explainable-machine-learning-challenge

[20] M. van Lent, W. Fisher, and M. Mancuso, “An explainable artificial
intelligence system for small-unit tactical behavior,” in Proc. 16th Conf:
Innov. Appl. Artif. Intell., 2004, pp. 900-907.

[21] W. R. Swartout and J. D. Moore, “Explanation in expert sys-
tems: A survey,” Univ. Southern California, Los Angeles, CA, USA,
Tech. Rep. ISI/RR-88-228, 1988.

[22] D. Doran, S. Schulz, and T. R. Besold. (2017). ““What does explainable
Al really mean? A new conceptualization of perspectives.” [Online].
Available: https://arxiv.org/abs/17 10.00794

[23] P. W. Koh and P. Liang. (2017). “Understanding black-box pre-
dictions via influence functions.” [Online]. Available: https://arxiv.
org/abs/1703.04730

[24] M. Bojarski et ai. (2017). “Explaining how a deep neural network
trained with end-to-end learning steers a car.” [Online]. Available:
https://arxiv.org/abs/1704.07911

(2

[3

[6

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]
[40]

[41]

[42]

[43]

[4]

[45]

[46]

[47]

[48]

[49]

Z. C. Lipton, “The mythos of model interpretability,” in Proc. ICML
Workshop Hum. Interpretability Mach. Learn., 2016, pp. 96-100.

A. Andrzejak, F. Langner, and S. Zabala, “Interpretable models from
distributed data via merging of decision trees,” in Proc. CIDM, Singapore,
Apr. 2013, pp. 1-9.

R. Piltaver, M. LuStrek, M. Gams, and S. Martinéic-Ipsic, ““Comprehen-
sibility of classification trees—Survey design validation,” in Proc. ITIS,
Smarje&ke toplice, Slovenia, 2014, pp. 5-7.

D. S. Weld and G. Bansal. (2018). ““The challenge of crafting intelligible
intelligence.” [Online]. Available: https://arxiv.org/abs/1803.04263
R.R. Suman, R. Mall, S. Sukumaran, and M. Satpathy, “Extracting state
models for black-box software components,” J. Object Technol., vol. 9,
no. 3, pp. 79-103, 2010.

V. Dignum, “Responsible artificial intelligence: Designing AI for human.
values,” ITU J., ICT Discoveries, vol. 1, pp. 1-8, Sep. 2017.

K. J. Danjuma. (2015). “Performance evaluation of machine learning
algorithms in post-operative life expectancy in the lung cancer patients.”
[Online]. Available: https://arxiv.org/abs/1504.04646

L. M. Chen, “Machine learning for data science: Mathematical or com-
putational,” in Mathematical Problems in Data Science. 2015, pp. 63-74.
T. Miller. (2017). “Explanation in artificial intelligence: Insights from the
social sciences.” [Online]. Available: https://arxiv.org/abs/1706.07269
A. Prabhakar, “Powerful but limited: A DARPA perspective on AI,” in
Proc. DARPA, 2017. Accessed: Jun. 6, 2018. [Online]. Available:
https://sites.nationalacademies.org/cs/groups/pgasite/documents/
webpage/pga_177035.pdf

S. Baum, “A survey of artificial general intelligence projects for ethics,
risk, and policy,” Global Catastrophic Risk Inst., Working Paper 17-1,
2017. [Online]. Available: https://gcrinstitute.org/about/

K. S. Gill, “Artificial super intelligence: Beyond rhetoric,” AJ Soc.,
vol. 31, no. 2, pp. 137-143, 2016.

R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad,
“Intelligible models for healthcare: Predicting pneumonia risk and hos-
pital 30-day readmission,” in Proc. 21th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, 2015, pp. 1721-1730.

A. Howard, C. Zhang, and E. Horvitz, “Addressing bias in machine
learning algorithms: A pilot study on emotion recognition for intelli-
gent systems,” in Proc. Adv. Robot. Social Impacts (ARSO), Mar. 2017,
pp. 1-7.

(2016). European Union General Data Protection Regulation (GDPR).
Accessed: Jun. 6, 2018. [Online]. Available: http://www.eugdpr.org/

D. Silver et ai., “Mastering the game of go without human knowledge,”
Nature, vol. 550, no. 7676, pp. 354-359, 2017.

P. Norvig. Google’s approach to artificial intelligence and machine
learning, UNSW, Sydney, NSW, Australia. Accessed: Jun. 6, 2018.
[Online]. Available: https://www.engineering.unsw.edu.au/video/
googles-approach-to-artificial-intelligence-and-machine-learning

M. McFarland. (2018). Uber Shuts Down Self-Driving Operations
in Arizona, CNN. Accessed Jun. 6, 2018. [Online]. Available:
http://money.cnn.com/201 8/05/23/technology/uber-arizona-self-
driving/index.html

M. Bojarski et ai. (2016). “End to end learning for self-driving cars.”
[Online]. Available: https://arxiv.org/abs/1604.07316

J. Haspiel et ai. (2018). Explanations and Expectations: Trust Build-
ing in Automated Vehicles, deepblue.lib.umich.edu. [Online]. Available:
https://doi.org/10.1145/3173386.3177057

A. Holzinger, C. Biemann, C. S. Pattichis, and D. B. Kell. (2017). “What
do we need to build explainable AI systems for the medical domain?”
[Online]. Available: https://arxiv.org/abs/1712.09923

G. J. Katuwal and R. Chen. (2016). Machine Learning Model Inter-
pretability for Precision Medicine. [Online]. Available: https://arxiv.
org/abs/1610.09045

Z. Che , S. Purushotham, R. Khemani, and Y. Liu, “Interpretable deep
models for ICU outcome prediction,” in Proc. AMIA Annu. Symp., 2017,
pp. 371-380.

J. Lightbourne, “Damned lies & criminal sentencing using evidence-
based tools,’ 15 Duke Law & Technol. Rev., Tech. Rep., 2017,
pp. 327-343. Accessed: Jun. 6, 2018. https://scholarship.law.duke.edu/
ditr/vol15/iss1/16

S. Tan, R. Caruana, G. Hooker, and Y. Lou. (2018). “Detecting bias in
black-box models using transparent model distillation.” [Online]. Avail-
able: https://arxiv.org/abs/1710.06169

VOLUME 6, 2018

[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

C. Howell, “A framework for addressing fairness in consequential
machine learning,” in Proc. FAT Conf., Tuts., 2018, pp. 1-2.

R. Berk and J. Bleich, “‘Statistical procedures for forecasting criminal
behavior: A comparative assessment,” Criminol. Public Policy, vol. 12,
no. 3, pp. 513-544, 2013.

Equifax. (2018). Equifax Launches NeuroDecision Technology.
Accessed: Jun. 6, 2018. [Online]. Available: https://investor.equifax.com/
news-and-events/news/201 8/03-26-2018-143044126

W. Knight. (2017). The U.S. military wants its autonomous machines to
explain themselves, MIT Technology Review. Accessed: Jun. 6, 2018.
[Online]. Available: https://www.technologyreview.com/s/603795/the-
us-military-wants-its-autonomous-machines-to-explain-themselves

A. Henelius, K. Puolamaki, and A. Ukkonen. (2017). “Interpreting clas-
sifiers through attribute interactions in datasets.’” [Online]. Available:
https://arxiv.org/abs/1707.07576

Future of Privacy Forum. (2017). Unfairness by Algorithm: Distilling the
Harms of Automated Decision-Making. Accessed: Jun. 6, 2018. [Online].
Available: https://fpf.org/wp-content/uploads/2017/12/FPF-Automated-
Decision-Making-Harms-and-Mitigation-Charts.pdf

R. Kass, T-Finin , “The need for user models in generating expert system
explanations,” PENN library, Tech. Rep., 1988.

A. Preece, “Asking ‘Why’ in AI: Explainability of intelligent systems—
Perspectives and challenges,” Intell. Syst. Accounting, Finance Manage.,
vol. 25, no. 1, pp. 63-72, 2018

S. Tan, K. C. Sim, and M. Gales, “Improving the interpretability of
deep neural networks with stimulated learning,” in Proc. IEEE Workshop
Autom. Speech Recognit. Understand. (ASRU), Dec. 2015, pp. 617-623.
P. Hall and N. Gill, An Introduction to Machine Learning Interpretability.
Newton, MA, USA: O’Reilly Media, 2018.

L. Breiman. (2001). Statistical Modeling: The Two Cultures,
Statistical Science. Accessed: Jun. 6, 2018. [Online]. Available:
http://bit.ly/2pwz6m5

A. Abdul, J. Vermeulen, D. Wang, B. Y. Lim, and M. Kankanhalli,
“Trends and trajectories for explainable, accountable and. intelligible
systems: An HCI research agenda,” in Proc. SIGCHI Conf. Hum. Factors
Comput. Syst. (CHI), 2018, p. 582.

F. Doshi-Velez and B. Kim. (2018).
science of interpretable machine learning.”
https://arxiv.org/abs/1702.08608

R. Guidotti, A. Monreale, E Turini, D. Pedreschi, and F. Giannotti.
(2018). “A survey of methods for explaining black box models.”
[Online]. Available: https://arxiv.org/abs/1802.01933

EK. Dodilovié, M. Bréic, and N. Hlupic, “Explainable artificial intel-
ligence: A survey,” in Proc. 41st Int. Conv. Inf. Commun. Technol.
Electron. Microelectron. (MIPRO), May 2018, pp. 0210-0215.

C. Wohlin, “Guidelines for snowballing in systematic literature studies
and a replication in software engineering,” in Proc. 18th Int. Conf. Eval.
Assessment Softw. Eng. (EASE), 2014, Art. no. 38.

B. Letham, C. Rudin, T. H. McCormick, and D. Madigan, “Interpretable
classifiers using rules and Bayesian analysis: Building a better stroke
prediction model,” Ann. Appi. Statist., vol. 9, no. 3, pp. 1350-1371, 2015.
K. Xu et al., “Show, attend and tell: Neural image caption generation
with visual attention,” in Proc. Int. Conf. Mach. Learn. (ICML), 2015,
pp. 1-10.

B. Ustun and C. Rudin, “‘Supersparse linear integer models for optimized
medical scoring systems,” Mach. Learn., vol. 102, no. 3, pp. 349-391,
2015.

S. Sarkar, “Accuracy and interpretability trade-offs in machine learning
applied to safer gambling,” in Proc. CEUR Workshop, 2016, pp. 79-87.
L. Breiman, “‘Statistical modeling: The two cultures (with comments and
a rejoinder by the author),” Stat. Sci., vol. 16, no. 3, pp. 199-231, 2001.
S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell, M. Riedl, and
A. Thomaz, “Learning from explanations using sentiment and advice
in RL,” IEEE Trans. Cogn. Develop. Syst., vol. 9, no. 1, pp. 44-55,
Mar. 2016.

A. Mahendran and A. Vedaldi, “Understanding deep image represen-
tations by inverting them,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. (CVPR), Jun. 2015, pp. 5188-5196.

T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “‘Dis-
tributed representations of words and phrases and their compositionality,”
in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2013, pp. 3111-3119.

G. Ras, M. van Gerven, and P. Haselager. (2018). “Explanation methods
in deep learning: Users, values, concerns and challenges.” [Online].
Available: https://arxiv.org/abs/1803.07517

“Towards a
[Online].

rigorous
Available:

52157
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

[75]

[76]

[77]

[78]

[79]

[80]

[81]

[82]

[83]

[84]

[85]

[86]

[87]

[88]

[89]

[90]

[91]

[92]

[93]

[94]

[95]

[96]

[97]

[98]

52158

A. Santoro et ai. (2017). “A simple neural network module for relational
reasoning.” [Online]. Available: https://arxiv.org/abs/1706.01427

R. B. Palm, U. Paquet, and O. Winther. (2017). “Recurrent rela-
tional networks for complex relational reasoning.” [Online]. Available:
https://arxiv.org/abs/1711.08028

Y. Dong, H. Su, J. Zhu, and B. Zhang, “Improving interpretability of
deep neural networks with semantic information,” in Proc. IEEE Conf:
Comput. Vis. Pattern Recognit. (CVPR), Mar. 2017, pp. 4306-4314.

C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling,
“Causal effect inference with deep latent-variable models,” in Proc. Adv.
Neural Inf: Process. Syst. (NIPS), 2017, pp. 6446-6456.

O. Goudet ef al. (2017). “Learning functional causal models with
generative neural networks.’ [Online]. Available: — https://arxiv.
org/abs/1709.05321

C. Yang, A. Rangarajan, and S. Ranka. (2018). “Global model inter-
pretation via recursive partitioning.” [Online]. Available: https://arxiv.
org/abs/1802.04253

M. A. Valenzuela-Escarcega, A. Nagesh, and M. Surdeanu. (2018).
“Lightly-supervised representation learning with global interpretability.”
[Online]. Available: https://arxiv.org/abs/1805.11545

A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune, “Syn-
thesizing the preferred inputs for neurons in neural networks via deep
generator networks,” in Proc. Adv. Neural Inf. Process. Syst. (NIPS),
2016, pp. 3387-3395.

D. Erhan, A. Courville, and Y. Bengio, “Understanding representations
learned in deep architectures,’ Dept. d’Informatique Recherche Opera-
tionnelle, Univ. Montreal, Montreal, QC, Canada, Tech. Rep. 1355, 2010.
M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why should i trust
you?’: Explaining the predictions of any classifier,’ in Proc. 22nd
ACM SIGKDD Int. Conf: Knowl. Discovery Data Mining, 2016,
pp. 1135-1144.

M. T. Ribeiro, S. Singh, and C. Guestrin, “Anchors: High-precision
model-agnostic explanations,” in Proc. AAAI Conf. Artif. Intell., 2018,
pp. 1-9.

J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L.
Wasserman, “‘Distribution-free predictive inference for regression,”
J. Amer Stat. Assoc., to be published. [Online]. Available:
http://www.stat.cmu.edu/~ryantibs/papers/conformal pdf

D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and
K-R. Miiller, ““How to explain individual classification decisions,” J.
Mach. Learn. Res., vol. 11, no. 6, pp. 1803-1831, 2010.

K. Simonyan, A. Vedaldi, and A. Zisserman. (2013). ““Deep inside con-
volutional networks: Visualising image classification models and saliency
maps.” [Online]. Available: https://arxiv.org/abs/1312.6034

M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-
tional networks,” in Proc. Eur. Conf. Comput. Vis. Zurich, Switzerland:
Springer, 2014, pp. 818-833.

B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and O. Torralba, “Learning
deep features for discriminative localization,” in Proc. IEEE Conf. Com-
put. Vis. Pattern Recognit., Jun. 2016, pp. 2921-2929.

M. Sundararajan, A. Taly, and Q. Yan. (2017). “Axiomatic attribution for
deep networks.” [Online]. Available: https://arxiv.org/abs/1703.01365
D. Smilkov, N. Thorat, B. Kim, F. Viégas, and M. Wattenberg. (2017).
“SmoothGrad: Removing noise by adding noise.” [Online]. Available:
https://arxiv.org/abs/1706.03825

M. Robnik-Sikonja and I. Kononenko, “Explaining classifications for
individual instances,” JEEE Trans. Knowl. Data Eng., vol. 20, no. 5,
pp. 589-600, May 2008.

G. Montavon, S. Lapuschkin, A. Binder, W. Samek, and K.-R. Miiller,
“Explaining nonlinear classification decisions with deep taylor decom-
position,” Pattern Recognit., vol. 65, pp. 211-222, May 2017.

S. Bach, A. Binder, K.-R. Miiller, and W. Samek, “Controlling explana-
tory heatmap resolution and semantics via decomposition depth,” in Proc.
IEEE Int. Conf. Image Process. (ICIP), Sep. 2016, pp. 2271-2275.

R. Fong and A. Vedaldi. (2017). “Interpretable explanations of
black boxes by meaningful perturbation.” [Online]. Available:
https://arxiv.org/abs/1704.03296

P. Dabkowski and Y. Gal, “‘Real time image saliency for black box
classifiers,” in Proc. Adv. Neural Inf. Process. Syst.,2017, pp. 6970-6979.
P-J. Kindermans ef a/., “Learning how to explain neural networks:
PatternNet and patternAttribution,” in Proc. Int. Conf. Learn. Repre-
sent., 2018, pp. 1-16. Accessed: Jun. 6, 2018. [Online]. Available:
https://openreview.net/forum?id=Hkn7CBaTW

[99]

[100]

[101]

[102]

[103]

[104]

[105]

[106]

[107]

[108]

[109]
[110]

(111)

(112]

113]

[114]

{115]

[116]

117]

[118]

[119]

[120]

(121)

[122]

[123]

A. Shrikumar, P. Greenside, A. Shcherbina, and A. Kundaje. (2016). “Not
just a black box: Interpretable deep learning by propagating activation
differences.” [Online]. Available: http://arxiv.org/abs/1605.01713

A. Ross, M. C. Hughes, and F. Doshi-Velez, “Right for the right reasons:
Training differentiable models by constraining their explanations,” in
Proce. Int. Joint Conf. Artif. Intell., 2017, pp. 2662-2670.

S. M. Lundberg and S. I. Lee, “A unified approach to interpreting
model predictions,” in Proc. Adv. Neural Inf. Process. Syst., 2017,
pp. 4768-4777.

R. Guidotti, A. Monreale, S. Ruggieri, D, Pedreschi, F. Turini, and
F. Giannotti. (2018). “Local rule-based explanations of black box deci-
sion systems.” [Online]. Available: https://arxiv.org/abs/1805.10820

D. Linsley, D. Scheibler, S$. Eberhardt, and T. Serre. (2018). “‘Global-
and-local attention networks for visual recognition.” [Online]. Available:
https://arxiv.org/abs/1805.08819

S. Seo, J. Huang, H. Yang, and Y. Liu, “Interpretable convolutional
neural networks with dual local and global attention for review rating
prediction,” in Proc. 11th ACM Conf. Recommender Syst. (RecSys), 2017,
pp. 297-305.

C. Molnar. (2018). interpretable Machine Learning: A Guide for Making
Black Box Models Explainable. Accessed: Jun. 6, 2018. [Online]. Avail-
able: https://christophm. github.io/interpretable-ml-book/

O. Bastani, C. Kim, and H. Bastani. (2017). “‘Interpretability via model
extraction.” [Online]. Available: https://arxiv.org/abs/1706.09773

J. J. Thiagarajan, B. Kailkhura, P. Sattigeri, and K. N. Ramamurthy.
(2016). “TreeView: Peeking into deep neural networks via feature-space
partitioning.” [Online]. Available: https://arxiv.org/abs/1611.07429

D. P. Green and H. L. Kern, “Modeling heterogeneous treatment effects
in large-scale experiments using Bayesian additive regression trees,” in
Proc. Annu. Summer Meeting Soc. Political Methodol., 2010, pp. 1-40.
H. A. Chipman, E. I. George, and R. E. McCulloch, “BART: Bayesian.
additive regression trees,” Appl. Statist., vol. 4, no. 1, pp. 266-298, 2010.
J. Elith, J. Leathwick, and T. Hastie, “A working guide to boosted
regression trees,” J. Animal Ecol., vol. 77, no. 4, pp. 802-813, 2008.

S. H. Welling, H. H. EF Refsgaard, P. B. Brockhoff, and
L. H. Clemmensen. (2016). “Forest floor visualizations of random
forests.” [Online]. Available: https://arxiv.org/abs/1605.09196

A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, ““Peeking inside the
black box: Visualizing statistical learning with plots of individual condi-
tional expectation,” J. Comput. Graph. Statist., vol. 24, no. 1, pp. 44-65,
2015, doi: 10.1080/10618600.2014.907095.

G. Casalicchio, C. Molnar, and B. Bischl. (2018). “Visualizing
the feature importance for black box models.” [Online]. Available:
https://arxiv.org/abs/1804.06620

U. Johansson, R. Konig, and I. Niklasson, “The truth is in there—Rule
extraction from opaque models using genetic programming,” in Proc.
FLAIRS Conf., 2004, pp. 658-663.

M. H. Aung ef ai., “Comparing analytical decision support models
through Boolean rule extraction: A case study of ovarian tumour malig-
nancy,” in Proc. Int. Symp. Neural Netw. Berlin, Germany: Springer,
2007, pp. 1177-1186.

T. Hailesilassie. (2017). ‘Rule extraction algorithm for deep neural net-
works: A review.” [Online]. Available: https://arxiv.org/abs/1610.05267
R. Andrews, J. Diederich, and A. B. Tickle, “Survey and critique of
techniques for extracting rules from trained artificial neural networks,”
Knowl.-Based Syst., vol. 8, no. 6, pp. 373-389, 1995.

T. GopiKrishna, “Evaluation of rule extraction algorithms,” Int. J. Data
Mining Knowl. Manage. Process, vol. 4, no. 3, pp. 9-19, 2014.

T. A. Etchells and P. J. G. Lisboa, “Orthogonal search-based rule
extraction (OSRE) for trained neural networks: A practical and efficient
approach,” IEEE Trans. Neural Netw., vol. 17, no. 2, pp. 374-384,
Mar. 2006.

N. Barakat and J. Diederich, “Eclectic nle-extraction from support vector
machines,” Int. J. Comput. Intell., vol. 2, no. 1, pp. 59-62, 2005.

P. Sadowski, J. Collado, D. Whiteson, and P. Baldi, “Deep learning,
dark knowledge, and dark matter,” in Proc. NIPS Workshop High-Energy
Phys. Mach. Learn. (PMLR), vol. 42, 2015, pp. 81-87.

G. Hinton, O. Vinyals, and J. Dean. (2015). “Distilling the knowledge in
a neural network.’ [Online]. Available: https://arxiv.org/abs/1503.02531
Z. Che, S. Purushotham, R. Khemani, and Y. Liu. (2015). “Distilling
knowledge from deep networks with applications to healthcare domain.”
[Online]. Available: https://arxiv.org/abs/1512.03542

VOLUME 6, 2018
A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAI

IEEE Access’

 

[124]

[125]

[126]

[127]

[128]

[129]

[130]

131]

[132]

[133]

[134]

[135]

[136]

[137]

[138]

[139]

[140]

141]

[142]

[143]

[144]

[145]

[146]

[147]

[148]

K. Xu, D. H. Park, D. H. Yi, and C. Sutton. (2018). “Interpreting deep
classifier by visual distillation of dark knowledge.” [Online]. Available:
https://arxiv.org/abs/1803.04042

S. Tan, “Interpretable approaches to detect bias in black-box models,” in
Proc. AAAVACM Conf. Al Ethics Soc., 2017, pp. 1-2.

S. Tan, R. Caruana, G. Hooker, and Y. Lou. (2018). “Auditing black-
box models using transparent model distillation with side information.”
[Online]. Available: https://arxiv.org/abs/1710.06169

S. Tan, R. Caruana, G. Hooker, and A. Gordo. (2018). “Transparent
model distillation.” [Online]. Available: https://arxiv.org/abs/1801.08640
Y. Zhang and B. Wallace. (2016). “A sensitivity analysis of (and practi-
tioners’ Guide to} convolutional neural networks for sentence classifica-
tion.” [Online]. Available: https://arxiv.org/abs/1510.03820

P. Cortez and M. J. Embrechts, “‘Using sensitivity analysis and visualiza-
tion techniques to open black box data mining models,” inf. Sci., vol. 225,
pp. 1-17, Mar. 2013.

P. Cortez and M. J. Embrechts, “Opening black box data mining models
using sensitivity analysis,” in Proc. IEEE Symp. Comput. Intell. Data
Mining (CIDM), Apr. 2011, pp. 341-348.

S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Miiller, and.
W. Samek, “On pixel-wise explanations for non-linear classifier deci-
sions by layer-wise relevance propagation,” PLoS ONE, vol. 10, no. 7,
p. e0130140, 2015.

A. Fisher, C. Rudin, and F. Dominici. (2018). “Model class
reliance: Variable importance measures for any machine learning
model class, from the ‘rashomon’ perspective.” [Online]. Available:
https://arxiv.org/abs/1801.01489

J. Bien and R. Tibshirani, “Prototype selection for interpretable classifi-
cation,” Ann. Appl. Statist., vol. 5, no. 4, pp. 2403-2424, 2011.

B. Kim, C. Rudin, and J. A. Shah, “The Bayesian case model: A gener-
ative approach for case-based reasoning and prototype classification,” in
Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 1952-1960.

K. S. Gurumoorthy, A. Dhurandhar, and G. Cecchi. (2017). “Pro-
toDash: Fast interpretable prototype selection.” [Online]. Available:
https://arxiv.org/abs/1707.01212

B. Kim, R. Khanna, and O. O. Koyejo, “Examples are not enough, learn
to criticize! criticism for interpretability,” in Proc. 29th Conf. Neural Inf:
Process. Syst. (NIPS), 2016, pp. 2280-2288.

S. Wachter, B. Mittelstadt, and C. Russell. (2017). ““Counterfactual expla-
nations without opening the black box: Automated decisions and the
GDPR.” [Online]. Available: https://arxiv.org/abs/1711.00399

X. Yuan, P. He, Q. Zhu, and X. Li. (2017). “Adversarial exam-
ples: Attacks and defenses for deep leaming.’’ [Online]. Available:
https://arxiv.org/abs/1712.07107

V. Schetinin ef ai., “Confident interpretation of Bayesian decision tree
ensembles for clinical applications,” [EEE Trans. Inf: Technol. Biomed.,
vol. 11, no. 3, pp. 312-319, May 2007.

S. Hara and K. Hayashi. (2016). “Making tree ensembles interpretable.”
[Online]. Available: https://arxiv.org/abs/1606.05390

H. F. Tan, G. Hooker, and M. T. Wells. (2016). “Tree space prototypes:
Another look at making tree ensembles interpretable.” [Online]. Avail-
able: https://arxiv.org/abs/1611.07115

R. D. Gibbons e¢ al., ““The CAD-MDD: A computerized adaptive diag-
nostic screening tool for depression,” J. Clin. Psychiatry, vol. 74, no. 7,
pp. 669-674, 2013.

S. Gareia, A. Fernandez, and F. Herrera, “Enhancing the effectiveness
and interpretability of decision tree and rule induction classifiers with
evolutionary training set selection over imbalanced problems,” Appi. Soft
Comput., vol. 9, no. 4, pp. 1304-1314, 2009.

F Wang and C. Rudin, “Falling rule lists,” in Proc. 18th Int. Conf. Artif
Intell. Statist. (AISTATS). San Diego, CA, USA: JMLR W&CP, 2015,
pp. 1013-1022.

G. Su, D. Wei, K. R. Varshney, and D. M. Malioutov. (2015). “‘Inter-
pretable two-level Boolean rule learning for classification.” [Online].
Available: https://arxiv.org/abs/1511.07361

D. M. Malioutov, K.R. Varshney, A. Emad, and S. Dash, “Learning inter-
pretable classification rules with boolean compressed sensing,” in Trans-
parent Data Mining for Big and Small Data. Springer, 2017, pp. 95-121.
S. Mishra, B. L. Sturm, and S. Dixon, “Local interpretable model-
agnostic explanations for music content analysis,” in Proc. ISMIR, 2017,
pp. 537-543.

S. Mohseni and E. D. Ragan. (2018). “A human-grounded evaluation
benchmark for local explanations of machine learning.” [Online]. Avail-
able: https://arxiv.org/abs/1801.05075

VOLUME 6, 2018

[149]

[150]

(151]

[152]

[153]

[154]

[155]

[156]

[157]

[158]

[159]

[160]

[161]

[162]

[163]

[164]

[165]

[166]

[167]

[168]

[169]

[170]

(171)

[172]

J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens,
“An empirical evaluation of the comprehensibility of decision table, tree
and rule based predictive models,” Decis. Support Syst., vol. 51, no. 1,
pp. 141-154, 2011.

A. Backhaus and U. Seiffert, “Quantitative measurements of model
interpretability for the analysis of spectral data,’ in Proc. IEEE Symp.
Comput. Intell. Data Mining (CIDM), 2013, pp. 18-25.

F Poursabzi-Sangdeh, D. G. Goldstein, J. M. Hofman, J. W. Vaughan, and
H. Wallach. (2018). “Manipulating and measuring model interpretabil-
ity.” [Online]. Available: https://arxiv.org/abs/1802.07810

D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. (2017). “Network
dissection: Quantifying interpretability of deep visual representations.”
[Online]. Available: https://arxiv.org/abs/1704.05796

M. J. Paul, “‘Interpretable machine learning: Lessons from topic mod-
eling,” in Proc. CHI Workshop Hum.-Centered Mach. Learn., 2016,
pp. 1-16.

L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and
L. Kagal. (2018). “Explaining explanations: An approach to eval-
uating interpretability of machine learning.’ [Online]. Available:
https://arxiv.org/abs/1806.00069

T. Miller, P. Howe, and L. Sonenberg, “Explainable Al: Beware of
inmates running the asylum,” in Proc. LJCAI Workshop Explainable AI
(XAI), 2017, pp. 36-42.

M. Bauer and S. Baldes, “An ontology-based interface for machine
learning,” in Proc. 10th Int. Conf. Intell. Interfaces, 2005, pp. 314-316.

J. Zhu, A. Liapis, S. Risi, R. Bidarra, and G. M. Youngblood, “Explain-
able AI for designers: A human-centered perspective on mixed-initiative
co-creation,” in Proc. IEEE Conf. Comput. Intell. Games (CIG), 2018,
pp. 458-465.

P. Tamagnini, J. Krause, A. Dasgupta, and E. Bertini, “Interpreting black-
box classifiers using instance-level visual explanations,” in Proc. 2nd
Workshop Hum.-Loop Data Anal., 2017, Art. no. 6.

FE. M. Hohman, M. Kahng, R. Pienta, and D. H. Chau. (2018). “Visual
analytics in deep learning: An interrogative survey for the next frontiers.”
[Online]. Available: https://arxiv.org/abs/1801.06889

A. Bunt, M. Lount, and C. Lauzon, “Are explanations always important?:
A study of deployed, low-cost intelligent interactive systems,” in Proc.
ACM Int. Conf. Intell. Interfaces, 2012, pp. 169-178.

T. Kulesza, S. Stumpf, M. Bumett, S. Yang, I. Kwan, and W.-K. Wong,
“Too much, too little, or just right? Ways explanations impact end users’
mental models,” in Proc. IEEE Symp. Vis. Lang. Hum.-Centric Comput.
(VL/HCC), Sep. 2013, pp. 3-10.

D. Holliday, S. Wilson, and S. Stumpf, “User trust in intelligent systems:
A jourmey over time,” in Proc. 21st Int. Conf. Intell. User Interfaces,
2016, pp. 164-168.

F. Offert. (2017). ““I know it when I see it’. Visualization and intuitive
interpretability.” [Online]. Available: https://arxiv.org/abs/1711.08042

J. Wang, R. Fujimaki, and Y. Motohashi, “Trading interpretabil-
ity for accuracy: Oblique treed sparse additive models,” in Proc.
21th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2015,
pp. 1245-1254.

G. Shmueli, “To explain or to predict?” Stat. Sei., vol. 25, no. 3,
pp. 289-310, 2010.

T. Yarkoni and J. Westfall, “Choosing prediction over explanation in
psychology: Lessons from machine learning,” Perspect. Psychol. Sci.,
vol. 12, no. 6, pp. 1100-1122, 2017.

A. Dhurandhar, V. Iyengar, R. Luss, and K. Shanmugam. (2017).
“TIP: Typifying the interpretability of procedures.” [Online]. Available:
https://arxiv.org/abs/1706.02952

N. Puri, P. Gupta, P. Agarwal, S. Verma, and B. Krishnamurthy. (2017).
“MAGIX: Model agnostic globally interpretable explanations.” [Online].
Available: https://arxiv.org/abs/1706.07160

K. R. Varshney, P. Khanduri, P. Sharma, S. Zhang, and
P. K. Varshney. (2018). “Why interpretability in machine leaming?
An answer using distributed detection and data fusion theory.” [Online].
Available: https://arxiv.org/abs/1806.09710

M. Fox, D. Long, and D. Magazzeni, “Explainable planning,” in Proc.
IICAI Workshop XAI, 2017, pp. 24-30.

T. Chakraborti, S. Sreedharan, Y. Zhang, and S. Kambhampati, “Plan
explanations as model reconciliation: Moving beyond Explanation as
Soliloquy,” in Proc. LJCAI, 2017, pp. 156-163.

Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, H. Zhuo, and
S. Kambhampati, “Plan explicability and predictability for robot task
planning,” in Proc. ICRA, May 2017, pp. 1313-1320.

52159
IEEE Access’

A. Adadi, M. Berrada: Peeking Inside the Black-Box: Survey on XAl

 

[173]

[174]

[175]

[176]

[177]

[178]

[179]

[180]

52160

M. Harbers, K. van den Bosch, and J.-J. Meyer, “Design and evaluation of
explainable BDI agents,” in Proc. IEEE/WIC/ACM Int. Conf. Web Inteil.,
Intell. Agent Technol., Aug. 2010, pp. 125-132.

M. Harbers, K. van den Bosch, and J.-J. Meyer, “Self-explaining agents in
virtual training,” presented EC-TEL PROLEARN Doctoral Consortium,
2009.

P. Langley, B. Meadows, M. Sridharan, and D. Choi, “Explainable
agency for intelligent autonomous systems,’ in Proc. AAAI, 2017,
pp. 4762-4763.

F. Kaptein, J. Broekens, K. Hindriks, and M. Neerincx, “The role of
emotion in self-explanations by cognitive agents,” in Proc. 7th Int. Conf:
Affect. Comput. Intell. Interact. Workshops Demos (ACIIW), Oct. 2017,
pp. 88-93.

M.A. Neerincx, J. van der Waa, F. Kaptein, and J. van Diggelen, “Using
perceptual and cognitive explanations for enhanced human-agent team
performance,” in Proc. Int. Conf. Eng. Psychol. Cogn. Ergonom. (EPCE),
2018, pp. 204-214.

FE. J.C. Garcia, D. A. Robb, X. Liu, A. Laskov, P. Patron, and H. Hastie,
“Explain yourself: A natural language interface for scrutable autonomous
robots,” in Proc. Explainable Robot. Syst. Workshop HRI, 2018.

E. Akyol, C. Langbort, and T. Basar. (2016). “Price of
transparency in strategic machine learning.” [Online]. Available:
https://arxiv.org/abs/1610.08210

M. Igami. (2017). “Artificial intelligence as structural estimation: Eco-
nomic interpretations of deep blue, bonanza, and AlphaGo.” [Online].
Available: https://arxiv.org/abs/17 10.10967

 

AMINA ADADI received the Degree in com-
puter engineering from the National School of
Applied Sciences of Fez in 2012 and the Ph.D.
degree in computer sciences from Sidi Mohammed
Ben Abdellah University, Fez, Morocco, in 2017.
Her current research interests include artificial
intelligence, machine learning, and semantic Web
services.

MOHAMMED BERRADA received the Ph.D.
degree in computer sciences from the Faculty of
Sciences, Sidi Mohammed Ben Abdellah Univer-
sity, Fez, Morocco, in 2008. He is currently a
Professor of computer sciences and the Manager
of the IT Department, National School of Applied
Sciences of Fez. His current research interests
include artificial intelligence, e-learning, enter-
prise architecture, and Web services.

VOLUME 6, 2018
1910.10045v2 [cs.AI] 26 Dec 2019

arXiv

Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
Opportunities and Challenges toward Responsible AI

Alejandro Barredo Arrieta*, Natalia Diaz-Rodriguez>, Javier Del Ser**“, Adrien Bennetot>**,
Siham Tabik®, Alberto Barbado!, Salvador Garcia®, Sergio Gil-Lopez*, Daniel Molina®,
Richard Benjamins", Raja Chatila‘, and Francisco Herrera®

*®TECNALIA, 48160 Derio, Spain
DENSTA, Institute Polytechnique Paris and INRIA Flowers Team, Palaiseau, France
‘University of the Basque Country (UPV/EHU), 48013 Bilbao, Spain
4 Basque Center for Applied Mathematics (BCAM), 48009 Bilbao, Bizkaia, Spain
°Segula Technologies, Pare d’activité de Pissaloup, Trappes, France
SInstitut des Systemes Intelligents et de Robotique, Sorbonne Université, France
8 DaSCI Andalusian Institute of Data Science and Computational Intelligence, University of Granada, 18071 Granada, Spain
"Telefonica, 28050 Madrid, Spain

 

Abstract

In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed
appropriately, may deliver the best of expectations over many application sectors across the field. For this
to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability,
an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural
Networks) that were not present in the last hype of AI (namely, expert systems and rule based models).
Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely
acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in
this article examines the existing literature and contributions already done in the field of XAI, including a
prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define
explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that
covers such prior conceptual propositions with a major focus on the audience for which the explainability
is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions
related to the explainability of different Machine Learning models, including those aimed at explaining
Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This
critical literature analysis serves as the motivating background for a series of challenges faced by XAI,
such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept
of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI
methods in real organizations with fairness, model explainability and accountability at its core. Our
ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve
as reference material in order to stimulate future research advances, but also to encourage experts and
professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any
prior bias for its lack of interpretability.

Keywords: Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion,
Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible
Artificial Intelligence.

 

 

*Corresponding author. TECNALIA. P. Tecnologico, Ed. 700. 48170 Derio (Bizkaia), Spain. E-mail: javier.delser @tecnalia.com

Preprint submitted to Information Fusion December 30, 2019
1. Introduction

Axtificial Intelligence (AD) lies at the core of many activity sectors that have embraced new information
technologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on the
paramount importance featured nowadays by intelligent machines endowed with learning, reasoning and
adaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented
levels of performance when learning to solve increasingly complex computational tasks, making them
pivotal for the future development of the human society [2]. The sophistication of Al-powered systems
has lately increased to such an extent that almost no human intervention is required for their design
and deployment. When decisions derived from such systems ultimately affect humans’ lives (as in e.g.
medicine, law or defense), there is an emerging need for understanding how such decisions are furnished
by AI methods [3].

While the very first AI systems were easily interpretable, the last years have witnessed the rise of
opaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning
(DL) models such as DNNs stems from a combination of efficient learning algorithms and their huge
parametric space. The latter space comprises hundreds of layers and millions of parameters, which makes
DNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency,
Le., the search for a direct understanding of the mechanism by which a model works [5].

As black-box Machine Learning (ML) models are increasingly being employed to make important
predictions in critical contexts, the demand for transparency is increasing from the various stakeholders in
AI [6]. The danger is on creating and using decisions that are not justifiable, legitimate, or that simply do
not allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of a
model are crucial, e.g., in precision medicine, where experts require far more information from the model
than a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomous
vehicles in transportation, security, and finance, among others.

In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and
trustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusing
solely on performance, the systems will be increasingly opaque. This is true in the sense that there is a
trade-off between the performance of a model and its transparency [10]. However, an improvement in the
understanding of a system can lead to the correction of its deficiencies. When developing a ML model,
the consideration of interpretability as an additional design driver can improve its implementability for 3
reasons:

e Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct
from bias in the training dataset.

e Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations
that could change the prediction.

e Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing
that an underlying truthful causality exists in the model reasoning.

All these means that the interpretation of the system should, in order to be considered practical,
provide either an understanding of the model mechanisms and predictions, a visualization of the model’s
discrimination rules, or hints on what could perturb the model [11].

In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI
(XAD) [7] proposes creating a suite of ML techniques that 1) produce more explainable models while
maintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to
understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent
partners. XAI draws as well insights from the Social Sciences [12] and considers the psychology of
explanation.
 

Hs Interpretable Artificial Intelligence
15) GE XAI
Hs Explainable Artificial Intelligence

Le
fang ~ 2
2 ot 5S

# of contributed works in the literature
nN
ot

 

 

 

2012 2013 2014 2015 2016 2017 2018 2019
(December 10th)

Figure 1: Evolution of the number of total publications whose title, abstract and/or keywords refer to the field of XAI during
the last years. Data retrieved from Scopus® (December 10th, 2019) by using the search terms indicated in the legend when
querying this database. It is interesting to note the latent need for interpretable AI models over time (which conforms to intuition, as
interpretability is a requirement in many scenarios), yet it has not been until 2017 when the interest in techniques to explain Al
models has permeated throughout the research community.

Figure | displays the rising trend of contributions on XAI and related concepts. This literature
outbreak shares its rationale with the research agendas of national governments and agencies. Although
some recent surveys [8, 13, 10, 14, 15, 16, 17] summarize the upsurge of activity in XAI across sectors
and disciplines, this overview aims to cover the creation of a complete unified framework of categories
and concepts that allow for scrutiny and understanding of the field of XAI methods. Furthermore, we pose
intriguing thoughts around the explainability of AI models in data fusion contexts with regards to data
privacy and model confidentiality. This, along with other research opportunities and challenges identified
throughout our study, serve as the pull factor toward Responsible Artificial Intelligence, term by which
we refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we
will later show in detail, model explainability is among the most crucial aspects to be ensured within this
methodological framework. All in all, the novel contributions of this overview can be summarized as
follows:

1. Grounded on a first elaboration of concepts and terms used in XAI-related research, we propose a
novel definition of explainability that places audience (Figure 2) as a key aspect to be considered when
explaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques,
from trustworthiness to privacy awareness, which round up the claimed importance of purpose and
targeted audience in model explainability.

2. We define and examine the different levels of transparency that a ML model can feature by itself, as
well as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that
are not transparent by design.

3. We thoroughly analyze the literature on XAI and related concepts published to date, covering ap-
proximately 400 contributions arranged into two different taxonomies. The first taxonomy addresses
the explainability of ML models using the previously made distinction between transparency and
post-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e.,
shallow) learning models. The second taxonomy deals with XAI methods suited for the explanation of
Deep Learning models, using classification criteria closely linked to this family of ML methods (e.g.
layerwise explanations, representation vectors, attention).

4. We enumerate a series of challenges of XAI that still remain insufficiently addressed to date. Specifi-
cally, we identify research needs around the concepts and metrics to evaluate the explainability of ML
models, and outline research directions toward making Deep Learning models more understandable.
We further augment the scope of our prospects toward the implications of XAI techniques in regards
to confidentiality, robustness in adversarial settings, data diversity, and other areas intersecting with
explainability.

5. After the previous prospective discussion, we arrive at the concept of Responsible Artificial Intelligence,
a manifold concept that imposes the systematic adoption of several AI principles for AI models to
be of practical use. In addition to explainability, the guidelines behind Responsible AI establish that
fairness, accountability and privacy should also be considered when implementing AI models in real
environments.

6. Since Responsible AI blends together model explainability and privacy/security by design, we call
for a profound reflection around the benefits and risks of XAI techniques in scenarios dealing with
sensitive information and/or confidential ML models. As we will later show, the regulatory push
toward data privacy, quality, integrity and governance demands more efforts to assess the role of XAI
in this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy and
security under different data fusion paradigms.

The remainder of this overview is structured as follows: first, Section 2 and subsections therein open a
discussion on the terminology and concepts revolving around explainability and interpretability in AI,
ending up with the aforementioned novel definition of interpretability (Subsections 2.1 and 2.2), and a
general criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceed
by reviewing recent findings on XAI for ML models (on transparent models and post-hoc techniques
respectively) that comprise the main division in the aforementioned taxonomy. We also include a review
on hybrid approaches among the two, to attain XAI. Benefits and caveats of the synergies among the
families of methods are discussed in Section 5, where we present a prospect of general challenges and
some consequences to be cautious about. Finally, Section 6 elaborates on the concept of Responsible
Artificial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the community
around this vibrant research area, which has the potential to impact society, in particular those sectors that
have progressively embraced ML as a core technology of their activity.

2. Explainability: What, Why, What For and How?

Before proceeding with our literature study, it is convenient to first establish a common point of
understanding on what the term explainability stands for in the context of AI and, more specifically,
ML. This is indeed the purpose of this section, namely, to pause at the numerous definitions that have
been done in regards to this concept (what?), to argue why explainability is an important issue in AI and
ML (why? what for?) and to introduce the general classification of XAI approaches that will drive the
literature study thereafter (how?).

2.1. Terminology Clarification

One of the issues that hinders the establishment of common grounds is the interchangeable misuse of
interpretability and explainability in the literature. There are notable differences among these concepts.
To begin with, interpretability refers to a passive characteristic of a model referring to the level at which
a given model makes sense for a human observer. This feature is also expressed as transparency. By
contrast, explainability can be viewed as an active characteristic of a model, denoting any action or
procedure taken by a model with the intent of clarifying or detailing its internal functions.

To summarize the most commonly used nomenclature, in this section we clarify the distinction and
similarities among terms often used in the ethical AI and XAI communities.

e Understandability (or equivalently, intelligibility) denotes the characteristic of a model to make a
human understand its function — how the model works — without any need for explaining its internal
structure or the algorithmic means by which the model processes data internally [18].

Comprehensibility: when conceived for ML models, comprehensibility refers to the ability of a
learning algorithm to represent its learned knowledge in a human understandable fashion [19, 20, 21].
This notion of model comprehensibility stems from the postulates of Michalski [22], which stated that
“the results of computer induction should be symbolic descriptions of given entities, semantically and
structurally similar to those a human expert might produce observing the same entities. Components of
these descriptions should be comprehensible as single ‘chunks’ of information, directly interpretable in
natural language, and should relate quantitative and qualitative concepts in an integrated fashion”.
Given its difficult quantification, comprehensibility is normally tied to the evaluation of the model
complexity [17].

Interpretability: it is defined as the ability to explain or to provide the meaning in understandable
terms to a human.

Explainability: explainability is associated with the notion of explanation as an interface between
humans and a decision maker that is, at the same time, both an accurate proxy of the decision maker
and comprehensible to humans [17].

Transparency: a model is considered to be transparent if by itself it is understandable. Since a model
can feature different degrees of understandability, transparent models in Section 3 are divided into three
categories: simulatable models, decomposable models and algorithmically transparent models [5].

In all the above definitions, understandability emerges as the most essential concept in XAI. Both
transparency and interpretability are strongly tied to this concept: while transparency refers to the
characteristic of a model to be, on its own, understandable for a human, understandability measures the
degree to which a human can understand a decision made by a model. Comprehensibility is also connected
to understandability in that it relies on the capability of the audience to understand the knowledge contained
in the model. All in all, understandability is a two-sided matter: model understandability and human
understandability. This is the reason why the definition of XAI given in Section 2.2 refers to the concept
of audience, as the cognitive skills and pursued goal of the users of the model have to be taken into
account jointly with the intelligibility and comprehensibility of the model in use. This prominent role
taken by understandability makes the concept of audience the cornerstone of XAI, as we next elaborate in
further detail.

2.2. What?

Although it might be considered to be beyond the scope of this paper, it is worth noting the discussion
held around general theories of explanation in the realm of philosophy [23]. Many proposals have been
done in this regard, suggesting the need for a general, unified theory that approximates the structure and
intent of an explanation. However, nobody has stood the critique when presenting such a general theory.
For the time being, the most agreed-upon thought blends together different approaches to explanation
drawn from diverse knowledge disciplines. A similar problem is found when addressing interpretability
in AI. It appears from the literature that there is not yet a common point of understanding on what
interpretability or explainability are. However, many contributions claim the achievement of interpretable
models and techniques that empower explainability.
To shed some light on this lack of consensus, it might be interesting to place the reference starting
point at the definition of the term Explainable Artificial Intelligence (XAI) given by D. Gunning in [7]:

 

“XAI will create a suite of machine learning techniques that enables human users to understand,
appropriately trust, and effectively manage the emerging generation of artificially intelligent partners”

 

 

 

This definition brings together two concepts (understanding and trust) that need to be addressed in
advance. However, it misses to consider other purposes motivating the need for interpretable AI models,
such as causality, transferability, informativeness, fairness and confidence [5, 24, 25, 26]. We will later
delve into these topics, mentioning them here as a supporting example of the incompleteness of the above
definition.

As exemplified by the definition above, a thorough, complete definition of explainability in AI
still slips from our fingers. A broader reformulation of this definition (e.g. “An explainable Artificial
Intelligence is one that produces explanations about its functioning”) would fail to fully characterize the
term in question, leaving aside important aspects such as its purpose. To build upon the completeness, a
definition of explanation is first required.

As extracted from the Cambridge Dictionary of English Language, an explanation is “the details or
reasons that someone gives to make something clear or easy to understand” [27]. In the context of an
ML model, this can be rephrased as: ”the details or reasons a model gives to make its functioning clear
or easy to understand”. It is at this point where opinions start to diverge. Inherently stemming from the
previous definitions, two ambiguities can be pointed out. First, the details or the reasons used to explain,
are completely dependent of the audience to which they are presented. Second, whether the explanation
has left the concept clear or easy to understand also depends completely on the audience. Therefore, the
definition must be rephrased to reflect explicitly the dependence of the explainability of the model on the
audience. To this end, a reworked definition could read as:

 

Given a certain audience, explainability refers to the details and reasons a model gives to make its
functioning clear or easy to understand.

 

 

 

Since explaining, as argumenting, may involve weighting, comparing or convincing an audience with
logic-based formalizations of (counter) arguments [28], explainability might convey us into the realm of
cognitive psychology and the psychology of explanations [7], since measuring whether something has
been understood or put clearly is a hard task to be gauged objectively. However, measuring to which
extent the internals of a model can be explained could be tackled objectively. Any means to reduce the
complexity of the model or to simplify its outputs should be considered as an XAI approach. How big
this leap is in terms of complexity or simplicity will correspond to how explainable the resulting model
is. An underlying problem that remains unsolved is that the interpretability gain provided by such XAI
approaches may not be straightforward to quantify: for instance, a model simplification can be evaluated
based on the reduction of the number of architectural elements or number of parameters of the model
itself (as often made, for instance, for DNNs). On the contrary, the use of visualization methods or natural
language for the same purpose does not favor a clear quantification of the improvements gained in terms
of interpretability. The derivation of general metrics to assess the quality of XAI approaches remain as
an open challenge that should be under the spotlight of the field in forthcoming years. We will further
discuss on this research direction in Section 5.

Explainability is linked to post-hoc explainability since it covers the techniques used to convert a
non-interpretable model into a explainable one. In the remaining of this manuscript, explainability will be
considered as the main design objective, since it represents a broader concept. A model can be explained,
but the interpretability of the model is something that comes from the design of the model itself. Bearing
these observations in mind, explainable AI can be defined as follows:

 

Given an audience, an explainable Artificial Intelligence is one that produces details or reasons to
make its functioning clear or easy to understand.

 

 

 
This definition is posed here as a first contribution of the present overview, implicitly assumes that the
ease of understanding and clarity targeted by XAI techniques for the model at hand reverts on different
application purposes, such as a better trustworthiness of the model’s output by the audience.

2.3. Why?

As stated in the introduction, explainability is one of the main barriers AI is facing nowadays in
regards to its practical implementation. The inability to explain or to fully understand the reasons by
which state-of-the-art ML algorithms perform as well as they do, is a problem that find its roots in two
different causes, which are conceptually illustrated in Figure 2.

Without a doubt, the first cause is the gap between the research community and business sectors,
impeding the full penetration of the newest ML models in sectors that have traditionally lagged behind
in the digital transformation of their processes, such as banking, finances, security and health, among
many others. In general this issue occurs in strictly regulated sectors with some reluctance to implement
techniques that may put at risk their assets.

The second axis is that of knowledge. AI has helped research across the world with the task of
inferring relations that were far beyond the human cognitive reach. Every field dealing with huge amounts
of reliable data has largely benefited from the adoption of AI and ML techniques. However, we are
entering an era in which results and performance metrics are the only interest shown up in research
studies. Although for certain disciplines this might be the fair case, science and society are far from being
concerned just by performance. The search for understanding is what opens the door for further model
improvement and its practical utility.

Who? Domain experts/users of the model (e.g. medical doctors, insurance agents) |?
Why? Trust the model itself, gain scientific knowledge Oe e8

 

 

 

 
 

   
 

 
 

Who? Users affected by model decisions
Why? Understand their situation, verify |?
fair decisions...

Who? Regulatory entities/agencies
Why? Certify model compliance with the]?
legislation in force, audits, ...

 
 

 

   
 

 

 

 

Target audience
in XAI

 

 
 

Who? Data scientists, developers, product owners... Who? Managers and executive board members
Why? Ensure/improve product efficiency, research, |? Why? Assess regulatory compliance, understand
new functionalities... corporate Al applications...

 
   
 
 

 

 

Figure 2: Diagram showing the different purposes of explainability in ML models sought by different audience profiles. Two goals
occur to prevail across them: need for model understanding, and regulatory compliance. Image partly inspired by the one presented
in [29], used with permission from IBM.

The following section develops these ideas further by analyzing the goals motivating the search for
explainable AI models.

2.4. What for?

The research activity around XAI has so far exposed different goals to draw from the achievement
of an explainable model. Almost none of the papers reviewed completely agrees in the goals required
to describe what an explainable model should compel. However, all these different goals might help
discriminate the purpose for which a given exercise of ML explainability is performed. Unfortunately,
scarce contributions have attempted to define such goals from a conceptual perspective [5, 13, 24, 30].
We now synthesize and enumerate definitions for these XAI goals, so as to settle a first classification
criteria for the full suit of papers covered in this review:
 

XAI Goal Main target audience (Fig. 2) References

 

Domain experts, users of the model
affected by decisions

Domain experts, managers and
Causality executive board members, [35, 38, 39, 40, 41, 42, 43]
regulatory entities/agencies

Trustworthiness [5, 10, 24, 32, 33, 34, 35, 36, 37]

 

 

[S, 44, 21, 26, 45, 30, 32, 37, 38, 39, 46, 47, 48, 49,
50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,
64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
78, 79, 80, 81, 82, 83, 84, 85]
[5, 44, 21, 25, 26, 45, 30, 32, 34, 35, 37, 38, 41, 46, 49,
50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65,
66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 86,
87, 88, 89, 59, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,
Informativeness All 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,
111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,
122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,
133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]

Transferability | Domain experts, data scientists

 

 

Domain experts, developers, [5, 45, 35, 46, 48, 54, 61, 72, 88, 89, 96, 108, 117,

 

Confidence managers, regulatory 119, 155]
entities/agencies
. Users affected by model decisions, [5, 24, 45, 35, 47, 99, 100, 101, 120, 121, 128, 156,
Fairness we :
regulatory entities/agencies 157, 158]

 

[21, 26, 30, 32, 37, 50, 53, 55, 62, 67, 68, 69, 70, 71,
Product owners, managers, users

 

Accessibility it pv ovl duenions 74, 75, 76, 86, 93, 94, 103, 105, 107, 108, 111, 112,
y 113, 114, 115, 124, 129]
Interactivity Domain experts, users affected by 137 59 59 65, 67, 74, 86, 124]

model decisions

 

Users affected by model decisions,

regulatory entities/agencies (89]

Privacy awareness

 

Table 1: Goals pursued in the reviewed literature toward reaching explainability, and their main target audience.

e Trustworthiness: several authors agree upon the search for trustworthiness as the primary aim of an
explainable AI model [31, 32]. However, declaring a model as explainable as per its capabilities of
inducing trust might not be fully compliant with the requirement of model explainability. Trustwor-
thiness might be considered as the confidence of whether a model will act as intended when facing a
given problem. Although it should most certainly be a property of any explainable model, it does not
imply that every trustworthy model can be considered explainable on its own, nor is trustworthiness
a property easy to quantify. Trust might be far from being the only purpose of an explainable model
since the relation among the two, if agreed upon, is not reciprocal. Part of the reviewed papers mention
the concept of trust when stating their purpose for achieving explainability. However, as seen in Table
1, they do not amount to a large share of the recent contributions related to XAI.

e Causality: another common goal for explainability is that of finding causality among data variables.
Several authors argue that explainable models might ease the task of finding relationships that, should
they occur, could be tested further for a stronger causal link between the involved variables [159, 160].
The inference of causal relationships from observational data is a field that has been broadly studied
over time [161]. As widely acknowledged by the community working on this topic, causality requires a
wide frame of prior knowledge to prove that observed effects are causal. A ML model only discovers
correlations among the data it learns from, and therefore might not suffice for unveiling a cause-effect
relationship. However, causation involves correlation, so an explainable ML model could validate
the results provided by causality inference techniques, or provide a first intuition of possible causal
relationships within the available data. Again, Table 1 reveals that causality is not among the most
important goals if we attend to the amount of papers that state it explicitly as their goal.

Transferability: models are always bounded by constraints that should allow for their seamless
transferability. This is the main reason why a training-testing approach is used when dealing with
ML problems [162, 163]. Explainability is also an advocate for transferability, since it may ease the
task of elucidating the boundaries that might affect a model, allowing for a better understanding and
implementation. Similarly, the mere understanding of the inner relations taking place within a model
facilitates the ability of a user to reuse this knowledge in another problem. There are cases in which the
lack of a proper understanding of the model might drive the user toward incorrect assumptions and
fatal consequences [44, 164]. Transferability should also fall between the resulting properties of an
explainable model, but again, not every transferable model should be considered as explainable. As
observed in Table 1, the amount of papers stating that the ability of rendering a model explainable is to
better understand the concepts needed to reuse it or to improve its performance is the second most used
reason for pursuing model explainability.

Informativeness: ML models are used with the ultimate intention of supporting decision making [92].
However, it should not be forgotten that the problem being solved by the model is not equal to that
being faced by its human counterpart. Hence, a great deal of information is needed in order to be able
to relate the user’s decision to the solution given by the model, and to avoid falling in misconception
pitfalls. For this purpose, explainable ML models should give information about the problem being
tackled. Most of the reasons found among the papers reviewed is that of extracting information about
the inner relations of a model. Almost all rule extraction techniques substantiate their approach on
the search for a simpler understanding of what the model internally does, stating that the knowledge
(information) can be expressed in these simpler proxies that they consider explaining the antecedent.
This is the most used argument found among the reviewed papers to back up what they expect from
reaching explainable models.

Confidence: as a generalization of robustness and stability, confidence should always be assessed
on a model in which reliability is expected. The methods to maintain confidence under control are
different depending on the model. As stated in [165, 166, 167], stability is a must-have when drawing
interpretations from a certain model. Trustworthy interpretations should not be produced by models
that are not stable. Hence, an explainable model should contain information about the confidence of its
working regime.

Fairness: from a social standpoint, explainability can be considered as the capacity to reach and
guarantee fairness in ML models. In a certain literature strand, an explainable ML model suggests a
clear visualization of the relations affecting a result, allowing for a fairness or ethical analysis of the
model at hand [3, 100]. Likewise, a related objective of XAT is highlighting bias in the data a model
was exposed to [168, 169]. The support of algorithms and models is growing fast in fields that involve
human lives, hence explainability should be considered as a bridge to avoid the unfair or unethical use
of algorithm’s outputs.

Accessibility: a minor subset of the reviewed contributions argues for explainability as the property
that allows end users to get more involved in the process of improving and developing a certain ML
model [37, 86] . It seems clear that explainable models will ease the burden felt by non-technical or
non-expert users when having to deal with algorithms that seem incomprehensible at first sight. This
concept is expressed as the third most considered goal among the surveyed literature.

Interactivity: some contributions [50, 59] include the ability of a model to be interactive with the user
as one of the goals targeted by an explainable ML model. Once again, this goal is related to fields in
which the end users are of great importance, and their ability to tweak and interact with the models is
what ensures success.

e Privacy awareness: almost forgotten in the reviewed literature, one of the byproducts enabled by ex-
plainability in ML models is its ability to assess privacy. ML models may have complex representations
of their learned patterns. Not being able to understand what has been captured by the model [4] and
stored in its internal representation may entail a privacy breach. Contrarily, the ability to explain the
inner relations of a trained model by non-authorized third parties may also compromise the differential
privacy of the data origin. Due to its criticality in sectors where XAI is foreseen to play a crucial role,
confidentiality and privacy issues will be covered further in Subsections 5.4 and 6.3, respectively.

This subsection has reviewed the goals encountered among the broad scope of the reviewed papers.
All these goals are clearly under the surface of the concept of explainability introduced before in this
section. To round up this prior analysis on the concept of explainability, the last subsection deals with
different strategies followed by the community to address explainability in ML models.

2.5. How?

The literature makes a clear distinction among models that are interpretable by design, and those
that can be explained by means of external XAI techniques. This duality could also be regarded as the
difference between interpretable models and model interpretability techniques; a more widely accepted
classification is that of transparent models and post-hoc explainability. This same duality also appears
in the paper presented in [17] in which the distinction its authors make refers to the methods to solve
the transparent box design problem against the problem of explaining the black-box problem. This
work, further extends the distinction made among transparent models including the different levels of
transparency considered.

Within transparency, three levels are contemplated: algorithmic transparency, decomposability and
simulatability!. Among post-hoc techniques we may distinguish among text explanations, visualizations,
local explanations, explanations by example, explanations by simplification and feature relevance. In this
context, there is a broader distinction proposed by [24] discerning between 1) opaque systems, where
the mappings from input to output are invisible to the user; 2) interpretable systems, in which users can
mathematically analyze the mappings; and 3) comprehensible systems, in which the models should output
symbols or rules along with their specific output to aid in the understanding process of the rationale
behind the mappings being made. This last classification criterion could be considered included within
the one proposed earlier, hence this paper will attempt at following the more specific one.

2.5.1. Levels of Transparency in Machine Learning Models

Transparent models convey some degree of interpretability by themselves. Models belonging to
this category can be also approached in terms of the domain in which they are interpretable, namely,
algorithmic transparency, decomposability and simulatability. As we elaborate next in connection to
Figure 3, each of these classes contains its predecessors, e.g. a simulatable model is at the same time a
model that is decomposable and algorithmically transparent:

e Simulatability denotes the ability of a model of being simulated or thought about strictly by a human,
hence complexity takes a dominant place in this class. This being said, simple but extensive (i.e., with
too large amount of rules) rule based systems fall out of this characteristic, whereas a single perceptron
neural network falls within. This aspect aligns with the claim that sparse linear models are more
interpretable than dense ones [170], and that an interpretable model is one that can be easily presented

 

'The alternative term simulability is also used in the literature to refer to the capacity of a system or process to be simulated.
However, we note that this term does not appear in current English dictionaries.

10
to a human by means of text and visualizations [32]. Again, endowing a decomposable model with
simulatability requires that the model has to be self-contained enough for a human to think and reason
about it as a whole.

e Decomposability stands for the ability to explain each of the parts of a model (input, parameter and
calculation). It can be considered as intelligibility as stated in [171]. This characteristic might empower
the ability to understand, interpret or explain the behavior of a model. However, as occurs with
algorithmic transparency, not every model can fulfill this property. Decomposability requires every
input to be readily interpretable (e.g. cumbersome features will not fit the premise). The added
constraint for an algorithmically transparent model to become decomposable is that every part of the
model must be understandable by a human without the need for additional tools.

e Algorithmic Transparency can be seen in different ways. It deals with the ability of the user to
understand the process followed by the model to produce any given output from its input data. Put
it differently, a linear model is deemed transparent because its error surface can be understood and
reasoned about, allowing the user to understand how the model will act in every situation it may
face [163]. Contrarily, it is not possible to understand it in deep architectures as the loss landscape
might be opaque [172, 173] since it cannot be fully observed and the solution has to be approximated
through heuristic optimization (e.g. through stochastic gradient descent). The main constraint for
algorithmically transparent models is that the model has to be fully explorable by means of mathematical
analysis and methods.

 

 

 

 

 

oO oO 0
If g(fa(e1), fe(ee)) > 5 If co > 180 then y=1 95% of the positive training samples
then y = 1, else y=0 Else if ¢1 + #3 > 150 then y=1 have 2 > 180+ Rule 1
fa(e1) = 1/e7, fa(e2) = log ee Else y = 0 90% of the positive training samples
o(f.9g) =1/F +9) a1: weight, co: height, «3: age have £1 + £3 > 1504 Rule 2

 

 

 

 

 

(a) (b) (c)

Figure 3: Conceptual diagram exemplifying the different levels of transparency characterizing a ML model My, with ¢ denoting
the parameter set of the model at hand: (a) simulatability; (b) decomposability; (c) algorithmic transparency. Without loss of
generality, the example focuses on the ML model as the explanation target. However, other targets for explainability may include a
given example, the output classes or the dataset itself.

2.5.2. Post-hoc Explainability Techniques for Machine Learning Models

Post-hoc explainability targets models that are not readily interpretable by design by resorting to
diverse means to enhance their interpretability, such as text explanations, visual explanations, local
explanations, explanations by example, explanations by simplification and feature relevance explanations
techniques. Each of these techniques covers one of the most common ways humans explain systems and
processes by themselves.

Further along this river, actual techniques, or better put, actual group of techniques are specified
to ease the future work of any researcher that intends to look up for an specific technique that suits its
knowledge. Not ending there, the classification also includes the type of data in which the techniques has
been applied. Note that many techniques might be suitable for many different types of data, although
the categorization only considers the type used by the authors that proposed such technique. Overall,
post-hoc explainability techniques are divided first by the intention of the author (explanation technique
e.g. Explanation by simplification), then, by the method utilized (actual technique e.g. sensitivity analysis)
and finally by the type of data in which it was applied (e.g. images).

11
e Text explanations deal with the problem of bringing explainability for a model by means of learning to
generate text explanations that help explaining the results from the model [169]. Text explanations also
include every method generating symbols that represent the functioning of the model. These symbols
may portrait the rationale of the algorithm by means of a semantic mapping from model to symbols.

e Visual explanation techniques for post-hoc explainability aim at visualizing the model’s behavior.
Many of the visualization methods existing in the literature come along with dimensionality reduction
techniques that allow for a human interpretable simple visualization. Visualizations may be coupled
with other techniques to improve their understanding, and are considered as the most suitable way to
introduce complex interactions within the variables involved in the model to users not acquainted to
ML modeling.

e Local explanations tackle explainability by segmenting the solution space and giving explanations
to less complex solution subspaces that are relevant for the whole model. These explanations can be
formed by means of techniques with the differentiating property that these only explain part of the
whole system’s functioning.

e Explanations by example consider the extraction of data examples that relate to the result generated by
a certain model, enabling to get a better understanding of the model itself. Similarly to how humans
behave when attempting to explain a given process, explanations by example are mainly centered in
extracting representative examples that grasp the inner relationships and correlations found by the
model being analyzed.

e Explanations by simplification collectively denote those techniques in which a whole new system is
rebuilt based on the trained model to be explained. This new, simplified model usually attempts at
optimizing its resemblance to its antecedent functioning, while reducing its complexity, and keeping a
similar performance score. An interesting byproduct of this family of post-hoc techniques is that the
simplified model is, in general, easier to be implemented due to its reduced complexity with respect to
the model it represents.

e Finally, feature relevance explanation methods for post-hoc explainability clarify the inner functioning
of a model by computing a relevance score for its managed variables. These scores quantify the affection
(sensitivity) a feature has upon the output of the model. A comparison of the scores among different
variables unveils the importance granted by the model to each of such variables when producing its
output. Feature relevance methods can be thought to be an indirect method to explain a model.

The above classification (portrayed graphically in Figure 4) will be used when reviewing spe-
cific/agnostic XAI techniques for ML models in the following sections (Table 2). For each ML model, a
distinction of the propositions to each of these categories is presented in order to pose an overall image of
the field’s trends.

3. Transparent Machine Learning Models

The previous section introduced the concept of transparent models. A model is considered to be
transparent if by itself it is understandable. The models surveyed in this section are a suit of transparent
models that can fall in one or all of the levels of model transparency described previously (namely,
simulatability, decomposability and algorithmic transparency). In what follows we provide reasons for
this statement, with graphical support given in Figure 5.

12
 

       
   

 

1"
Ls ]
Ls]

  

Model Black-box Feature
simplification model relevance

 

 

 

 

_— 1 me
x = (2',...,27)
x;: Input instance

 

 

 

” Explanatory examples
for the model:”

    

 

 

Figure 4: Conceptual diagram showing the different post-hoc explainability approaches available fora ML model My.

3.1. Linear/Logistic Regression

Logistic Regression (LR) is a classification model to predict a dependent variable (category) that is
dichotomous (binary). However, when the dependent variable is continuous, linear regression would
be its homonym. This model takes the assumption of linear dependence between the predictors and the
predicted variables, impeding a flexible fit to the data. This specific reason (stiffness of the model) is the
one that maintains the model under the umbrella of transparent methods. However, as stated in Section 2,
explainability is linked to a certain audience, which makes a model fall under both categories depending
who is to interpret it. This way, logistic and linear regression, although clearly meeting the characteristics
of transparent models (algorithmic transparency, decomposability and simulatability), may also demand
post-hoc explainability techniques (mainly, visualization), particularly when the model is to be explained
to non-expert audiences.

The usage of this model has been largely applied within Social Sciences for quite a long time,
which has pushed researchers to create ways of explaining the results of the models to non-expert
users. Most authors agree on the different techniques used to analyze and express the soundness of LR
[174, 175, 176, 177], including the overall model evaluation, statistical tests of individual predictors,
goodness-of-fit statistics and validation of the predicted probabilities. The overall model evaluation
shows the improvement of the applied model over a baseline, showing if it is in fact improving the model
without predictions. The statistical significance of single predictors is shown by calculating the Wald
chi-square statistic. The goodness-of-fit statistics show the quality of fitness of the model to the data
and how significant this is. This can be achieved by resorting to different techniques e.g. the so-called
Hosmer-Lemeshow (H-L) statistic. The validation of predicted probabilities involves testing whether the
output of the model corresponds to what is shown by the data. These techniques show mathematical ways
of representing the fitness of the model and its behavior.

Other techniques from other disciplines besides Statistics can be adopted for explaining these re-

13
 

Transparent ML Models

 

 

 

Model Post-hoc
el Simulatability Decomposability Algorithmic Transparency analysis
Predictors are human readable and Variables are still readable, butthe number, ae
. a . ntencn es a or ou P&T Variables and interactions are too complex
Linear/Logistic Regression _ interactions among them are kept toa _of interactions and predictors involved in ° ‘ Not needed
me oto be analyzed without mathematical tools
minimum them have grown to force decomposition
‘A human can simulate and obtain the - Human-readabie rules that explain the
se m " The model comprises rules that do not alter
a prediction of a decision tree on his/her own, eT Knowledge leamed from data and allows
Decision Trees data whatsoever, and preserves their Not needed

without requiring any mathematical readability for a direct understanding of the prediction
background process

The amount of variables is too high and/or

The complexity of the model (number of ‘the similarity measure is too complex to be

variables, their understandability and the able to simulate the model completely, but

 

The similarity measure cannot be
decomposed and/or the number of

K-Nearest Neighbors variables is so high that the user has to rely Not needed

similarity measure under use) matches _the similarity measure and the set of : ve
' a ee : on mathematical and statistical tools to
human naive capabilities for simulation variables can be decomposed and analyzed

separately analyze the model

 

Rules have become so complicated (and

‘Variables included in rules are readable, The size of the rule set becomes too large °
the rule set size has grown so much) that

 

 

Rule Based Learners —_and the size of the rule set is manageable to be analyzed without decomposing it into 7 Not needed
, mathematical tools are needed for
by a human user without external help small rule chunks nathems ;
inspecting the model behaviour
‘Variables and the interaction among them - Due to their complexity, variables and
noe : Interactions become too complex tobe | . .
vs as per the smooth functions involved in the ee > interactions cannot be analyzed without the
General Additive Models rns ive simulated, so decomposition techniques are non ° nut Not needed
model must be constrained within human . " application of mathematical and statistical
aa " required for analyzing the model
capabilities for understanding tools
a an Statistical relationships cannot be
Statistical relationships modeled among _ oe . st P
a ee ced the ake thermaclone Statistical relationships involve so many interpreted even if already decomposed,
Bayesian Models variables that they must be decomposed in. and predictors are so complex that model Not needed

should be directly understandable by the

 

 

 

 

 

© marginals so as to ease their analysis can be only analyzed with mathematical
target audience tools
Tice Froombies x x x Neeied: Usually Model simplification or
Feature relevance techniques
Suppor Vector Mackines x x x Neeied: Usually Model simplification or
Local explanations techniques
Needed: Usually Model simplification,
Multi-layer Neural Network x x x Feature relevance or Visualization
techniques
Convolutional Neural Network x x x Needed: Usually Feature relevance or
Visualization techniques
Rocuront Newal Network x x x Needed: Usually Feature relevance

techniques

 

Table 2: Overall picture of the classification of ML models attending to their level of explainability.

gression models. Visualization techniques are very powerful when presenting statistical conclusions to
users not well-versed in statistics. For instance, the work in [178] shows that the usage of probabilities to
communicate the results, implied that the users where able to estimate the outcomes correctly in 10% of
the cases, as opposed to 46% of the cases when using natural frequencies. Although logistic regression is
among the simplest classification models in supervised learning, there are concepts that must be taken
care of.

In this line of reasoning, the authors of [179] unveil some concerns with the interpretations derived
from LR. They first mention how dangerous it might be to interpret log odds ratios and odd ratios as
substantive effects, since they also represent unobserved heterogeneity. Linked to this first concern,
[179] also states that a comparison between these ratios across models with different variables might be
problematic, since the unobserved heterogeneity is likely to vary, thereby invalidating the comparison.
Finally they also mention that the comparison of these odds across different samples, groups and time is
also risky, since the variation of the heterogeneity is not known across samples, groups and time points.
This last paper serves the purpose of visualizing the problems a model’s interpretation might entail, even
when its construction is as simple as that of LR.

Also interesting is to note that, for a model such as logistic or linear regression to maintain decompos-
ability and simulatability, its size must be limited, and the variables used must be understandable by their
users. As stated in Section 2, if inputs to the model are highly engineered features that are complex or
difficult to understand, the model at hand will be far from being decomposable. Similarly, if the model is
so large that a human cannot think of the model as a whole, its simulatability will be put to question.

3.2. Decision Trees

Decision trees are another example of a model that can easily fulfill every constraint for transparency.
Decision trees are hierarchical structures for decision making used to support regression and classification
problems [132, 180]. In the simplest of their flavors, decision trees are simulatable models. However,
their properties can render them decomposable or algorithmically transparent.

14
 
    
 

127 22 bh
y = wits + weve + wo i)
Training 2 tay" a

dataset 2

Training
dataset

m2y
Training

dataset
Ly Class O % ClassO
Support: 70% test .
r2 wy | ? Impurity: 0.1 y. ? / ay

Straightforward what-if testing & similar training instances
Simple univariate thresholds Prediction by majority voting
wo (intercept): y for a test instance Direct support and impurity measures Simulatable, decomposable
with average normalized features Simulatable, decomposable Algorithmic transparency (lazy training)

(a) (b) (c)
a kugatha yo y)) <1 fer) + we folas P(yler, #2) & p(yler joules)
GC C) = t “1 is vee ets ° GC Dy ti eace io) * © GC C) (1)

high then y=0

 

   

 

 

 

 

 

w;: increase in y if a;
increases by one unit

    
   
   

  

 

 

 

 

    
 

   
  
 

Plylzi)
o

 

 

 

 

 

 
 

i i z i
Training — Ifz2 is low then y =O Training 92) y , Training @2) s
dataset 9 dataset 2 y dataset *
@ AL 2
Linguistic rules: easy to interpret(“\ Simulatable, decomposable (9 The independence assumption permits:

   
 

to assess the contribution of each variable
Simulatable, decomposable
Algorithmic transparency (distribution fitting)

Interpretability depends on link PAY
function g(z), the selected f;(x:)
and the sparseness of [w1,..., wa]

Simulatable if ruleset coverage and
specifity are kept constrained
Fuzziness improves interpretability

(d) (e) (f)

Figure 5: Graphical illustration of the levels of transparency of different ML models considered in this overview: (a) Linear
regression; (b) Decision trees; (c) K-Nearest Neighbors; (d) Rule-based Learners; (e) Generalized Additive Models; (f) Bayesian
Models.

 

 

 

 

 

Decision trees have always lingered in between the different categories of transparent models. Their
utilization has been closely linked to decision making contexts, being the reason why their complexity
and understandability have always been considered a paramount matter. A proof of this relevance can
be found in the upsurge of contributions to the literature dealing with decision tree simplification and
generation [132, 180, 181, 182]. As noted above, although being capable of fitting every category within
transparent models, the individual characteristics of decision trees can push them toward the category of
algorithmically transparent models. A simulatable decision tree is one that is manageable by a human
user. This means its size is somewhat small and the amount of features and their meaning are easily
understandable. An increment in size transforms the model into a decomposable one since its size impedes
its full evaluation (simulation) by a human. Finally, further increasing its size and using complex feature
relations will make the model algorithmically transparent loosing the previous characteristics.

Decision trees have long been used in decision support contexts due to their off-the-shelf transparency.
Many applications of these models fall out of the fields of computation and AI (even information
technologies), meaning that experts from other fields usually feel comfortable interpreting the outputs of
these models [183, 184, 185]. However, their poor generalization properties in comparison with other
models make this model family less interesting for their application to scenarios where a balance between
predictive performance is a design driver of utmost importance. Tree ensembles aim at overcoming such
a poor performance by aggregating the predictions performed by trees learned on different subsets of
training data. Unfortunately, the combination of decision trees looses every transparent property, calling
for the adoption of post-hoc explainability techniques as the ones reviewed later in the manuscript.

15
3.3. K-Nearest Neighbors

Another method that falls within transparent models is that of K-Nearest Neighbors (KNN), which
deals with classification problems in a methodologically simple way: it predicts the class of a test sample
by voting the classes of its K nearest neighbors (where the neighborhood relation is induced by a measure
of distance between samples). When used in the context of regression problems, the voting is replaced by
an aggregation (e.g. average) of the target values associated with the nearest neighbors.

In terms of model explainability, it is important to observe that predictions generated by KNN models
rely on the notion of distance and similarity between examples, which can be tailored depending on the
specific problem being tackled. Interestingly, this prediction approach resembles that of experience-based
human decision making, which decides upon the result of past similar cases. There lies the rationale
of why KNN has also been adopted widely in contexts in which model interpretability is a requirement
[186, 187, 188, 189]. Furthermore, aside from being simple to explain, the ability to inspect the reasons
by which a new sample has been classified inside a group and to examine how these predictions evolve
when the number of neighbors K is increased or decreased empowers the interaction between the users
and the model.

One must keep in mind that as mentioned before, KNN’s class of transparency depends on the features,
the number of neighbors and the distance function used to measure the similarity between data instances.
A very high K impedes a full simulation of the model performance by a human user. Similarly, the usage
of complex features and/or distance functions would hinder the decomposability of the model, restricting
its interpretability solely to the transparency of its algorithmic operations.

3.4. Rule-based Learning

Rule-based learning refers to every model that generates rules to characterize the data it is intended to
learn from. Rules can take the form of simple conditional if-then rules or more complex combinations of
simple rules to form their knowledge. Also connected to this general family of models, fuzzy rule based
systems are designed for a broader scope of action, allowing for the definition of verbally formulated
rules over imprecise domains. Fuzzy systems improve two main axis relevant for this paper. First, they
empower more understandable models since they operate in linguistic terms. Second, they perform better
that classic rule systems in contexts with certain degrees of uncertainty. Rule based learners are clearly
transparent models that have been often used to explain complex models by generating rules that explain
their predictions [126, 127, 190, 191].

Rule learning approaches have been extensively used for knowledge representation in expert systems
[192]. However, a central problem with rule generation approaches is the coverage (amount) and the
specificity (length) of the rules generated. This problem relates directly to the intention for their use in
the first place. When building a rule database, a typical design goal sought by the user is to be able to
analyze and understand the model. The amount of rules in a model will clearly improve the performance
of the model at the stake of compromising its intepretability. Similarly, the specificity of the rules plays
also against interpretability, since a rule with a high number of antecedents an/or consequences might
become difficult to interpret. In this same line of reasoning, these two features of a rule based learner
play along with the classes of transparent models presented in Section 2. The greater the coverage or
the specificity is, the closer the model will be to being just algorithmically transparent. Sometimes, the
reason to transition from classical rules to fuzzy rules is to relax the constraints of rule sizes, since a
greater range can be covered with less stress on interpretability.

Rule based learners are great models in terms of interpretability across fields. Their natural and
seamless relation to human behaviour makes them very suitable to understand and explain other models.
If a certain threshold of coverage is acquired, a rule wrapper can be thought to contain enough information
about a model to explain its behavior to a non-expert user, without forfeiting the possibility of using the
generated rules as an standalone prediction model.

16
3.5. General Additive Models

In statistics, a Generalized Additive Model (GAM) is a linear model in which the value of the
variable to be predicted is given by the aggregation of a number of unknown smooth functions defined
for the predictor variables. The purpose of such model is to infer the smooth functions whose aggregate
composition approximates the predicted variable. This structure is easily interpretable, since it allows the
user to verify the importance of each variable, namely, how it affects (through its corresponding function)
the predicted output.

Similarly to every other transparent model, the literature is replete with case studies where GAMs
are in use, specially in fields related to risk assessment. When compared to other models, these are
understandable enough to make users feel confident on using them for practical applications in finance
[193, 194, 195], environmental studies [196], geology [197], healthcare [44], biology [198, 199] and
energy [200]. Most of these contributions use visualization methods to further ease the interpretation of
the model. GAMs might be also considered as simulatable and decomposable models if the properties
mentioned in its definitions are fulfilled, but to an extent that depends roughly on eventual modifications
to the baseline GAM model, such as the introduction of link functions to relate the aggregation with the
predicted output, or the consideration of interactions between predictors.

All in all, applications of GAMs like the ones exemplified above share one common factor: under-
standability. The main driver for conducting these studies with GAMs is to understand the underlying
relationships that build up the cases for scrutiny. In those cases the research goal is not accuracy for its
own sake, but rather the need for understanding the problem behind and the relationship underneath the
variables involved in data. This is why GAMs have been accepted in certain communities as their de facto
modeling choice, despite their acknowledged misperforming behavior when compared to more complex
counterparts.

3.6. Bayesian Models

A Bayesian model usually takes the form of a probabilistic directed acyclic graphical model whose
links represent the conditional dependencies between a set of variables. For example, a Bayesian network
could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the
network can be used to compute the probabilities of the presence of various diseases. Similar to GAMs,
these models also convey a clear representation of the relationships between features and the target, which
in this case are given explicitly by the connections linking variables to each other.

Once again, Bayesian models fall below the ceiling of Transparent models. Its categorization leaves
it under simulatable, decomposable and algorithmically transparent. However, it is worth noting that
under certain circumstances (overly complex or cumbersome variables), a model may loose these first
two properties. Bayesian models have been shown to lead to great insights in assorted applications such
as cognitive modeling [201, 202], fishery [196, 203], gaming [204], climate [205], econometrics [206] or
robotics [207]. Furthermore, they have also been utilized to explain other models, such as averaging tree
ensembles [208].

4. Post-hoc Explainability Techniques for Machile Learning Models: Taxonomy, Shallow Models
and Deep Learning

When ML models do not meet any of the criteria imposed to declare them transparent, a separate
method must be devised and applied to the model to explain its decisions. This is the purpose of post-hoc
explainability techniques (also referred to as post-modeling explainability), which aim at communicating
understandable information about how an already developed model produces its predictions for any given
input. In this section we categorize and review different algorithmic approaches for post-hoc explainability,
discriminating among 1) those that are designed for their application to ML models of any kind; and 2)
those that are designed for a specific ML model and thus, can not be directly extrapolated to any other

17
learner. We now elaborate on the trends identified around post-hoc explainability for different ML models,
which are illustrated in Figure 6 in the form of hierarchical bibliographic categories and summarized next:

e Model-agnostic techniques for post-hoc explainability (Subsection 4.1), which can be applied seam-
lessly to any ML model disregarding its inner processing or internal representations.

e Post-hoc explainability that are tailored or specifically designed to explain certain ML models. We
divide our literature analysis into two main branches: contributions dealing with post-hoc explainability
of shallow ML models, which collectively refers to all ML models that do not hinge on layered
structures of neural processing units (Subsection 4.2); and techniques devised for deep learning models,
which correspondingly denote the family of neural networks and related variants, such as convolutional
neural networks, recurrent neural networks (Subsection 4.3) and hybrid schemes encompassing deep
neural networks and transparent models. For each model we perform a thorough review of the latest
post-hoc methods proposed by the research community, along with a identification of trends followed
by such contributions.

e We end our literature analysis with Subsection 4.4, where we present a second taxonomy that com-
plements the more general one in Figure 6 by classifying contributions dealing with the post-hoc
explanation of Deep Learning models. To this end we focus on particular aspects related to this family
of black-box ML methods, and expose how they link to the classification criteria used in the first
taxonomy.

4.1. Model-agnostic Techniques for Post-hoc Explainability

Model-agnostic techniques for post-hoc explainability are designed to be plugged to any model
with the intent of extracting some information from its prediction procedure. Sometimes, simplification
techniques are used to generate proxies that mimic their antecedents with the purpose of having something
tractable and of reduced complexity. Other times, the intent focuses on extracting knowledge directly
from the models or simply visualizing them to ease the interpretation of their behavior. Following the
taxonomy introduced in Section 2, model-agnostic techniques may rely on model simplification, feature
relevance estimation and visualization techniques:

e Explanation by simplification. They are arguably the broadest technique under the category of model
agnostic post-hoc methods. Local explanations are also present within this category, since sometimes,
simplified models are only representative of certain sections of a model. Almost all techniques taking
this path for model simplification are based on rule extraction techniques. Among the most known
contributions to this approach we encounter the technique of Local Interpretable Model-Agnostic
Explanations (LIME) [32] and all its variations [214, 216]. LIME builds locally linear models around
the predictions of an opaque model to explain it. These contributions fall under explanations by
simplification as well as under local explanations. Besides LIME and related flavors, another approach
to rule extraction is G-REX [212]. Although it was not originally intended for extracting rules from
opaque models, the generic proposition of G-REX has been extended to also account for model
explainability purposes [190, 211]. In line with rule extraction methods, the work in [215] presents a
novel approach to learn rules in CNF (Conjunctive Normal Form) or DNF (Disjunctive Normal Form)
to bridge from a complex model to a human-interpretable model. Another contribution that falls off the
same branch is that in [218], where the authors formulate model simplification as a model extraction
process by approximating a transparent model to the complex one. Simplification is approached from a
different perspective in [120], where an approach to distill and audit black box models is presented. In
it, two main ideas are exposed: a method for model distillation and comparison to audit black-box risk
scoring models; and an statistical test to check if the auditing data is missing key features it was trained
with. The popularity of model simplification is evident, given it temporally coincides with the most

18
jenae Waa
[Decision Tree:

[Transparent Modelg

Hl

 

ano
Sensiivity: [ea 2a

[Game theory inspired:

[Feature relevance explanation
[Woda Agroste interaction based:

[Others [[iao.ta 25,20.)

[Ormers:
[Decisron Tree/Pratctype:
[Feature relevance explanation] [Feature importance Feontribution® [[ros_To1, 10,70
[Rule-based learner:
(Probabilistic: [a7 2am]

 

   
   

   
 

 

[Ensembles and Multiple Classifier System]

 

[Variable importance / attribution: (joa 2a [ea =]

  
  
   
 

   

[Explanation by simplification

“eature relevance explanatior “eatare Contribution / Statistics: [[e@=y]ii16. 2]
[Rule-based learner:

[Decision Tree: [juice 7 #197, 1s, 7 DED)

[Post- Hoc Explainability|
Support Vector

 

 

   
 
 

[Explanation by simplification

 
     

 

[Feature relevance explanation

[Multi-Layer Neural Networks] [Local explanation] [Decision Tree Sensitivity: Teal]

[Explanation by simplification]

 

  
   
 
     

 
   

[Feature relevance explanation
[Feature Extraction:

    
   

[Fitter 7 Activation: [jue 17, We, 150,265,270,

      
 
   
   
       
 
      

 

  

(Convolutional Neural Networks

 

Visual explanation] Sensitivity / Saliency:

[Layer modification: [faz 7

[Model combination:

    

(Architecture modification] Attention networks: [lor a2 ao]

 

[Loss modification:

fRule-based learner: [ia]

[Explanation by simplification

[Feature relevance explanation

[Visual explanation
Arquitecture modification

     

[Activation propagation:

 
   

 

[Recurrent Neural Networks

 
 
 
 

[Loss 7 Layer modification:

   
  

      

fOmhers: [isn eas

Figure 6: Taxonomy of the reviewed literature and trends identified for explainability techniques related to different ML models.
References boxed in blue, green and red correspond to XAI techniques using image, text or tabular data, respectively. In order
to build this taxonomy, the literature has been analyzed in depth to discriminate whether a post-hoc technique can be seamlessly
applied to any ML model, even if, e.g., explicitly mentions Deep Learning in its title and/or abstract.

recent literature on XAI, including techniques such as LIME or G-REX. This symptomatically reveals

that this post-hoc explainability approach is envisaged to continue playing a central role on XAI.

e Feature relevance explanation techniques aim to describe the functioning of an opaque model by

19
ranking or measuring the influence, relevance or importance each feature has in the prediction output by
the model to be explained. An amalgam of propositions are found within this category, each resorting
to different algorithmic approaches with the same targeted goal. One fruitful contribution to this path
is that of [224] called SHAP (SHapley Additive exPlanations). Its authors presented a method to
calculate an additive feature importance score for each particular prediction with a set of desirable
properties (local accuracy, missingness and consistency) that its antecedents lacked. Another approach
to tackle the contribution of each feature to predictions has been coalitional Game Theory [225] and
local gradients [234]. Similarly, by means of local gradients [230] test the changes needed in each
feature to produce a change in the output of the model. In [228] the authors analyze the relations and
dependencies found in the model by grouping features, that combined, bring insights about the data.
The work in [173] presents a broad variety of measures to tackle the quantification of the degree of
influence of inputs on outputs of systems. Their QII (Quantitative Input Influence) measures account
for correlated inputs while measuring influence. In contrast, in [222] the authors build upon the existing
SA (Sensitivity Analysis) to construct a Global SA which extends the applicability of the existing
methods. In [227] a real-time image saliency method is proposed, which is applicable to differentiable
image classifiers. The study in [123] presents the so-called Automatic STRucture [Dentification method
(ASTRID) to inspect which attributes are exploited by a classifier to generate a prediction. This method
finds the largest subset of features such that the accuracy of a classifier trained with this subset of
features cannot be distinguished in terms of accuracy from a classifier built on the original feature set.
In [221] the authors use influence functions to trace a model’s prediction back to the training data, by
only requiring an oracle version of the model with access to gradients and Hessian-vector products.
Heuristics for creating counterfactual examples by modifying the input of the model have been also
found to contribute to its explainability [236, 237]. Compared to those attempting explanations by
simplification, a similar amount of publications were found tackling explainability by means of feature
relevance techniques. Many of the contributions date from 2017 and some from 2018, implying that as
with model simplification techniques, feature relevance has also become a vibrant subject study in the
current XAI landscape.

e Visual explanation techniques are a vehicle to achieve model-agnostic explanations. Representative
works in this area can be found in [222], which present a portfolio of visualization techniques to help in
the explanation of a black-box ML model built upon the set of extended techniques mentioned earlier
(Global SA). Another set of visualization techniques is presented in [223]. The authors present three
novel SA methods (data based SA, Monte-Carlo SA, cluster-based SA) and one novel input importance
measure (Average Absolute Deviation). Finally, [238] presents ICE (Individual Conditional Expecta-
tion) plots as a tool for visualizing the model estimated by any supervised learning algorithm. Visual
explanations are less common in the field of model-agnostic techniques for post-hoc explainability.
Since the design of these methods must ensure that they can be seamlessly applied to any ML model
disregarding its inner structure, creating visualizations from just inputs and outputs from an opaque
model is a complex task. This is why almost all visualization methods falling in this category work
along with feature relevance techniques, which provide the information that is eventually displayed to
the end user.

Several trends emerge from our literature analysis. To begin with, rule extraction techniques prevail
in model-agnostic contributions under the umbrella of post-hoc explainability. This could have been
intuitively expected if we bear in mind the wide use of rule based learning as explainability wrappers
anticipated in Section 3.4, and the complexity imposed by not being able to get into the model itself.
Similarly, another large group of contributions deals with feature relevance. Lately these techniques are
gathering much attention by the community when dealing with DL models, with hybrid approaches that
utilize particular aspects of this class of models and therefore, compromise the independence of the feature
relevance method on the model being explained. Finally, visualization techniques propose interesting

20
ways for visualizing the output of feature relevance techniques to ease the task of model’s interpretation.
By contrast, visualization techniques for other aspects of the trained model (e.g. its structure, operations,
etc) are tightly linked to the specific model to be explained.

4.2. Post-hoc Explainability in Shallow ML Models

Shallow ML covers a diversity of supervised learning models. Within these models, there are strictly
interpretable (transparent) approaches (e.g. KNN and Decision Trees, already discussed in Section 3).
However, other shallow ML models rely on more sophisticated learning algorithms that require additional
layers of explanation. Given their prominence and notable performance in predictive tasks, this section
concentrates on two popular shallow ML models (tree ensembles and Support Vector Machines, SVMs)
that require the adoption of post-hoc explainability techniques for explaining their decisions.

4.2.1. Tree Ensembles, Random Forests and Multiple Classifier Systems

Tree ensembles are arguably among the most accurate ML models in use nowadays. Their advent
came as an efficient means to improve the generalization capability of single decision trees, which are
usually prone to overfitting. To circumvent this issue, tree ensembles combine different trees to obtain an
aggregated prediction/regression. While it results to be effective against overfitting, the combination of
models makes the interpretation of the overall ensemble more complex than each of its compounding tree
learners, forcing the user to draw from post-hoc explainability techniques. For tree ensembles, techniques
found in the literature are explanation by simplification and feature relevance techniques; we next examine
recent advances in these techniques.

To begin with, many contributions have been presented to simplify tree ensembles while maintaining
part of the accuracy accounted for the added complexity. The author from [119] poses the idea of training
a single albeit less complex model from a set of random samples from the data (ideally following the real
data distribution) labeled by the ensemble model. Another approach for simplification is that in [118], in
which authors create a Simplified Tree Ensemble Learner (STEL). Likewise, [122] presents the usage
of two models (simple and complex) being the former the one in charge of interpretation and the latter
of prediction by means of Expectation-Maximization and Kullback-Leibler divergence. As opposed to
what was seen in model-agnostic techniques, not that many techniques to board explainability in tree
ensembles by means of model simplification. It derives from this that either the proposed techniques are
good enough, or model-agnostic techniques do cover the scope of simplification already.

Following simplification procedures, feature relevance techniques are also used in the field of tree
ensembles. Breiman [286] was the first to analyze the variable importance within Random Forests. His
method is based on measuring MDA (Mean Decrease Accuracy) or MIE (Mean Increase Error) of the
forest when a certain variable is randomly permuted in the out-of-bag samples. Following this contribution
[241] shows, in an real setting, how the usage of variable importance reflects the underlying relationships
of a complex system modeled by a Random Forest. Finally, a crosswise technique among post-hoc
explainability, [240] proposes a framework that poses recommendations that, if taken, would convert
an example from one class to another. This idea attempts to disentangle the variables importance in a
way that is further descriptive. In the article, the authors show how these methods can be used to elevate
recommendations to improve malicious online ads to make them rank higher in paying rates.

Similar to the trend shown in model-agnostic techniques, for tree ensembles again, simplification and
feature relevance techniques seem to be the most used schemes. However, contrarily to what was observed
before, most papers date back from 2017 and place their focus mostly on bagging ensembles. When
shifting the focus towards other ensemble strategies, scarce activity has been recently noted around the
explainability of boosting and stacking classifiers. Among the latter, it is worth highlighting the connection
between the reason why a compounding learner of the ensemble produces an specific prediction on a given
data, and its contribution to the output of the ensemble. The so-called Stacking With Auxiliary Features
(SWAF) approach proposed in [242] points in this direction by harnessing and integrating explanations in

21
stacking ensembles to improve their generalization. This strategy allows not only relying on the output
of the compounding learners, but also on the origin of that output and its consensus across the entire
ensemble. Other interesting studies on the explainability of ensemble techniques include model-agnostic
schemes such as DeepSHAP [226], put into practice with stacking ensembles and multiple classifier
systems in addition to Deep Learning models; the combination of explanation maps of multiple classifiers
to produce improved explanations of the ensemble to which they belong [243]; and recent insights dealing
with traditional and gradient boosting ensembles [287, 288].

4.2.2. Support Vector Machines

Another shallow ML model with historical presence in the literature is the SVM. SVM models are
more complex than tree ensembles, with a much opaquer structure. Many implementations of post-hoc
explainability techniques have been proposed to relate what is mathematically described internally in
these models, to what different authors considered explanations about the problem at hand. Technically,
an SVM constructs a hyper-plane or set of hyper-planes in a high or infinite-dimensional space, which
can be used for classification, regression, or other tasks such as outlier detection. Intuitively, a good
separation is achieved by the hyperplane that has the largest distance (so-called functional margin) to the
nearest training-data point of any class, since in general, the larger the margin, the lower the generalization
error of the classifier. SVMs are among the most used ML models due to their excellent prediction
and generalization capabilities. From the techniques stated in Section 2, post-hoc explainability applied
to SVMs covers explanation by simplification, local explanations, visualizations and explanations by
example.

Among explanation by simplification, four classes of simplifications are made. Each of them dif-
ferentiates from the other by how deep they go into the algorithm inner structure. First, some authors
propose techniques to build rule based models only from the support vectors of a trained model. This is
the approach of [93], which proposes a method that extracts rules directly from the support vectors of a
trained SVM using a modified sequential covering algorithm. In [57] the same authors propose eclectic
rule extraction, still considering only the support vectors of a trained model. The work in [94] generates
fuzzy rules instead of classical propositional rules. Here, the authors argue that long antecedents reduce
comprehensibility, hence, a fuzzy approach allows for a more linguistically understandable result. The
second class of simplifications can be exemplified by [98], which proposed the addition of the SVM’s
hyperplane, along with the support vectors, to the components in charge of creating the rules. His method
telies on the creation of hyper-rectangles from the intersections between the support vectors and the
hyper-plane. In a third approach to model simplification, another group of authors considered adding
the actual training data as a component for building the rules. In [126, 244, 246] the authors proposed a
clustering method to group prototype vectors for each class. By combining them with the support vectors,
it allowed defining ellipsoids and hyper-rectangles in the input space. Similarly in [106], the authors
proposed the so-called Hyper-rectangle Rule Extraction, an algorithm based on SVC (Support Vector
Clustering) to find prototype vectors for each class and then define small hyper-rectangles around. In
[105], the authors formulate the rule extraction problem as a multi-constrained optimization to create a
set of non-overlapping rules. Each rule conveys a non-empty hyper-cube with a shared edge with the
hyper-plane. In a similar study conducted in [245], extracting rules for gene expression data, the authors
presented a novel technique as a component of a multi-kernel SVM. This multi-kernel method consists
of feature selection, prediction modeling and rule extraction. Finally, the study in [134] makes use of a
growing SVC to give an interpretation to SVM decisions in terms of linear rules that define the space in
Voronoi sections from the extracted prototypes.

Leaving aside rule extraction, the literature has also contemplated some other techniques to contribute
to the interpretation of SVMs. Three of them (visualization techniques) are clearly used toward explaining
SVM models when used for concrete applications. For instance, [77] presents an innovative approach to
visualize trained SVM to extract the information content from the kernel matrix. They center the study

22
on Support Vector Regression models. They show the ability of the algorithm to visualize which of the
input variables are actually related with the associated output data. In [68] a visual way combines the
output of the SVM with heatmaps to guide the modification of compounds in late stages of drug discovery.
They assign colors to atoms based on the weights of a trained linear SVM that allows for a much more
comprehensive way of debugging the process. In [116] the authors argue that many of the presented
studies for interpreting SVMs only account for the weight vectors, leaving the margin aside. In their study
they show how this margin is important, and they create an statistic that explicitly accounts for the SVM
margin. The authors show how this statistic is specific enough to explain the multivariate patterns shown
in neuroimaging.

Noteworthy is also the intersection between SVMs and Bayesian systems, the latter being adopted
as a post-hoc technique to explain decisions made by the SVM model. This is the case of [248] and
[247], which are studies where SVMs are interpreted as MAP (Maximum A Posteriori) solutions to
inference problems with Gaussian Process priors. This framework makes tuning the hyper-parameters
comprehensible and gives the capability of predicting class probabilities instead of the classical binary
classification of SVMs. Interpretability of SVM models becomes even more involved when dealing
with non-CPD (Conditional Positive Definite) kernels that are usually harder to interpret due to missing
geometrical and theoretical understanding. The work in [102] revolves around this issue with a geometrical
interpretation of indefinite kernel SVMs, showing that these do not classify by hyper-plane margin
optimization. Instead, they minimize the distance between convex hulls in pseudo-Euclidean spaces.

A difference might be appreciated between the post-hoc techniques applied to other models and those
noted for SVMs. In previous models, model simplification in a broad sense was the prominent method
for post-hoc explainability. In SVMs, local explanations have started to take some weight among the
propositions. However, simplification based methods are, on average, much older than local explanations.

As a final remark, none of the reviewed methods treating SVM explainability are dated beyond 2017,
which might be due to the progressive proliferation of DL models in almost all disciplines. Another
plausible reason is that these models are already understood, so it is hard to improve upon what has
already been done.

4.3. Explainability in Deep Learning

Post-hoc local explanations and feature relevance techniques are increasingly the most adopted
methods for explaining DNNs. This section reviews explainability studies proposed for the most used
DL models, namely multi-layer neural networks, Convolutional Neural Networks (CNN) and Recurrent
Neural Networks (RNN).

4.3.1. Multi-layer Neural Networks

From their inception, multi-layer neural networks (also known as multi-layer perceptrons) have been
warmly welcomed by the academic community due to their huge ability to infer complex relations among
variables. However, as stated in the introduction, developers and engineers in charge of deploying these
models in real-life production find in their questionable explainability a common reason for reluctance.
That is why neural networks have been always considered as black-box models. The fact that explainability
is often a must for the model to be of practical value, forced the community to generate multiple
explainability techniques for multi-layer neural networks, including model simplification approaches,
feature relevance estimators, text explanations, local explanations and model visualizations.

Several model simplification techniques have been proposed for neural networks with one single
hidden layer, however very few works have been presented for neural networks with multiple hidden
layers. One of these few works is DeepRED algorithm [257], which extends the decompositional approach
to rule extraction (splitting at neuron level) presented in [259] for multi-layer neural network by adding
more decision trees and rules.

Some other works use model simplification as a post-hoc explainability approach. For instance, [56]
presents a simple distillation method called Interpretable Mimic Learning to extract an interpretable model

23
by means of gradient boosting trees. In the same direction, the authors in [135] propose a hierarchical
partitioning of the feature space that reveals the iterative rejection of unlikely class labels, until association
is predicted. In addition, several works addressed the distillation of knowledge from an ensemble of
models into a single model [80, 289, 290] .

Given the fact that the simplification of multi-layer neural networks is more complex as the number of
layers increases, explaining these models by feature relevance methods has become progressively more
popular. One of the representative works in this area is [60], which presents a method to decompose the
network classification decision into contributions of its input elements. They consider each neuron as an
object that can be decomposed and expanded then aggregate and back-propagate these decompositions
through the network, resulting in a deep Taylor decomposition. In the same direction, the authors in [110]
proposed DeepLIFT, an approach for computing importance scores in a multi-layer neural network. Their
method compares the activation of a neuron to the reference activation and assigns the score according to
the difference.

On the other hand, some works try to verify the theoretical soundness of current explainability methods.
For example, the authors in [262], bring up a fundamental problem of most feature relevance techniques,
designed for multi-layer networks. They showed that two axioms that such techniques ought to fulfill
namely, sensitivity and implementation invariance, are violated in practice by most approaches. Following
these axioms, the authors of [262] created integrated gradients, a new feature relevance method proven
to meet the aforementioned axioms. Similarly, the authors in [61] analyzed the correctness of current
feature relevance explanation approaches designed for Deep Neural Networks, e,g., DeConvNet, Guided
BackProp and LRP, on simple linear neural networks. Their analysis showed that these methods do not
produce the theoretically correct explanation and presented two new explanation methods PatternNet and
PatternAttribution that are more theoretically sound for both, simple and deep neural networks.

4.3.2. Convolutional Neural Networks

Currently, CNNs constitute the state-of-art models in all fundamental computer vision tasks, from
image classification and object detection to instance segmentation. Typically, these models are built as
a sequence of convolutional layers and pooling layers to automatically learn increasingly higher level
features. At the end of the sequence, one or multiple fully connected layers are used to map the output
features map into scores. This structure entails extremely complex internal relations that are very difficult
to explain. Fortunately, the road to explainability for CNNs is easier than for other types of models, as the
human cognitive skills favors the understanding of visual data.

Existing works that aim at understanding what CNNs learn can be divided into two broad categories:
1) those that try to understand the decision process by mapping back the output in the input space to
see which parts of the input were discriminative for the output; and 2) those that try to delve inside the
network and interpret how the intermediate layers see the external world, not necessarily related to any
specific input, but in general.

One of the seminal works in the first category was [291]. When an input image runs feed-forward
through a CNN, each layer outputs a number of feature maps with strong and soft activations. The authors
in [291] used Deconvnet, a network designed previously by the same authors [142] that, when fed with a
feature map from a selected layer, reconstructs the maximum activations. These reconstructions can give
an idea about the parts of the image that produced that effect. To visualize these strongest activations in
the input image, the same authors used the occlusion sensitivity method to generate a saliency map [136],
which consists of iteratively forwarding the same image through the network occluding a different region
at a time.

To improve the quality of the mapping on the input space, several subsequent papers proposed
simplifying both the CNN architecture and the visualization method. In particular, [96] included a global
average pooling layer between the last convolutional layer of the CNN and the fully-connected layer that
predicts the object class. With this simple architectural modification of the CNN, the authors built a class

24
activation map that helps identify the image regions that were particularly important for a specific object
class by projecting back the weights of the output layer on the convolutional feature maps. Later, in [143],
the authors showed that max-pooling layers can be used to replace convolutional layers with a large stride
without loss in accuracy on several image recognition benchmarks. They obtained a cleaner visualization
than Deconvnet by using a guided backpropagation method.

To increase the interpretability of classical CNNs, the authors in [113] used a loss for each filter in
high level convolutional layers to force each filter to learn very specific object components. The obtained
activation patterns are much more interpretable for their exclusiveness with respect to the different labels
to be predicted. The authors in [72] proposed visualizing the contribution to the prediction of each single
pixel of the input image in the form of a heatmap. They used a Layer-wise Relevance Propagation (LRP)
technique, which relies on a Taylor series close to the prediction point rather than partial derivatives at
the prediction point itself. To further improve the quality of the visualization, attribution methods such
as heatmaps, saliency maps or class activation methods (GradCAM [292]) are used (see Figure 7). In
particular, the authors in [292] proposed a Gradient-weighted Class Activation Mapping (Grad-CAM),
which uses the gradients of any target concept, flowing into the final convolutional layer to produce a
coarse localization map, highlighting the important regions in the image for predicting the concept.

 

(a) Heatmap [168] (b) Attribution [293] (c) Grad-CAM [292]

Figure 7: Examples of rendering for different XAI visualization techniques on images.

In addition to the aforementioned feature relevance and visual explanation methods, some works
proposed generating text explanations of the visual content of the image. For example, the authors in [91]
combined a CNN feature extractor with an RNN attention model to automatically learn to describe the
content of images. In the same line, [278] presented a three-level attention model to perform a fine-grained
classification task. The general model is a pipeline that integrates three types of attention: the object
level attention model proposes candidate image regions or patches from the input image, the part-level
attention model filters out non-relevant patches to a certain object, and the last attention model localizes
discriminative patches. In the task of video captioning, the authors in [111] use a CNN model combined
with a bi-directional LSTM model as encoder to extract video features and then feed these features to an
LSTM decoder to generate textual descriptions.

One of the seminal works in the second category is [137]. In order to analyse the visual information
contained inside the CNN, the authors proposed a general framework that reconstruct an image from the
CNN internal representations and showed that several layers retain photographically accurate information
about the image, with different degrees of geometric and photometric invariance. To visualize the notion
of aclass captured by a CNN, the same authors created an image that maximizes the class score based on
computing the gradient of the class score with respect to the input image [272]. In the same direction,
the authors in [268] introduced a Deep Generator Network (DGN) that generates the most representative
image for a given output neuron in a CNN.

For quantifying the interpretability of the latent representations of CNNs, the authors in [125] used a
different approach called network dissection. They run a large number of images through a CNN and then
analyze the top activated images by considering each unit as a concept detector to further evaluate each

25
unit for semantic segmentation. This paper also examines the effects of classical training techniques on
the interpretability of the learned model.

Although many of the techniques examined above utilize local explanations to achieve an overall
explanation of a CNN model, others explicitly focus on building global explanations based on locally found
prototypes. In [263, 294], the authors empirically showed how local explanations in deep networks are
strongly dominated by their lower level features. They demonstrated that deep architectures provide strong
priors that prevent the altering of how these low-level representations are captured. All in all, visualization
mixed with feature relevance methods are arguably the most adopted approach to explainability in CNNs.

Instead of using one single interpretability technique, the framework proposed in [295] combines
several methods to provide much more information about the network. For example, combining feature
visualization (what is a neuron looking for?) with attribution (how does it affect the output?) allows
exploring how the network decides between labels. This visual interpretability interface displays different
blocks such as feature visualization and attribution depending on the visualization goal. This interface
can be thought of as a union of individual elements that belong to layers (input, hidden, output), atoms (a
neuron, channel, spatial or neuron group), content (activations — the amount a neuron fires, attribution —
which classes a spatial position most contributes to, which tends to be more meaningful in later layers), and
presentation (information visualization, feature visualization). Figure 8 shows some examples. Attribution
methods normally rely on pixel association, displaying what part of an input example is responsible for
the network activating in a particular way [293].

    

(a) Neuron (b) Channel (c) Layer

Figure 8: Feature visualization at different levels of a certain network [293].

 

(a) Original image (b) Explaining electric guitar (c) Explaining acoustic guitar

Figure 9: Examples of explanation when using LIME on images [71].
A much simpler approach to all the previously cited methods was proposed in LIME framework
[71], as was described in Subsection 4.1 LIME perturbs the input and sees how the predictions change.

In image classification, LIME creates a set of perturbed instances by dividing the input image into
interpretable components (contiguous superpixels), and runs each perturbed instance through the model

26
to get a probability. A simple linear model learns on this data set, which is locally weighted. At the end of
the process, LIME presents the superpixels with highest positive weights as an explanation (see Figure 9).
A completely different explainability approach is proposed in adversarial detection. To understand
model failures in detecting adversarial examples, the authors in [264] apply the k-nearest neighbors
algorithm on the representations of the data learned by each layer of the CNN. A test input image is
considered as adversarial if its representations are far from the representations of the training images.

4.3.3. Recurrent Neural Networks

As occurs with CNNs in the visual domain, RNNs have lately been used extensively for predictive
problems defined over inherently sequential data, with a notable presence in natural language processing
and time series analysis. These types of data exhibit long-term dependencies that are complex to be
captured by a ML model. RNNs are able to retrieve such time-dependent relationships by formulating the
retention of knowledge in the neuron as another parametric characteristic that can be learned from data.

Few contributions have been made for explaining RNN models. These studies can be divided into two
groups: 1) explainability by understanding what a RNN model has learned (mainly via feature relevance
methods); and 2) explainability by modifying RNN architectures to provide insights about the decisions
they make (local explanations).

In the first group, the authors in [280] extend the usage of LRP to RNNs. They propose a specific
propagation rule that works with multiplicative connections as those in LSTMs (Long Short Term Memory)
units and GRUs (Gated Recurrent Units). The authors in [281] propose a visualization technique based on
finite horizon n-grams that discriminates interpretable cells within LSTM and GRU networks. Following
the premise of not altering the architecture, [296] extends the interpretable mimic learning distillation
method used for CNN models to LSTM networks, so that interpretable features are learned by fitting
Gradient Boosting Trees to the trained LSTM network under focus.

Aside from the approaches that do not change the inner workings of the RNNs, [285] presents RETAIN
(REverse Time AttentloN) model, which detects influential past patterns by means of a two-level neural
attention model. To create an interpretable RNN, the authors in [283] propose an RNN based on SISTA
(Sequential Iterative Soft-Thresholding Algorithm) that models a sequence of correlated observations
with a sequence of sparse latent vectors, making its weights interpretable as the parameters of a principled
statistical model. Finally, [284] constructs a combination of an HMM (Hidden Markov Model) and an
RNN, so that the overall model approach harnesses the interpretability of the HMM and the accuracy of
the RNN model.

4.3.4. Hybrid Transparent and Black-box Methods

The use of background knowledge in the form of logical statements or constraints in Knowledge
Bases (KBs) has shown to not only improve explainability but also performance with respect to purely
data-driven approaches [297, 298, 299]. A positive side effect shown is that this hybrid approach provides
robustness to the learning system when errors are present in the training data labels. Other approaches
have shown to be able to jointly learn and reason with both symbolic and sub-symbolic representations
and inference. The interesting aspect is that this blend allows for expressive probabilistic-logical reasoning
in an end-to-end fashion [300]. A successful use case is on dietary recommendations, where explanations
are extracted from the reasoning behind (non-deep but KB-based) models [301].

Future data fusion approaches may thus consider endowing DL models with explainability by external-
izing other domain information sources. Deep formulation of classical ML models has been done, e.g. in
Deep Kalman filters (DKFs) [302], Deep Variational Bayes Filters (DVBFs) [303], Structural Variational
Autoencoders (SVAE) [304], or conditional random fields as RNNs [305]. These approaches provide
deep models with the interpretability inherent to probabilistic graphical models. For instance, SVAE
combines probabilistic graphical models in the embedding space with neural networks to enhance the
interpretability of DKFs. A particular example of classical ML model enhanced with its DL counterpart is

27
Deep Nearest Neighbors DkNN [264], where the neighbors constitute human-interpretable explanations
of predictions. The intuition is based on the rationalization of a DNN prediction based on evidence.
This evidence consists of a characterization of confidence termed credibility that spans the hierarchy of
representations within a DNN, that must be supported by the training data [264].

 

 

Transparent design methods

x Black-box Y e Decision Tree

— ML model My > A (Fuzzy) rule-based learning
C C » e KNN

Prediction

 

 

 

 

 

 

 

 

 

 

pep tion

Figure 10: Pictorial representation of a hybrid model. A neural network considered as a black-box can be explained by associating
it to a more interpretable model such as a Decision Tree [306], a (fuzzy) rule-based system [19] or KNN [264].

A different perspective on hybrid XAI models consists of enriching black-box models knowledge
with that one of transparent ones, as proposed in [24] and further refined in [169] and [307]. In particular,
this can be done by constraining the neural network thanks to a semantic KB and bias-prone concepts
[169], or by stacking ensembles jointly encompassing white- and black-box models [307].

Other examples of hybrid symbolic and sub-symbolic methods where a knowledge-base tool or
graph-perspective enhances the neural (e.g., language [308]) model are in [309, 310]. In reinforcement
learning, very few examples of symbolic (graphical [311] or relational [75, 312]) hybrid models exist,
while in recommendation systems, for instance, explainable autoencoders are proposed [313]. A specific
transformer architecture symbolic visualization method (applied to music) pictorially shows how soft-max
attention works [314]. By visualizing self-reference, i.e., the last layer of attention weights, arcs show
which notes in the past are informing the future and how attention is skip over less relevant sections.
Transformers can also help explain image captions visually [315].

Another hybrid approach consists of mapping an uninterpretable black-box system to a white-box twin
that is more interpretable. For example, an opaque neural network can be combined with a transparent
Case Based Reasoning (CBR) system [316, 317]. In [318], the DNN and the CBR (in this case a KNN) are
paired in order to improve interpretability while keeping the same accuracy. The explanation by example
consists of analyzing the feature weights of the DNN which are then used in the CBR, in order to retrieve
nearest-neighbor cases to explain the DNNs prediction.

4.4, Alternative Taxonomy of Post-hoc Explainability Techniques for Deep Learning

DL is the model family where most research has been concentrated in recent times and they have
become central for most of the recent literature on XAI. While the division between model-agnostic
and model-specific is the most common distinction made, the community has not only relied on this
criteria to classify XAI methods. For instance, some model-agnostic methods such as SHAP [224] are
widely used to explain DL models. That is why several XAI methods can be easily categorized in
different taxonomy branches depending on the angle the method is looked at. An example is LIME
which can also be used over CNNs, despite not being exclusive to deal with images. Searching within
the alternative DL taxonomy shows us that LIME can explicitly be used for Explaining a Deep Network
Processing, as a kind of Linear Proxy Model. Another type of classification is indeed proposed in [13]
with a segmentation based on 3 categories. The first category groups methods explaining the processing of
data by the network, thus answering to the question “why does this particular input leads to this particular
output?” The second one concerns methods explaining the representation of data inside the network, i.e.,
answering to the question “what information does the network contain?”. The third approach concerns

28
models specifically designed to simplify the interpretation of their own behavior. Such a multiplicity of
classification possibilities leads to different ways of constructing XAI taxonomies.

Linear Proxy Models
(32!

Decision Trees
182, 257, 258, 259]

       
  

 
 
  

Explanation of Deep
Network Processing

    

Automatic-Rule Extraction
[217, 251, 252, 253, 254, 255, 256, 319, 320, 321]

Bho, 323]

  
 
 
 
 

   
   
  

  
 

Salience Ma,
196, 136, 261, 262, 272,

 
   

Role of Individual Units}

Explanation of Deep
[125, 326, 327, 328, 329]

[Network Representation}

   
 

  
 

Role of Representation Vectors|
144]

 
     

     
  

  
  

 
   
   
 

Attention Networks
[267, 278, 330, 331, 332, 333, 334)

     
 

 

lement
, 34L, 342]

Representation Disentangl
[113, 379, 335, 336, 337, 338, 339, 3

Explanation Producing
Systems

 

 
      
   
       
    

Explanation Generation|
[276, 343, 344, 345]

     
   

Neural-symbolic S

'stems|
, 298, 299, 300]

 
  
   

  

 
   

  
  

B-enhanced Systems|

K
[24, 169, 301, 308, 309, 310]

 
   

[Deep Formulation|
[264, 302, 303, 304, 305]

Hybrid Transparent
and Black-box Methods

   

Relational Reasoning,
(75, 312, 313, 314]

(Case-base Reasoning|
[316, 317, 318]

  

(a)

XAT in ML fs. 6)

Simplification

   
   

XAI in DL (ie 12)
Explanation of Deep
Network Processing

Explanation of Deep
Learning Representation

Explanation Producing
Systems

Hybrid Transparent and
Black-box Methods

‘Text Explanation

 

 

 

Visual explanation

Feature Relevance

[i Fatt

 

 

|

  

 

(b)

Figure 11: (a) Alternative Deep Learning specific taxonomy extended from the categorization from [13]; and (b) its connection to

the taxonomy in Figure 6.

Figure 11 shows the alternative Deep Learning taxonomy inferred from [13]. From the latter, it can be
deduced the complementarity and overlapping of this taxonomy to Figure 6 as:

e Some methods [272, 280] classified in distinct categories (namely feature relevance for CNN and
feature relevance for RNN) in Figure 6 are included in a single category (Explanation of Deep Network
Processing with Salience Mapping) when considering the classification from [13].

e Some methods [82, 144] are classified on a single category (Explanation by simplification for Multi-
Layer Neural Network) in Figure 6 while being in 2 different categories (namely, Explanation of Deep
Network Processing with Decision Trees and Explanation of Deep Network Representation with the
Role of Representation Vectors) in [13], as shown in Figure 11.

A classification based on explanations of model processing and explanations of model representation
is relevant, as it leads to a differentiation between the execution trace of the model and its internal data
structure. This means that depending of the failure reasons of a complex model, it would be possible to
pick-up the right X AI method according to the information needed: the execution trace or the data structure.
This idea is analogous to testing and debugging methods used in regular programming paradigms [346].

5. XAI: Opportunities, Challenges and Future Research Needs

We now capitalize on the performed literature review to put forward a critique of the achievements,
trends and challenges that are still to be addressed in the field of explainability of ML and data fusion
models. Actually our discussion on the advances taken so far in this field has already anticipated some
of these challenges. In this section we revisit them and explore new research opportunities for XAI,
identifying possible research paths that can be followed to address them effectively in years to come:

29
e When introducing the overview in Section 1 we already mentioned the existence of a tradeoff between
model interpretability and performance, in the sense that making a ML model more understandable
could eventually degrade the quality of its produced decisions. In Subsection 5.1 we will stress on the
potential of XAI developments to effectively achieve an optimal balance between the interpretability
and performance of ML models.

In Subsection 2.2 we stressed on the imperative need for reaching a consensus on what explainability
entails within the AI realm. Reasons for pursuing explainability are also assorted and, under our
own assessment of the literature so far, not unambiguously mentioned throughout related works. In
Subsection 5.2 we will further delve into this important issue.

e Given its notable prevalence in the XAI literature, Subsections 4.3 and 4.4 revolved on the explainability
of Deep Learning models, examining advances reported so far around a specific bibliographic taxonomy.
We go in this same direction with Subsection 5.3, which exposes several challenges that hold in regards
to the explainability of this family of models.

e Finally, we close up this prospective discussion with Subsections 5.4 to 5.8, which place on the table
several research niches that despite its connection to model explainability, remain insufficiently studied
by the community.

Before delving into these identified challenges, it is important to bear in mind that this prospective
section is complemented by Section 6, which enumerates research needs and open questions related to
XAI within a broader context: the need for responsible AI.

5.1. On the Tradeoff between Interpretability and Performance

The matter of interpretability versus performance is one that repeats itself through time, but as any
other big statement, has its surroundings filled with myths and misconceptions.

As perfectly stated in [347], it is not necessarily true that models that are more complex are inherently
more accurate. This statement is false in cases in which the data is well structured and features at our
disposal are of great quality and value. This case is somewhat common in some industry environments,
since features being analyzed are constrained within very controlled physical problems, in which all of
the features are highly correlated, and not much of the possible landscape of values can be explored in
the data [348]. What can be hold as true, is that more complex models enjoy much more flexibility than
their simpler counterparts, allowing for more complex functions to be approximated. Now, returning to
the statement “models that are more complex are more accurate”, given the premise that the function
to be approximated entails certain complexity, that the data available for study is greatly widespread
among the world of suitable values for each variable and that there is enough data to harness a complex
model, the statement presents itself as a true statement. It is in this situation that the trade-off between
performance and interpretability can be observed. It should be noted that the attempt at solving problems
that do not respect the aforementioned premises will fall on the trap of attempting to solve a problem that
does not provide enough data diversity (variance). Hence, the added complexity of the model will only
fight against the task of accurately solving the problem.

In this path toward performance, when the performance comes hand in hand with complexity, in-
terpretability encounters itself on a downwards slope that until now appeared unavoidable. However,
the apparition of more sophisticated methods for explainability could invert or at least cancel that slope.
Figure 12 shows a tentative representation inspired by previous works [7], in which XAI shows its
power to improve the common trade-off between model interpretability and performance. Another aspect
worth mentioning at this point due to its close link to model interpretability and performance is the
approximation dilemma: explanations made for a ML model must be made drastic and approximate
enough to match the requirements of the audience for which they are sought, ensuring that explanations
are representative of the studied model and do not oversimplify its essential features.

30
Hybrid modelling approaches

A XAI’s future New explainability-preserving modelling approaches
research arena. Interpretable feature engineering

High

 
 
  

Post-hoc explainability techniques
Interpretability-driven model designs

Model accuracy

m4
°
4

 

 

o
°
4

High
Model interpretability

Figure 12: Trade-off between model interpretability and performance, and a representation of the area of improvement where the
potential of XAI techniques and tools resides.

5.2. On the Concept and Metrics

The literature clearly asks for an unified concept of explainability. In order for the field to thrive,
it is imperative to place a common ground upon which the community is enabled to contribute new
techniques and methods. A common concept must convey the needs expressed in the field. It should
propose a common structure for every XAI system. This paper attempted a new proposition of a concept
of explainability that is built upon that from Gunning [7]. In that proposition and the following strokes to
complete it (Subsection 2.2), explainability is defined as the ability a model has to make its functioning
clearer to an audience. To address it, post-hoc type methods exist. The concept portrayed in this survey
might not be complete but as it stands, allows for a first common ground and reference point to sustain
a profitable discussion in this matter. It is paramount that the field of XAI reaches an agreement in this
respect combining the shattered efforts of a widespread field behind the same banner.

Another key feature needed to relate a certain model to this concrete concept is the existence of a
metric. A metric, or group of them should allow for a meaningful comparison of how well a model fits
the definition of explainable. Without such tool, any claim in this respect dilutes among the literature, not
providing a solid ground on which to stand. These metrics, as the classic ones (accuracy, F1, sensitivity...),
should express how well the model performs in a certain aspect of explainability. Some attempts have been
done recently around the measurement of XAI, as reviewed thoroughly in [349, 350]. In general, XAT
measurements should evaluate the goodness, usefulness and satisfaction of explanations, the improvement
of the mental model of the audience induced by model explanations, and the impact of explanations on the
performance of the model and on the trust and reliance of the audience. Measurement techniques surveyed
in [349] and [350] (e.g., goodness checklist, explanation satisfaction scale, elicitation methods for mental
models, computational measures for explainer fidelity, explanation trustworthiness and model reliability)
seem to be a good push in the direction of evaluating XAI techniques. Unfortunately, conclusions drawn
from these overviews are aligned with our prospects on the field: more quantifiable, general XAI metrics
are really needed to support the existing measurement procedures and tools proposed by the community.

This survey does not tackle the problem of designing such a suite of metrics, since such a task should
be approached by the community as a whole prior acceptance of the broader concept of explainabil-
ity, which on the other hand, is one of the aims of the current work. Nevertheless, we advocate for
further efforts towards new proposals to evaluate the performance of XAI techniques, as well as compar-
ison methodologies among XAI approaches that allow contrasting them quantitatively under different

31
application context, models and purposes.

5.3. Challenges to achieve Explainable Deep Learning

While many efforts are currently being made in the area of XAI, there are still many challenges to be
faced before being able to obtain explainability in DL models. First, as explained in Subsection 2.2, there
is a lack of agreement on the vocabulary and the different definitions surrounding XAI. As an example,
we often see the terms feature importance and feature relevance referring to the same concept. This is
even more obvious for visualization methods, where there is absolutely no consistency behind what is
known as saliency maps, salient masks, heatmaps, neuron activations, attribution, and other approaches
alike. As XATis a relatively young field, the community does not have a standardized terminology yet.

As it has been commented in Subsection 5.1, there is a trade-off between interpretability and accuracy
[13], i.e., between the simplicity of the information given by the system on its internal functioning, and
the exhaustiveness of this description. Whether the observer is an expert in the field, a policy-maker or a
user without machine learning knowledge, intelligibility does not have to be at the same level in order
to provide the audience an understanding [6]. This is one of the reasons why, as mentioned above, a
challenge in XAI is establishing objective metrics on what constitutes a good explanation. A possibility
to reduce this subjectivity is taking inspiration from experiments on human psychology, sociology or
cognitive sciences to create objectively convincing explanations. Relevant findings to be considered when
creating an explainable AI model are highlighted in [12]: First, explanations are better when constrictive,
meaning that a prerequisite for a good explanation is that it does not only indicate why the model made a
decision X, but also why it made decision X rather than decision Y. It is also explained that probabilities
are not as important as causal links in order to provide a satisfying explanation. Considering that black box
models tend to process data in a quantitative manner, it would be necessary to translate the probabilistic
results into qualitative notions containing causal links. In addition, they state that explanations are
selective, meaning that focusing solely on the main causes of a decision-making process is sufficient. It
was also shown that the use of counterfactual explanations can help the user to understand the decision of
a model [40, 42, 351].

Combining connectionist and symbolic paradigms seems a favourable way to address this challenge
[169, 299, 312, 352, 353]. On one hand, connectionist methods are more precise but opaque. On the other
hand, symbolic methods are popularly considered less efficient, while they offer a greater explainability
thus respecting the conditions mentioned above:

e The ability to refer to established reasoning rules allows symbolic methods to be constrictive.

e The use of a KB formalized e.g. by an ontology can allow data to be processed directly in a qualitative
way.

e Being selective is less straightforward for connectionist models than for symbolic ones.

Recalling that a good explanation needs to influence the mental model of the user, i.e. the repre-
sentation of the external reality using, among other things, symbols, it seems obvious that the use of
the symbolic learning paradigm is appropriate to produce an explanation. Therefore, neural-symbolic
interpretability could provide convincing explanations while keeping or improving generic performance
[297].

As stated in [24], a truly explainable model should not leave explanation generation to the users as
different explanations may be deduced depending on their background knowledge. Having a semantic
representation of the knowledge can help a model to have the ability to produce explanations (e.g., in
natural language [169]) combining common sense reasoning and human-understandable features.

Furthermore, until an objective metric has been adopted, it appears necessary to make an effort to
rigorously formalize evaluation methods. One way may be drawing inspiration from the social sciences,
e.g., by being consistent when choosing the evaluation questions and the population sample used [354].

32
A final challenge XAI methods for DL need to address is providing explanations that are accessible
for society, policy makers and the law as a whole. In particular, conveying explanations that require
non-technical expertise will be paramount to both handle ambiguities, and to develop the social right to
the (not-yet available) right for explanation in the EU General Data Protection Regulation (GDPR) [355].

5.4. Explanations for Al Security: XAI and Adversarial Machine Learning

Nothing has been said about confidentiality concerns linked to XAI. One of the last surveys very
briefly introduced the idea of algorithm property and trade secrets [14]. However, not much attention
has been payed to these concepts. If confidential is the property that makes something secret, in the
AI context many aspects involved in a model may hold this property. For example, imagine a model
that some company has developed through many years of research in a specific field. The knowledge
synthesized in the model built might be considered to be confidential, and it may be compromised even
by providing only input and output access [356]. The latter shows that, under minimal assumptions,
data model functionality stealing is possible. An approach that has served to make DL models more
robust against intellectual property exposure based on a sequence of non accessible queries is in [357].
This recent work exposes the need for further research toward the development of XAI tools capable of
explaining ML models while keeping the model’s confidentiality in mind.

Ideally, XAI should be able to explain the knowledge within an AI model and it should be able to
reason about what the model acts upon. However, the information revealed by XAI techniques can be used
both to generate more effective attacks in adversarial contexts aimed at confusing the model, at the same
time as to develop techniques to better protect against private content exposure by using such information.
Adversarial attacks [358] try to manipulate a ML algorithm after learning what is the specific information
that should be fed to the system so as to lead it to a specific output. For instance, regarding a supervised
ML classification model, adversarial attacks try to discover the minimum changes that should be applied
to the input data in order to cause a different classification. This has happened regarding computer vision
systems of autonomous vehicles; a minimal change in a stop signal, imperceptible to the human eye, led
vehicles to detect it as a 45 mph signal [359]. For the particular case of DL models, available solutions
such as Cleverhans [360] seek to detect adversarial vulnerabilities, and provide different approaches
to harden the model against them. Other examples include AlfaSVMLib [361] for SVM models, and
AdversarialLib [362] for evasion attacks. There are even available solutions for unsupervised ML, like
clustering algorithms [363].

While XAI techniques can be used to furnish more effective adversarial attacks or to reveal confidential
aspects of the model itself, some recent contributions have capitalized on the possibilities of Generative
Adversarial Networks (GANS [364]), Variational Autoencoders [365] and other generative models towards
explaining data-based decisions. Once trained, generative models can generate instances of what they
have learned based on a noise input vector that can be interpreted as a latent representation of the data at
hand. By manipulating this latent representation and examining its impact on the output of the generative
model, it is possible to draw insights and discover specific patterns related to the class to be predicted.
This generative framework has been adopted by several recent studies [366, 367] mainly as an attribution
method to relate a particular output of a Deep Learning model to their input variables. Another interesting
research direction is the use of generative models for the creation of counterfactuals, i.e., modifications
to the input data that could eventually alter the original prediction of the model [368]. Counterfactual
prototypes help the user understand the performance boundaries of the model under consideration for
his/her improved trust and informed criticism. In light of this recent trend, we definitely believe that there
is road ahead for generative ML models to take their part in scenarios demanding understandable machine
decisions.

5.5. XAI and Output Confidence
Safety issues have also been studied in regards to processes that depend on the output of AI models,
such as vehicular perception and self-driving in autonomous vehicles, automated surgery, data-based

33
support for medical diagnosis, insurance risk assessment and cyber-physical systems in manufacturing,
among others [369]. In all these scenarios erroneous model outputs can lead to harmful consequences,
which has yielded comprehensive regulatory efforts aimed at ensuring that no decision is made solely on
the basis of data processing [3].

In parallel, research has been conducted towards minimizing both risk and uncertainty of harms
derived from decisions made on the output of a ML model. As a result, many techniques have been
reported to reduce such a risk, among which we pause at the evaluation of the model’s output confidence
to decide upon. In this case, the inspection of the share of epistemic uncertainty (namely, the uncertainty
due to lack of knowledge) of the input data and its correspondence with the model’s output confidence
can inform the user and eventually trigger his/her rejection of the model’s output [370, 371]. To this end,
explaining via XAI techniques which region of the input data the model is focused on when producing a
given output can discriminate possible sources of epistemic uncertainty within the input domain.

5.6. XAI, Rationale Explanation, and Critical Data Studies

When shifting the focus to the research practices seen in Data Science, it has been noted that
reproducibility is stringently subject not only to the mere sharing of data, models and results to the
community, but also to the availability of information about the full discourse around data collection,
understanding, assumptions held and insights drawn from model construction and results’ analyses [372].
In other words, in order to transform data into a valuable actionable asset, individuals must engage in
collaborative sense-making by sharing the context producing their findings, wherein context refers to sets
of narrative stories around how data were processed, cleaned, modeled and analyzed. In this discourse
we find also an interesting space for the adoption of XAI techniques due to their powerful ability to
describe black-box models in an understandable, hence conveyable fashion towards colleagues from
Social Science, Politics, Humanities and Legal fields.

XAlI can effectively ease the process of explaining the reasons why a model reached a decision in an
accessible way to non-expert users, i.e. the rationale explanation. This confluence of multi-disciplinary
teams in projects related to Data Science and the search for methodologies to make them appraise the
ethical implications of their data-based choices has been lately coined as Critical Data studies [373]. It
is in this field where XAI can significantly boost the exchange of information among heterogeneous
audiences about the knowledge learned by models.

5.7. XAI and Theory-guided Data Science

We envision an exciting synergy between the XAI realm and Theory-guided Data Science, a paradigm
exposed in [374] that merges both Data Science and the classic theoretical principles underlying the
application/context where data are produced. The rationale behind this rising paradigm is the need for data-
based models to generate knowledge that is the prior knowledge brought by the field in which it operates.
This means that the model type should be chosen according to the type of relations we intend to encounter.
The structure should also follow what is previously known. Similarly, the training approach should not
allow for the optimization process to enter regions that are not plausible. Accordingly, regularization
terms should stand the prior premises of the field, avoiding the elimination of badly represented true
relations for spurious and deceptive false relations. Finally, the output of the model should inform about
everything the model has come to learn, allowing to reason and merge the new knowledge with what was
already known in the field.

Many examples of the implementation of this approach are currently available with promising results.
The studies in [375]-[382] were carried out in diverse fields, showcasing the potential of this new paradigm
for data science. Above all, it is relevant to notice the resemblance that all concepts and requirements of
Theory-guided Data Science share with XAI. All the additions presented in [374] push toward techniques
that would eventually render a model explainable, and furthermore, knowledge consistent. The concept
of knowledge from the beginning, central to Theory-guided Data Science, must also consider how

34
the knowledge captured by a model should be explained for assessing its compliance with theoretical
principles known beforehand. This, again, opens a magnificent window of opportunity for XAI.

5.8. Guidelines for ensuring Interpretable Al Models

Recent surveys have emphasized on the multidisciplinary, inclusive nature of the process of making
an Al-based model interpretable. Along this process, it is of utmost importance to scrutinize and take into
proper account the interests, demands and requirements of all stakeholders interacting with the system to
be explained, from the designers of the system to the decision makers consuming its produced outputs
and users undergoing the consequences of decisions made therefrom.

Given the confluence of multiple criteria and the need for having the human in the loop, some
attempts at establishing the procedural guidelines to implement and explain AI systems have been recently
contributed. Among them, we pause at the thorough study in [383], which suggests that the incorporation
and consideration of explainability in practical AI design and deployment workflows should comprise
four major methodological steps:

1. Contextual factors, potential impacts and domain-specific needs must be taken into account when
devising an approach to interpretability: These include a thorough understanding of the purpose for
which the AI model is built, the complexity of explanations that are required by the audience, and the
performance and interpretability levels of existing technology, models and methods. The latter pose a
reference point for the AI system to be deployed in lieu thereof.

2. Interpretable techniques should be preferred when possible: when considering explainability in the
development of an AI system, the decision of which XAI approach should be chosen should gauge
domain-specific risks and needs, the available data resources and existing domain knowledge, and the
suitability of the ML model to meet the requirements of the computational task to be addressed. It is in
the confluence of these three design drivers where the guidelines postulated in [383] (and other studies
in this same line of thinking [384]) recommend first the consideration of standard interpretable models
rather than sophisticated yet opaque modeling methods. In practice, the aforementioned aspects
(contextual factors, impacts and domain-specific needs) can make transparent models preferable
over complex modeling alternatives whose interpretability require the application of post-hoc XAI
techniques. By contrast, black-box models such as those reviewed in this work (namely, support
vector machines, ensemble methods and neural networks) should be selected only when their superior
modeling capabilities fit best the characteristics of the problem at hand.

3. Ifa black-box model has been chosen, the third guideline establishes that ethics-, fairness- and safety-
related impacts should be weighed. Specifically, responsibility in the design and implementation of
the AI system should be ensured by checking whether such identified impacts can be mitigated and
counteracted by supplementing the system with XAI tools that provide the level of explainability
required by the domain in which it is deployed. To this end, the third guideline suggests 1) a detailed
articulation, examination and evaluation of the applicable explanatory strategies, 2) the analysis of
whether the coverage and scope of the available explanatory approaches match the requirements of
the domain and application context where the model is to be deployed; and 3) the formulation of
an interpretability action plan that sets forth the explanation delivery strategy, including a detailed
time frame for the execution of the plan, and a clearance of the roles and responsibilities of the team
involved in the workflow.

4. Finally, the fourth guideline encourages to rethink interpretability in terms of the cognitive skills,
capacities and limitations of the individual human. This is an important question on which studies
on measures of explainability are intensively revolving by considering human mental models, the
accessibility of the audience to vocabularies of explanatory outcomes, and other means to involve the
expertise of the audience into the decision of what explanations should provide.

35
We foresee that the set of guidelines proposed in [383] and summarized above will be complemented
and enriched further by future methodological studies, ultimately heading to a more responsible use of AI.
Methodological principles ensure that the purpose for which explainability is pursued is met by bringing
the manifold of requirements of all participants into the process, along with other universal aspects of
equal relevance such as no discrimination, sustainability, privacy or accountability. A challenge remains
in harnessing the potential of XAI to realize a Responsible AI, as we discuss in the next section.

6. Toward Responsible AI: Principles of Artificial Intelligence, Fairness, Privacy and Data Fusion

Over the years many organizations, both private and public, have published guidelines to indicate
how AI should be developed and used. These guidelines are commonly referred to as AI principles, and
they tackle issues related to potential AI threats to both individuals and to the society as a whole. This
section presents some of the most important and widely recognized principles in order to link XAI -
which normally appears inside its own principle — to all of them. Should a responsible implementation
and use of AI models be sought in practice, it is our firm claim that XAI does not suffice on its own. Other
important principles of Artificial Intelligence such as privacy and fairness must be carefully addressed
in practice. In the following sections we elaborate on the concept of Responsible AI, along with the
implications of XAI and data fusion in the fulfillment of its postulated principles.

6.1. Principles of Artificial Intelligence

A recent review of some of the main AI principles published since 2016 appears in [385]. In this
work, the authors show a visual framework where different organizations are classified according to the
following parameters:

e Nature, which could be private sector, government, inter-governmental organization, civil society or
multistakeholder.

e Content of the principles: eight possible principles such as privacy, explainability, or fairness. They
also consider the coverage that the document grants for each of the considered principles.

e Target audience: to whom the principles are aimed. They are normally for the organization that
developed them, but they could also be destined for another audience (see Figure 2).

e Whether or not they are rooted on the International Human Rights, as well as whether they explicitly
talk about them.

For instance, [386] is an illustrative example of a document of AI principles for the purpose of
this overview, since it accounts for some of the most common principles, and deals explicitly with
explainability. Here, the authors propose five principles mainly to guide the development of AI within
their company, while also indicating that they could also be used within other organizations and businesses.

The authors of those principles aim to develop AI in a way that it directly reinforces inclusion, gives
equal opportunities for everyone, and contributes to the common good. To this end, the following aspects
should be considered:

e The outputs after using AI systems should not lead to any kind of discrimination against individuals
or collectives in relation to race, religion, gender, sexual orientation, disability, ethnic, origin or any
other personal condition. Thus, a fundamental criteria to consider while optimizing the results of an AI
system is not only their outputs in terms of error optimization, but also how the system deals with those
groups. This defines the principle of Fair Al.

36
e People should always know when they are communicating with a person, and when they are commu-
nicating with an AI system. People should also be aware if their personal information is being used
by the AI system and for what purpose. It is crucial to ensure a certain level of understanding about
the decisions taken by an AI system. This can be achieved through the usage of XAI techniques. It
is important that the generated explanations consider the profile of the user that will receive those
explanations (the so-called audience as per the definition given in Subsection 2.2) in order to adjust the
transparency level, as indicated in [45]. This defines the principle of Transparent and Explainable AI.

e AI products and services should always be aligned with the United Nation’s Sustainable Development
Goals [387] and contribute to them in a positive and tangible way. Thus, AI should always generate
a benefit for humanity and the common good. This defines the principle of Human-centric Al (also
referred to as Al for Social Good [388]).

e Al systems, specially when they are fed by data, should always consider privacy and security standards
during all of its life cycle. This principle is not exclusive of AI systems since it is shared with many
other software products. Thus, it can be inherited from processes that already exist within a company.
This defines the principle of Privacy and Security by Design, which was also identified as one of
the core ethical and societal challenges faced by Smart Information Systems under the Responsible
Research and Innovation paradigm (RRI, [389]). RRI refers to a package of methodological guidelines
and recommendations aimed at considering a wider context for scientific research, from the perspective
of the lab to global societal challenges such as sustainability, public engagement, ethics, science
education, gender equality, open access, and governance. Interestingly, RRI also requires openness and
transparency to be ensured in projects embracing its principles, which links directly to the principle of
Transparent and Explainable AI mentioned previously.

e The authors emphasize that all these principles should always be extended to any third-party (providers,
consultants, partners...).

Going beyond the scope of these five AI principles, the European Commission (EC) has recently
published ethical guidelines for Trustworthy AI [390] through an assessment checklist that can be
completed by different profiles related to AI systems (namely, product managers, developers and other
roles). The assessment is based in a series of principles: 1) human agency and oversight; 2) technical
robustness and safety; 3) privacy and data governance; 4) transparency, diversity, non-discrimination and
fairness; 5) societal and environmental well-being; 6) accountability. These principles are aligned with
the ones detailed in this section, though the scope for the EC principles is more general, including any
type of organization involved in the development of AI.

It is worth mentioning that most of these AI principles guides directly approach XAI as a key aspect
to consider and include in AI systems. In fact, the overview for these principles introduced before [385],
indicates that 28 out of the 32 AI principles guides covered in the analysis, explicitly include XAI as a
crucial component. Thus, the work and scope of this article deals directly with one of the most important
aspects regarding AI at a worldwide level.

6.2. Fairness and Accountability

As mentioned in the previous section, there are many critical aspects, beyond XAI, included within
the different AI principles guidelines published during the last decade. However, those aspects are not
completely detached from XAJI; in fact, they are intertwined. This section presents two key components
with a huge relevance within the AI principles guides, Fairness and Accountability. It also highlights how
they are connected to XAI.

37
6.2.1. Fairness and Discrimination

It is in the identification of implicit correlations between protected and unprotected features where
XAI techniques find their place within discrimination-aware data mining methods. By analyzing how
the output of the model behaves with respect to the input feature, the model designer may unveil hidden
correlations between the input variables amenable to cause discrimination. XAI techniques such as SHAP
[224] could be used to generate counterfactual outcomes explaining the decisions of a ML model when
fed with protected and unprotected variables.

Recalling the Fair AI principle introduced in the previous section, [386] reminds that fairness is a
discipline that generally includes proposals for bias detection within datasets regarding sensitive data that
affect protected groups (through variables like gender, race...). Indeed, ethical concerns with black-box
models arise from their tendency to unintentionally create unfair decisions by considering sensitive factors
such as the individual’s race, age or gender [391]. Unfortunately, such unfair decisions can give rise to
discriminatory issues, either by explicitly considering sensitive attributes or implicitly by using factors
that correlate with sensitive data. In fact, an attribute may implicitly encode a protected factor, as occurs
with postal code in credit rating [392]. The aforementioned proposals centered on fairness aspects permit
to discover correlations between non-sensitive variables and sensitive ones, detect imbalanced outcomes
from the algorithms that penalize a specific subgroup of people (discrimination), and mitigate the effect
of bias on the model’s decisions. These approaches can deal with:

e Individual fairness: here, fairness is analyzed by modeling the differences between each subject and the
rest of the population.

e Group fairness: it deals with fairness from the perspective of all individuals.
e Counterfactual fairness: it tries to interpret the causes of bias using, for example, causal graphs.
The sources for bias, as indicated in [392], can be traced to:
e Skewed data: bias within the data acquisition process.
e Tainted data: errors in the data modelling definition, wrong feature labelling, and other possible causes.

e Limited features: using too few features could lead to an inference of false feature relationships that
can lead to bias.

e Sample size disparities: when using sensitive features, disparities between different subgroups can
induce bias.

e Proxy features: there may be correlated features with sensitive ones that can induce bias even when the
sensitive features are not present in the dataset.

The next question that can be asked is what criteria could be used to define when AI is not biased. For
supervised ML, [393] presents a framework that uses three criteria to evaluate group fairness when there
is a sensitive feature present within the dataset:

e Independence: this criterion is fulfilled when the model predictions are independent of the sensitive
feature. Thus, the proportion of positive samples (namely, those ones belonging to the class of interest)
given by the model is the same for all the subgroups within the sensitive feature.

e Separation: it is met when the model predictions are independent of the sensitive feature given the
target variable. For instance, in classification models, the True Positive (TP) rate and the False Positive

(FP) rate are the same in all the subgroups within the sensitive feature. This criteria is also known as
Equalized Odds.

38
e Sufficiency: it is accomplished when the target variable is independent of the sensitive feature given
the model output. Thus, the Positive Predictive Value is the same for all subgroups within the sensitive
feature. This criteria is also known as Predictive Rate Parity.

Although not all of the criteria can be fulfilled at the same time, they can be optimized together in
order to minimize the bias within the ML model.

There are two possible actions that could be used in order to achieve those criteria. On one hand,
evaluation includes measuring the amount of bias present within the model (regarding one of the criteria
aforementioned). There are many different metrics that can be used, depending on the criteria considered.
Regarding independence criterion, possible metrics are statistical parity difference or disparate impact.
In case of the separation criterion, possible metrics are equal opportunity difference and average odds
difference [393]. Another possible metric is the Theil index [394], which measures inequality both in
terms of individual and group fairness.

On the other hand, mitigation refers to the process of fixing some aspects in the model in order to
remove the effect of the bias in terms of one or several sensitive features. Several techniques exist within
the literature, classified in the following categories:

e Pre-processing: these groups of techniques are applied before the ML model is trained, looking to
remove the bias at the first step of the learning process. An example is Reweighing [395], which
modifies the weights of the features in order to remove discrimination in sensitive attributes. Another
example is [396], which hinges on transforming the input data in order to find a good representation
that obfuscates information about membership in sensitive features.

e In-processing: these techniques are applied during the training process of the ML model. Normally,
they include Fairness optimization constraints along with cost functions of the ML model. An example
is Adversarial Debiasing, [397]. This technique optimizes jointly the ability of predicting the target
variable while minimizing the ability of predicting sensitive features using a GAN.

e Post-processing: these techniques are applied after the ML model is trained. They are less intrusive
because they do not modify the input data or the ML model. An example is Equalized Odds [393]. This
techniques allows to adjust the thresholds in the classification model in order to reduce the differences
between the TP rate and the FP rate for each sensitive subgroup.

Even though these references apparently address an AI principle that appears to be independent of
XAI, the literature shows that they are intertwined. For instance, the survey in [385] evinces that 26 out
of the 28 AI principles that deal with XAI, also talk about fairness explicitly. This fact elucidates that
organizations usually consider both aspects together when implementing Responsible AI.

The literature also exploses that XAI proposals can be used for bias detection. For example, [398]
proposes a framework to visually analyze the bias present in a model (both for individual and group
fairness). Thus, the fairness report is shown just like the visual summaries used within XAI. This
explainability approach eases the understanding and measurement of bias. The system must report that
there is bias, justify it quantitatively, indicate the degree of fairness, and explain why a user or group
would be treated unfairly with the available data. Similarly, XAI techniques such as SHAP [224] could be
used to generate counterfactual outcomes explaining the decisions of a ML model when fed with protected
and unprotected variables. By identifying implicit correlations between protected and unprotected features
through XAI techniques, the model designer may unveil hidden correlations between the input variables
amenable to cause discrimination.

Another example is [399], where the authors propose a fair-by-design approach in order to develop
ML models that jointly have less bias and include as explanations human comprehensible rules. The
proposal is based in self-learning locally generative models that use only a small part of the whole
dataset available (weak supervision). It first finds recursively relevant prototypes within the dataset, and

39
extracts the empirical distribution and density of the points around them. Then it generates rules in an
IF/THEN format that explain that a data point is classified within a specific category because it is similar
to some prototypes. The proposal then includes an algorithm that both generates explanations and reduces
bias, as it is demonstrated for the use case of recidivism using the Correctional Offender Management
Profiling for Alternative Sanctions (COMPAS) dataset [400]. The same goal has been recently pursued in
[401], showing that post-hoc XAI techniques can forge fairer explanations from truly unfair black-box
models. Finally, CERTIFAI (Counterfactual Explanations for Robustness, Transparency, Interpretability,
and Fairness of Artificial Intelligence models) [402] uses a customized genetic algorithm to generate
counterfactuals that can help to see the robustness of a ML model, generate explanations, and examine
fairness (both at the individual level and at the group level) at the same time.

Strongly linked to the concept of fairness, much attention has been lately devoted to the concept of
data diversity, which essentially refers to the capability of an algorithmic model to ensure that all different
types of objects are represented in its output [403]. Therefore, diversity can be thought to be an indicator
of the quality of a collection of items that, when taking the form of a model’s output, can quantify the
proneness of the model to produce diverse results rather than highly accurate predictions. Diversity comes
into play in human-centered applications with ethical restrictions that permeate to the AI modeling phase
[404]. Likewise, certain AI problems (such as content recommendation or information retrieval) also
aim at producing diverse recommendations rather than highly-scoring yet similar results [405, 406]. In
these scenarios, dissecting the internals of a black-box model via XAI techniques can help identifying the
capability of the model to maintain the input data diversity at its output. Learning strategies to endow a
model with diversity keeping capabilities could be complemented with XAI techniques in order to shed
transparency over the model internals, and assess the effectiveness of such strategies with respect to the
diversity of the data from which the model was trained. Conversely, XAI could help to discriminate which
parts of the model are compromising its overall ability to preserve diversity.

6.2.2. Accountability
Regarding accountability, the EC [390] defines the following aspects to consider:

e Auditability: it includes the assessment of algorithms, data and design processes, but preserving the
intellectual property related to the AI systems. Performing the assessment by both internal and external
auditors, and making the reports available, could contribute to the trustworthiness of the technology.
When the AI system affects fundamental rights, including safety-critical applications, it should always
be audited by an external third party.

e Minimization and reporting of negative impacts: it consists of reporting actions or decisions that yield
a certain outcome by the system. It also comprises the assessment of those outcomes and how to
respond to them. To address that, the development of AI systems should also consider the identification,
assessment, documentation and minimization of their potential negative impacts. In order to minimize
the potential negative impact, impact assessments should be carried out both prior to and during the
development, deployment and use of AI systems. It is also important to guarantee protection for anyone
who raises concerns about an AI system (e.g., whistle-blowers). All assessments must be proportionate
to the risk that the AI systems pose.

e Trade-offs: in case any tension arises due to the implementation of the above requirements, trade-offs
could be considered but only if they are ethically acceptable. Such trade-offs should be reasoned,
explicitly acknowledged and documented, and they must be evaluated in terms of their risk to ethical
principles. The decision maker must be accountable for the manner in which the appropriate trade-off
is being made, and the trade-off decided should be continually reviewed to ensure the appropriateness
of the decision. If there is no ethically acceptable trade-off, the development, deployment and use of
the AI system should not proceed in that form.

40
e Redress: it includes mechanisms that ensure an adequate redress for situations when unforeseen unjust
adverse impacts take place. Guaranteeing a redress for those non-predicted scenarios is a key to ensure
trust. Special attention should be paid to vulnerable persons or groups.

These aspects addressed by the EC highlight different connections of XAI with accountability. First,
XAI contributes to auditability as it can help explaining AI systems for different profiles, including
regulatory ones. Also, since there is a connection between fairness and XAI as stated before, XAI can
also contribute to the minimization and report of negative impacts.

6.3. Privacy and Data Fusion

The ever-growing number of information sources that nowadays coexist in almost all domains of
activity calls for data fusion approaches aimed at exploiting them simultaneously toward solving a learning
task. By merging heterogeneous information, data fusion has been proven to improve the performance of
ML models in many applications, such as industrial prognosis [348], cyber-physical social systems [407]
or the Internet of Things [408], among others. This section speculates with the potential of data fusion
techniques to enrich the explainability of ML models, and to compromise the privacy of the data from
which ML models are learned. To this end, we briefly overview different data fusion paradigms, and later
analyze them from the perspective of data privacy. As we will later, despite its relevance in the context of
Responsible AI, the confluence between XAI and data fusion is an uncharted research area in the current
research mainstream.

6.3.1. Basic Levels of Data Fusion

We depart from the different levels of data fusion that have been identified in comprehensive surveys
on the matter [409, 410, 411, 412]. In the context of this subsection, we will distinguish among fusion at
data level, fusion at model level and fusion at knowledge level. Furthermore, a parallel categorization can
be established depending on where such data is processed and fused, yielding centralized and distributed
methods for data fusion. In a centralized approach, nodes deliver their locally captured data to a centralized
processing system to merge them together. In contrast, in a distributed approach, each of the nodes merges
its locally captured information, eventually sharing the result of the local fusion with its counterparts.

Fusion through the information generation process has properties and peculiarities depending on
the level at which the fusion is performed. At the so-called data level, fusion deals with raw data. As
schematically shown in Figure 13, a fusion model at this stage receives raw data from different information
sources, and combines them to create a more coherent, compliant, robust or simply representative data
flow. On the other hand, fusion at the model level aggregates models, each learned from a subset of the
data sets that were to be fused. Finally, at the knowledge level the fusion approach deals with knowledge in
the form of rules, ontologies or other knowledge representation techniques with the intention of merging
them to create new, better or more complete knowledge from what was originally provided. Structured
knowledge information is extracted from each data source and for every item in the data set using multiple
knowledge extractors (e.g. a reasoning engine operating on an open semantic database). All produced
information is then fused to further ensure the quality, correctness and manageability of the produced
knowledge about the items in the data set.

Other data fusion approaches exist beyonds the ones represented in Figure 13. As such, data-level
fusion can be performed either by a technique specifically devoted to this end (as depicted in Figure 13.b)
or, instead, performed along the learning process of the ML model (as done in e.g. DL models). Similarly,
model-level data fusion can be made by combining the decisions of different models (as done in tree
ensembles).

6.3.2. Emerging Data Fusion Approaches
In the next subsection we examine other data fusion approaches that have recently come into scene
due to their implications in terms of data privacy:

Al
Di : ith dataset

Dr : Fused data

KB : Knowledge Base
: Predictions

    
  

    

   
 

 
    

> Knowledge

<4 extractor
Ora Knowledge |

extractor
Knowledge
extractor

Model Fusion
Knowledge Fusion

    

Data Fusion technique

Client-server
delivery

 

 

T Secure
Aggregation

{> : encrypted mode] (Py) y
| information (e.g. . ML
| gradients) View V v

--- He | >: model update
(ec) (£)

Figure 13: Diagrams showing different levels at which data fusion can be performed: (a) data level; (b) model level; (c) knowledge
level; (d) Big Data fusion; (e) Federated Learning and (f) Multiview Learning.

 

 

 

 

Joint optimization
+ Fusion

 

 

 

 

e In Big Data fusion (Figure 13.d), local models are learned on a split of the original data sources, each
submitted to a Worker node in charge of performing this learning process (Map task). Then, a Reduce
node (or several Reduce nodes, depending on the application) combines the outputs produced by each
Map task. Therefore, Big Data fusion can be conceived as a means to distribute the complexity of learn-
ing a ML model over a pool of Worker nodes, wherein the strategy to design how information/models
are fused together between the Map and the Reduce tasks is what defines the quality of the finally
generated outcome [413].

e By contrast, in Federated Learning [414, 415, 416], the computation of ML models is made on data
captured locally by remote client devices (Figure 13.e). Upon local model training, clients transmit
encrypted information about their learned knowledge to a central server, which can take the form of
layer-wise gradients (in the case of neural ML models) or any other model-dependent content alike. The
central server aggregates (fuses) the knowledge contributions received from all clients to yield a shared
model harnessing the collected information from the pool of clients. It is important to observe that no
client data is delivered to the central server, which elicits the privacy-preserving nature of Federated
Learning. Furthermore, computation is set closer to the collected data, which reduces the processing
latency and alleviates the computational burden of the central server.

Finally, Multiview Learning [417] constructs different views of the object as per the information
contained in the different data sources (Figure 13.f). These views can be produced from multiple
sources of information and/or different feature subsets [418]. Multiview Learning devises strategies
to jointly optimize ML models learned from the aforementioned views to enhance the generalization
performance, specially in those applications with weak data supervision and hence, prone to model
overfitting. This joint optimization resorts to different algorithmic means, from co-training to co-
regularization [419].

6.3.3. Opportunities and Challenges in Privacy and Data Fusion under the Responsible AI Paradigm

AI systems, specially when dealing with multiple data sources, need to explicitly include privacy
considerations during the system’s life cycle. This is specially critical when working with personal data,

42
because respecting people’s right to privacy should always be addressed. The EC highlights that privacy
should also address data governance, covering the quality and integrity of the used data [390]. It should
also include the definition of access protocols and the capability to process data in a way that ensures
privacy. The EC guide breaks down the privacy principle into three aspects:

e Privacy and data protection: they should be guaranteed in AI systems throughout its entire lifecycle. It
includes both information provided by users and information generated about those users derived from
their interactions with the system. Since digital information about a user could be used in a negative
way against them (discrimination due to sensitive features, unfair treatment...), it is crucial to ensure
proper usage of all the data collected.

e Quality and integrity of data: quality of data sets is fundamental to reach good performance with AI
systems that are fueled with data, like ML. However, sometimes the data collected contains socially
constructed biases, inaccuracies, errors and mistakes. This should be tackled before training any model
with the data collected. Additionally, the integrity of the data sets should be ensured.

e Access to data: if there is individual personal data, there should always be data protocols for data
governance. These protocols should indicate who may access data and under which circumstances.

The aforementioned examples from the EC shows how data fusion is directly intertwined with privacy
and with fairness, regardless of the technique employed for it.

Notwithstanding this explicit concern from regulatory bodies, loss of privacy has been compromised
by DL methods in scenarios where no data fusion is performed. For instance, a few images are enough
to threaten users’ privacy even in the presence of image obfuscation [420], and the model parameters of
a DNN can be exposed by simply performing input queries on the model [356, 357]. An approach to
explain loss of privacy is by using privacy loss and intent loss subjective scores. The former provides a
subjective measure of the severity of the privacy violation depending on the role of a face in the image,
while the latter captures the intent of the bystanders to appear in the picture. These kind of explanations
have motivated, for instance, secure matching cryptographic protocols for photographer and bystanders to
preserve privacy [356, 421, 422]. We definite advocate for more efforts invested in this direction, namely,
in ensuring that XAI methods do not pose a threat in regards to the privacy of the data used for training
the ML model under target.

When data fusion enters the picture, different implications arise with the context of explainability
covered in this survey. To begin with, classical techniques for fusion at the data level only deal with data
and have no connection to the ML model, so they have little to do with explainability. However, the
advent of DL models has blurred the distinction between information fusion and predictive modeling. The
first layers of DL architectures are in charge of learning high-level features from raw data that possess
relevance for the task at hand. This learning process can be thought to aim at solving a data level fusion
problem, yet in a directed learning fashion that makes the fusion process tightly coupled to the task to be
solved.

In this context, many techniques in the field of XAI have been proposed to deal with the analysis of
correlation between features. This paves the way to explaining how data sources are actually fused through
the DL model, which can yield interesting insights on how the predictive task at hand induces correlations
among the data sources over the spatial and/or time domain. Ultimately, this gained information on the
fusion could not only improve the usability of the model as a result of its enhanced understanding by the
user, but could also help identifying other data sources of potential interest that could be incorporated to
the model, or even contribute to a more efficient data fusion in other contexts.

Unfortunately, this previously mentioned concept of fusion at data level contemplates data under
certain constraints of known form and source origin. As presented in [423], the Big Data era presents
an environment in which these premises cannot be taken for granted, and methods to board Big Data
fusion (as that illustrated in Figure 13.d) have to be thought. Conversely, a concern with model fusion

43
context emerges in the possibility that XAI techniques could be explanatory enough to compromise the
confidentiality of private data. This could eventually occur if sensitive information (e.g. ownership) could
be inferred from the explained fusion among protected and unprotected features.

When turning our prospects to data fusion at model level, we have already argued that the fusion of the
outputs of several transparent models (as in tree ensembles) could make the overall model opaque, thereby
making it necessary to resort to post-hoc explainability solutions. However, model fusion may entail other
drawbacks when endowed with powerful post-hoc XAI techniques. Let us imagine that relationships of
a model’s input features have been discovered by means of a post-hoc technique) and that one of those
features is hidden or unknown. Will it be possible to infer another model’s features if that previous feature
was known to be used in that model? Would this possibility uncover a problem as privacy breaches in
cases in which related protected input variables are not even shared in the first place?

To get the example clearer, in [424] a multiview perspective is utilized in which different single views
(representing the sources they attend to) models are fused. These models contain among others, cell-phone
data, transportation data, etc. which might introduce the problem that information that is not even shared
can be discovered through other sources that are actually shared. In the example above, what if instead of
features, a model shares with another a layer or part of its architecture as in Federated Learning? Would
this sharing make possible to infer information from that exchanged part of its model, to the extent of
allowing for the design of adversarial attacks with better success rate upon the antecedent model?

If focused at knowledge level fusion, a similar reasoning holds: XAI comprises techniques that extract
knowledge from ML model(s). This ability to explain models could have an impact on the necessity of
discovering new knowledge through the complex interactions formed within ML models. If so, XAI might
enrich knowledge fusion paradigms, bringing the possibility of discovering new knowledge extractors
of relevance for the task at hand. For this purpose, it is of paramount importance that the knowledge
extracted from a model by means of XAI techniques can be understood and extrapolated to the domain
in which knowledge extractors operate. The concept matches with ease with that of transfer learning
portrayed in [425]. Although XAI is not contemplated in the surveyed processes of extracting knowledge
from models trained in certain feature spaces and distributions, to then be utilized in environments where
previous conditions do not hold, when deployed, XAI can pose a threat if the explanations given about the
model can be reversely engineered through the knowledge fusion paradigm to eventually compromise, for
instance, the differential privacy of the overall model.

The distinction between centralized and distributed data fusion also spurs further challenges in
regards to privacy and explainability. The centralized approach does not bring any further concerns
that those presented above. However, distributed fusion does arise new problems. Distributed fusion
might be applied for different reasons, mainly due to environmental constraints or due to security or
privacy issues. The latter context may indulge some dangers. Among other goals (e.g. computational
efficiency), model-level data fusion is performed in a distributed fashion to ensure that no actual data is
actually shared, but rather parts of an ML model trained on local data. This rationale lies at the heart
of Federated Learning, where models exchange locally learned information among nodes. Since data
do not leave the local device, only the transmission of model updates is required across distributed
devices. This lightens the training process for network-compromised settings and guarantees data privacy
[416]. Upon the use of post-hoc explainability techniques, a node could disguise sensitive information
about the local context in which the received ML model part was trained. In fact, it was shown that a
black-box model based on a DNN from which an input/output query interface is given can be used to
accurately predict every single hyperparameter value used for training, allowing for potential privacy-
related consequences [357, 420, 421]. This relates to studies showing that blurring images does not
guarantee privacy preservation.

Data fusion, privacy and model explainability are concepts that have not been analysed together so far.
From the above discussion it is clear that there are unsolved concerns and caveats that demand further
study by the community in forthcoming times.

44
6.4. Implementing Responsible Al Principles in an Organization

While increasingly more organizations are publishing AI principles to declare that they care about
avoiding unintended negative consequences, there is much less experience on how to actually implement
the principles into an organization. Looking at several examples of principles declared by different
organizations [385], we can divide them into two groups:

e Al-specific principles that focus on aspects that are specific to AI, such as explainability, fairness and
human agency.

e End-to-end principles that cover all aspects involved in AI, including also privacy, security and safety.

The EC Guidelines for Trustworthy AI are an example of end-to-end principles [390], while those of
Telefonica (a large Spanish ICT company operating worldwide) are more Al-specific [386]. For example,
safety and security are relevant for any connected IT system, and therefore also for AI systems. The
same holds for privacy, but it is probably true that privacy in the context of AI systems is even more
important than for general IT systems, due to the fact that ML models need huge amounts of data and
most importantly, because XAI tools and data fusion techniques pose new challenges to preserve the
privacy of protected records.

When it comes to implement the AI Principles into an organization, it is important to operationalize
the Al-specific parts and, at the same time, leverage the processes already existing for the more generic
principles. Indeed, in many organizations there already exist norms and procedures for privacy, security
and safety. Implementing AI principles requires a methodology such as that presented in [386] that breaks
down the process into different parts. The ingredients of such a methodology should include, at least:

e AI principles (already discussed earlier), which set the values and boundaries.
e Awareness and training about the potential issues, both technical and non-technical.

e A questionnaire that forces people to think about certain impacts of the AI system (impact explanation).
This questionnaire should give concrete guidance on what to do if certain undesired impacts are
detected.

e Tools that help answering some of the questions, and help mitigating any problems identified. XAI
tools and fairness tools fall in this category, as well as other recent proposals such as model cards [426].

e A governance model assigning responsibilities and accountabilities (responsibility explanation). There
are two philosophies for governance: 1) based on committees that review and approve AI developments,
and 2) based on the self-responsibility of the employees. While both are possible, given the fact
that agility is key for being successful in the digital world, it seems wiser to focus on awareness and
employee responsibility, and only use committees when there are specific, but important issues.

From the above elaborations, it is clear that the implementation of Responsible AI principles in
companies should balance between two requirements: 1) major cultural and organizational changes
needed to enforce such principles over processes endowed with AI functionalities; and 2) the feasibility
and compliance of the implementation of such principles with the IT assets, policies and resources already
available at the company. It is in the gradual process of rising corporate awareness around the principles
and values of Responsible AI where we envision that XAI will make its place and create huge impact.

7. Conclusions and Outlook

This overview has revolved around eXplainable Artificial Intelligence (CAI, which has been identified
in recent times as an utmost need for the adoption of ML methods in real-life applications. Our study

45
has elaborated on this topic by first clarifying different concepts underlying model explainability, as well
as by showing the diverse purposes that motivate the search for more interpretable ML methods. These
conceptual remarks have served as a solid baseline for a systematic review of recent literature dealing with
explainability, which has been approached from two different perspectives: 1) ML models that feature
some degree of transparency, thereby interpretable to an extent by themselves; and 2) post-hoc XAI
techniques devised to make ML models more interpretable. This literature analysis has yielded a global
taxonomy of different proposals reported by the community, classifying them under uniform criteria.
Given the prevalence of contributions dealing with the explainability of Deep Learning models, we have
inspected in depth the literature dealing with this family of models, giving rise to an alternative taxonomy
that connects more closely with the specific domains in which explainability can be realized for Deep
Learning models.

We have moved our discussions beyond what has been made so far in the XAI realm toward the concept
of Responsible AI, a paradigm that imposes a series of AI principles to be met when implementing AI
models in practice, including fairness, transparency, and privacy. We have also discussed the implications
of adopting XAI techniques in the context of data fusion, unveiling the potential of XAI to compromise
the privacy of protected data involved in the fusion process. Implications of XAI in fairness have also
been discussed in detail. This vision of XAI as a core concept to ensure the aforementioned principles for
Responsible AI is summarized graphically in Figure 14.

 
      

Tnterpretability

Performance

 

 

 

Fairness Privacy Accountability
Responsible
Al
: Security &
Ethics Transparency Safety

 

 

 

 

 

 

 

Figure 14: Summary of XAI challenges discussed in this overview and its impact on the principles for Responsible AI.

Our reflections about the future of XAI, conveyed in the discussions held throughout this work,
agree on the compelling need for a proper understanding of the potentiality and caveats opened up by
XAI techniques. It is our vision that model interpretability must be addressed jointly with requirements
and constraints related to data privacy, model confidentiality, fairness and accountability. A responsible
implementation and use of AI methods in organizations and institutions worldwide will be only guaranteed
if all these AI principles are studied jointly.

Acknowledgments

Alejandro Barredo-Arrieta, Javier Del Ser and Sergio Gil-Lopez would like to thank the Basque
Government for the funding support received through the EMAITEK and ELKARTEK programs. Javier
Del Ser also acknowledges funding support from the Consolidated Research Group MATHMODE
(IT1294-19) granted by the Department of Education of the Basque Government. Siham Tabik, Salvador
Garcia, Daniel Molina and Francisco Herrera would like to thank the Spanish Government for its funding
support (SMART-DaSCI project, TIN2017-89517-P), as well as the BBVA Foundation through its Ayudas

46
Fundacion BBVA a Equipos de Investigacion Cientifica 2018 call (DeepSCOP project). This work was
also funded in part by the European Union’s Horizon 2020 research and innovation programme AI4EU
under grant agreement 825619. We also thank Chris Olah, Alexander Mordvintsev and Ludwig Schubert
for borrowing images for illustration purposes. Part of this overview is inspired by a preliminary work of
the concept of Responsible AI: R. Benjamins, A. Barbado, D. Sierra, “Responsible AI by Design”, to
appear in the Proceedings of the Human-Centered AI: Trustworthiness of AI Models & Data (HAD track
at AAAI Fall Symposium, DC, November 7-9, 2019 [386].

References

[1]

[2]
[3]

[4]
[5]
[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

S. J. Russell, P. Norvig, Artificial intelligence: a modern approach, Malaysia; Pearson Education
Limited,, 2016.

D. M. West, The future of work: robots, AI, and automation, Brookings Institution Press, 2018.

B. Goodman, S. Flaxman, European union regulations on algorithmic decision-making and a right
to explanation, AI Magazine 38 (3) (2017) 50-57.

D. Castelvecchi, Can we open the black box of AI?, Nature News 538 (7623) (2016) 20.
Z. C. Lipton, The mythos of model interpretability, Queue 16 (3) (2018) 30:31-30:57.

A. Preece, D. Harborne, D. Braines, R. Tomsett, S. Chakraborty, Stakeholders in Explainable AI
(2018). arXiv:1810.00184.

D. Gunning, Explainable artificial intelligence (xAI), Tech. rep., Defense Advanced Research
Projects Agency (DARPA) (2017).

E. Tjoa, C. Guan, A survey on explainable artificial intelligence (XAI): Towards medical XAI
(2019). arXiv:1907.07374.

J. Zhu, A. Liapis, S. Risi, R. Bidarra, G. M. Youngblood, Explainable AI for designers: A human-
centered perspective on mixed-initiative co-creation, 2018 IEEE Conference on Computational
Intelligence and Games (CIG) (2018) 1-8.

F. K. DoSilovié, M. Bréié, N. Hlupié, Explainable artificial intelligence: A survey, in: 41st
International Convention on Information and Communication Technology, Electronics and Micro-
electronics (MIPRO), 2018, pp. 210-215.

P. Hall, On the Art and Science of Machine Learning Explanations (2018). arxiv:1810.
02909.

T. Miller, Explanation in artificial intelligence: Insights from the social sciences, Artif. Intell. 267
(2019) 1-38.

L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, L. Kagal, Explaining Explanations: An
Overview of Interpretability of Machine Learning (2018). arXiv:1806.00069.

A. Adadi, M. Berrada, Peeking inside the black-box: A survey on explainable artificial intelligence
(XAD, IEEE Access 6 (2018) 52138-52160.

O. Biran, C. Cotton, Explanation and justification in machine learning: A survey, in: IJCAI-17
workshop on explainable AI (XAD, Vol. 8, 2017, p. 1.

47
[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

S. T. Shane T. Mueller, R. R. Hoffman, W. Clancey, G. Klein, Explanation in Human-AI Systems: A
Literature Meta-Review Synopsis of Key Ideas and Publications and Bibliography for Explainable
AI, Tech. rep., Defense Advanced Research Projects Agency (DARPA) XAI Program (2019).

R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods
for explaining black box models, ACM Computing Surveys 51 (5) (2018) 93:1-93:42.

G. Montavon, W. Samek, K.-R. Mller, Methods for interpreting and understanding deep neural
networks, Digital Signal Processing 73 (2018) 1-15. doi:10.1016/4.dsp.2017.10.011.

A. Fernandez, F. Herrera, O. Cordon, M. Jose del Jesus, F. Marcelloni, Evolutionary fuzzy systems
for explainable artificial intelligence: Why, when, what for, and where to?, IEEE Computational
Intelligence Magazine 14 (1) (2019) 69-81.

M. Gleicher, A framework for considering comprehensibility in modeling, Big data 4 (2) (2016)
75-88.

M. W. Craven, Extracting comprehensible models from trained neural networks, Tech. rep., Univer-
sity of Wisconsin-Madison Department of Computer Sciences (1996).

R. S. Michalski, A theory and methodology of inductive learning, in: Machine learning, Springer,
1983, pp. 83-134.

J. Diez, K. Khalifa, B. Leuridan, General theories of explanation: buyer beware, Synthese 190 (3)
(2013) 379-396.

D. Doran, S. Schulz, T. R. Besold, What does explainable AI really mean? a new conceptualization
of perspectives (2017). arXiv:1710.00794.

F. Doshi-Velez, B. Kim, Towards a rigorous science of interpretable machine learning (2017).
arXiv:1702.08608.

A. Vellido, J. D. Martin-Guerrero, P. J. Lisboa, Making machine learning models interpretable., in:
European Symposium on Artificial Neural Networks, Computational Intelligence and Machine
Learning (ESANN), Vol. 12, Citeseer, 2012, pp. 163-172.

E. Walter, Cambridge advanced learner’s dictionary, Cambridge University Press, 2008.
P. Besnard, A. Hunter, Elements of Argumentation, The MIT Press, 2008.

F Rossi, AI Ethics for Enterprise AI (2019).
URL https://economics.harvard.edu/files/economics/files/rossi-
francesca_4-22-19_ai-ethics—for-enterprise-ai_ec3118-hbs.pdf

A. Holzinger, C. Biemann, C. S. Pattichis, D. B. Kell, What do we need to build explainable Ai
systems for the medical domain? (2017). arXiv:1712.09923.

B. Kim, E. Glassman, B. Johnson, J. Shah, iBCM: Interactive bayesian case model empowering
humans via intuitive interaction, Tech. rep., MIT-CSAIL-TR-2015-010 (2015).

M. T. Ribeiro, S. Singh, C. Guestrin, Why should I trust you?: Explaining the predictions of any
classifier, in: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
ACM, 2016, pp. 1135-1144.

M. Fox, D. Long, D. Magazzeni, Explainable planning (2017). arXiv:1709.10256.

48
[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

H. C. Lane, M. G. Core, M. Van Lent, S. Solomon, D. Gomboc, Explainable artificial intelligence
for training and tutoring, Tech. rep., University of Southern California (2005).

W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, B. Yu, Interpretable machine learning:
definitions, methods, and applications (2019). arxXiv:1901.04592.

J. Haspiel, N. Du, J. Meyerson, L. P. Robert Jr, D. Tilbury, X. J. Yang, A. K. Pradhan, Explana-
tions and expectations: Trust building in automated vehicles, in: Companion of the ACM/IEEE
International Conference on Human-Robot Interaction, ACM, 2018, pp. 119-120.

A. Chander, R. Srinivasan, S. Chelian, J. Wang, K. Uchino, Working with beliefs: AI transparency
in the enterprise., in: Workshops of the ACM Conference on Intelligent User Interfaces, 2018.

A. B. Tickle, R. Andrews, M. Golea, J. Diederich, The truth will come to light: Directions and
challenges in extracting the knowledge embedded within trained artificial neural networks, IEEE
Transactions on Neural Networks 9 (6) (1998) 1057-1068.

C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, M. Welling, Causal effect inference with
deep latent-variable models, in: Advances in Neural Information Processing Systems, 2017, pp.
6446-6456.

O. Goudet, D. Kalainathan, P. Caillou, I. Guyon, D. Lopez-Paz, M. Sebag, Learning functional
causal models with generative neural networks, in: Explainable and Interpretable Models in
Computer Vision and Machine Learning, Springer, 2018, pp. 39-80.

S. Athey, G. W. Imbens, Machine learning methods for estimating heterogeneous causal effects,
stat 1050 (5) (2015).

D. Lopez-Paz, R. Nishihara, S. Chintala, B. Scholkopf, L. Bottou, Discovering causal signals in
images, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2017, pp. 6979-6987.

C. Barabas, K. Dinakar, J. Ito, M. Virza, J. Zittrain, Interventions over predictions: Reframing the
ethical debate for actuarial risk assessment (2017). arXiv:1712.08238.

R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, N. Elhadad, Intelligible models for healthcare:
Predicting pneumonia risk and hospital 30-day readmission, in: Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, 2015,
pp. 1721-1730.

A. Theodorou, R. H. Wortham, J. J. Bryson, Designing and implementing transparency for real
time inspection of autonomous robots, Connection Science 29 (3) (2017) 230-241.

W. Samek, T. Wiegand, K.-R. Miiller, Explainable artificial intelligence: Understanding, visualizing
and interpreting deep learning models (2017). arXiv:1708.08296.

C. Wadsworth, F. Vera, C. Piech, Achieving fairness through adversarial learning: an application to
recidivism prediction (2018). arXiv:1807.00199.

X. Yuan, P. He, Q. Zhu, X. Li, Adversarial examples: Attacks and defenses for deep learning, IEEE
Transactions on Neural Networks and Learning Systems 30 (9) (2019) 2805-2824.

B. Letham, C. Rudin, T. H. McCormick, D. Madigan, et al., Interpretable classifiers using rules
and bayesian analysis: Building a better stroke prediction model, The Annals of Applied Statistics
9 (3) (2015) 1350-1371.

49
[50]

[51]

[52]
[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

M. Harbers, K. van den Bosch, J.-J. Meyer, Design and evaluation of explainable BDI agents, in:
IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,
Vol. 2, IEEE, 2010, pp. 125-132.

M. H. Aung, P. G. Lisboa, T. A. Etchells, A. C. Testa, B. Van Calster, S. Van Huffel, L. Valentin,
D. Timmerman, Comparing analytical decision support models through boolean rule extraction:
A case study of ovarian tumour malignancy, in: International Symposium on Neural Networks,
Springer, 2007, pp. 1177-1186.

A. Weller, Challenges for transparency (2017). arXiv:1708.01870.

A. A. Freitas, Comprehensible classification models: a position paper, ACM SIGKDD explorations
newsletter 15 (1) (2014) 1-10.

V. Schetinin, J. E. Fieldsend, D. Partridge, T. J. Coats, W. J. Krzanowski, R. M. Everson, T. C.
Bailey, A. Hernandez, Confident interpretation of bayesian decision tree ensembles for clinical
applications, IEEE Transactions on Information Technology in Biomedicine 11 (3) (2007) 312-319.

D. Martens, J. Vanthienen, W. Verbeke, B. Baesens, Performance of classification models from a
user perspective, Decision Support Systems 51 (4) (2011) 782-793.

Z. Che, S. Purushotham, R. Khemani, Y. Liu, Interpretable deep models for ICU outcome prediction,
in: AMIA Annual Symposium Proceedings, Vol. 2016, American Medical Informatics Association,
2016, p. 371.

N. Barakat, J. Diederich, Eclectic rule-extraction from support vector machines, International
Journal of Computer, Electrical, Automation, Control and Information Engineering 2 (5) (2008)
1672-1675.

E J.C. Garcia, D. A. Robb, X. Liu, A. Laskov, P. Patron, H. Hastie, Explain yourself: A natural
language interface for scrutable autonomous robots (2018). arXiv:1803.02088.

P. Langley, B. Meadows, M. Sridharan, D. Choi, Explainable agency for intelligent autonomous
systems, in: AAAI Conference on Artificial Intelligence, 2017, pp. 4762-4763.

G. Montavon, S. Lapuschkin, A. Binder, W. Samek, K.-R. Miiller, Explaining nonlinear classifica-
tion decisions with deep taylor decomposition, Pattern Recognition 65 (2017) 211-222.

P-J. Kindermans, K. T. Schiitt, M. Alber, K.-R. Miiller, D. Erhan, B. Kim, S. Dahne, Learning how
to explain neural networks: Patternnet and patternattribution (2017). arXiv:1705.05598.

G. Ras, M. van Gerven, P. Haselager, Explanation methods in deep learning: Users, values,
concerns and challenges, in: Explainable and Interpretable Models in Computer Vision and
Machine Learning, Springer, 2018, pp. 19-36.

S. Bach, A. Binder, K.-R. Miiller, W. Samek, Controlling explanatory heatmap resolution and
semantics via decomposition depth, in: IEEE International Conference on Image Processing (ICIP),
IEEE, 2016, pp. 2271-2275.

G. J. Katuwal, R. Chen, Machine learning model interpretability for precision medicine (2016).
arXiv:1610.09045.

M. A. Neerincx, J. van der Waa, F. Kaptein, J. van Diggelen, Using perceptual and cognitive expla-
nations for enhanced human-agent team performance, in: International Conference on Engineering
Psychology and Cognitive Ergonomics, Springer, 2018, pp. 204-214.

50
[66]

[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

[76]

[77]

[78]

[79]

[80]

[81]

J. D. Olden, D. A. Jackson, Illuminating the black box: a randomization approach for understanding
variable contributions in artificial neural networks, Ecological modelling 154 (1-2) (2002) 135-150.

J. Krause, A. Perer, K. Ng, Interacting with predictions: Visual inspection of black-box machine
learning models, in: CHI Conference on Human Factors in Computing Systems, ACM, 2016, pp.
5686-5697.

L. Rosenbaum, G. Hinselmann, A. Jahn, A. Zell, Interpreting linear support vector machine models
with heat map molecule coloring, Journal of Cheminformatics 3 (1) (2011) 11.

J. Tan, M. Ung, C. Cheng, C. S. Greene, Unsupervised feature construction and knowledge
extraction from genome-wide assays of breast cancer with denoising autoencoders, in: Pacific
Symposium on Biocomputing Co-Chairs, World Scientific, 2014, pp. 132-143.

S. Krening, B. Harrison, K. M. Feigh, C. L. Isabell, M. Riedl, A. Thomaz, Learning from expla-
nations using sentiment and advice in RL, IEEE Transactions on Cognitive and Developmental
Systems 9 (1) (2017) 44-S5.

M. T. Ribeiro, S. Singh, C. Guestrin, Model-agnostic interpretability of machine learning (2016).
arXiv:1606.05386.

S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Mtiller, W. Samek, On pixel-wise expla-
nations for non-linear classifier decisions by layer-wise relevance propagation, PloS one 10 (7)
(2015) e0130140.

T. A. Etchells, P. J. Lisboa, Orthogonal search-based rule extraction (OSRE) for trained neural
networks: a practical and efficient approach, IEEE Transactions on Neural Networks 17 (2) (2006)
374-384.

Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, H. H. Zhuo, S$. Kambhampati, Plan expli-
cability and predictability for robot task planning, in: 2017 IEEE International Conference on
Robotics and Automation (ICRA), IEEE, 2017, pp. 1313-1320.

A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, T. Lillicrap,
A simple neural network module for relational reasoning, in: Advances in Neural Information
Processing Systems, 2017, pp. 4967-4976.

C.-Y. J. Peng, T.-S. H. So, F. K. Stage, E. P.S. John, The use and interpretation of logistic regression
in higher education journals: 1988-1999, Research in Higher Education 43 (3) (2002) 259-293.

B. Ustiin, W. Melssen, L. Buydens, Visualisation and interpretation of support vector regression
models, Analytica Chimica Acta 595 (1-2) (2007) 299-309.

Q. Zhang, Y. Yang, H. Ma, Y. N. Wu, Interpreting CNNs via decision trees, in: IEEE Conference
on Computer Vision and Pattern Recognition, 2019, pp. 6261-6270.

M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth, F. Doshi-Velez, Beyond sparsity: Tree
regularization of deep models for interpretability, in: AAAI Conference on Artificial Intelligence,
2018, pp. 1670-1678.

G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network (2015). arXiv:
1503.02531.

N. Frosst, G. Hinton, Distilling a neural network into a soft decision tree (2017). arXiv:
1711.09784.

31
[82]

[83]

[84]

[85]

[86]

[87]

[88]

[89]

[90]

[91]

[92]

[93]

[94]

[95]

[96]

[97]

M. G. Augasta, T. Kathirvalavakumar, Reverse engineering the neural networks for rule extraction
in classification problems, Neural Processing Letters 35 (2) (2012) 131-150.

Z.-H. Zhou, Y. Jiang, S.-F. Chen, Extracting symbolic rules from trained neural network ensembles,
AI Communications 16 (1) (2003) 3-15.

H. F. Tan, G. Hooker, M. T. Wells, Tree space prototypes: Another look at making tree ensembles
interpretable (2016). arXiv:1611.07115.

R. C. Fong, A. Vedaldi, Interpretable explanations of black boxes by meaningful perturbation, in:
IEEE International Conference on Computer Vision, 2017, pp. 3429-3437.

T. Miller, P, Howe, L. Sonenberg, Explainable AI: Beware of inmates running the asylum, in:
International Joint Conference on Artificial Intelligence, Workshop on Explainable AI (XAT),
Vol. 36, 2017, pp. 36-40.

R. Goebel, A. Chander, K. Holzinger, F Lecue, Z. Akata, S. Stumpf, P. Kieseberg, A. Holzinger,
Explainable AI: the new 42?, in: International Cross-Domain Conference for Machine Learning
and Knowledge Extraction, Springer, 2018, pp. 295-303.

V. Belle, Logic meets probability: Towards explainable AI systems for uncertain worlds, in:
International Joint Conference on Artificial Intelligence, 2017, pp. 5116-5120.

L. Edwards, M. Veale, Slave to the algorithm: Why a right to an explanation is probably not the
remedy you are looking for, Duke L. & Tech. Rev. 16 (2017) 18.

Y. Lou, R. Caruana, J. Gehrke, G. Hooker, Accurate intelligible models with pairwise interactions,
in: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM,
2013, pp. 623-631.

K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, Y. Bengio, Show, attend
and tell: Neural image caption generation with visual attention, in: International Conference on
Machine Learning, 2015, pp. 2048-2057.

J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, B. Baesens, An empirical evaluation of the
comprehensibility of decision table, tree and rule based predictive models, Decision Support
Systems 51 (1) (2011) 141-154.

N. H. Barakat, A. P. Bradley, Rule extraction from support vector machines: A sequential covering
approach, IEEE Transactions on Knowledge and Data Engineering 19 (6) (2007) 729-741.

F.C. Adriana da Costa, M. M. B. Vellasco, R. Tanscheit, Fuzzy rule extraction from support vector
machines, in: International Conference on Hybrid Intelligent Systems, IEEE, 2005, pp. 335-340.

D. Martens, B. Baesens, T. Van Gestel, J. Vanthienen, Comprehensible credit scoring models using
tule extraction from support vector machines, European Journal of Operational Research 183 (3)
(2007) 1466-1476.

B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for discriminative
localization, in: IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2921-
2929.

R. Krishnan, G. Sivakumar, P. Bhattacharya, Extracting decision trees from trained neural networks,
Pattern Recognition 32 (12) (1999) 1999-2009.

52
[98]

[99]

[100]

[101]

[102]

[103]

[104]

[105]

[106]

[107]

[108]

[109]

[110]

[111]

[112]

[113]

X. Fu, C. Ong, S. Keerthi, G. G. Hung, L. Goh, Extracting the knowledge embedded in support
vector machines, in: [EEE International Joint Conference on Neural Networks, Vol. 1, IEEE, 2004,
pp. 291-296.

B. Green, “Fair” risk assessments: A precarious approach for criminal justice reform, in: 5th
Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2018.

A. Chouldechova, Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments, Big Data 5 (2) (2017) 153-163.

M. Kim, O. Reingold, G. Rothblum, Fairness through computationally-bounded awareness, in:
Advances in Neural Information Processing Systems, 2018, pp. 4842-4852.

B. Haasdonk, Feature space interpretation of SVMs with indefinite kernels, IEEE Transactions on
Pattern Analysis and Machine Intelligence 27 (4) (2005) 482-492.

A. Palezewska, J. Palezewski, R. M. Robinson, D. Neagu, Interpreting random forest classification
models using a feature contribution method, in: Integration of Reusable Systems, Springer, 2014,
pp. 193-218.

S. H. Welling, H. H. Refsgaard, P. B. Brockhoff, L. H. Clemmensen, Forest floor visualizations of
random forests (2016). arXiv:1605.09196.

G. Fung, S. Sandilya, R. B. Rao, Rule extraction from linear support vector machines, in: ACM
SIGKDD International Conference on Knowledge Discovery in Data Mining, ACM, 2005, pp.
32-40.

Y. Zhang, H. Su, T. Jia, J. Chu, Rule extraction from trained support vector machines, in: Pacific-
Asia Conference on Knowledge Discovery and Data Mining, Springer, 2005, pp. 61-70.

D. Linsley, D. Shiebler, S. Eberhardt, T. Serre, Global-and-local attention networks for visual
recognition (2018). arXiv:1805.08819.

S.-M. Zhou, J. Q. Gan, Low-level interpretability and high-level interpretability: a unified view
of data-driven interpretable fuzzy system modelling, Fuzzy Sets and Systems 159 (23) (2008)
3091-3131.

J. Burrell, How the machine ‘thinks’: Understanding opacity in machine learning algorithms, Big
Data & Society 3 (1) (2016) 1-12.

A. Shrikumar, P. Greenside, A. Shcherbina, A. Kundaje, Not just a black box: Learning important
features through propagating activation differences (2016). arxXiv:1605.01713.

Y. Dong, H. Su, J. Zhu, B. Zhang, Improving interpretability of deep neural networks with
semantic information, in: IEEE Conference on Computer Vision and Pattern Recognition, 2017,
pp. 4306-4314.

G. Ridgeway, D. Madigan, T. Richardson, J. O’ Kane, Interpretable boosted naive bayes classi-
fication., in: ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 1998, pp.
101-104.

Q. Zhang, Y. Nian Wu, S.-C. Zhu, Interpretable convolutional neural networks, in: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 8827-8836.

53
[114]

[115]

[116]

[117]

[118]
[119]

[120]

[121]

[122]
[123]

[124]

[125]

[126]

[127]

[128]

[129]

[130]

S. Seo, J. Huang, H. Yang, Y. Liu, Interpretable convolutional neural networks with dual local and
global attention for review rating prediction, in: Proceedings of the Eleventh ACM Conference on
Recommender Systems, ACM, 2017, pp. 297-305.

K. Larsen, J. H. Petersen, E. Budtz-Jorgensen, L. Endahl, Interpreting parameters in the logistic
regression model with random effects, Biometrics 56 (3) (2000) 909-914.

B. Gaonkar, R. T. Shinohara, C. Davatzikos, A. D. N. Initiative, et al., Interpreting support vector
machine models for multivariate group wise analysis in neuroimaging, Medical image analysis
24 (1) (2015) 190-204.

K. Xu, D. H. Park, C. Yi, C. Sutton, Interpreting deep classifier by visual distillation of dark
knowledge (2018). arXiv:1803.04042.

H. Deng, Interpreting tree ensembles with intrees (2014). arXiv:1408.5456.

P. Domingos, Knowledge discovery via multiple models, Intelligent Data Analysis 2 (1-4) (1998)
187-202.

S. Tan, R. Caruana, G. Hooker, Y. Lou, Distill-and-compare: Auditing black-box models using
transparent model distillation, in: AAAI/ACM Conference on AI, Ethics, and Society, ACM, 2018,
pp. 303-310.

R. A. Berk, J. Bleich, Statistical procedures for forecasting criminal behavior: A comparative
assessment, Criminology & Public Policy 12 (3) (2013) 513-544.

S. Hara, K. Hayashi, Making tree ensembles interpretable (2016). arXiv:1606.05390.

A. Henelius, K. Puolamaki, A. Ukkonen, Interpreting classifiers through attribute interactions in
datasets (2017). arXiv:1707.07576.

H. Hastie, F. J. C. Garcia, D. A. Robb, P. Patron, A. Laskov, MIRIAM: a multimodal chat-based
interface for autonomous systems, in: ACM International Conference on Multimodal Interaction,
ACM, 2017, pp. 495-496.

D. Bau, B. Zhou, A. Khosla, A. Oliva, A. Torralba, Network dissection: Quantifying interpretability
of deep visual representations, in: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2017, pp. 6541-6549.

H. Niifiez, C. Angulo, A. Catala, Rule extraction from support vector machines., in: European
Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning
(ESANN), 2002, pp. 107-112.

H. Ntifiez, C. Angulo, A. Catala, Rule-based learning systems for support vector machines, Neural
Processing Letters 24 (1) (2006) 1-18.

M. Kearns, S. Neel, A. Roth, Z. S. Wu, Preventing fairness gerrymandering: Auditing and learning
for subgroup fairness (2017). arXiv:1711.05144.

E. Akyol, C. Langbort, T. Basar, Price of transparency in strategic machine learning (2016).
arXiv:1610.08210.

D. Erhan, A. Courville, Y. Bengio, Understanding representations learned in deep architectures,
Department dInformatique et Recherche Operationnelle, University of Montreal, QC, Canada,
Tech. Rep 1355 (2010) 1.

54
[131]

[132]

[133]

[134]

[135]

[136]

[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

[146]

[147]

[148]

Y. Zhang, B. Wallace, A sensitivity analysis of (and practitioners’ guide to) convolutional neural
networks for sentence classification (2015). arXiv:1510.03820.

J. R. Quinlan, Simplifying decision trees, International journal of man-machine studies 27 (3)
(1987) 221-234.

Y. Zhou, G. Hooker, Interpreting models via single tree approximation (2016). arxiv:1610.
09036.

A. Navia-Vazquez, E. Parrado-Hernandez, Support vector machine interpretation, Neurocomputing
69 (13-15) (2006) 1754-1759.

J. J. Thiagarajan, B. Kailkhura, P. Sattigeri, K. N. Ramamurthy, Treeview: Peeking into deep neural
networks via feature-space partitioning (2016). arXiv:1611.07429.

M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: European
conference on computer vision, Springer, 2014, pp. 818-833.

A. Mahendran, A. Vedaldi, Understanding deep image representations by inverting them, in:
Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5188-
5196.

J. Wagner, J. M. Kohler, T. Gindele, L. Hetzel, J. T. Wiedemer, S. Behnke, Interpretable and
fine-grained visual explanations for convolutional neural networks, in: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2019, pp. 9097-9107.

A. Kanehira, T. Harada, Learning to explain with complemental examples, in: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 8603-8611.

D. W. Apley, Visualizing the effects of predictor variables in black box supervised learning models
(2016). arXiv:1612.08468.

M. Staniak, P. Biecek, Explanations of Model Predictions with live and breakDown Packages, The
R Journal 10 (2) (2018) 395-409.

M. D. Zeiler, D. Krishnan, G. W. Taylor, R. Fergus, Deconvolutional networks., in: CVPR, Vol. 10,
2010, p. 7.

J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller, Striving for simplicity: The all
convolutional net (2014). arXiv:1412.6806.

B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, R. Sayres, Interpretability beyond
feature attribution: Quantitative testing with concept activation vectors (TCAV) (2017). arXiv:
1711.11279.

A. Polino, R. Pascanu, D. Alistarh, Model compression via distillation and quantization (2018).
arXiv:1802.05668.

W. J. Murdoch, A. Szlam, Automatic rule extraction from long short term memory networks (2017).
arXiv:1702.02540.

M. W. Craven, J. W. Shavlik, Using sampling and queries to extract rules from trained neural
networks, in: Machine learning proceedings 1994, Elsevier, 1994, pp. 37-45.

A. D. Arbatli, H. L. Akin, Rule extraction from trained neural networks using genetic algorithms,
Nonlinear Analysis: Theory, Methods & Applications 30 (3) (1997) 1639-1648.

55
[149]

[150]
[151]

[152]

[153]

[154]

[155]

[156]

[157]
[158]

[159]

[160]

[161]
[162]
[163]

[164]

[165]

[166]

[167]
[168]

U. Johansson, L. Niklasson, Evolving decision trees using oracle guides, in: 2009 IEEE Symposium
on Computational Intelligence and Data Mining, IEEE, 2009, pp. 238-244.

T. Lei, R. Barzilay, T. Jaakkola, Rationalizing neural predictions (2016). arXiv:1606.04155.

A. Radford, R. Jozefowicz, I. Sutskever, Learning to generate reviews and discovering sentiment
(2017). arXiv:1704.01444.

R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, D. Batra, Grad-CAM: Why did
you say that? (2016).

R. Shwartz-Ziv, N. Tishby, Opening the black box of deep neural networks via information (2017).
arXiv:1703.00810.

J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson, Understanding neural networks through
deep visualization (2015). arXiv:1506.06579.

P. E. Pope, S. Kolouri, M. Rostami, C. E. Martin, H. Hoffmann, Explainability methods for graph
convolutional neural networks, in: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2019, pp. 10772-10781.

P. Gajane, M. Pechenizkiy, On formalizing fairness in prediction with machine learning (2017).
arXiv:1710.03184.

C. Dwork, C. Ilvento, Composition of fairsystems (2018). arXiv:1806.06122.

S. Barocas, M. Hardt, A. Narayanan, Fairness and Machine Learning, fairmlbook.org, 2019,
http://www. fairmlbook.org.

H.-X. Wang, L. Fratiglioni, G. B. Frisoni, M. Viitanen, B. Winblad, Smoking and the occurence of
alzheimet’s disease: Cross-sectional and longitudinal data in a population-based study, American
journal of epidemiology 149 (7) (1999) 640-644.

P. Rani, C. Liu, N. Sarkar, E. Vanman, An empirical study of machine learning techniques for affect
recognition in human-robot interaction, Pattern Analysis and Applications 9 (1) (2006) 58-69.

J. Pearl, Causality, Cambridge university press, 2009.
M. Kuhn, K. Johnson, Applied predictive modeling, Vol. 26, Springer, 2013.

G. James, D. Witten, T. Hastie, R. Tibshirani, An introduction to statistical learning, Vol. 112,
Springer, 2013.

C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, R. Fergus, Intriguing
properties of neural networks (2013). arXiv:1312.6199.

D. Ruppert, Robust statistics: The approach based on influence functions, Taylor & Francis, 1987.

S. Basu, K. Kumbier, J. B. Brown, B. Yu, Iterative random forests to discover predictive and
stable high-order interactions, Proceedings of the National Academy of Sciences 115 (8) (2018)
1943-1948.

B. Yu, et al., Stability, Bernoulli 19 (4) (2013) 1484-1500.

K. Burns, L. A. Hendricks, K. Saenko, T. Darrell, A. Rohrbach, Women also Snowboard: Over-
coming Bias in Captioning Models (2018). arXiv:1803.09797.

56
[169]

[170]

[171]

[172]

[173]

[174]

[175]

[176]

[177]

[178]

[179]

[180]

[181]
[182]
[183]

[184]

[185]

[186]

A. Bennetot, J.-L. Laurent, R. Chatila, N. Diaz-Rodriguez, Towards explainable neural-symbolic
visual reasoning, in: NeSy Workshop IJCAI 2019, Macau, China, 2019.

R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical
Society: Series B (Methodological) 58 (1) (1996) 267-288.

Y. Lou, R. Caruana, J. Gehrke, Intelligible models for classification and regression, in: ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2012, pp.
150-158.

K. Kawaguchi, Deep learning without poor local minima, in: Advances in neural information
processing systems, 2016, pp. 586-594.

A. Datta, S. Sen, Y. Zick, Algorithmic transparency via quantitative input influence: Theory and
experiments with learning systems, in: 2016 IEEE symposium on security and privacy (SP), IEEE,
2016, pp. 598-617.

Z. Bursac, C. H. Gauss, D. K. Williams, D. W. Hosmer, Purposeful selection of variables in logistic
regression, Source code for biology and medicine 3 (1) (2008) 17.

J. Jaccard, Interaction effects in logistic regression: Quantitative applications in the social sciences,
Sage Thousand Oaks, CA, 2001.

D. W. Hosmer Jr, S. Lemeshow, R. X. Sturdivant, Applied logistic regression, Vol. 398, John Wiley
& Sons, 2013.

C.-Y. J. Peng, K. L. Lee, G. M. Ingersoll, An introduction to logistic regression analysis and
reporting, The journal of educational research 96 (1) (2002) 3-14.

U. Hoffrage, G. Gigerenzer, Using natural frequencies to improve diagnostic inferences, Academic
medicine 73 (5) (1998) 538-540.

C. Mood, Logistic regression: Why we cannot do what we think we can do, and what we can do
about it, European sociological review 26 (1) (2010) 67-82.

H. Laurent, R. L. Rivest, Constructing optimal binary decision trees is Np-complete, Information
processing letters 5 (1) (1976) 15-17.

P.E. Utgoff, Incremental induction of decision trees, Machine learning 4 (2) (1989) 161-186.
J. R. Quinlan, Induction of decision trees, Machine learning 1 (1) (1986) 81-106.

L. Rokach, O. Z. Maimon, Data mining with decision trees: theory and applications, Vol. 69, World
scientific, 2014.

S. Rovnyak, S. Kretsinger, J. Thorp, D. Brown, Decision trees for real-time transient stability
prediction, IEEE Transactions on Power Systems 9 (3) (1994) 1417-1426.

H. Nefeslioglu, E. Sezer, C. Gokceoglu, A. Bozkir, T. Duman, Assessment of landslide suscep-
tibility by decision trees in the metropolitan area of istanbul, turkey, Mathematical Problems in
Engineering 2010 (2010) Article ID 901095.

S. B. Imandoust, M. Bolandraftar, Application of k-nearest neighbor (knn) approach for predicting
economic events: Theoretical background, International Journal of Engineering Research and
Applications 3 (5) (2013) 605-610.

57
[187]

[188]

[189]

[190]

[191]

[192]

[193]

[194]

[195]

[196]

[197]

[198]

[199]

[200]

[201]

[202]

L. Li, D. M. Umbach, P. Terry, J. A. Taylor, Application of the GA/KNN method to SELDI
proteomics data, Bioinformatics 20 (10) (2004) 1638-1640.

G. Guo, H. Wang, D. Bell, Y. Bi, K. Greer, An KNN model-based approach and its application in
text categorization, in: International Conference on Intelligent Text Processing and Computational
Linguistics, Springer, 2004, pp. 559-570.

S. Jiang, G. Pang, M. Wu, L. Kuang, An improved k-nearest-neighbor algorithm for text catego-
rization, Expert Systems with Applications 39 (1) (2012) 1503-1509.

U. Johansson, R. Kénig, L. Niklasson, The truth is in there-rule extraction from opaque models
using genetic programming., in: FLAIRS Conference, Miami Beach, FL, 2004, pp. 658-663.

J. R. Quinlan, Generating production rules from decision trees., in: ijcai, Vol. 87, Citeseer, 1987,
pp. 304-307.

P. Langley, H. A. Simon, Applications of machine learning and rule induction, Communications of
the ACM 38 (11) (1995) 54-64.

D. Berg, Bankruptcy prediction by generalized additive models, Applied Stochastic Models in
Business and Industry 23 (2) (2007) 129-143.

R. Calabrese, et al., Estimating bank loans loss given default by generalized additive models, UCD
Geary Institute Discussion Paper Series, WP2012/24 (2012).

P. Taylan, G.-W. Weber, A. Beck, New approaches to regression by generalized additive models and
continuous optimization for modern applications in finance, science and technology, Optimization
56 (5-6) (2007) 675-698.

H. Murase, H. Nagashima, S. Yonezaki, R. Matsukura, T. Kitakado, Application of a generalized
additive model (GAM) to reveal relationships between environmental factors and distributions of
pelagic fish and krill: a case study in sendai bay, Japan, ICES Journal of Marine Science 66 (6)
(2009) 1417-1424.

N. Tomié, S. Bozié, A modified geosite assessment model (M-GAM) and its application on the
lazar canyon area (serbia), International journal of environmental research 8 (4) (2014) 1041-1052.

A. Guisan, T. C. Edwards Jr, T. Hastie, Generalized linear and generalized additive models in
studies of species distributions: setting the scene, Ecological Modelling 157 (2-3) (2002) 89-100.

P. Rothery, D. B. Roy, Application of generalized additive models to butterfly transect count data,
Journal of Applied Statistics 28 (7) (2001) 897-909.

A. Pierrot, Y. Goude, Short-term electricity load forecasting with generalized additive models, in:
16th Intelligent System Applications to Power Systems Conference, ISAP 2011, IEEE, 2011, pp.
410-415.

T. L. Griffiths, C. Kemp, J. B. Tenenbaum, Bayesian models of cognition. (4 2008).
doi:10.1184/R1/6613682.v1.

URL https://kilthub.cmu.edu/articles/Bayesian_models_of_
cognition/ 6613682

B. H. Neelon, A. J. OMalley, S.-L. T. Normand, A bayesian model for repeated measures zero-
inflated count data with application to outpatient psychiatric service use, Statistical modelling
10 (4) (2010) 421-439.

58
[203]

[204]

[205]

[206]

[207]

[208]

[209]

[210]

[211]

[212]

[213]

[214]

[215]

[216]

[217]

[218]

[219]

M. McAllister, G. Kirkwood, Bayesian stock assessment: a review and example application using
the logistic model, ICES Journal of Marine Science 55 (6) (1998) 1031-1060.

G. Synnaeve, P. Bessiere, A bayesian model for opening prediction in RTS games with application
to starcraft, in: Computational Intelligence and Games (CIG), 2011 IEEE Conference on, IEEE,
2011, pp. 281-288.

S.-K. Min, D. Simonis, A. Hense, Probabilistic climate change predictions applying bayesian model
averaging, Philosophical transactions of the royal society of london a: mathematical, physical and
engineering sciences 365 (1857) (2007) 2103-2116.

G. Koop, D. J. Poirier, J. L. Tobias, Bayesian econometric methods, Cambridge University Press,
2007.

A. R. Cassandra, L. P. Kaelbling, J. A. Kurien, Acting under uncertainty: Discrete bayesian models
for mobile-robot navigation, in: Proceedings of IEEE/RSJ International Conference on Intelligent
Robots and Systems. IROS’96, Vol. 2, IEEE, 1996, pp. 963-972.

H. A. Chipman, E. I George, R. E. McCulloch, Bayesian cart model search, Journal of the
American Statistical Association 93 (443) (1998) 935-948.

B. Kim, C. Rudin, J. A. Shah, The bayesian case model: A generative approach for case-based
reasoning and prototype classification, in: Advances in Neural Information Processing Systems,
2014, pp. 1952-1960.

B. Kim, R. Khanna, O. O. Koyejo, Examples are not enough, learn to criticize! criticism for
interpretability, in: Advances in Neural Information Processing Systems, 2016, pp. 2280-2288.

U. Johansson, L. Niklasson, R. K6nig, Accuracy vs. comprehensibility in data mining models,
in: Proceedings of the seventh international conference on information fusion, Vol. 1, 2004, pp.
295-300.

R. Konig, U. Johansson, L. Niklasson, G-rex: A versatile framework for evolutionary data mining,
in: 2008 IEEE International Conference on Data Mining Workshops, IEEE, 2008, pp. 971-974.

H. Lakkaraju, E. Kamar, R. Caruana, J. Leskovec, Interpretable & explorable approximations of
black box models (2017). arXiv:1707.01154.

S. Mishra, B. L. Sturm, S. Dixon, Local interpretable model-agnostic explanations for music
content analysis., in: ISMIR, 2017, pp. 537-543.

G. Su, D. Wei, K. R. Varshney, D. M. Malioutov, Interpretable two-level boolean rule learning for
classification (2015). arXiv:1511.07361.

M. T. Ribeiro, S. Singh, C. Guestrin, Nothing else matters: Model-agnostic explanations by
identifying prediction invariance (2016). arXiv:1611.05817.

M. W. Craven, Extracting comprehensible models from trained neural networks, Ph.D. thesis,
aAI9700774 (1996).

O. Bastani, C. Kim, H. Bastani, Interpretability via model extraction (2017). arXiv:1706.
09773.

G. Hooker, Discovering additive structure in black box functions, in: Proceedings of the tenth
ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2004,
pp. 575-580.

59
[220]

[221]

[222]

[223]

[224]

[225]

[226]

[227]

[228]

[229]

[230]

[231]

[232]

[233]

[234]

[235]

[236]

P. Adler, C. Falk, S. A. Friedler, T. Nix, G. Rybeck, C. Scheidegger, B. Smith, S. Venkatasubra-
manian, Auditing black-box models for indirect influence, Knowledge and Information Systems
54 (1) (2018) 95-122.

P. W. Koh, P. Liang, Understanding black-box predictions via influence functions, in: Proceedings
of the 34th International Conference on Machine Learning-Volume 70, JMLR. org, 2017, pp.
1885-1894.

P. Cortez, M. J. Embrechts, Opening black box data mining models using sensitivity analysis, in:
2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2011, pp.
341-348.

P. Cortez, M. J. Embrechts, Using sensitivity analysis and visualization techniques to open black
box data mining models, Information Sciences 225 (2013) 1-17.

S. M. Lundberg, S.-I. Lee, A unified approach to interpreting model predictions, in: Advances in
Neural Information Processing Systems, 2017, pp. 4765-4774.

I. Kononenko, et al., An efficient explanation of individual classifications using game theory,
Journal of Machine Learning Research 11 (Jan) (2010) 1-18.

H. Chen, S. Lundberg, S.-I. Lee, Explaining models by propagating shapley values of local
components (2019). arXiv:arXiv:1911.11888.

P. Dabkowski, Y. Gal, Real time image saliency for black box classifiers, in: Advances in Neural
Information Processing Systems, 2017, pp. 6967-6976.

A. Henelius, K. Puolamaki, H. Bostrém, L. Asker, P. Papapetrou, A peek into the black box:
exploring classifiers by randomization, Data mining and knowledge discovery 28 (5-6) (2014)
1503-1529.

J. Moeyersoms, B. d’ Alessandro, F. Provost, D. Martens, Explaining classification models built on
high-dimensional sparse data (2016). arXiv:1607.06280.

D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, K.-R. MAiZller, How to
explain individual classification decisions, Journal of Machine Learning Research 11 (Jun) (2010)
1803-1831.

J. Adebayo, L. Kagal, Iterative orthogonal feature projection for diagnosing bias in black-box
models (2016). arXiv:1611.04967.

R. Guidotti, A. Monreale, S. Ruggieri, D. Pedreschi, F. Turini, F. Giannotti, Local rule-based
explanations of black box decision systems (2018). arXiv:1805.10820.

S. Krishnan, E. Wu, Palm: Machine learning explanations for iterative debugging, in: Proceedings
of the 2nd Workshop on Human-In-the-Loop Data Analytics, ACM, 2017, p. 4.

M. Robnik-Sikonja, I. Kononenko, Explaining classifications for individual instances, IEEE
Transactions on Knowledge and Data Engineering 20 (5) (2008) 589-600.

M. T. Ribeiro, S. Singh, C. Guestrin, Anchors: High-precision model-agnostic explanations, in:
AAAI Conference on Artificial Intelligence, 2018, pp. 1527-1535.

D. Martens, F. Provost, Explaining data-driven document classifications, MIS Quarterly 38 (1)
(2014) 73-100.

60
[237]

[238]

[239]

[240]

[241]

[242]

[243]

[244]

[245]

[246]

[247]

[248]

[249]

[250]

[251]

[252]

D. Chen, S. P. Fraiberger, R. Moakler, F. Provost, Enhancing transparency and control when
drawing data-driven inferences about individuals, Big data 5 (3) (2017) 197-212.

A. Goldstein, A. Kapelner, J. Bleich, E. Pitkin, Peeking inside the black box: Visualizing statistical
learning with plots of individual conditional expectation, Journal of Computational and Graphical
Statistics 24 (1) (2015) 44-65.

G. Casalicchio, C. Molnar, B. Bischl, Visualizing the feature importance for black box models,
in: Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
Springer, 2018, pp. 655-670.

G. Tolomei, F. Silvestri, A. Haines, M. Lalmas, Interpretable predictions of tree-based ensembles via
actionable feature tweaking, in: Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, ACM, 2017, pp. 465-474.

L. Auret, C. Aldrich, Interpretation of nonlinear relationships between process variables by use of
random forests, Minerals Engineering 35 (2012) 27-42.

N. F. Rajani, R. Mooney, Stacking with auxiliary features for visual question answering, in: Proceed-
ings of the 2018 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 2217-2226.

N. F. Rajani, R. J. Mooney, Ensembling visual explanations, in: Explainable and Interpretable
Models in Computer Vision and Machine Learning, Springer, 2018, pp. 155-172.

H. Ntifiez, C. Angulo, A. Catala, Rule-based learning systems for support vector machines, Neural
Processing Letters 24 (1) (2006) 1-18.

Z. Chen, J. Li, L. Wei, A multiple kernel support vector machine scheme for feature selection and
rule extraction from gene expression data of cancer tissue, Artificial Intelligence in Medicine 41 (2)
(2007) 161-175.

H. Nujiez, C. Angulo, A. Catala, Support vector machines with symbolic interpretation, in: VII
Brazilian Symposium on Neural Networks, 2002. SBRN 2002. Proceedings., IEEE, 2002, pp.
142-147.

P. Sollich, Bayesian methods for support vector machines: Evidence and predictive class probabili-
ties, Machine learning 46 (1-3) (2002) 21-52.

P. Sollich, Probabilistic methods for support vector machines, in: Advances in neural information
processing systems, 2000, pp. 349-355.

W. Landecker, M. D. Thomure, L. M. Bettencourt, M. Mitchell, G. T. Kenyon, S. P. Brumby,
Interpreting individual classifications of hierarchical networks, in: 2013 IEEE Symposium on
Computational Intelligence and Data Mining (CIDM), IEEE, 2013, pp. 32-38.

A. Jakulin, M. MoZina, J. Demar, I. Bratko, B. Zupan, Nomograms for visualizing support vector
machines, in: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge
discovery in data mining, ACM, 2005, pp. 108-117.

L. Fu, Rule generation from neural networks, IEEE Transactions on Systems, Man, and Cybernetics
24 (8) (1994) 1114-1124.

G. G. Towell, J. W. Shavlik, Extracting refined rules from knowledge-based neural networks,
Machine Learning 13 (1) (1993) 71-101.

61
[253]

[254]

[255]

[256]

[257]

[258]

[259]

[260]

[261]

[262]

[263]

[264]

[265]

[266]

[267]

[268]

[269]

S. Thrun, Extracting rules from artificial neural networks with distributed representations, in:
Proceedings of the 7th International Conference on Neural Information Processing Systems,
NIPS’94, 1994, pp. 505-512.

R. Setiono, W. K. Leow, FERNN: An algorithm for fast extraction of rules from neural networks,
Applied Intelligence 12 (1) (2000) 15-25.

I. A. Taha, J. Ghosh, Symbolic interpretation of artificial neural networks, IEEE Transactions on
Knowledge and Data Engineering 11 (3) (1999) 448-463.

H. Tsukimoto, Extracting rules from trained neural networks, IEEE Transactions on Neural
Networks 11 (2) (2000) 377-389.

J. R. Zilke, E. L. Mencia, F. Janssen, Deepred-tule extraction from deep neural networks, in:
International Conference on Discovery Science, Springer, 2016, pp. 457-473.

G. P. J. Schmitz, C. Aldrich, F 8. Gouws, ANN-DT: an algorithm for extraction of decision trees
from artificial neural networks, IEEE Transactions on Neural Networks 10 (6) (1999) 1392-1401.

M. Sato, H. Tsukimoto, Rule extraction from neural networks via decision tree induction, in:
IJCNN’01. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222),
Vol. 3, IEEE, 2001, pp. 1870-1875.

R. Féraud, F. Clérot, A methodology to explain neural network classification, Neural networks
15 (2) (2002) 237-246.

A. Shrikumar, P. Greenside, A. Kundaje, Learning Important Features Through Propagating
Activation Differences (2017). arXiv:1704.02685.

M. Sundararajan, A. Taly, Q. Yan, Axiomatic attribution for deep networks, in: International
Conference on Machine Learning, Vol. 70, JMLR. org, 2017, pp. 3319-3328.

J. Adebayo, J. Gilmer, I. Goodfellow, B. Kim, Local explanation methods for deep neural networks
lack sensitivity to parameter values (2018). arXiv:1810.03307.

N. Papernot, P. McDaniel, Deep k-nearest neighbors: Towards confident, interpretable and robust
deep learning (2018). arXiv:1803.04765.

J. Li, X. Chen, E. Hovy, D. Jurafsky, Visualizing and understanding neural models in NLP (2015).
arXiv:1506.01066.

S. Tan, K. C. Sim, M. Gales, Improving the interpretability of deep neural networks with stimulated
learning, in: 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),
IEEE, 2015, pp. 617-623.

L. Rieger, C. Singh, W. J. Murdoch, B. Yu, Interpretations are useful: penalizing explanations to
align neural networks with prior knowledge (2019). arXiv: arXiv:1909.13584.

A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, J. Clune, Synthesizing the preferred inputs for
neurons in neural networks via deep generator networks, in: Advances in Neural Information
Processing Systems, 2016, pp. 3387-3395.

Y. Li, J. Yosinski, J. Clune, H. Lipson, J. E. Hopcroft, Convergent learning: Do different neural
networks learn the same representations?, in: ICLR, 2016.

62
[270]

[271]

[272]

[273]

[274]

[275]
[276]

[277]

[278]

[279]

[280]

[281]

[282]

[283]

[284]

[285]

[286]

M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, S. Liu, Towards better analysis of deep convolutional neural
networks, IEEE transactions on visualization and computer graphics 23 (1) (2016) 91-100.

Y. Goyal, A. Mohapatra, D. Parikh, D. Batra, Towards transparent AI systems: Interpreting visual
question answering models (2016). arXiv:1608.08974.

K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualising image
classification models and saliency maps (2013). arXiv:1312.6034.

A. Nguyen, J. Yosinski, J. Clune, Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, 2015, pp. 427-436.

J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko,
T. Darrell, Long-term recurrent convolutional networks for visual recognition and description,
in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp.
2625-2634.

M. Lin, Q. Chen, S. Yan, Network in network (2013). arXiv:1312.4400.

L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell, Generating Visual
Explanations (2016). arXiv:1603.08507.

F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, X. Tang, Residual attention
network for image classification, in: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2017, pp. 3156-3164.

T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, Z. Zhang, The application of two-level attention models
in deep convolutional neural network for fine-grained image classification, in: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 842-850.

Q. Zhang, R. Cao, Y. Nian Wu, S.-C. Zhu, Growing Interpretable Part Graphs on ConvNets via
Multi-Shot Learning (2016). arXiv:1611.04246.

L. Arras, G. Montavon, K.-R. Miiller, W. Samek, Explaining recurrent neural network predictions
in sentiment analysis (2017). arXiv:1706.07206.

A. Karpathy, J. Johnson, L. Fei-Fei, Visualizing and understanding recurrent networks (2015).
arXiv:1506.02078.

J. Clos, N. Wiratunga, S. Massie, Towards explainable text classification by jointly learning lexicon
and modifier terms, in: ]CAI-17 Workshop on Explainable AI (XAD, 2017, p. 19.

S. Wisdom, T. Powers, J. Pitton, L. Atlas, Interpretable recurrent neural networks using sequential
sparse recovery (2016). arXiv:1611.07252.

V. Krakovna, F. Doshi-Velez, Increasing the interpretability of recurrent neural networks using
hidden markov models (2016). arXiv:1606.05320.

E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, W. Stewart, Retain: An interpretable
predictive model for healthcare using reverse time attention mechanism, in: Advances in Neural
Information Processing Systems, 2016, pp. 3504-3512.

L. Breiman, Classification and regression trees, Routledge, 2017.

63
[287]

[288]

[289]

[290]

[291]

[292]

[293]

[294]

[295]

[296]

[297]

[298]

[299]

[300]

[301]

[302]
[303]

A. Lucic, H. Haned, M. de Rijke, Explaining predictions from tree-based boosting ensembles
(2019). arXiv:arXiv:1907.02582.

S. M. Lundberg, G. G. Erion, S.-L Lee, Consistent individualized feature attribution for tree
ensembles (2018). arXiv:arXiv:1802.03888.

C. Bucilud, R. Caruana, A. Niculescu-Mizil, Model compression, in: ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, ACM, 2006, pp. 535-541.

R. Traoré, H. Caselles-Dupré, T. Lesort, T. Sun, G. Cai, N. D. Rodriguez, D. Filliat, DisCoRL:
Continual reinforcement learning via policy distillation (2019). arxXiv:1907.05855.

M. D. Zeiler, G. W. Taylor, R. Fergus, et al., Adaptive deconvolutional networks for mid and high
level feature learning., in: ICCV, Vol. 1, 2011, p. 6.

R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-cam: Visual
explanations from deep networks via gradient-based localization, in: Proceedings of the IEEE
International Conference on Computer Vision, 2017, pp. 618-626.

C. Olah, A. Mordvintsev, L. Schubert, Feature visualization., DistillHttps://distill pub/2017/feature-
visualization (2017). do1:10.23915/distill.00007.

J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, B. Kim, Sanity checks for saliency
maps, in: Advances in Neural Information Processing Systems, 2018, pp. 9505-9515.

C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, A. Mordvintsev, The building
blocks of interpretability, Distill (2018).
URL https://distill.pub/2018/building—-blocks/

Z. Che, S. Purushotham, R. Khemani, Y. Liu, Distilling knowledge from deep networks with
applications to healthcare domain (2015). arXiv:1512.03542.

I. Donadello, L. Serafini, A. D. Garcez, Logic tensor networks for semantic image interpretation,
Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI
(2017) 1596-1602.

I. Donadello, Semantic image interpretation-integration of numerical data and logical knowledge
for cognitive vision, Ph.D. thesis, University of Trento (2018).

A. S. d’Avila Garcez, M. Gori, L. C. Lamb, L. Serafini, M. Spranger, S$. N. Tran, Neural-symbolic
computing: An effective methodology for principled integration of machine learning and reasoning
(2019). arXiv:1905.06088.

R. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester, L. De Raedt, DeepProbLog: Neural
probabilistic logic programming, in: Advances in Neural Information Processing Systems 31, 2018,
pp. 3749-3759.

I. Donadello, M. Dragoni, C. Eccher, Persuasive explanation of reasoning inferences on dietary
data, in: First Workshop on Semantic Explainability @ ISWC 2019, 2019.

R. G. Krishnan, U. Shalit, D. Sontag, Deep Kalman Filters (2015). arXiv:1511.05121.

M. Karl, M. Soelch, J. Bayer, P. van der Smagt, Deep Variational Bayes Filters: Unsupervised
Learning of State Space Models from Raw Data (2016). arXiv:1605.06432.

64
[304]

[305]

[306]

[307]

[308]

[309]

[310]

(311)
[312]

[313]

(314]

[315]

[316]

[317]

[318]

[319]

[320]

M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, S. R. Datta, Composing graphical
models with neural networks for structured representations and fast inference, in: Advances in
Neural Information Processing Systems 29, 2016, pp. 2946-2954.

S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, P. H. Torr,
Conditional random fields as recurrent neural networks, in: Proceedings of the IEEE international
conference on computer vision, 2015, pp. 1529-1537.

N. Narodytska, A. Ignatiev, F. Pereira, J. Marques-Silva, Learning optimal decision trees with SAT,
in: Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,
WCAI-18, 2018, pp. 1362-1368.

O. Loyola-Gonzalez, Black-box vs. white-box: Understanding their advantages and weaknesses
from a practical point of view, IEEE Access 7 (2019) 154096-154113.

F Petroni, T. Rocktschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, S. Riedel, Language models
as knowledge bases? (2019). arXiv:1909.01066.

K. Bollacker, N. Diaz-Rodriguez, X. Li, Extending knowledge graphs with subjective influence
networks for personalized fashion, in: E. Portmann, M. E. Tabacchi, R. Seising, A. Habenstein
(Eds.), Designing Cognitive Cities, Springer International Publishing, 2019, pp. 203-233.

W. Shang, A. Trott, S. Zheng, C. Xiong, R. Socher, Learning world graphs to accelerate hierarchical
reinforcement learning (2019). arXiv:1907.00664.

M. Zolotas, Y. Demiris, Towards explainable shared control using augmented reality, 2019.

M. Garnelo, K. Arulkumaran, M. Shanahan, Towards deep symbolic reinforcement learning (2016).
arXiv:1609.05518.

V. Bellini, A. Schiavone, T. Di Noia, A. Ragone, E. Di Sciascio, Knowledge-aware autoencoders
for explainable recommender systems, in: Proceedings of the 3rd Workshop on Deep Learning for
Recommender Systems, DLRS 2018, 2018, pp. 24-31.

C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, C. Hawthorne, A. M. Dai, M. D. Hoffman,
D. Eck, Music transformer: Generating music with long-term structure (2018). arxiv:1809.
04281.

M. Cornia, L. Baraldi, R. Cucchiara, Smart: Training shallow memory-aware transformers for
robotic explainability (2019). arXiv:1910.02974.

A. Aamodt, E. Plaza, Case-based reasoning: Foundational issues, Methodological Variations, and
System Approaches 7 (1) (1994) 39-59.

R. Caruana, Case-based explanation for artificial neural nets, in: Artificial Neural Networks in
Medicine and Biology, Proceedings of the ANNIMAB-1 Conference, 2000, pp. 303-308.

M. T. Keane, E. M. Kenny, The Twin-System Approach as One Generic Solution for XAI: An
Overview of ANN-CBR Twins for Explaining Deep Learning (2019). arXiv:1905.08069.

T. Hailesilassie, Rule extraction algorithm for deep neural networks: A review (2016). arXiv:
1610.05267.

J. M. Benitez, J. L. Castro, I. Requena, Are artificial neural networks black boxes?, IEEE Trans.
Neural Networks 8 (5) (1997) 1156-1164.

65
[321]

[322]

[323]

[324]

[325]

[326]

[327]

[328]

[329]

[330]

[331]

[332]

[333]

[334]

[335]

[336]

[337]

U. Johansson, R. Knig, L. Niklasson, Automatically balancing accuracy and comprehensibility in
predictive modeling, in: Proceedings of the 8th International Conference on Information Fusion,
Vol. 2, 2005, p. 7 pp.

D. Smilkov, N. Thorat, B. Kim, F. Viégas, M. Wattenberg, SmoothGrad: removing noise by adding
noise (2017). arXiv:1706.03825.

M. Ancona, E. Ceolini, C. Oztireli, M. Gross, Towards better understanding of gradient-based
attribution methods for Deep Neural Networks (2017). arXiv:1711.06104.

J. Yosinski, J. Clune, Y. Bengio, H. Lipson, How transferable are features in deep neural networks?
(2014). arXiv:1411.1792.

A. Sharif Razavian, H. Azizpour, J. Sullivan, 8. Carlsson, CNN Features off-the-shelf: an Astound-
ing Baseline for Recognition (2014). arXiv:1403. 6382.

S. Du, H. Guo, A. Simpson, Self-driving car steering angle prediction based on image recognition,
Tech. rep., Technical Report, Stanford University (2017).

B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Object Detectors Emerge in Deep Scene
CNNs (2014). arXiv:1412.6856.

Y. Zhang, X. Chen, Explainable Recommendation: A Survey and New Perspectives (2018).
arXiv:1804.11192.

J. Frankle, M. Carbin, The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
(2018). arXiv:1803.03635.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,
Attention Is All You Need (2017). arXiv:1706.03762.

J. Lu, J. Yang, D. Batra, D. Parikh, Hierarchical question-image co-attention for visual question
answering, in: Proceedings of the 30th International Conference on Neural Information Processing
Systems, NIPS’ 16, 2016, pp. 289-297.

A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, D. Batra, Human Attention in Visual Question
Answering: Do Humans and Deep Networks Look at the Same Regions? (2016). arXiv:
1606.03556.

D. Huk Park, L. A. Hendricks, Z. Akata, A. Rohrbach, B. Schiele, T. Darrell, M. Rohrbach,
Multimodal Explanations: Justifying Decisions and Pointing to the Evidence (2018). arXiv:
1802 .08129.

A. Slavin Ross, M. C. Hughes, F. Doshi-Velez, Right for the Right Reasons: Training Differentiable
Models by Constraining their Explanations (2017). arXiv:1703.03717.

I. T. Jolliffe, Principal Component Analysis and Factor Analysis, Springer New York, 1986, pp.
115-128.

A. Hyvrinen, E. Oja, Oja, e.: Independent component analysis: Algorithms and applications. neural
networks 13(4-5), 411-430, Neural networks 13 (2000) 411-430.

M. W. Berry, M. Browne, A. N. Langville, V. P. Pauca, R. J. Plemmons, Algorithms and applications
for approximate nonnegative matrix factorization, Computational Statistics & Data Analysis 52
(2007) 155-173.

66
[338] D. P. Kingma, M. Welling, Auto-Encoding Variational Bayes (2013). arXiv:1312.6114.

[339] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. M. Botvinick, S. Mohamed, A. Lerchner,
beta-vae: Learning basic visual concepts with a constrained variational framework, in: ICLR, 2017.

[340] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, P. Abbeel, InfoGAN: Interpretable
Representation Learning by Information Maximizing Generative Adversarial Nets (2016). arXiv:
1606.03657.

[341] Q. Zhang, Y. Yang, Y. Liu, Y. Nian Wu, S.-C. Zhu, Unsupervised Learning of Neural Networks to
Explain Neural Networks (2018). arXiv:1805.07468.

[342] S. Sabour, N. Frosst, G. E Hinton, Dynamic Routing Between Capsules (2017). arXiv:1710.
09829.

[343] A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Batra, D. Parikh, VQA: Visual Question
Answering (2015). arXiv:1505.00468.

[344] A. Fukui, D. Huk Park, D. Yang, A. Rohrbach, T. Darrell, M. Rohrbach, Multimodal Compact
Bilinear Pooling for Visual Question Answering and Visual Grounding (2016). arxiv:1606.
01847.

[345] D. Bouchacourt, L. Denoyer, EDUCE: explaining model decisions through unsupervised concepts
extraction (2019). arXiv:1905.11852.

[346] C. Hofer, M. Denker, S. Ducasse, Design and Implementation of a Backward-In-Time Debugger,
in: NODe 2006, Vol. P-88 of Lecture Notes in Informatics, 2006, pp. 17-32.

[347] C. Rudin, Please stop explaining black box models for high stakes decisions (2018). arXiv:
1811.10154.

[348] A. Diez-Olivan, J. Del Ser, D. Galar, B. Sierra, Data fusion and machine learning for industrial
prognosis: Trends and perspectives towards Industry 4.0, Information Fusion 50 (2019) 92-111.

[349] R. R. Hoffman, S. T. Mueller, G. Klein, J. Litman, Metrics for explainable ai: Challenges and
prospects (2018). arXiv:arXiv:1812.04608.

[350] S. Mohseni, N. Zarei, E. D. Ragan, A multidisciplinary survey and framework for design and
evaluation of explainable ai systems (2018). arXiv:arXiv:1811.11839.

[351] R.M. J. Byrne, Counterfactuals in explainable artificial intelligence (XAI): Evidence from human
reasoning, in: Proceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence, IJCAI-19, 2019, pp. 6276-6282.

[352] M. Garnelo, M. Shanahan, Reconciling deep learning with symbolic artificial intelligence: repre-
senting objects and relations, Current Opinion in Behavioral Sciences 29 (2019) 17-23.

[353] G. Marra, F. Giannini, M. Diligenti, M. Gori, Integrating learning and reasoning with deep logic
models (2019). arXiv:1901.04195.

[354] K. Kelley, B. Clark, V. Brown, J. Sitzia, Good practice in the conduct and reporting of survey
research, International Journal for Quality in Health Care 15 (3) (2003) 261-266.

[355] S. Wachter, B. Mittelstadt, L. Floridi, Why a right to explanation of automated decision-making
does not exist in the general data protection regulation, International Data Privacy Law 7 (2) (2017)
76-99.

67
[356]

[357]

[358]

[359]

[360]

[361]

[362]

[363]

[364]

[365]

[366]

[367]

[368]

[369]

[370]

[371]

[372]

T. Orekondy, B. Schiele, M. Fritz, Knockoff nets: Stealing functionality of black-box models
(2018). arXiv:1812.02766.

S. J. Oh, B. Schiele, M. Fritz, Towards reverse-engineering black-box neural networks, in: Explain-
able AI: Interpreting, Explaining and Visualizing Deep Learning, Springer, 2019, pp. 121-144.

I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing adversarial examples (2014).
arXiv:1412.6572.

K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, D. Song,
Robust physical-world attacks on deep learning models (2017). arXiv:1707.08945.

I. J. Goodfellow, N. Papernot, P. D. McDaniel, cleverhans v0.1: an adversarial machine learning
library (2016). arXiv:1610.00768.

H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, F. Roli, Support vector machines under
adversarial label contamination, Neurocomputing 160 (C) (2015) 53-62.

B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndié, P. Laskov, G. Giacinto, FE. Roli, Evasion
attacks against machine learning at test time, in: Proceedings of the 2013th European Conference
on Machine Learning and Knowledge Discovery in Databases - Volume Part III, ECMLPKDD’ 13,
2013, pp. 387-402.

B. Biggio, I. Pillai, S. R. Bulo, D. Ariu, M. Pelillo, F. Roli, Is data clustering in adversarial settings
secure? (2018). arXiv:1811.09982.

Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan, Y. Zheng, Recent progress on generative adversarial
networks (gans): A survey, IEEE Access 7 (2019) 36322-36333.

D. Charte, F. Charte, S. Garcia, M. J. del Jesus, F Herrera, A practical tutorial on autoencoders
for nonlinear feature fusion: Taxonomy, models, software and guidelines, Information Fusion 44
(2018) 78-96.

C. EF Baumgartner, L. M. Koch, K. Can Tezcan, J. Xi Ang, E. Konukoglu, Visual feature attribution
using wasserstein gans, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 8309-8319.

C. Biffi, O. Oktay, G. Tarroni, W. Bai, A. De Marvao, G. Doumou, M. Rajchl, R. Bedair, S. Prasad,
S. Cook, et al., Learning interpretable anatomical features through deep generative models: Ap-
plication to cardiac remodeling, in: International Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2018, pp. 464-471.

S. Liu, B. Kailkhura, D. Loveland, Y. Han, Generative counterfactual introspection for explainable
deep learning (2019). arXiv: arXiv:1907.03077.

K. R. Varshney, H. Alemzadeh, On the safety of machine learning: Cyber-physical systems,
decision sciences, and data products, Big data 5 (3) (2017) 246-255.

G. M. Weiss, Mining with rarity: a unifying framework, ACM Sigkdd Explorations Newsletter
6 (1) (2004) 7-19.

J. Attenberg, P. Ipeirotis, F. Provost, Beat the machine: Challenging humans to find a predictive
model’s unknown unknowns, Journal of Data and Information Quality (JDIQ) 6 (1) (2015) 1.

G. Neff, A. Tanweer, B. Fiore-Gartland, L. Osburn, Critique and contribute: A practice-based
framework for improving critical data studies and data science, Big data 5 (2) (2017) 85-97.

68
[373]

[374]

[375]

[376]

[377]

[378]

[379]

[380]

[381]

[382]

[383]

[384]

[385]

[386]
[387]

A. Iliadis, F Russo, Critical data studies: An introduction, Big Data & Society 3 (2) (2016)
2053951716674238.

A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar,
N. Samatova, V. Kumar, Theory-guided data science: A new paradigm for scientific discovery
from data, IEEE Transactions on Knowledge and Data Engineering 29 (10) (2017) 2318-2331.

G. Hautier, C. C. Fischer, A. Jain, T. Mueller, G. Ceder, Finding natures missing ternary oxide
compounds using machine learning and density functional theory, Chemistry of Materials 22 (12)
(2010) 3762-3767.

C. C. Fischer, K. J. Tibbetts, D. Morgan, G. Ceder, Predicting crystal structure by merging data
mining with quantum mechanics, Nature materials 5 (8) (2006) 641.

S. Curtarolo, G. L. Hart, M. B. Nardelli, N. Mingo, S. Sanvito, O. Levy, The high-throughput
highway to computational materials design, Nature materials 12 (3) (2013) 191.

K. C. Wong, L. Wang, P. Shi, Active model with orthotropic hyperelastic material for cardiac image
analysis, in: International Conference on Functional Imaging and Modeling of the Heart, Springer,
2009, pp. 229-238.

J. Xu, J. L. Sapp, A. R. Dehaghani, F. Gao, M. Horacek, L. Wang, Robust transmural electrophysi-
ological imaging: Integrating sparse and dynamic physiological models into ecg-based inference,
in: International Conference on Medical Image Computing and Computer- Assisted Intervention,
Springer, 2015, pp. 519-527.

T. Lesort, M. Seurin, X. Li, N. Daz-Rodrguez, D. Filliat, Unsupervised state representation learning
with robotic priors: a robustness benchmark (2017). arXiv:arXiv:1709.05185.

J. Z. Leibo, Q. Liao, F. Anselmi, W. A. Freiwald, T. Poggio, View-tolerant face recognition and
hebbian learning imply mirror-symmetric neural tuning to head orientation, Current Biology 27 (1)
(2017) 62-67.

F. Schrodt, J. Kattge, H. Shan, F. Fazayeli, J. Joswig, A. Banerjee, M. Reichstein, G. Bonisch,
S. Dfaz, J. Dickie, et al., Bhpmf—a hierarchical bayesian approach to gap-filling and trait prediction
for macroecology and functional biogeography, Global Ecology and Biogeography 24 (12) (2015)
1510-1521.

D. Leslie, Understanding artificial intelligence ethics and safety (2019). arXiv:arXiv:1906.
05684, doi:10.5281/zenodo.3240529.

C. Rudin, Stop explaining black box machine learning models for high stakes decisions and use
interpretable models instead (2018). arXiv:arXiv:1811.10154.

J. Fjeld, H. Hilligoss, N. Achten, M. L. Daniel, J. Feldman, S. Kagay, Principled artificial intelli-
gence: A map of ethical and rights-based approaches (2019).
URL https://ai-hr.cyber.harvard.edu/images/primp-viz.pdf

R. Benjamins, A. Barbado, D. Sierra, Responsible AI by design (2019). arxXiv:1909.12838.

United-Nations, Transforming our world: the 2030 agenda for sustainable development, Tech. rep.,
eSocialSciences (2015).
URL https://EconPapers.repec.org/RePEc:ess:wpaper:1d:7559

69
[388]

[389]

[390]

[391]

[392]
[393]

[394]

[395]

[396]

[397]

[398]

[399]

[400]

[401]

[402]

[403]

[404]
[405]

G. D. Hager, A. Drobnis, F. Fang, R. Ghani, A. Greenwald, T. Lyons, D. C. Parkes, J. Schultz,
S. Saria, S. F Smith, M. Tambe, Artificial intelligence for social good (2019). arXiv:arXiv:
1901.05406.

B.C. Stahl, D. Wright, Ethics and privacy in ai and big data: Implementing responsible research
and innovation, IEEE Security & Privacy 16 (3) (2018) 26-33.

High Level Expert Group on Artificial Intelligence, Ethics guidelines for trustworthy ai, Tech. rep.,
European Commission (2019).

B. d’ Alessandro, C. O’ Neil, T. LaGatta, Conscientious classification: A data scientist’s guide to
discrimination-aware classification, Big data 5 (2) (2017) 120-134.

S. Barocas, A. D. Selbst, Big data’s disparate impact, Calif. L. Rev. 104 (2016) 671.

M. Hardt, E. Price, N. Srebro, et al., Equality of opportunity in supervised learning, in: Advances
in neural information processing systems, 2016, pp. 3315-3323.

T. Speicher, H. Heidari, N. Grgic-Hlaca, K. P. Gummadi, A. Singla, A. Weller, M. B. Zafar, A
unified approach to quantifying algorithmic unfairness: Measuring individual group unfairness
via inequality indices, in: Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery Data Mining, ACM, 2018, pp. 2239-2248.

F. Kamiran, T. Calders, Data preprocessing techniques for classification without discrimination,
Knowledge and Information Systems 33 (1) (2012) 1-33.

R. Zemel, Y. Wu, K. Swersky, T. Pitassi, C. Dwork, Learning fair representations, in: International
Conference on Machine Learning, 2013, pp. 325-333.

B. H. Zhang, B. Lemoine, M. Mitchell, Mitigating unwanted biases with adversarial learning, in:
Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, ACM, 2018, pp.
335-340.

Y. Ahn, Y.-R. Lin, Fairsight: Visual analytics for fairness in decision making, IEEE transactions on
visualization and computer graphics (2019).

E. Soares, P. Angelov, Fair-by-design explainable models for prediction of recidivism, arXiv
preprint arXiv:1910.02043 (2019).

J. Dressel, H. Farid, The accuracy, fairness, and limits of predicting recidivism, Science advances
4 (1) (2018) eaa05580.

U. Aivodji, H. Arai, O. Fortineau, S. Gambs, S. Hara, A. Tapp, Fairwashing: the risk of rationaliza-
tion, in: International Conference on Machine Learning, 2019, pp. 161-170.

S. Sharma, J. Henderson, J. Ghosh, Certifai: Counterfactual explanations for robustness,
transparency, interpretability, and fairness of artificial intelligence models, arXiv preprint
arXiv:1905.07857 (2019).

M. Drosou, H. Jagadish, E. Pitoura, J. Stoyanovich, Diversity in big data: A review, Big data 5 (2)
(2017) 73-84.

J. Lerman, Big data and its exclusions, Stan. L. Rev. Online 66 (2013) 55.
R. Agrawal, S. Gollapudi, A. Halverson, S. Ieong, Diversifying search results, in: Proceedings of

the second ACM international conference on web search and data mining, ACM, 2009, pp. 5-14.

70
[406]

[407]

[408]

[409]

[410]

[411]

[412]

[413]

[414]

[415]

[416]

[417]

[418]

[419]

[420]

[421]

[422]

B. Smyth, P. McClave, Similarity vs. diversity, in: International conference on case-based reasoning,
Springer, 2001, pp. 347-361.

P. Wang, L. T. Yang, J. Li, J. Chen, S. Hu, Data fusion in cyber-physical-social systems: State-of-
the-art and perspectives, Information Fusion 51 (2019) 42-57.

W. Ding, X. Jing, Z. Yan, L. T. Yang, A survey on data fusion in internet of things: Towards secure
and privacy-preserving fusion, Information Fusion 51 (2019) 129-144.

A. Smirnov, T. Levashova, Knowledge fusion patterns: A survey, Information Fusion 52 (2019)
31-40.

W. Ding, X. Jing, Z. Yan, L. T. Yang, A survey on data fusion in internet of things: Towards secure
and privacy-preserving fusion, Information Fusion 51 (2019) 129-144.

P. Wang, L. T. Yang, J. Li, J. Chen, S. Hu, Data fusion in cyber-physical-social systems: State-of-
the-art and perspectives, Information Fusion 51 (2019) 42-57.

B. P. L. Lau, S. H. Marakkalage, Y. Zhou, N. U. Hassan, C. Yuen, M. Zhang, U.-X. Tan, A survey
of data fusion in smart city applications, Information Fusion 52 (2019) 357-374.

S. Ramirez-Gallego, A. Femdndez, S. Garcia, M. Chen, F. Herrera, Big data: Tutorial and guidelines
on information and process fusion for analytics algorithms with mapreduce, Information Fusion 42
(2018) 51-61.

J. Koneény, H. B. McMahan, D. Ramage, P. Richtrik, Federated optimization: Distributed machine
learning for on-device intelligence (2016). arXiv:1610.02527.

B. McMahan, E. Moore, D. Ramage, S. Hampson, B. A. y Arcas, Communication-efficient
learning of deep networks from decentralized data, in: Artificial Intelligence and Statistics, 2017,
pp. 1273-1282.

J. Konetny, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, D. Bacon, Federated learning:
Strategies for improving communication efficiency (2016). arXiv: 1610.05492.

S. Sun, A survey of multi-view machine learning, Neural computing and applications 23 (7-8)
(2013) 2031-2038.

R. Zhang, F. Nie, X. Li, X. Wei, Feature selection with multi-view data: A survey, Information
Fusion 50 (2019) 158-167.

J. Zhao, X. Xie, X. Xu, S. Sun, Multi-view learning overview: Recent progress and new challenges,
Information Fusion 38 (2017) 43-54.

S. J. Oh, R. Benenson, M. Fritz, B. Schiele, Faceless person recognition: Privacy implications
in social media, in: Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam,
Proceedings, Part III, 2016, pp. 19-35.

P. Aditya, R. Sen, P. Druschel, S. Joon Oh, R. Benenson, M. Fritz, B. Schiele, B. Bhattacharjee,
T. T. Wu, I-pic: A platform for privacy-compliant image capture, in: Proceedings of the 14th annual
international conference on mobile systems, applications, and services, ACM, 2016, pp. 235-248.

Q. Sun, A. Tewari, W. Xu, M. Fritz, C. Theobalt, B. Schiele, A hybrid model for identity obfuscation
by face replacement, in: Proceedings of the European Conference on Computer Vision (ECCV),
2018, pp. 553-569.

71
[423] X. L. Dong, D. Srivastava, Big data integration, in: 2013 IEEE 29th international conference on
data engineering (ICDE), IEEE, 2013, pp. 1245-1248.

[424] D. Zhang, J. Zhao, F. Zhang, T. He, comobile: Real-time human mobility modeling at urban scale
using multi-view learning, in: Proceedings of the 23rd SIGSPATIAL International Conference on
Advances in Geographic Information Systems, ACM, 2015, p. 40.

[425] S. J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on knowledge and data
engineering 22 (10) (2009) 1345-1359.

[426] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji,
T. Gebru, Model cards for model reporting, in: Proceedings of the Conference on Fairness,
Accountability, and Transparency, ACM, 2019, pp. 220-229.

72
Check for
updates

Explainable Artificial Intelligence:
Concepts, Applications, Research
Challenges and Visions

Luca Longo\=)@®, Randy Goebel?@®, Freddy Lecue®“*@®, Peter Kieseberg’®,
and Andreas Holzinger?°®

' School of Computer Science, Technological University Dublin, Dublin, Ireland

luca. longo@tudublin.ie

2 xAl-Lab, Alberta Machine Intelligence Institute, University of Alberta,

Edmonton, Canada
rgoebel@ualberta.ca
3 Tnria, Sophia Antipolis, Biot, France
freddy.lecue@inria.fr
* Thales, Montreal, Canada
® JRC Blockchains, University of Applied Sciences St. Pélten, Sankt Pélten, Austria
Peter. Kieseberg@fhstp.ac.at
© Human-Centered AI Lab, Institute for Medical Informatics, Statistics
and Documentation, Medical University Graz, Graz, Austria
andreas. holzinger@medunigraz.at

Abstract. The development of theory, frameworks and tools for
Explainable AI (XAI) is a very active area of research these days, and
articulating any kind of coherence on a vision and challenges is itself a
challenge. At least two sometimes complementary and colliding threads
have emerged. The first focuses on the development of pragmatic tools for
increasing the transparency of automatically learned prediction models,
as for instance by deep or reinforcement learning. The second is aimed
at anticipating the negative impact of opaque models with the desire
to regulate or control impactful consequences of incorrect predictions,
especially in sensitive areas like medicine and law. The formulation of
methods to augment the construction of predictive models with domain
knowledge can provide support for producing human understandable
explanations for predictions. This runs in parallel with AI regulatory
concerns, like the European Union General Data Protection Regulation,
which sets standards for the production of explanations from automated
or semi-automated decision making. Despite the fact that all this research
activity is the growing acknowledgement that the topic of explainability
is essential, it is important to recall that it is also among the oldest fields
of computer science. In fact, early AI was re-traceable, interpretable,
thus understandable by and explainable to humans. The goal of this
research is to articulate the big picture ideas and their role in advancing
the development of XAI systems, to acknowledge their historical roots,
and to emphasise the biggest challenges to moving forward.

© IFIP International Federation for Information Processing 2020
Published by Springer Nature Switzerland AG 2020

A. Holzinger et al. (Eds.):; CD-MAKE 2020, LNCS 12279, pp. 1-16, 2020.
https://doi.org/10.1007/978-3-030-57321-8_1
2 L. Longo et al.

Keywords: Explainable artificial intelligence - Machine learning -
Explainability

1 Introduction

Machine learning is often viewed as the technology belonging to the future
in many application fields [46], ranging from pure commodities like recom-
mender systems for music, to automatic diagnosis of cancer or control models
for autonomous transportation. However, one fundamental issue lies within the
realm of explainability [60]. More precisely, most of the existing learning algo-
rithms can often lead to robust and accurate models from data, but in application
terms, they fail to provide end-users with descriptions on how they built them,
or to produce convincing explanations for their predictions [7]. In many sensi-
tive applications, such as in medicine, law, and other sectors where the main
workers are not computer scientists or engineers, the direct application of these
learning algorithms and complex models, without human oversight, is currently
inappropriate. The reasons are not only technical, like the accuracy of a model,
its stability to decisions and susceptibility to attacks, but often arise from soci-
ological concerns, practically settling on the issue of trust. In fact, one of the
principal reasons to produce an explanation is to gain the trust of users [13].
Trust is the main way to enhance the confidence of users with a system [66] as
well as their comfort while using and governing it [41]. Trust connects to ethics
and the intensity of regulatory activities, as for instance the General Data Pro-
tection Regulation in the European Union, leads to many legal and even ethical
questions: responsibility for safety, liability for malfunction, and tradeoffs therein
must inform decision makers at the highest level. Many methods of explainabil-
ity for data-driven models have emerged in the years, at a growing rate. On
the one hand, a large body of work have focused on building post-hoc methods
mainly aimed at wrapping fully trained models, often referred to black-boxes,
with an explainability layer [37]. A smaller body of research works, on the other
hand, have concentrated on creating self-explainable and interpretable models
by incorporating explainability mechanisms during their training, often referred
to as the ante-hoc phase [7]. Despite the fact that all this research activity is
the growing acknowledgement of the topic of explainability [68], by now referred
to as Explainable Artificial Intelligence (XAT) [54], it is important to recall that
it is also among the oldest fields of computer science. In fact, early AI was re-
traceable, interpretable, thus understandable by and explainable to humans. For
these reasons, many scholars have tried to review research works in the field
[1,3,22,51,72]. These reviews reveals the needs for a variety of kinds of explana-
tion, for the identification of methods for explainability and their evaluation as
well as the need to calibrate the tradeoffs in the degree or level of explanation
appropriate for a broad spectrum of applications.

The goal of this research is to articulate the big picture ideas and their role
in advancing the development of XAI systems, to acknowledge their historical
roots, and to emphasise the biggest challenges to moving forward. The reminder
XAI: Concepts, Applications, Research Challenges and Visions 3

of the paper focuses on relevant notions and concepts for explainability in Sect. 2.
It then continues in Sect. 3 with descriptions on the applications of methods for
XAI and on domains and areas in which these can have a significant impact.
A discussion on the research challenges surrounding XAI is presented in Sect. 4.
Eventually, recommendations and visions follow by presenting what we believe
scholars should focus on in the development of future explainable AI systems.

2 Notions and Related Concepts

A serious challenge for any attempt to articulate the current concepts for XAI
is that there is a very high volume of current activity, both on the research side
(22,66, 72], and in aggressive industrial developments, where any XAI functions
can provide a market advantage to all for profit applications of AI [23,60]. In
addition, there remains a lack of consensus on terminology, for example as noted
within, there are a variety of definitions for the concept of interpretation, but
little current connection to the formal history, that means in formal theories of
explanation or causation [30]. One recent paper [4] provides an organizing frame-
work based on comparing levels of explanation with levels of autonomous driving.
The goal is to identify foundational XAI concepts like relationships to historical
work on explanation, especially scientific ones, or the importance of interactive
explanation as well as the challenge of their evaluation. Note further that the
need for a complex system to provide explanations of activities, including pre-
dictions, is not limited to those with components created by machine learning
(example in [53]). Pragmatically, the abstract identification of a scientific expla-
nation that enables an explainee to recreate an experiment or prediction can arise
in very simple circumstances. For example, one can evaluate an explanation by
simply noting whether it is sufficient to achieve an explainee’s intended task. For
example, in Fig. 1, the pragmatic value of an Ikea visual assembly “explanation”
is whether the assembler explainee can achieve the assembly using the diagram.

Overall and within this broad spectrum of ideas related to explanation, there
is some focus on the foundational connection between explanation and that of
abductive reasoning. For example, the historical notion of scientific explanation
has been the subject of much debate in the community of science and philosophy
[70]. Some propose that a theory of explanation should include both scientific
and other simpler forms of explanation. Consequently, it has been a common goal
to formulate principles that can confirm an explanation as a scientific one. Aris-
totle is generally considered to be the first philosopher to articulate an opinion
that knowledge becomes scientific when it tries to explain the causes of “why.”
His view urges that science should not only keep facts, but also describe them
in an appropriate explanatory framework [15]. In addition to this theoretical
view, empiricists also maintain a belief that the components of ideas should
be acquired from perceptions with which humans become familiar through sen-
sory experience. The development of the principles of scientific explanation from
this perspective prospered with the so-called Deductive-Nomological (DN) model
that was described by Hempel in [24-26], and by Hempel and Oppenheim in [27].
4 L. Longo et al.

There is a more pragmatic AI historical research thread that connects scien-
tific explanation to AI implementations of abductive reasoning. One such thread,
among many, begins with Pople in 1973 [59], Poole et al. in 1987 [58], Muggleton
in 1991 [52], to Evans et al. in 2018 [14]. Pople described an algorithm for abduc-
tion applied to medical diagnosis. Poole et al. provided an extension to first order
logic which could subsume non-monotonic reasoning theories and also identify
explanatory hypothesis for any application domain. Muggleton proposed a fur-
ther refinement referred to as inductive logic programming where hypotheses
are identified by inductive constraints within any logic, including higher-order
logics. Finally, the adoption of this thread of reasoning have been generalised
to explanation based on inductive logic programming by Evans et al. [14]. This
most recent work connects with information theoretic ideas used to compare dif-
ferences in how to learn probability distributions that are modeled by machine
learning methods.

Interpreting and explaining a model trained from data by employing a
machine learning technique is not an easy task. A body of literature has focused
on tackling this by attempting at defining the concept of interpretability. This has
lead to the formation of many types of explanation, with several attributes and
structures. For example, it seems to human nature to assign causal attribution
of events [23], and we possess an innate psychological tendency to anthropomor-
phism. As a consequence, an Al-based system that purports to capture causal
relations should be capable of providing a causal explanation of its inferential
process (example in [55]). Causality can been considered a fundamental attribute
of explainability, especially when scientific explainability carries a responsibility
to help the explainee reconstruct the inferential process leading to a prediction.
Many have noted this role on how explanations should make the causal relation-
ships between the inputs and the outputs of a model explicit [17,30,41,51].

Despite the fact that data-driven models are extremely good at discover-
ing associations in the data, unfortunately they can not guarantee causality of
these associations. The objective of significantly inferring causal relationships
depends on prior knowledge, and very often some of the discovered associations
might be completely unexpected, not interpretable nor explainable. As pointed
by [1], the decisions taken considering the output of a model should be clearly
explainable to support their justifiability. These explanations should allow the
identification of potential flows both in a model, enhancing its transparency, the
knowledge discovery process, supporting its controllability and improvement of
its accuracy. Although the importance of explainability is clear, the definition of
objective criteria to evaluate methods for XAI and validate their explanations
is still lacking. Numerous notions underlying the effectiveness of explanations
were identified from the fields of Philosophy, Psychology and Cognitive Science.
These were related to the way humans define, generate, select, evaluate and
present explanations [50].
XAI: Concepts, Applications, Research Challenges and Visions 5

3 Applications and Impact Areas

Explainable artificial intelligence has produced many methods so far and it has
been applied in many domains, with different expected impacts [21]. In these
applications, the production of explanations for black box predictions requires
a companion method to extract or lift correlative structures from deep-learned
models into vocabularies appropriate for user level explanations. Initial activities
focused on deep learning image classification with explanations emerging as heat
maps created on the basis of gaps in probability distributions between a learned
model and an incorrect prediction [5]. However, the field has become so diverse
in methods, often determined by domain specific issues and attributes, that it
is scarcely possible to get in-depth knowledge on the whole of it. Additionally,
one major aspect though is the problem of explainable AI, where lot of problems
have been emerged and illustrated in the literature, especially from not being
able to provide explanations. While all of these topics require long and in-depth
discussions and are certainly of significant importance for the future of several
AI methods in many application domains, we want to focus on the benefits
that can be reaped from explainability. This means not focusing on the issues
of incomplete and imperfect technologies as a stopping point for applications,
but discussing novel solutions provided by explainable AI. A discussion of some,
partly prominent and partly surprising examples follows, with arguments on why
a certain amount of explainability - as a reflection - is required for more advanced
AI. There are many sectors that already have fully functional applications based
on machine learning, but still serious problems in applying them exist. These
are often caused by failing to be capable to explain how these methods work.
In other words, it is known that they work, but the concrete results cannot
be explained. Many of these applications either come from safety critical or
personally sensitive domains, thus a lot of attention is put on explanations of
the inferences of trained models, usually predictions or classifications.

Threat Detection and Triage - The detection of threats and efficient triage
have been core topics in the area of IT-Security for at least the past three
decades. This started with research in the area of code analysis and signature
based AntiVirus-Software, moving towards automated decompilation and code
analysis, as well as supporting the automated analysis of network monitoring
information for triage. Currently, fully automated threat detection and triage is
not available in real life systems due to the complexity of the task and the prob-
lem with false positives, even though several different approaches exist. These
also include strategies that do not try to detect actual threats, but rather filter-
ing out all known legit network travel and thus drastically reducing the amount
of information requiring manual analysis [56]. Still, a major problem without
explainability lies in the opaque nature of these methods, thus not being able
to fully understand their inner functioning and how an inference was reached.
Explainability could greatly enhance the detection capabilities, especially since
dynamic effects, such as changing user behavior, could be modelled and intro-
duced earlier into the algorithms without generating a large set of false positives.
6 L. Longo et al.

Explainable Object Detection - Object detection is usually performed from
a large portfolio of artificial neural networks (ANN) architectures such as YOLO,
trained on large amount of labelled data. In such contexts, explaining object
detections is rather difficult if not impossible due to the high complexity of the
hyperparameters (number of layers, filters, regularisers, optimizer, loss function)
of the most accurate ANNs. Therefore, explanations of an object detection task
are limited to features involved in the data and modeled in the form of saliency
maps [11] or at best to examples [40], or prototypes [35]. They are the state-
of-the-art approaches but explanations are limited by data frames feeding the
ANNs. Industrial applications embedding object detection, such as obstacles
detection for trains, do require human-like rational for ensuring the system can
be guaranteed, even certified [39].

Protection Against Adversarial ML - In adversarial machine learning,
attackers try to manipulate the results of learning algorithms by inserting specif
ically crafted data in the learning process [32], in order to lead a model to learn
erroneous things. Detection of such a manipulation is not trivial, especially
in contexts with big data, where no model exists before the analysis phase.
While there are several proposals on how to deal with this issue [16], some
of them employ neural sub-networks for differentiating between malicious and
benign input data like [49]. In this specific circumstance, explainability would
have a great impact as it will support the task of uncovering such a manip-
ulation far more quickly, efficiently and without actually finding the examples
that have been manipulated, thus greatly enhancing trust in machine learning
inferences [31].

Open Source Intelligence (OSINT) - In Open Source Intelligence [19], infor-
mation retrieval is purely reduced to openly available information, as contrary to
Signals Intelligence (SIGINT). However, there are several major issues surround-
ing OSINT, especially referring to context, languages and the amount of informa-
tion available. Similarly, another problem lies in deciding how much a source is
trusted, and what level of impact news of sources shall have on the result of their
aggregations. This is especially important when considering adversarial attacks
against OSINT methods and systems [12]. Explainability could provide means
for detecting these attacks, with an impact on mitigating their influence. Further-
more, the information that an attack against an intelligent system was launched
is also a valuable input from an intelligence perspective, so explainability might
lead to additional valuable information. However, not all false information exists
due to malice, especially when reporting very recent events: information parti-
cles might be wrong, misleading or simply unknown at the time of reporting.
OSINT becomes especially complex in case of ongoing events, where facts change
every minute, either due to knew intelligence, or simply because of changes in
the event itself. Explainability would allow to estimate the effects of incorrect
information particles on the overall machine learning outcomes, thus allowing,
for instance, to give error margins on reported numbers.
XAI: Concepts, Applications, Research Challenges and Visions 7

Trustworthy (autonomous) Medical Agents - Several architectures for
integrating machine learning into medical decision making have been devised
in the past. These are based upon a doctor-in-the-loop approach whereby doc-
tors act as input providers to machine learning algorithms. These can lead to
suggestions related to diagnosis or treatment that can be subsequently reviewed
by the doctors themselves, who, in turn, can provide feedback in a loop to fur-
ther enhance modeling [34]. Additionally, the mechanism can also introduces
external knowledge to support decision making aimed at incorporating the latest
findings in the underlying medical field.

Autonomous Vehicles - While certainly being developed within machine
learning, explainability would be beneficial for the area of autonomous vehicles,
especially considering autonomous cars. In cases of car accidents, explanations
can help trace the reasons why an autonomous vehicle behaved in a certain why
and took certain actions. Consequently this can not only lead to safer vehicles,
but it also can help solve issues in court faster, greatly enhancing trust towards
these novel ML-based technologies and especially the resulting artifacts [20].

4 Research Challenges

A number of research challenges surrounding the development of methods for
explainability exist, including technical, legal and practical challenges.

4.1 Technical Challenges

XAI Systems Evaluation. The comprehensive study of what explanation
means from a sociological viewpoint [50] begs a difficult issue that is both tech-
nical and non-technical: how does one evaluate the quality of an explanation? It is
not a surprise that the quality or value of an explanation is at least partly deter-
mined by the receiver of an explanation, sometimes referred to as the “explainee”.
An easy way to frame the challenge of evaluating explanations, with respect to
an explainee, arises from observing the history of the development of evaluation
techniques from the field of data visualization [36]. A simple example of “visual
explanation” can frame the general evaluation problem for all explanations as
follows. Consider the IKEA assembly diagram, rendered in Fig. 1. A simple list of
requirements to assess explanation quality emerges from considering the IKEA
assembly instructions as a visual explanation of how to assemble the piece of
furniture. In this case, the visual explanation is intended to guide all explainees,
and not just a single individual, to the successful assembly of the furniture item.
One measure of quality is simply to test whether any individual explainee can
use the visual explanation to complete the assembly. Another measure is about
whether the visual explanation is clear and unambiguous, so that the assembly
is time efficient. In the case of Fig. 1, the sequencing of steps might be misinter-
preted by an explainee, and that the simple use of circular arrows to indicate
motion may also be ambiguous.
8 L. Longo et al.

 

Fig. 1. An IKEA visual explanation for furniture assembly

Overall, and as anticipated in the general evaluation of explanation sys-
tems, one could design cognitive experiments to determine, over an experimental
human cohort, which portions of the explanation clearly lead to correct infer-
ences, and those which are more difficult to correctly understand. This means
that XAI system requirements should include the need to produce an explicit rep-
resentation of all the components in a way that supports the appropriate inter-
pretation of the visual classification of components. One can generalize visual
explanations to the full repertoire that might obtain for a general XAI system.
This means a set of representations of the semantics of an underlying domain of
application that can provide support to construct an explanation that is under-
stood by a human explainee.

XAI Interpretation. Even though XAI systems are supposed to expose the
functioning of a learning technique as well as a set of justification of a model’s
inferences, it remains rather difficult for a human to interpret them. Explanations
are not the final words of an intelligent system but rather the intermediate layer
that requires knowledge expertise, context and common-sense characterization
for appropriate and correct human interpretation and decision-making [44,64].
Semantics, knowledge graphs [38] and their machine learning representations
[6] or similar technical advancements are interesting avenues to be considered
for pushing the interpretation at the next right level of knowledge expertise.
These might also include the addition of argumentative capabilities, as applied
in [45,61] to produce rational and justifiable explanations [62].

4.2 Legal Challenges

While the theoretical ground work in AI stays on the very theoretical side and
is thus typically considered to be not problematic from a legal point of view,
the actual application of XAI methods in a certain domain can have serious
legal implications. This is especially important when considering working with
sensitive information. Here, it has yet to be researched whether the explainabil-
ity related to a model might be used to infer information about individuals, for
instance, by using it with slightly different data sets. This technique has been
used in many variations in IT-Security, especially considering anonymized data
XAI: Concepts, Applications, Research Challenges and Visions 9

sets or partially released sensitive information as a basis to gather more intelli-
gence on the people involved [9]. Similar attacks have already been proposed and
carried out against machine learned models [65] and these allowed to produce a
great amount of information, hidden correlations and causalities that were used
to infer sensitive information.

Concepts like federated machine learning are built on the notion of executing
machine learning algorithms locally on sensitive data sets and then exchanging
the resulting feature sets in order to be combined centrally. These are in contrast
to more traditional approaches that collect all sensitive data centrally and then
run the learning algorithms. One challenge for federated machine learning is to
achieve model robustness but greatly focus on protective sensitive inferences.
This justifies the need for more applicable anonymisation techniques, as many
of the current methods are unsuitable for many application scenarios, either due
to performance or quality issues [47,48]. In addition, other legal challenges exist
such as the right to be forgotten [67]. This ‘reflects the claim of an individual
to have certain data deleted so that third persons can no longer trace them’.
This fair right is accompanied by technical difficulties ranging from the issue
related to the deletion of entries in modern systems, to the problem of inferring
information on individuals from aggregates and especially the removal of said
individuals from the aggregation process.

Despite the aforementioned challenges, positive benefits can be brought by
explainability to the area of machine learning and AI as a whole with respect to
legal issues. While the issue of transparency, a key requirement in the General
Data Protection Regulation (GDPR), can be a rather a hard issue to tackle, this
could change with explainability providing detailed insight, where, when and to
what extent personal data of a single individual was involved in a data analysis
workflow [69]. While this is currently not a binding requirement to provide that
level of details [69], this could be a game changer regarding acceptance of AI,
as well as increasing privacy protection in a data driven society. Furthermore, a
significant problem currently tackled in machine learning is bias [71], especially
since simple methods for tackling the issue have shown to be ineffective [33].
Explainability could support this combat and thus provide a better legal standing
for the results derived from data driven systems, especially when used for socio-
economic purposes.

4.3 Practical Challenges

One of the most crucial success factors of AI generally and XAI specifically, is
to ensure effective human-AlI interfaces to enable a usable and useful interac-
tion between humans and AI [2]. Such goals have been discussed in the HCI
community for decades [10], but it was not really seen as important in the Al
community. Now the needs and demands of XAJI for ‘explainable user interfaces’
may finally stimulate to realise advanced human-centered concepts similar to the
early visions of Vannevar Bush in 1945 [8]. Here, the goal is to explore both the
explainability side, that means the artificial explanation generated by machines,
as well as the human side, that means the human understanding. In an ideal
10 L. Longo et al.

world, both machine explanations and human understanding would be identical,
and congruent with the ground truth, which is defined for both machines and
humans equally. However, in the real world we face two significant problems:

— the ground truth cannot always be fully defined, as for instance when con-
cerned with medical diagnoses [57| when there is high uncertainty;

— human models such as scientific, world, problem solving models, are often
based on causality, in the sense of Judea Pearl [55], which is very challenging
as current machine learning does not incorporate them and simply follows
pure correlation.

Practically speaking, current XAI methods mainly focus on highlighting input—
relevant parts, for example via heat-mapping, that significantly contributed to
a certain output, or the most relevant features of a training data set that influ-
enced the most the model accuracy. Unfortunately, they do not incorporate the
notion of human model, and therefore there is a need to take also into account
the concept of causability [30]. In detail, in line with the concept of usability [28],
causability is defined as ‘the extent to which an explanation of a statement to a
human expert achieves a specified level of causal understanding with effective-
ness, efficiency and satisfaction in a specified context of use and the particular
contextual understanding capabilities of a human’. Following this concept, it
becomes possible to measure the quality of explanations in the same terms as
usability (effectiveness, efficiency and satisfaction in a specified context of use),
for example with a measurement scale [29].

5 Recommendations and Visions

Machine learning, as a solid research area within artificial intelligence, has
undoubtedly impacted the field by providing scholars with a robust suite of
methods for modeling complex, non-linear phenomena. With the growing body of
work in the last decade on deep learning, this impact has significantly expanded
to many applications areas. However, despite the widely acknowledged capability
of machine and deep learning to allow scholars to induce accurate models from
data and extract relevant patterns, accelerating scientific discovery [18], there
is the problem of their interpretability and explainability. For this reason, the
last few years have seen a growing body of work on research in methods aimed
at explaining the inner functioning of data-driven models and the learning tech-
niques used to induce them. Currently and generally recognised as a core area of
AI, eXplainable Artificial Intelligence (XAI) has produced a plethora of methods
for model interpretability and explainability. Hundred of scientific articles are
published each month in many workshops, conferences and presented at sympo-
sium around the world. Some of them focus on wrapping trained models with
explanatory layers, such as knowledge graphs [39]. Other try to embed the con-
cept of explainability during training, and some of them try to merge learning
capabilities with symbolic reasoning [43]. Explainability is a concept borrowed
XAI: Concepts, Applications, Research Challenges and Visions 11

from psychology, since it is strictly connected to humans, that is difficult to oper-
ationalise. A precise formalisation of the construct of explainability is far from
being a trivial task as multiple attributes can participate in its definition [61].
Similarly, the attributes might interact with each other, adding complexity in
the definition of an objective measure of explainability [42,63]. For these reasons,
the last few years have seen also a growing body of research on approaches for
evaluating XAI methods. In other words, approaches that are more focused on
the explanations generated by XAI solutions, their structure, efficiency, efficacy
and impact on humans understanding.

The first recommendation to scholars willing to perform scientific research
on explainable artificial intelligence and create XAI methods is to firstly focus
on the structure of explanations, the attributes of explainability and the way
they can influence humans. This links computer science with psychology. The
second recommendation is to define the context of explanations, taking into
consideration the underlying domain of application, who they will serve and
how. Ultimately, explanations are effective when they help end-users to build a
complete and correct mental representation of the inferential process of a given
data-driven model. Work on this direction should also focus on which type of
explanation can be provided to end-users, including textual, visual, numerical,
rules-based or mixed solutions. This links computer science with the behavioural
and social sciences. The third recommendation is to clearly define the scope of
explanations. This might involve the creation of a method that provide end-users
with a suite of local explanations for each input instance or the formation of a
method that focuses more on generating explanations on a global level aimed
at understanding a model as a whole. This links computer science to statistics
and mathematics. The final recommendation is to involve humans, as ultimate
users of XAT methods, within the loop of model creation, exploitation, as well as
the enhancement of its interpretability and explainability. This can include the
development of interactive interfaces that allow end-users to navigate through
models, understanding their inner logic at a local or global level, for existing
or new input instances. This links artificial intelligence with human-computer
interaction.

The visions behind explainable artificial intelligence are certainly numerous.
Probably the most important is the creation of models with high accuracy as well
as high explainability. The trade-off between these two sides is well known, and
usually, increments in one dimension means decrements in the other dimension.
Creating interpretable and explainable models that are also highly accurate is
the ideal scenario, but since this has been demonstrated to be a hard problem
with currents methods of learning and explainability, further research is needed.
One possible solution is the creation of models that are fully transparent at all
stages of model formation, exploitation and exploration and that are capable
of providing local and global explanations. This leads to another vision, which
is the use of methods that embed learning capabilities and symbolic reasoning.
The former is aimed at generating models and representations with high accu-
racy for predictive and forecasting purposes, while the latter to explain these
12

L. Longo et al.

representations in highly interpretable natural language terms, aligned to the
way human understand and reason.

Acknowledgements. R.Goebel would like to acknowledge the support of the Alberta
Machine Intelligence Institute, which is one of the three Pan Canadian AI Centres.
A.Holzinger would like to acknowledge the support of the Austrian Science Fund
(FWF), Project: P-32554 “Explainable AI - A reference model of explainable Arti-
ficial Intelligence for the Medical Domain”.

References

10.

11.

12.

13.

14.

15.

16.

. Adadi, A., Berrada, M.: Peeking inside the black-box: a survey on explainable

artificial intelligence (XAI). IEEE Access 6, 52138-52160 (2018)

. Amershi, S., et al.: Guidelines for human-AI interaction. In: Proceedings of the

2019 CHI Conference on Human Factors in Computing Systems. ACM (2019)

. Arras, L., Osman, A., Miller, K.R., Samek, W.: Evaluating recurrent neural net-

work explanations. In: Proceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP, Florence, Italy, pp. 113-
126. Association for Computational Linguistics (2019)

. Atakishiyev, S., et al.: A multi-component framework for the analysis and design

of explainable artificial intelligence. (arXiv:2005.01908v1 [cs.AI]) (2020)

. Babiker, H.K.B., Goebel, R.: An introduction to deep visual explanation. In: NIPS

2017 - Workshop Interpreting, Explaining and Visualizing Deep Learning (2017)

. Bianchi, F., Rossiello, G., Costabello, L., Palmonari, M., Minervini, P.: Knowledge

graph embeddings and explainable AI. CoRR, abs/2004.14843 (2020)

. Biran, O., Cotton, C.: Explanation and justification in machine learning: a survey.

In: IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI), Melbourne,
Australia, pp. 8-13. International Joint Conferences on Artificial Intelligence Inc.
(2017)

. Bush, V.: As we may think. Atl. Mon. 176(1), 101-108 (1945)
. Cai, Z., He, Z., Guan, X., Li, Y.: Collective data-sanitization for preventing sensi-

tive information inference attacks in social networks. IEEE Trans. Depend. Secure
Comput. 15(4), 577-590 (2016)

Card, S.K., Moran, T.P., Newell, A.: Psychol. Hum. Comput. Interact. Erlbaum,
Hillsdale (NJ) (1983)

Chang, C.-H., Creager, E., Goldenberg, A., Duvenaud, D.: Interpreting neural
network classifications with variational dropout saliency maps. Proc. NIPS 1(2),
1-9 (2017)

Devine, S.M., Bastian, N.D.: Intelligent systems design for malware classification
under adversarial conditions. arXiv preprint, arXiv:1907.03149 (2019)

Dzindolet, M.T., Peterson, $.A., Pomranky, R.A., Pierce, L.G., Beck, H.P.: The
role of trust in automation reliance. Int. J. hum. Comput. Stud. 58(6), 697-718
(2003)

Evans, R., Greffenstette, E.: Learning explanatory rules from noisy data. J. Artif.
Intell. Res. 61, 1-64 (2018)

Falcon, A.: Aristotle on causality. Stanford Encyclopedia of Philosophy (2006).
(https: //plato.stanford.edu

Feinman, R., Curtin, R.R., Shintre, $., Gardner, A.B.: Detecting adversarial sam-
ples from artifacts. arXiv preprint, arXiv:1703.00410 (2017)
17.

18.

19.

20.

21.

22.

23.

24.

25.

26.
27.

28.

29.

30.

31.

32.

33.

34.

XAI: Concepts, Applications, Research Challenges and Visions 13

Fox, M., Long, D., Magazzeni, D.: Explainable planning. In: IJCAI 2017 Work-
shop on Explainable Artificial Intelligence (XAI), Melbourne, Australia, pp. 24-30.
International Joint Conferences on Artificial Intelligence Inc (2017)

Gil, Y., Greaves, M., Hendler, J., Hirsh, H.: Amplify scientific discovery with arti-
ficial intelligence. Science 346(6206), 171-172 (2014)

Glassman, M., Kang, M.J.: Intelligence in the internet age: the emergence and
evolution of open source intelligence (OSINT). Comput. Hum. Behav. 28(2), 673-
682 (2012)

Glomsrud, J.A., @degardstuen, A., Clair, A.L.S., Smogeli, @.: Trustworthy versus
explainable AI in autonomous vessels. In: Proceedings of the International Seminar
on Safety and Security of Autonomous Vessels (ISSAV) and European STAMP
Workshop and Conference (ESWC) 2019, pp. 37-47. Sciendo (2020)

Goebal, R., et al.: Explainable AI: the new 42? In: Holzinger, A., Kieseberg, P.,
Tjoa, A.M., Weippl, E. (eds.) CD-MAKE 2018. LNCS, vol. 11015, pp. 295-303.
Springer, Cham (2018). https: //doi.org/10.1007/978-3-319-99740-7_21

Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., Pedreschi, D.: A
survey of methods for explaining black box models. ACM Comput. Surv. (CSUR)
51(5), 93:1-93:42 (2018)

Ha, T., Lee, $., Kim, S.: Designing explainability of an artificial intelligence sys-
tem. In: Proceedings of the Technology, Mind, and Society, p. 1, article no. 14,
Washington, District of Columbia, USA. ACM (2018)

Hempel, C.G.: The function of general laws in history. J. Philos. 39(2), 35-48
(1942)

Hempel, C.G.: The theoretician’s dilemma: a study in the logic of theory construc-
tion. Minnesota Stud. Philos. Sci. 2, 173-226 (1958)

Hempel, C.G.: Aspects of Scientific Explanation. Free Press, New York (1965)
Hempel, C.G., Oppenheim, P.: Studies in the logic of explanation. Philos. Sci.
15(2), 135-175 (1948)

Holzinger, A.: Usability engineering methods for software developers. Commun.
ACM 48(1), 71-74 (2005)

Holzinger, A., Carrington, A., Miiller, H.: Measuring the quality of explanations:
the System Causability Scale (SCS). KI - Kiinstliche Intelligenz 34(2), 193-198
(2020). https: //doi.org/10.1007/s13218-020-00636-z

Holzinger, A., Langs, G., Denk, H., Zatloukal, K., Mueller, H.: Causability and
explainability of artificial intelligence in medicine. Wiley Interdiscip. Rev. Data
Min. Knowl. Discov. 9(4), e1312 (2019)

Holzinger, K., Mak, K., Kieseberg, P., Holzinger, A.: Can we trust machine learning
results? Artificial intelligence in safety-critical decision support. ERCIM NEWS
112, 42-43 (2018)

Huang, L., Joseph, A.D., Nelson, B., Rubinstein, B.I., Tygar, J.D.: Adversarial
machine learning. In: Proceedings of the 4th ACM workshop on Security and arti-
ficial intelligence, pp. 43-58 (2011)

Kamishima, T., Akaho, S., Asoh, H., Sakuma, J.: Fairness-aware classifier with
prejudice remover regularizer. In: Flach, P.A., De Bie, T., Cristianini, N. (eds.)
ECML PKDD 2012. LNCS (LNAD), vol. 7524, pp. 35-50. Springer, Heidelberg
(2012). https: / /doi.org/10.1007/978-3-642-33486-3_3

Kieseberg, P., Malle, B., Friihwirt, P., Weippl, E., Holzinger, A.: A tamper-proof
audit and control system for the doctor in the loop. Brain Inform. 3(4), 269-279
(2016). https: //doi.org/10.1007/s40708-016-0046-2
35.

36.

37.

38.

39.

40.

41.

42.

43.

44.

45.

46.

47.

L. Longo et al.

Kim, B., Koyejo, O., Khanna, R.: Examples are not enough, learn to criticize! Crit-
icism for interpretability. In: Advances in Neural Information Processing Systems
29: Annual Conference on Neural Information Processing Systems 2016, Barcelona,
Spain, 5-10 December, pp. 2280-2288 (2016)

Lam, H., Bertini, E., Isenberg, P., Plaisant, C., Carpendale, S.: Empirical studies
in information visualization: seven scenarios. IEEE Trans. Graph. Vis. Comput.
18(9), 1520-1536 (2012)

Laugel, T., Lesot, M.J., Marsala, C., Renard, X., Detyniecki, M.: The dangers
of post-hoc interpretability: unjustified counterfactual explanations. In: Proceed-
ings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
(IJCAD, Macao, China, pp. 2801-2807 2019. International Joint Conferences on
Artificial Intelligence Organization (2019)

Lécué, F.: On the role of knowledge graphs in explainable AI. Semant. Web 11(1),
41-51 (2020)

Lécué, F., Pommellet, T.: Feeding machine learning with knowledge graphs for
explainable object detection. In: Sudrez-Figueroa, M.C., Cheng, G., Gentile, A.L.,
Guéret, C., Keet, C.M., Bernstein, A., (eds.) Proceedings of the ISWC 2019 Satel-
lite Tracks (Posters & Demonstrations, Industry, and Outrageous Ideas) co-located
with 18th International Semantic Web Conference (ISWC 2019), 26-30 October
2019, Auckland, New Zealand, volume 2456 of CEUR Workshop Proceedings, pp.
277-280. CEUR-WS.org (2019)

Li, O., Liu, H., Chen, C., Rudin, C.: Deep learning for case-based reasoning through
prototypes: a neural network that explains its predictions. In: Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), 2-7 Febru-
ary 2018, New Orleans, Louisiana, USA, pp. 3530-3537 (2018)

Lipton, Z.C.: The mythos of model interpretability. Commun. ACM 61(10), 36-43
(2018)

Longo, L.: Argumentation for knowledge representation, conflict resolution, defea-
sible inference and its integration with machine learning. In: Holzinger, A. (ed.)
Machine Learning for Health Informatics. LNCS (LNAD), vol. 9605, pp. 183-208.
Springer, Cham (2016). https: //doi.org/10.1007/978-3-319-50478-0_9

Longo, L., Dondio, P.: Defeasible reasoning and argument-based systems in medi-
cal fields: an informal overview. In: 2014 IEEE 27th International Symposium on
Computer-Based Medical Systems, pp. 376-381. IEEE (2014)

Longo, L., Hederman, L.: Argumentation theory for decision support in health-
care: a comparison with machine learning. In: Imamura, K., Usui, S., Shirao, T.,
Kasamatsu, T., Schwabe, L., Zhong, N. (eds.) BHI 2013. LNCS (LNAD), vol. 8211,
pp. 168-180. Springer, Cham (2013). https: //doi.org/10.1007/978-3-319-02753-
117

Longo, L., Kane, B., Hederman, L.: Argumentation theory in health care. In: 2012
25th International Symposium on Computer-Based Medical Systems (CBMS), pp.
1-6. IEEE (2012)

Makridakis, 5.: The forthcoming artificial intelligence (AI) revolution: its impact
on society and firms. Futures 90, 46-60 (2017)

Malle, B., Kieseberg, P., Holzinger, A.: Do not disturb? Classifier behav-
jor on perturbed datasets. In: Holzinger, A., Kieseberg, P., Tjoa, A.M.,
Weippl, E. (eds.) CD-MAKE 2017. LNCS, vol. 10410, pp. 155-173.
Springer, Cham (2017).  https://doi.org/10.1007/978-3-319-66808-6_11
48.

49.

50.

51.

52.

53.

54.

55.

56.

57.

58.

59.

60.

61.

62.

63.

XAI: Concepts, Applications, Research Challenges and Visions 15

Malle, B., Kieseberg, P., Weippl, E., Holzinger, A.: The right to be forgot-
ten: towards machine learning on perturbed knowledge bases. In: Buccafurri, F.,
Holzinger, A., Kieseberg, P., Tjoa, A.M., Weippl, E. (eds.) CD-ARES 2016. LNCS,
vol. 9817, pp. 251-266. Springer, Cham (2016). https: //doi.org/10.1007/978-3-319-
45507-5_17

Metzen, J.H., Genewein, T., Fischer, V., Bischoff, B.: On detecting adversarial
perturbations. arXiv preprint, arXiv:1702.04267 (2017)

Miller, T.: Explanation in artificial intelligence: insights from the social sciences.
Artif. Intell. 267, 1-38 (2019)

Miller, T., Howe, P., Sonenberg, L.: Explainable AI: beware of inmates running
the asylum or: how i learnt to stop worrying and love the social and behavioural
sciences. In: IJCAI Workshop on Explainable AI (XAI), Melbourne, Australia, pp.
36-42. International Joint Conferences on Artificial Intelligence Inc. (2017)
Muggleton, S.: Inductive logic programming. New Generat. Comput. 8(4), 295-318
1991

Vinee I, Jannach, D.: A systematic review and taxonomy of explanations in
decision support and recommender systems. User Model. User Adap. Interact.
27(3), 393-444 (2017). https: //doi.org/10.1007/s11257-017-9195-0

Paez, A.: The pragmatic turn in explainable artificial intelligence (XAI). Mind.
Mach. 29, 1-19 (2019)

Pearl, J.: Causality: Models, Reasoning, and Inference, 2nd edn. Cambridge Uni-
versity Press, Cambridge (2009)

Pirker, M., Kochberger, P., Schwandter, S.: Behavioural comparison of systems
for anomaly detection. In: Proceedings of the 13th International Conference on
Availability, Reliability and Security, pp. 1-10 (2018)

Pohn, B., Kargl, M., Reihs, R., Holzinger, A., Zatloukal, k., Miler, H.: Towards
a deeper understanding of how a pathologist makes a diagnosis: visualization of
the diagnostic process in histopathology. In: IEEE Symposium on Computers and
Communications (ISCC 2019). IEEE (2019)

Poole, D., Goebel, R., Aleliunas, R.: Theorist: A logical reasoning system for
defaults and diagnosis. The Knowledge Frontier. Symbolic Computation (Artifi-
cial Intelligence), pp. 331-352 (1987). https: //doi.org/10.1007/978- 1-4612-4792-
0-13

Pople, H.: On the mechanization of abductive logic. In: IJCAI’1973: Proceedings
of the 3rd International Joint Conference on Artificial Intelligence, pp. 147-152.
Morgan Kaufmann Publishers (1973)

Preece, A.: Asking ”why” in AI: explainability of intelligent systems-perspectives
and challenges. Intell. Syst. Account. Financ. Manage. 25(2), 63-72 (2018)
Rizzo, L., Longo, L.: Inferential models of mental workload with defeasible argu-
mentation and non-monotonic fuzzy reasoning: a comparative study. In: Proceed-
ings of the 2nd Workshop on Advances in Argumentation in Artificial Intelligence,
co-located with XVII International Conference of the Italian Association for Artifi-
cial Intelligence, AP @AI*IA 2018, 20-23 November 2018, Trento, Italy, pp. 11-26
2018

tooo. L., Longo, L.: A qualitative investigation of the explainability of defeasible
argumentation and non-monotonic fuzzy reasoning. In: Proceedings for the 26th
AIAIT Irish Conference on Artificial Intelligence and Cognitive Science Trinity Col-
lege Dublin, 6-7 December 2018, Dublin, Ireland, pp. 138-149 (2018)

Rizzo, L., Longo, L.: An empirical evaluation of the inferential capacity of defeasible
argumentation, non-monotonic fuzzy reasoning and expert systems. Expert Syst.
Appl. 147, 113220 (2020)
64.

65.

66.

67.

68.

69.

70.

71.

72.

L. Longo et al.

Rizzo, L., Majnaric, L., Longo, L.: A comparative study of defeasible argumen-
tation and non-monotonic fuzzy reasoning for elderly survival prediction using
biomarkers. In: Ghidini, C., Magnini, B., Passerini, A., Traverso, P. (eds.) AI*IA
2018. LNCS (LNAI), vol. 11298, pp. 197-209. Springer, Cham (2018). https: //doi.
org/10.1007/978-3-030-03840-3_15

Shokri, R., Stronati, M., Song, C., Shmatikov, V.: Membership inference attacks
against machine learning models. In: 2017 IEEE Symposium on Security and Pri-
vacy (SP), pp. 3-18. IEEE (2017)

Tintarev, N., Masthoff, J.: A survey of explanations in recommender systems. In:
IEEE 23rd international conference on data engineering workshop, pp. 801-810,
Istanbul, ‘Turkey. IEEE (2007)

Villaronga, E.F., Kieseberg, P., Li, T.: Humans forget, machines remember: arti-
ficial intelligence and the right to be forgotten. Comput. Law Secur. Rev. 34(2),
304-313 (2018)

Vilone, G., Longo, L.: Explainable artificial intelligence: a systematic review.
CoRR, abs/2006.00093 (2020)

Wachter, S., Mittelstadt, B., Floridi, L.: Transparent, explainable, and accountable
AI for robotics. Sci. Robot. 2(6) (2017)

Woodward, J.: Scientific explanation. Stanford Encyclopedia of Philosophy (2003).
(https: //plato.stanford.edu

Yapo, A., Weiss, J.: Ethical implications of bias in machine learning. In: HICCS
2018, Proceedings of the 51st Hawaii International Conference on System Sciences
(2018)

Zhang, Q., Zhu, 8.: Visual interpretability for deep learning: a survey. Front.
Inform. Technol. Electron. Eng. 19(1), 27-39 (2018). https://doi.org/10.1631/
FITEE.1700808
 

European
Commission

 

JRC TECHNICAL REPORT

Robustness and Explainability
of Artificial Intelligence

From technical to
policy solutions

Hamon, Ronan
Junklewitz, Henrik

Sanchez, Ignacio

2020

: real
attack society task approaches

useresearch algorithm

complex new makingaspect hand
e@Xamp 1 €personal

human pl p mechanism

various application technical
set d ] OO vulnerabilities echnica
« O regulation
coprRk NOQE C22 Cight context A5eSSTEnk,
automatedimpact ° vu
current interpretability Cc gs a machine ‘user
7 7 . Ls 6 information per ormance 4
explainabilityg.=¢ t ears
3
scientific subject o Ww a S SyS em training
technique os maficious reliabilityrecss
European provide © & science explanation niGve el

transparency "rQbUStNeSS jccupolicy

ed 2 t a thrs at decisi . on.

. practice (£2,402 cas
input

cybersecurity image
citizen

wlohe

fa -f6) caf) EUR 30040 EN
@igtr 4

 
This publication is a Technical report by the Joint Research Centre (JRC), the European Commission's science and knowledge service. It
aims to provide evidence-based scientific support to the European policymaking process. The scientific output expressed does not imply a
policy position of the European Commission. Neither the European Commission nor any person acting on behalf of the Commission is
responsible for the use that might be made of this publication. For information on the methodology and quality underlying the data used
in this publication for which the source is neither Eurostat nor other Commission services, users should contact the referenced source. The
designations employed and the presentation of material on the maps do not imply the expression of any opinion whatsoever on the part
of the European Union concerning the legal status of any country, territory, city or area or of its authorities, or concerning the delimitation
of its frontiers or boundaries.

EU Science Hub
https://ec.europa.eu/jre

JIRC119336
EUR 30040 EN

PDF ISBN 978-92-765-14660-5 ISSN 1831-9424 doi:10.2760/57493

© European Union, 2020

®

=D

The reuse policy of the Eurapean Commission is implemented by the Commission Decision 2011/833/EU of 12 December 2011 on the
reuse of Commission documents (OJ L 330, 14.12.2011, p. 39). Except otherwise noted, the reuse of this document is authorised under
the Creative Commons Attribution 4.0 International (CC BY 4.0) licence (https://creativecommons org/licenses/by/4.0/}. This means that
reuse is allowed provided appropriate credit is given and any changes are indicated. For any use or reproduction of photos or other
material that is not owned by the EU, permission must be sought directly from the copyright holders.

 

All content © European, 2020

How to cite this report: Hamon, R., Junklewitz, H., Sanchez, |. Robustness and Explainability of Artificial Intelligence - From technical to
policy solutions, EUR 30040, Publications Office of the European Union, Luxembourg, Luxembourg, 2020, ISBN 978-92-79-14660-5
(online), doi:10.2760/57493 (online), JRC119336.
Contents

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Executive summary 1
Abstract 3
1 Introduction 4
2 Background: Perspectives from Society and Policy making 6
2.1 Policy initiatives 6
2.2. Data governance and decision-making 7
2.3. From Data Governance to Al Governance 9

3. Artificial Intelligence and its Robustness and Explainability: Overview and limitations ...msnsssussuenenneenenees LO
3.1 Artificial intelligence and Machine Learning: A short overview 10
3.1.1 Machine learning 10

3.1.2 — Artificial General Intelligence ll

3.2. Transparency of Al systems 11
3.2.1 Documentation of specifications 12

3.2.2 Interpretability and understandability. 12

3.2.3. Aspects of interpretability. 12

3.2.4 — Interpretable models vs. post-hoc interpretability 13

3.2.5 Interpretability vs. accuracy 14

3.3 Reliability of Al systems 14
3.3.1 Evaluation of performances 15

3.3.2 Vulnerabilities of machine learning 15

Data poisoning 16

Crafting of adversarial examples 17

Model flaws 17

3.3.3. Approaches to increase the reliability of machine learning MOeLS ..u. we nsmmmnenunemeemennnennemeeel 7

Data sanitization 18

Robust learning 18

Extensive testing 18

Formal verification 19

3.4 Protection of data in Al systems 19
3.4.1 Threats against data 20

3.4.2 Differential privacy 20

3.4.3 Distributed and federated learning 20

3.4.4 — Training over encrypted data 21

4 From technical to policy solutions 22
4.1 Certification of the robustness of Al systems 22
4.1.1 Impact assessments of Al systems 22

 

41.2 Testing 23
42 Standardization

 

4.2.1 Known vulnerabilities

 

 

4.2.2 Systematic transparency
4.2.3 Understandable explanation

5 Conclusion

 

 

References

 

List of boxes
List of figures
List of tables

23
23
24
24
25
26
32
33
34
Executive summary

In the light of the recent advances in artificial intelligence (Al), the challenges posed by its use in an ever-
increasing number of areas have serious implications for EU citizens and organisations. The consequences will
gradually become a highly debated topic in our society: Al is starting to play a crucial part in systems for
decision-making and autonomous processes, with potential consequences for our lives. A major concern
comes from the various and serious vulnerabilities that affect Al techniques. These vulnerabilities could
strongly impact the robustness of current systems, leading them into uncontrolled behaviour, and allowing
potential adversaries to deceive algorithms to their own advantages.

In addition to that, Al is also becoming a key technology in automated decision-making systems based on
personal data, which may potentially have significant impacts on the fundamental rights of individuals. In this
situation, the General Data Protection Regulation (GDPR), applicable since 2018, has introduced a set of rights
that relate to the explainability of Al and, in particular, that provide any individual subject to such a decision
the possibility to ask for an explanation. The recognized opaqueness of the latest generation of Al systems
raises the issue of how to ensure that EU citizens are able to assert this right in contexts where a decision
taken by an Al-powered system has a negative impact on their life.

The scientific community took the measure of these concerns at an early stage, and started to provide
technical solutions to increase both the robustness and the explainability of Al systems. Despite these efforts,
there is still a gap between the resulting scientific outcomes of research and the legitimate expectations the
society may have on this novel technology. Even if Al systems currently stay under human supervision in
relatively controlled environment, Al is expected to be deployed on a much larger scale in the next years, and
this situation calls for an appropriate answer from regulatory bodies.

To this end, the European Commission has committed itself to set up the principles of a trustworthy and
secure use of Al in the digital society. Built on the multiple initiatives linked to the cybersecurity of digital
systems, in particular the Cybersecurity Act, proposed in 2017, that introduced an EU-wide cybersecurity
certification framework for digital products, services and processes, an ecosystem around Al technologies is
rapidly emerging to favour innovation while protecting fundamental rights. The new Commission which took
office in December 2019 is also determined to foster and regulate Al at the EU level, based on the policy
activities that have been implemented since 2018.

This Technical Report by the European Commission Joint Research Centre (JRC) aims to contribute to this
movement for the establishment of a sound regulatory framework for Al, by making the connection between
the principles embodied in these policy activities and the ongoing technical discussions within the scientific
community. The individual objectives of this report are:

— to provide a policy-oriented description of the current perspectives of Al and its implications in society;

— to provide an objective view on the current landscape of Al, focusing on the aspects of robustness and
explainability. This include a technical discussion of the current risks associated with Al in terms of
cybersecurity, safety, and data protection;

— to present the scientific solutions that are currently under active development in the Al community to
mitigate these risks;

— to put forward a list of recommendations presenting policy-related considerations for the attention of the
policy makers to establish a set of standardisation and certification tools for Al systems.

From a technical perspective, Al today is dominated by machine learning techniques, which encompass
various mathematical methods to extract and exploit relevant information from large collection of data. This
aspect is what makes both the strength and weakness of automatic decision-making systems: a machine
learning model is able to automatically learn complex structures from data, however the learned patterns are
limited in its understanding of the world, by the lack of explicit rules or logical mechanisms. Machine learning
algorithms are powerful to extract correlations between all sorts of complex data, but there is no strong
guarantee that these correlations are meaningful and correspond to actual causal relationships. Furthermore,
the complexity of models, in particular in the state-of-the-art deep learning techniques, often prevents their
inspection and control by human operators. In this way, Al is certainly a vector of innovation, but also a source
of major challenges with respect to cybersecurity, safety, and explainability.
Three important topics, deemed as essential for a right deployment of Al in the society in relation to these
issues, are highlighted and discussed with regard to the current technical solutions existing in the scientific
literature:

1. Transparency of models: it relates to the documentation of the Al processing chain, including the
technical principles of the model, and the description of the data used for the conception of the
model. This also encompasses elements that provide a good understanding of the model, and related
to the interpretability and explainability of models;

2. Reliability of models: it concerns the capacity of the models to avoid failures or malfunction, either
because of edge cases or because of malicious intentions. The main vulnerabilities of Al models have
to be identified, and technical solutions have to be implemented to make sure that autonomous
systems will not fail or be manipulated by an adversary;

3. Protection of data in models: The security of data used in Al models needs to be preserved. In the
case of sensitive data, for instance personal data, the risks should be managed by the application of
proper organisational and technical controls.

Several points emerged from this analysis. First, it is important to note that these topics are not mutually
exclusive but on the contrary complementary. Notably, even if the explainability of models is a key point for
their transparency, it is also a concept of paramount importance to assess the reliability of a model and its
exposure to failures. Second, it has to be noted that the fast progress of Al and its current industrialization
should not overshadow the lack of efficient measures to mitigate the risks associated to Al, these questions
being still open problems in the scientific community. Lastly, these topics only address some aspects of the
key requirements generally established for a trustworthy and secure use of Al, they are however of prime
importance and directly connected to other ethical concerns, such as the fairness or the accountability of Al
systems.

The conclusion of this report is that it is essential to take into consideration the current technological
advances of Al for the establishment of a regulatory framework that will promote a responsible use of the Al
technology, while preserving innovation. As of now, several avenues for reflection could be considered to
undertake the implementation of standards in Al technologies, as well as of security and reliability
certifications of Al components embedded in real-world products and services. These avenues include:

1. developing a methodology to evaluate the impacts of Al systems on society built on the model
of the Data Protection Impact Assessments (DPIA) introduced in the GDPR, that would provide an
assessment of the risks in the usage of Al techniques to the users and organisations;

2. introducing standardized methodologies to assess the robustness of Al models, in particular
to determine their field of action with respect to the data that have been used for the training, the
type of mathematical model, or the context of use, amongst others factors;

3. raising awareness among Al practitioners through the publication of good practices regarding to
known vulnerabilities of Al models and technical solutions to address them;

4. promoting transparency in the conception of machine learning models, emphasizing the need of an
explainability-by-design approach for Al systems with potential negative impacts on
fundamental rights of users.

The importance of the establishment of good practices and threat-driven procedures is of paramount
importance to strengthen the trust in Al systems. This implies that all risks for the interest and rights of users,
in the broad sense, should be taken into consideration, and appropriate safeguards measures have to be
implemented, based on current scientific knowledge.
Abstract

In the light of the recent advances in artificial intelligence (Al), the serious negative consequences of its use
for EU citizens and organisations have led to multiple initiatives from the European Commission to set up the
principles of a trustworthy and secure Al. Among the identified requirements, the concepts of robustness and
explainability of Al systems have emerged as key elements for a future regulation of this technology.

This Technical Report by the European Commission Joint Research Centre (JRC) aims to contribute to this
movement for the establishment of a sound regulatory framework for Al, by making the connection between
the principles embodied in current regulations regarding to the cybersecurity of digital systems and the
protection of data, the policy activities concerning Al, and the technical discussions within the scientific
community of Al, in particular in the field of machine learning, that is largely at the origin of the recent
advancements of this technology.

The individual objectives of this report are to provide a policy-oriented description of the current perspectives
of Al and its implications in society, an objective view on the current landscape of Al, focusing of the aspects
of robustness and explainability. This also include a technical discussion of the current risks associated with Al
in terms of security, safety, and data protection, and a presentation of the scientific solutions that are
currently under active development in the Al community to mitigate these risks.

This report puts forward several policy-related considerations for the attention of policy makers to establish a
set of standardisation and certification tools for Al. First, the development of methodologies to evaluate the
impacts of Al on society, built on the model of the Data Protection Impact Assessments (DPIA) introduced in
the General Data Protection Regulation (GDPR), is discussed. Secondly, a focus is made on the establishment
of methodologies to assess the robustness of systems that would be adapted to the context of use. This
would come along with the identification of known vulnerabilities of Al systems, and the technical solutions
that have been proposed in the scientific community to address them. Finally, the aspects of transparency
and explainability of Al are discussed, including the explainability-by-design approaches for Al models.
1 Introduction

It is unanimously accepted that our times are witnessing a new technological revolution, as large parts of
society and industry are subject to an ongoing process of digitization. The increased connectivity between
people and devices, the access to enormous amounts of data and the ever-growing processing power have
enabled the emergence of technologies that will change our relationship to the world and boost innovations
by being the driver of formidable discoveries.

Artificial Intelligence (Al) is a prime technology that emerged from these major changes, and has achieved
recently tremendous progress in many areas, enabling the automation of various cognitive tasks that were
previously out of reach of computers. It is expected that Al will profoundly reshape the social and economic
landscape of our societies [1], and it will definitively spark innovations and foster creative solutions in fields
such as medicine, transportation, finance, or machine translation, to name but a few. While Al will likely bring
significant advances in these domains, it becomes also crucial to get a sense of the side effects that these
automatic reasoning systems will inevitably cause after their integration in production products and services.
The risks on the fundamental rights of citizens and organisations are serious, and Al will have significant
impacts in the society. These impacts raise legitimate concerns on topics such as data protection,
cybersecurity, privacy, reliability, fairness, or trustworthiness. Users must be guaranteed that their
fundamental rights are protected and that products and services are safe to use. Furthermore, the rise of
autonomy that is allowed by Al techniques requires a proper assessment of their capacity to work reliably in
uncontrolled environments.

Even if this technology is still in its infancy, Al systems have already caused disastrous accidents, from the
dramatic accident of an autonomous car in 2018 with one fatality in Arizona [2] to the more anecdotal but
still problematic case of Microsoft’s chat Bot Tay [3]. In the first example, the computer-based model used for
the recognition of persons and objects misclassified a pedestrian as an object, leading to wrong decision-
making processes. The second example vividly showcased how easily a complex Al system can be attacked by
ill-intentional actors to deceive an automatic text-based bot into having ethically completely unacceptable
discussions, displaying racist and misogynistic behaviour. These are not isolated examples, and the number of
failures and the seriousness of harm is likely to grow as Al will be more and more common.

Al today is dominated by machine learning techniques, whose main feature is to build a reasoning system
directly from data, often in large quantities, without explicit rules to generate the result of the process. The
genericity of these techniques makes them very attractive in a wide range of applications. Furthermore, the
machine learning community has adopted from its beginning an open approach for collaboration and
dissemination, with a large collection of resources, from software, to datasets, to documentation, freely
available to everyone. This approach boosted the popularity of machine learning in the scientific and
engineering communities, and its adoption by practitioners of many sectors, taking advantage of the huge
amount of data collected in digital systems.

At the same time, the recent application of the General Data Protection Regulation (GDPR) [4] has created a
new framework for companies that want to use personal data in automated decision-making systems. This
framework introduced new rights for data subjects, bring new challenges in the case of Al-powered systems.
One of them concerns the question of the interpretation of obtained outcomes, especially when these
outcomes may have potential negative consequences for the data subjects. Al methods are famously known
to have limited capacity to provide the reasoning principles behind a decision, mainly due to the fact that the
logic is automatically inferred from vast amounts of data, and embedded in complex mathematical structures
that are successful but very opaque for humans. The explainability of methods is then becoming crucial in this
context to ensure the rights of individuals to understand a decision concerning them.

Al systems fall under the scope of the Cybersecurity Act, proposed by the European Commission in 2017, and
that introduced an EU-wide cybersecurity certification framework for digital products, services and processes.
In the same way cybersecurity techniques have evolved over time to adapt to new devices and new practices,
the need to secure and improve reliability of Al systerns has become prevalent, as their opaqueness exposes
them to strong defects, both intentional and non-intentional. Inside the Al community, connected discussions
around Al safety [5], or biases and accountability in Al [6, 7] have emerged, and led to significant advances on
this question. Thus, explainability and robustness are largely intertwined: understanding the mechanisms of a
system is a standard approach to guarantee its reliability.

In light of the serious threats that Al poses to all parts of the European society, from the violation and
endangerment of the rights of citizens, to the jeopardizing of activities of European business, to the potential
uses by malicious actors, regulating frameworks need to consider the uptake of Al technologies and their
complications to mitigate its risks and ensure a trustworthy environment for its development.

This report aims to identify some of these limitations, and to provide with potential solutions for policy
makers. Its central angle is to make the connection between the legitimate expectations in terms of
robustness and explainability and the current scientific landscape of Al on these topics. The individual
objectives of this report are:

— to provide a policy-oriented description of the current perspectives of Al and its implications in society
(Section 2);

— to provide an objective view on the current landscape of the current landscape of Artificial Intelligence
(Al), focusing of the aspects of robustness and explainability. This includes a technical discussion of the
current risks associated with Al in terms of cybersecurity, safety, and data protection, and the scientific
solutions that are currently under active development in the Al community to mitigate these risks (Section
3);

— to put forward a list of recommendations presenting policy-related considerations at the attention of the
policy makers to establish a set of standardisation and certification tools for Al, built on state-of-the-art
techniques, with an emphasis on their scope and limitations (Section 4).

More broadly, the scope of this report is to explore the technical and scientific backgrounds related to the
requirement of robustness and explainability of Al, and on that basis to chart out on which aspects concrete
policy actions and regulations are the most needed, and to which extent it is possible to implement them with
the current technology.
2 Background: Perspectives from Society and Policy making

As with every novel technology left unregulated, Al can and will simply be employed according to its user’s
preferences, regardless of the lawfulness of their intentions or ethical considerations. Until recently, its use
did not raise a lot of concerns as it was limited to scientific works or commercial activities with only a limited
scope. Now that Al has become widely deployed, it is a prime task for society and policy making to come to an
agreement on necessary boundaries, regulations and standards for Al. In the last three years, many reports
have been released to provide general Al policy strategies and high-level guidelines for the use of Al in
mainstream activities, either by national institutions [8, 9, 10, 11, 12, 13, 14, 15, 16, 17], international
organisations [18, 19], or policy-oriented think tanks [20, 21].

The most important narrative throughout these documents is that Al is both opportunity and challenge. Many
policy recommendations are, thus, broadly set along two lines: fostering Al technology as an enabler for
innovation, increased resilience and better competitiveness, while being increasingly aware of the lack of
robustness of such systems and the exploitation of vulnerabilities or outright malicious use by threat actors
and adversaries. For policy regulations, challenges and opportunities need to be weighed against each other.
This implies that trust in researchers and innovators needs to be maintained, since regulation of a novel
technology should not end in impeding the possibility to explore the capabilities and limitations of a
technology. Conversely, it is mandatory for researchers and innovators to understand the need of policy
makers to set boundaries and standards to prevent possible violations that would be contrary to European
values.

2.1 Policy initiatives

The political guidelines for the new European Commission, which took office in December 2019 [22] give high
prominence to the need for fostering and regulating Al at EU wide level: The human and ethical implications
of Al are especially stated as one of the few highlight policy initiatives that the new Commission wants to
tackle within the first 100 days, through cross-cutting measures.

10 April 2018 1 June 2018 7 June 2018 18 Dec. 2018 8 April 2019 26 June 2019
Digital Day Al expert Horizon Europe Consultation Commission Policy &
Declaration: group programme on draft ethics presents investment
Member States appointed proposed: guidelines by Al approach to recommendations
sign up to & Al Alliance largest EU R&l expert group. build trust in by Al expert
cooperate on Al. launched. programme ever human-centric group. Pilot phase
with €100 billion. Al. of Al ethics
guidelines
25 April 2018 6 June 2018 7 December 2018 1 Jan. 2019 9 April 2019 Early 2020
European Al strategy: increase Digital Europe Coordinated plan AI4EU Digital Day Evaluation
public and private investments programme with Member States: project - Presenta- report of
to €20 billion per year over proposed: boost Al made in launch. tion and pilot phase.
the next decade, prepare for €2.5 billion Europe. All EU to discussions
socio-economic changes, and for the countries to develop on Al ethics
ensure appropriate ethical and deployment own Al strategies by guidelines.
legal framework. of Al. mid-2019.

Figure 1: Al for Europe: Roadmap

(source: EU Al factsheet?)

The undertaking of the von der Leyen Commission rests on the initiatives launched by the previous European
Commission. The roadmap Al for Europe (see Figure 1) details the various focused policy activities concerning
Al that have taken place since 2018:

— the initial European Commission Al strategy 2018 [23], focusing on the Digital Single Market, common
investments and setting up an ethical and legal framework for Al in Europe;

— the Coordinated Action Plan 2018 in continuation of the initial Al strategy [24]: all EU Member States
aim to set up their own Al strategies within this EU framework by 2019. In support of this initiative, a

 

1 https: //ec.europa.eu/digital-single-market/en/news/factsheet-artificial-intelligence-europe
Science for Policy by JRC on Artificial Intelligence [1] was released last year. The present report grew out
of the chapter on the cybersecurity perspective of Al of said report, which described in the robustness of
Al algorithms against malicious action;

— the appointment of an independent high level expert group (HLEG) on Al by the European
Commission to provide input and counsel from academia and industry [25, 26];

— various EU wide projects under cooperation with, or guidance of, the European Commission: the Al4EU
project? and the Al Watch observatory”, and the European Al alliance’.

These initiatives should be seen in the more global legal context around the cybersecurity of digital systerns
and the initiatives concerning the management of data, that both have been at the centre of various
regulations these last years in the EU.

2.2 Data governance and decision-making

Data are a key aspect in Al techniques, and the widespread use of data management systems that has been
enabled by the digitalization of services have led to the emergence of principles to properly store and handle
these data. The concept of Data Governance has appeared to describe the set of practices, procedures norms
and rules to ensure that data collected by organizations are well-managed (lawfulness, accountability, etc.).
The European Commission has been particularly active to set up a legal framework that implements these
principles, in order to provide an environment that both boost innovation while protecting fundamental rights:
The Regulation on a framework for the free flow of non-personal data [27] aims at removing obstacles to the
free movement of non-personal data, while the General Data Protection Regulation (GDPR) [4] provides a
framework adapted for the handling of personal data by any organization.

The GDPR became applicable in Europe in 2018 putting forward a modernised set of rules fit for the digital
age. It is of particular interest for Al, as it introduces specific elements to tackle the growing adoption of Al in
decision-making systems based on personal data.

Several of the provisions of the GDPR relate to this topic. The recital 71 of the regulation already foresees
cases in which algorithms are used for profiling or to automate decision-making, and it introduces and
motivates the need to introduce safequards for such processes. These safeguards aim to protect against
potential adverse consequences that profiling or automatic decision-making can have on data subjects.

The application of Al to personal data, or more generically automatic processing, is considered in the GDPR
under different circumstances. The first one is in profiling, which is defined in Article 4 as Any form of
automated processing of personal data consisting of the use of personal data to evaluate certain personal
aspects relating to a natural person. The second one is solely automated decision-making, which is defined
in [28] as the ability to make decisions by technological means without human involvement. This refers to the
broader notion of the application of algorithms for the purposes of decision-making, which may or may not
involve some form of profiling in doing so. Several examples on this topic can be found in [28].

Recital 71 contextualises the set of data subject rights relevant to both profiling and automated decision-
making that are developed in the several articles of the regulation, including the right to information, to
obtain human intervention, to express his/her point of view, to obtain an explanation of the outcome of a
decision and to challenge such decision.

Article 13 and 14 of the GDPR require that data subjects are informed about the existence of automated
decision making, including but not limited to profiling. Further, the articles require that data controllers provide
data subjects with information about the underlying mechanisms (logics) behind the automated decision-
making performed and the significance and potential consequences of such processing. The right of access
(article 15) also include similar provisions, granting data subjects the right to access their personal data and
obtain such information about its processing from the data controllers.

These articles refer respectively to Article 22 where additional specific provisions on automated individual
decision-making are introduced. Data subjects have the right not to be subject to a decision exclusively based
on automated processing if such decision affects him/her legally or significantly in any other way, unless any
of the exceptions foreseen in Paragraph 2 applies (necessary for a contract, authorised by Union or Member
State Law or based on data subject explicit consent).

 

https: //www.aideu.eu/
*https://ec.europa.eu/knowledge4policy/ai-watch/about_en
“https: //ec.europa.eu/digital-single-market /en/european-ai-alliance
Article 22 emphasises the requirement to implement appropriate measures to safeguard the rights and
freedoms of data subjects. In those cases, where automated decision-making takes place, it does so by
granting data subjects the right to obtain human intervention, to express their point of view and to be able to
contest the decision taken. The guidelines released on this topic by the Article 29 Working Party [28] state that
human intervention implies that the human-in-the-loop should refer to someone with the appropriate
authority and capability to change the decision.

It is clear how the requirement of explainability is relevant for the envisaged safequards. Human supervision
can only be effective if the person reviewing the process can be in a position to assess the algorithmic
processing carried out. This implies that such processing should be understandable. Further, explainability is
also key to ensure that data subjects are able to express their point of view and are capable of contesting the
decision. As it is stated in the Article 29 guidelines, data subjects will only be able to do that if they fully
understand how the automated decision was made an on which bases.

The Article 29 guidelines provide in Annex 1 a set of good practice recommendations for data controllers, with
respect to the several rights and provisions of the GDPR that are of relevance for profiling and automated
decision making. On top of the generic transparency requirements, as commented in [29], data controllers
have the specific requirement to provide understandable explanations to data subjects in automated decision-
making. Since the entered into force of the GDPR the academic community has debated about this right to
explanation [30]. The intention of the present report is not to enter into this legal discussion, but rather to
focus on the immediate challenges that the usage of machine learning techniques presents in this regard. It is
easy to see how the current technical limitations of Al models can cause difficulties in the practical
implementation of provisions described in the GDPR and the related data subject rights.

One of these limitations relate to the interpretability of Al models, which connects to the right of data
subjects to receive meaning information about the logics involved in automatic decision-making processes
and the right that data subjects have to be able to contest a decision, both of which require the provision of
understandable explanations to them.

Table 1: Data protection rights introduced in the GDPR linked to algorithmic processing of data (see [28, 31] for more

 

 

details)
Right GDPR References
Right to be notified of solely automated decision making Art. 13, Art. 14

 

Right of notification and access to information about logics involved in]|Art. 13, Art. 14, Art. 15
automated processing

 

Right on information of significance of and potential effects of solely|Art. 13, Art. 14. Art. 15
automated decision making

 

 

 

 

 

Right not to be subject to solely automated decision making Art. 22

Right to contest a decision in solely automated decision making Art. 22

Right to obtain human intervention Recital 71, Art. 22
Right to obtain an explanation Recital 71

 

 

 

Table 1 summarises the several rights of the GDPR that refer to any of the forms of automatic processing
accounted for in the regulation. In addition to these rights, all the other ones generically referring to the
processing of personal data also apply, namely the right of information (in a broader sense), the right of
access, the right of rectification and the right to object [28], as well as the specific legal basis (art.6) used by
the data controller (usually data subject consent).
2.5 From Data Governance to Al Governance

A similar movement to data governance, now for the management of Artificial Intelligence is currently gaining
momentum [32, 33] to make sure that the design and the implementation of Al systems are aligned with
values and responsibilities of organisations and society.

To this end, on the one hand, the new Commission policy initiatives focus on fostering investment research
and development, attracting and building up skills and expertise, strengthening European cooperation and
increasing access to data for Al development.

On the other hand, various organisations have stressed the need of ethical principles and increased
trustworthiness of Al [34, 35, 36, 37]. From the European perspective, the HLEG already published their Ethical
guidelines for trustworthy Al [25, 26, 35, 36] under the previous Commission. The guidelines clearly define the
boundaries of current Al system’s capabilities and chart out what is needed to make them trustworthy from a
complete societal point of view, including ethical and legal considerations. As the current main pillar from
which to derive guidance on Al development from the policy perspective, the guidelines list seven broad key
requirements [26]:

1. human agency and oversight: protection of fundamental rights, interaction between humans and
Al systems;

2. technical robustness and safety: resilience, accuracy, reliability of Al systerns;

3. privacy and data governance: data protection, data management, privacy rights;
4. transparency: traceability, explainability, communication;

5. diversity, non-discrimination and fairness: accessibility, lawfulness;

6. environmental and societal well-being: sustainability, social and societal impact;
7. accountability: auditability, reporting, responsibility.

It is noteworthy that the requirements (1), (2), (3) and (4) are emphasizing elements that directly link to the
fields of Al robustness, explainability, legal considerations of data protection and cybersecurity.
3. Artificial Intelligence and its Robustness and Explainability: Overview
and limitations

3.1 Artificial intelligence and Machine Learning: A short overview

Artificial Intelligence (Al) is generally defined as methods capable of rational and autonomous reasoning,
action or decision making, and/or adaptation to complex environment, and to previously unseen
circumstances [38, 25]. It is deeply rooted in the field of computer science, and regroups different sets of
techniques that led to significant advances in the automation of human level tasks in the second half of the
twentieth century, nonetheless without much impact beyond academic circles. Much of the real and tangible
successes of Al today can be directly linked to the hugely successful subfield of machine learning, and in
particular the rising technique of deep learning that reached tremendous milestones these last years in
computer vision, natural language processing or game reasoning. In this report, only the machine learning
aspect of Al will be considered, as it encompasses the vast majority of the Al landscape.

3.1.1 Machine learning

Machine learning [39, 40] consists in a set of mathematical techniques at the intersection of algorithmic,
statistical learning and optimisation theory, that aim to extracting information from a set of examples
(images, sensor records, text, etc.) in order to solve a problem related to this data (classification, recognition,
generation, etc.). Machine learning can be roughly divided into three paradigms: In the supervised setting, each
example includes a label, that can be either categorical or scalar: For a given input, the model aims to predict
the right label. In the unsupervised (or self-supervised) setting, no label is provided, the model aiming at
learning a new representation that groups the examples based on their similarity. The last category regroups
reinforcement learning techniques, in which an agent is trained to perform a complex sequence of actions
autonomously in a complex environment, in order to maximize a reward function. In today’s machine learning
applications, supervised techniques are predominant, and have been mainly applied to decision-making
systems.

Although distinct, the tools and technologies overlap over the three categories, and most approaches, such as
decision trees, linear regression, or SVMs, have been used over the decades in any of the three paradigms. In
these traditional machine learning approaches, descriptors of the data, called features, are constructed and
used to train a mathematical model, that will discover and extract statistical patterns that are significant for
the problem one aims to solve. Nowadays, deep learning [41] (or the use of multi-layered neural networks)
has gained special interest from the scientific and engineering communities, in particular for its capacity to
include the crafting of features as part of the learning process, dramatically increasing the performances.
They are, in particular, very well adapted to learn new representations of data that are more compact and
more informative. Deep learning excels in tasks related to perception (image characterization, sound
recognition, etc.) where traditional machine learning techniques struggle to handle the high dimensionality of
data. It has also led to generative tools, such as GANs [42], that have been a major improvement in the field
of computer-assisted synthesis, with a capacity to generate original data with a striking realism.

Al systems are inherently more complex than classical decision-making systems: they are composed of a
possibly non-linear feedback loop system between an algorithm and a dataset which, together, constitute a
learning model that acts as the actual reasoning systems, outputting predictions based on inputs. This model
is then embedded into a more traditional program, often combined with other pieces of code or software
architecture, possibly implemented using different programming tools than those used to write the Al models.
If these systems are introduced in a careless manner, the integrity of the whole architecture is possibly
threatened since the security controls in place may not be adequate anymore.

10
 

 

Examples of Al decision-making systems

1. An environmental association wants to release a phone application to their members that allow them
to automatically identify using supervised learning the type of butterfly they may encounter just by using
their camera. To do that, the data science team in charge of the project first asks its members to take
pictures of butterflies, and manually determines the species based on a nomenclature. Once the number of
images is sufficient, a computer vision model is trained to automatically determine the species of butterflies.

2. A commercial bank uses machine learning for marketing purposes. Based on the payments made by
the customers, several predefined attributes are extracted (number of operations per day, total amount spent
per day, etc.). From these features, unsupervised techniques are implemented to separate customers in
different categories based on their behavioural activities. These categories are then used to design
customised marketing strategies.

3. An advertisement company wants to increase the number of clicks in the ads displayed to visitors of
a website. To do that, a reinforcement learning model is used to generate an eye-catching advertisement,
featuring the image of the product and a generated slogan. The model can adapt the way the elements are
arranged, as well as the text, and the format. The system gets a reward every time a visitor clicks on the ad.

4. A physics research group wants to simulate the signals that will be returned by a sensor. To do that,
the group has at its disposal millions of experimental signals that have been acquired in many different
conditions, with a wide range of sensors. The team builds then a generative model that will simulate, based
on statistical correlations, the signals that one may obtain in unknown experimental conditions.

 

3.1.2 Artificial General Intelligence

As for many technological advances, discussions of Al bear the danger of overestimating its capabilities and
robustness [43], or getting lost in marketing and buzz wording [44, 45]. Al is often used for any type of
automated decision system [7] and unfortunately tends to be improperly used as a marketing argument.

On a different note, ideas such as conscious Al, Al replacing humans for almost any kind of work, or generally,
Al being credited as an all-purpose solution to every problem, are far from being achieved and even relevant
for the actual discussion of current Al systems.

Nevertheless, even if researchers in the Al community agree on the unlikeliness of the emergence of a
human-level intelligence in the coming decades, the ever-growing place of Al in our life raises relevant
concerns that experts try to address. Linked to the inner mechanisms of Al (robustness, explainability,
transparency, data protection amongst others) Al may turn out to be harmful in a situation where autonomy
is given to a system without appropriate safeguards to monitor its activity, independent of the specifications
of the model. The development of mechanisms to create safe Al systems by design in this setting is still an
active area of research [46]. Appropriate answers to ensure the transparency and reliability of Al systems, as
discussed below, would be one of the solutions to address this issue and keep Al beneficial for humanity.

3.2 Transparency of Al systems

Transparency usually refers to the possibility to have a complete view on a system, i.e., all aspects are visible
and can be scrutinized for analysis. In the case of Al systems, three levels of transparency can be
distinguished:

1. Implementation: at this level, the way the model acts on the input data to output a prediction is
known, including the technical principles of the model (e.g., sequence of operations, set of conditions,
etc.) and the associated parameters (e.g., coefficients, weights, thresholds, etc.). This is the standard
level of transparency of most open source models available on the Internet and provided by
researchers. Such systems are often referred as white-box model, in contrast to black-box model
where the model is unknown;

2. Specifications: this refers to all information that led to the obtained implementation, including
details about the specifications of the model (e.g., task, objectives, context, etc.) training dataset, the
training procedure (e.g., hyper-parameters, cost function, etc.), the performances, as well as any
element that allows to reproduce from scratch the implementation. Research papers usually fulfil in
part this level of transparency;

ll

 
3. Interpretability: this corresponds to the understanding of the underlying mechanisms of the model
(e.g., the logical principles behind the processing of data, the reason behind an output, etc.). This also
includes the demonstration that the algorithm follows the specifications and is aligned with human
values (e.g., in terms of fairness). This aspect is discussed more broadly in the next section. In a
general manner, this level of transparency is not achieved in current Al systems.

Most real-world Al systems used in production are not transparent, either because the implementation and
the specifications are not publicly available (for matter of intellectual property for instance), and/or because
the model is too complicated and no simple interpretation of results can be made. In the following, two
approaches to increase the transparency of models are discussed.

3.2.1 Documentation of specifications

Faced with a large number of models aimed for various different tasks and developed in many different
contexts, there is a need to provide a consistent documentation alongside code implementation to identify use
cases and follow a traceability process essential for industry applications, particularly in sensitive areas. One
reason for that comes from the development of Al, which has been and is still driven by the research
community, who does not have the same practices as industrial actors and has a tendency to release model
implementations without proper specifications nor maintenance procedures, other than an elusive research
paper. To remedy this situation, some works have been proposed to provide templates to document a
machine learning model.

In [47] a template is introduced to properly document the training dataset. This documentation is structured
into seven categories: Motivation of the dataset, its composition, the collection process, the preprocessing
applied to the data, including cleaning and labelling, its expected uses, the ways of distribution, and finally the
procedure implemented for its maintenance. A similar approach is considered in [48] but more focused on the
model, describing the data used for the training and the evaluation phase, technical details about the model;
the way it has been trained, as well as various measures of performances.

3.2.2 Interpretability and understandability

Although the advent of widespread use of machine learning and especially deep neural network techniques
has clearly induced current discussions about the need for more interpretable Al models, this topic is not
novel: Early Al research on expert systems in the 1980s already raised questions about Al explainability
(see [49, 50] for a more detailed review). Nonetheless, discussions about explainable Al have significantly
broadened: from a growing literature of technical work on interpretable models and explainable Al [51, 52], to
an ongoing discussion about the precise meaning and definition of explainability and interpretability [53, 50,
54], to more procedural questions about the evaluation of existing frameworks [55], or even to input from
social science about the meaning of explanation [56].

As noted in [56], most methods and tools introduced by researchers in the Al community to explain Al systems
do not rely on a formal definition of what an explanation is, albeit this question has been the subject of works
in fields such as cognitive science or social psychology [57, 58] for decades or even centuries. On the contrary,
explanatory approaches in Al rely on the idea of providing elements to explain the results to a human in
understandable terms, without agreeing on a common formal definition that varies depending on author and
context [53, 50, 54].

3.2.3. Aspects of interpretability

In most cases, interpretability is often (but not always) loosely defined as a variant of how well a human
could understand the decisions of an autonomous algorithmic system [56, 55, 49]. The interpretability of
predictive models can be characterized following different aspects. Generally, there are two main objectives
pursued behind interpretability approaches. In the global interpretability setup, the elements that need to be
explained cover all the steps of the machine learning processing chain. They include:

— the logic of the model: what kind of features are used, and how they are considered to return the outputs.
This can be based on conditions derived from the comparison of the value of features, or on linear and
non-linear operations of those features.

— a description of the kind of data that are expected to be used in the model, including the boundaries of
the input space (e.g., only valid for male individuals aged from 50 to 80, or for images taken by
commercial cameras in daylight). Datasets often contain biases that can have a strong influence on the

12
mechanisms learned by the model. Careful description of the dataset (see [47, 48] for examples of a
template for such description) is then essential to understand what is learned by a model.

— in the case of classification tasks, how the decision is taken using the output values (e.g., in the case of
thresholding, how the threshold is chosen).

The second approach focuses on providing an explanation for a single prediction made by the system using
specific input data. While this explanation can be given at the light of the understanding of the global model,
specific approaches can be designed to explain the decision. This can be done for instance by highlighting the
most prominent features that come into play in the decisions, or by generating counterfactual
explanations [59], that return which features should be changed to modify the decision (e.g., in the case of
credit scoring, what are the requirements that are not achieved by the customer whose application has been
denied).

 

 

Example of a counterfactual explanation

An Al scoring system is implemented in a university to automatically allocate scholarship based on school
results of the previous year. The process also takes into account the average academic standard of the class
of the applicant. A student A has been denied the application. To justify the decision, the system returns the
following counterfactual explanations: You would have obtained the scholarship if one of the following
conditions were reached:

the average score over all topics was higher than 14 (currently: 12.6);

the score in mathematics was higher than 11 (currently: 12) AND the average score of the class in
physics was lower than 11 (currently: 13);

the mark in physics was higher than the average mark in physics (currently: 11 < 12.6).

 

3.2.4 Interpretable models vs. post-hoc interpretability
Two approaches in interpretable Al are generally considered, depending on the nature of the model:

— Post-hoc interpretability is used to extract explanations from black box model that are not inherently
interpretable, such as high dimensional models (e.g., deep learning models) that include a tremendous
number of parameters. The interpretation is done through reverse engineering, by selectively querying the
model to reveal some of its properties. Many approaches from the literature of post-hoc interpretability
aim to train an openly interpretable surrogate model on the basis of these queries [51].

— Interpretable models: these models are fully or partially designed to provide reliable and easy to
understand explanations of the prediction they output from the start [52]. The problem is that it stands to
reason as to whether it is always possible to design an appropriate interpretable model to the desired
accuracy. The feasibility of this approach is highly debated, especially in application cases where the most
accurate solutions are usually provided by complex models such as deep neural networks.

It is worth noting that most methods for interpretability are themselves based on statistical tools that are
subject to uncertainty or errors. Their outputs do not then constitute a true statement but should also be
carefully analysed and understood.

 

 

Example of post-hoc interpretability method in computer vision

Several techniques to explain the decision made by a classifier for computer vision tasks. Generally, it consists
in defining the area of interest, that has been found relevant by the classifier for the decision. In this example,
the decision of a computer vision model trained on the ImageNet dataset [60], a widely-used dataset used for
computer vision, is explained using a method introduced in [119]. In Figure 2, the explanation of the decision
to correctly classify the image as squirrel is displayed, highlighting the area comprising the head of the
animal.

 

13

 

 
 

Figure 2: Explanation provided by the method introduced in [119], indicating which area of the image has been found to
be relevant for the decision of the classifier.

3.2.5 Interpretability vs. accuracy

The aforementioned problems with designing interpretable models are part of a larger discussion, in which it
is debated whether a trade-off exists in machine learning model design between interpretability and accuracy.
Usually, more complex models are employed in pursuit of higher accuracies or to achieve more complex tasks.
Making those models more interpretable in turn seems to almost inevitably come with a loss in these
features. On the other hand, the assumption that under given constraints better results can only be achieved
with a more complex model, can be challenged, especially when good feature engineering is combined with
simpler but robust models [52]. Yet from another angle, the very notion of what a complex and less
interpretable model means might depend on the point of view, constraints or situation [53].

The question of how much the outputs of a given algorithm are still understandable for a human or even
fundamentally uniquely explainable (e.g., because of non-linear functions employed in many machine learning
models) is crucial for a reliable assessment of its security.

3.3. Reliability of Al systems

Despite their performances, Al systems are not yet considered as reliable enough to be fully autonomous in
complex environments without human supervision. Beyond the classical software vulnerabilities that are
inherent to any piece of software, and that will not be discussed here, their characteristic opens up new
surface of vulnerabilities that are still largely little known. The case of the first fatal accident involving an
autonomous car and a pedestrian that happened in 2018 in Arizona is an example of the global lack of
reliability of Al systems. In the Vehicle Automation Report of the accident [2] written by the National
Transportation Safety Board is reported the predictions made by the system a few seconds before the impact.
In a nutshell, the pedestrian was alternatively detected as a vehicle (and then considered as traveling in the
other lane), and as an unknown (static) object. 2.5 seconds before impact, it was seen as a bicycle, and 1.2
seconds before as being on the path of the car. An alarm was raised, and 0.02s before impact the operator
took control of the wheel. One observation made is that the system design did not include a consideration for
jaywalking pedestrians.

A distinction is made here between two signs indicating that a learned machine learning model is not reliable:

1. Poor performances: the model cannot perform well in the task in conditions that are considered as
normal for humans;

2. Vulnerabilities: the model performs well but has vulnerabilities that may lead to malfunctions in
specific conditions. These malfunctions may appear either naturally in the course of the execution of
the program, or be intentionally provoked by an adversary with malicious intentions.

Assessing the reliability of a system requires then to consider these two aspects.

14
3.3.1 Evaluation of performances

The evaluation of performances is an important aspect that is central during the conception of a machine
learning model. It is a multi-faceted question that include amongst others the choice of the right metrics and
of the procedure of evaluation.

The choice of the right metric is crucial to assess its capacity to solve the problem. It is generally driven by the
kind of data, the choice of the class of models, and above the specifications of the task. No matter how good
is the chosen metric, it will still be an approximation, and therefore it will not encompass all the aspects of
the studied task. The danger then lies in optimizing the model in order to maximize the performance metric,
that may lead to a model with good performances but not adapted to the actual problem.°.

 

 

Example of the evaluation of performances on a binary decision making system

A binary decision-making system is considered, to determine, based on medical images, if a tumour is present
or not. Even though the models output a dichotomous answer (True or False), the algorithm first computes a
probability, that acts as a level of confidence of the model on the decision, and is then transformed into a
decision after application of a threshold. The threshold has a significant influence on the performances, and
its choice is guided by the problem: a low threshold will label more instances as positive, limiting the risk of
missing a true positive, but increasing the number of false positive. Conversely, a high threshold will reduce
the number of detected samples, while increasing the risk of missing a relevant example. Existing measures
of performances (such as AUC) takes into account this phenomenon, but determining a right value above
which a performance metrics is acceptable may turn out to be problematic.

 

As for the evaluation procedure, beyond the common practices already well-established in the machine
learning community (splitting the dataset into a training set and a test set, imbalance of the dataset taken
into consideration), several points are worth to be mentioned in relation with reliability, inspired by clinical
trials performed in health context and discussed in [60].

The first point is the importance of an external validation, independent from the training phase, to limit
overfitting, which occurs when the model does not learn any meaningful pattern but only memorize the input
data, reducing greatly the generalization power of the model. Even if external validation can be to some
degree compared to the testing phase in machine learning procedures, in which the model is evaluated on
previously unseen data, it goes beyond by extending the testing to other form of validation, described in [60]
as temporal and geographical to highlight the fact that the data have been collected at a different time and
at a different location. While this has to be adapted to the domain of application, the idea that a proper
validation has to be performed in different situations is crucial to correctly assess the model.

The second point emphasized in [60] is the risk of spectrum bias, that refers to the presence of examples in
the dataset that does not reflect the diversity and the complexity of situations, i.e., the spectrum of examples
does not reflect the real spectrum. It implies that good performances on obvious examples are not sufficient
to assess the capacity of the model to correctly handle more ambiguous situations.

Finally, this study identifies the risk that despite high performances, the system does not provide a real
benefit for the users, or lead to a blind trust in a Al system that even with high performances stays prone to
errors.

3.3.2 Vulnerabilities of machine learning

In an adversarial context, Artificial Intelligence models open new vectors of attacks compared to classic
software: as displayed in Figure 3, representing the paradigm change induced by Al components in the
cybersecurity of digital systems, vulnerabilities can be exploited at the level of the different elements present
in the Al processing chain, multiplying the potential threats and then the global risk of failures.

 

5 This is summarized in [10] as an adage known as the Goodheart’s law: When a measure becomes a target, it
ceases to be a good measure.

15

 
SREP
*

wumemeny

 

i
|
mn

 

 

 

 

Cee

Algorithm

sO

Model Attack

 
  

seme eee

 

    

 

 

 

 

: <> :

: = Data Poisoning :

as <
Algorithm

Figure 3: Paradigm change in the cybersecurity of systems because of the introduction of Al components.

Although many common techniques require the knowledge of the parameters of the model (white-box
settings) to exploit these vulnerabilities and build attacks, this is unlikely to be a limiting factor for attackers:
first, it has been shown that it is feasible to perform these attacks in a black-box setting with comparable
performances [61], by approximating the model through a limited series of well-designed queries. Second,
adversarial examples have been shown to possess a property of transferability in many configurations [62],
i.e., malicious samples from a model, designed by the attacker, can be still effective against a target model.
Finally, any breach of security of the system storing the Al model could be exploited in order to discover its
mathematical structure, and plan a more comprehensive attack.

Typical vulnerabilities intrinsically linked to Al systems [43, 63, 64], include the following ones.

Data poisoning

It consists in deliberately introducing false data at the training stage of the model [65]. This can be done to
neutralize a system, reduce its performance, or silently introduce a backdoor exploitable by the adversary.
Data poisoning relies on the capacity of models to learn new patterns along the time by constant retraining
almost in real time using newly acquired data. This design opens the possibility for an attacker to inject
gradually benign data that will progressively drift away the decision boundaries of the algorithm [63]. In a
similar way, reinforcement learning systems can easily be misled to maximize wrong goals by corrupting the
reward channels of their agents [66, 67, 68].

This attack can also be performed at the production stage, by an attacker who would have access to the
training data, but also who would have the control of a pretrained model. The training of a model, especially
the most complicated ones, requires indeed tremendous amount of data and huge computational and human
resources, and it is common to reuse models that have been trained by third party. An adversary could use
this opportunity to conceal backdoors that it could exploit subsequently.

16
Crafting of adversarial examples

The most active research domain currently is without doubt the domain of adversarial examples. It consists in
using input data to the trained machine learning model, which are deliberately designed to be
misclassified [72, 64, 73]. The development of techniques to this end has been an active area of research,
with a host of different types of attacks [74, 75, 76, 77, 61, 78], relying mostly on optimisation procedure to
synthesize adversarial examples. Most of the attacks focused on classification task in computer vision, this
subdomain being one of the most active area of deep learning research. Deep learning image analysis
systems have been proved to be sensitive to input image spoofing of various kinds [72, 74, 79].

school bus (1.00) Perturbation guacamole (0.98)

 

Figure 4: Illustration of an adversarial example using Basic Iterative Method [69]. The classifier used is Inception v3 [70].
The image comes from the validation of the ImageNet dataset [71] (left) Original image, correctly classified as a school
bus. (middle) Perturbation added to the image, with a 10x amplification (right) Adversarial example, wrongly classified
with high confidence.

 

 

Example of an adversarial example in computer vision

In Figure 4 is displayed an illustration of an attack on a standard classifier using basic projected gradient
descent. While the object is correctly classified after the training of the model, adding a small perturbation on
the pixels of the image, almost imperceptible to humans, significantly degrades the performance of the
classifier, which assigns a wrong label with a high confidence. Adversarial attacks are nonetheless not limited
to image classification, and has also been successfully applied in different contexts, such as image
segmentation [80], object recognition [81], speech recognition [82], text summarization [83], as well as
unsupervised [84] or generative [77] models.

 

Model flaws

It consists in taking advantage of the inherent weaknesses of the mathematical procedures involved in the
learning process of the model [5, 63]. The usage of a specific architecture known to be susceptible to various
phenomena, such as the presence of noise, can enable the attacker to fool the system.

Despite the relatively insecure settings in which those attacks occur, the impact on real-world activities is
expected to be profound, as the implementation of these attacks is already feasible in a more constrained
context. In [69, 85, 74] are discussed several approaches to construct adversarial examples robust to several
transformations happening in the physical world, such as viewpoint shifts, noise, low or high brightness, and
so forth and so on. This has practical consequences, for example on the development of autonomous cars
that strongly rely on computer vision techniques [86, 87]. Adversarial attacks can also be directly performed
on the car itself after exploiting a vulnerability of the car’s software, as in [88].

3.3.3 Approaches to increase the reliability of machine learning models

Designing Al specific security controls is an active, albeit young, field of research in adversarial machine
learning, that naturally arises with the advent of attack against machine learning models. Depending on
circumstances, such as the intention of the attack, the type of vulnerability, and the kind of model, two main
approaches have been proposed to mitigate the risks of wrong behaviour of models [89, 63, 5, 90, 91, 92]:

17

 
These approaches follow the well-known security-by-design principle, i.e. taking the security of a software or
application into account from the beginning of the design process. It should be noted that for machine
learning based Al systems this approach likely implies a degradation of performance that is inherent to
statistical approaches [93].

Data sanitization

Cleaning the training data of all potentially malicious content before training the model is a way to prevent
data poisoning [94]. Depending on the situation, another Al system can be employed to act as a filter, or
classical input sanitization based on handcrafted rules can be used. In very important circumstances though,
human intervention might be inevitable.

Robust learning

Redesigning the learning procedure to be robust against malicious action, especially adversarial examples [95,
96]. This entails explicit training against known adversarial examples, as well as redesigning the mathematical
foundation of the algorithms by employing techniques from statistics, such as regularization and robust
inference.

It is interesting to note that, as it has been the case in cybersecurity for decades, there is a constant race
between attackers and defenders to attack and protect Al models. As an example, we can cite a robust
learning technique called distillation [97], which acts on the outputs of the model to reduce its sensitivity to
adversarial examples. This defence has been broken [98], then improved to resist to these attacks [99], and
we can expect this process to keep going. The underlying mechanisms are then very similar to those presents
in traditional cybersecurity systems, and similar strategies could be implemented.

Extensive testing

The testing of a model cannot be restricted to a single dataset. Rigorous benchmarking requires the take into
account of edge cases that can arise either because a given example has not been taken into account in the
training data, or because the input data is slightly corrupted, and not recognizable by the model.

Frost

Contrast Brightness

 

Speckle blur Elastic blur

Defocus

  

Figure S: Illustration of the different alterations defined in [100], performed on an image of a boat from the ImageNet
dataset [71], to mimic different weather conditions or noise that may appear on the sensors.

18
 

 

Example of augmented dataset for testing in computer vision

In [100] several approaches have been proposed to mimic several conditions that may appear in the capture
of images. It includes weather conditions (snow, fog, brightness, etc.), as well as different types of noise that
may appear on the pictures taken by optical sensors, such as Gaussian noise, speckle, motion blur, etc. In
Figure 5, an example of such alterations is given on an image of a boat extracted from the ImageNet
dataset [71], a widely-used dataset used for computer tasks such as classification, segmentation or
recognition. Using techniques defined in [100], a new dataset called ImageNet-C is generated, enabling
designers of computer vision to assess the robustness of their model against these alterations.

 

Formal verification

Formal verification is a very active field in computer science who aims to prove the correctness of a software
or hardware systems with respect to specified properties, using mathematical proofs. Two main properties are
often investigated:

— (Un)satisfiability: checking if for a given input, getting a certain output is (not) feasible;
— Robustness: checking if adding noise to a given input changes its output.

In a supervised learning context, most issues can be formulated as a satisfiability problem, i.e., checking that
the model cannot output a wrong label. Verifying the satisfiability property is nonetheless in the majority of
cases not possible, as the input space is infinite, and not known. This issue is circumvented by considering
instead an evaluation of the robustness of the model at the inputs present in the dataset, as an
approximation to the real input space. These methods aim to guarantee that for all points in a neighbourhood
of a given input, the outputs stay the same.

 

 

Example of a formal verification procedure in malware analysis

In a malware analysis context, one example of satisfiability property that is desirable is that a malicious file
should not be classified as benign. In practice though, it is not possible to have access to all (existing and non-
existing) malicious files to verify the property, and there is no formal specification of what is a malicious file.
Using existing samples, modifying the samples while preserving their maliciousness (that can be practically
challenging) can ensure that at the neighbourhood of the known samples, the model correctly classifies the
sample as malicious.

 

In a nutshell, methods used for formal verification of deep neural networks rely on the same idea as for SMT
problems, where linear constraints are combined to derive the domain space of acceptable solutions. For the
verification of the robustness property, it consists in deriving the constraint applied to the outputs, given the
operations successively applied on the inputs. The last step consists then in verifying that this domain is
included in the same decision region as the outputs.

When those operations are linear, i.e., adding or multiplying a factor to the inputs affects likewise the outputs,
the verification is straightforward as efficient algorithms, such as the simplex algorithms, exist and are easily
scalable. Modern machine learning algorithms, such as neural networks, are nonetheless highly non-linear,
and does not fall within the scope of application of these algorithms. Several techniques have been
introduced in the case of deep learning: in [101] approximations of non-linear functions are used to perform
the verification; in [102], the regions around inputs is propagated at each layer of the network; in [103], the
inputs are clustered to derive safe regions in which the classifier is robust.

Despite the promising results, these techniques are not able yet to scale up to consider large networks.

3.4 Protection of data in Al systems

Machine learning models are built on large amount of data, extracting statistical patterns to solve a specific
task. The dataset used for the training can nonetheless be sensitive, either because it contains personal
information (medical records, emails, geolocation, etc.) or because the content is restricted (intellectual
property, strategic systems, etc.). For personal data, systems should be compliant in regards to the legislation
on privacy and data protection, and then appropriate technical and organizational measures should be put in
place to implement data protection principles. An example is the EU General Data Protection Regulation
(GDPR) that applies in all EU member states. The application of anonymization or pseudonymization [104] to
these data is a safeguard recommended by the GDPR, although the feasibility to do so highly depends on the

19

 

 
context of the application and furthers the complexity of the employed systerns even more, potentially
impacting the explainability of the Al system.

More generally, building an Al system based on sensitive data requires then to ensure that all actors involved
in the machine learning pipeline, from the collection of data, to their processing, to the training of the model,
to its maintenance, and to its use, are considered trustworthy to handle the data.

In this section are described the main risks regarding to the confidentiality of data, as well as several
mechanisms to mitigate them and guarantee some levels of data protection. These mechanisms are not
exclusive, and should be combined and appended to classic approaches used for the protection of data
management systems.

3.4.1 Threats against data

The quality and correctness of the training data is of paramount importance to ensure that Al systems
employing machine learning techniques, designed to be trained with data, operate properly. Together with the
machine learning algorithm in charge of building the model, the training data is part of the Al system and, as
such, it forms part of the scope of security that needs to be protected. It is, therefore, crucial to ensure the
security of data sets in terms of their confidentiality, integrity and availability, as well as their compliance
with possible data protection frameworks.

In practice, many challenges have to be faced given the complex supply and processing chain involved in
complex Al systems. The high computational cost of machine learning algorithms often requires for instance
to outsource to an external contractor the training of models. It may not be possible however to fully trust all
the actors involved in the process for the protection of sensitive data. Two risks are considered:

1. sensitive data are directly accessible to an untrustworthy actor, due to malicious intent or
vulnerabilities in the data infrastructure;

2. sensitive data may leak from the model after the training.

The second risk makes reference to the capacity of memorization of machine learning models. Models are
indeed trained to extract patterns from data, and usually store them as parameters of the models, for
instance under the form of weights. The aim of the training phase is to make the model memorize
generalizable patterns, that will be relevant for data that are not present in the training dataset. Nevertheless,
these patterns can be very similar to the training data, and can be retrieved by adversaries: An example of
such an attack is described in [105], where credit card numbers are extracted from a natural language
processing model developed for autocompletion and that has been trained on vast amounts of text data.
Memorization is often associated with overfitting. Preventing overfitting though regularization techniques
limits the memorization effect, but only partially, as various techniques have been designed to recover
degraded but exploitable data present in the training set [106, 107].

3.4.2 Differential privacy

Differential privacy [108, 109] consists in adding noise to the training data to reduce the influence of each
individual sample on the output. The implementation introduced in [110] of differential privacy for deep
learning consists in adding noise to the gradients at each iteration of the iterative training algorithm. It
provides probabilistic guarantees on the level of privacy reached by a model, i.e., how hard it is to retrieve the
actual training data from the model. It acts as a regularization technique, and in this respect it prevents
overfitting but also may greatly reduce the performances of the systems in terms of accuracy if the level of
privacy is too high.

Differential privacy comes in addition of additional measures to increase the level of protection, such as
preventing the direct access to the parameters of the models, and limiting the mumber of queries an
adversary might be able to do on a system.

3.4.3 Distributed and federated learning

Distributed and federated are two different situations where the learning of the model is not performed by a
single actor, bust instead by a multitude of different parties that may or may not be connected between each
other. In distributed learning, all parties are learning the same model, and shares information about the
gradients. With federated learning, only parameters of the model are exchanged between actors. In this
setting, each actor has only access to its part of the dataset, while taking advantage of a more robust model

20
that is trained using various source of data. Though information about the training data can still leak through
the model, this greatly reduces the disclosure of sensitive data.

3.4.4 Training over encrypted data

Fully homomorphic encryption is a special kind of cryptography methods that allows to perform additions and
multiplications on encrypted data. Its integration in machine learning algorithms in still in its early
stages [111], but it does suggest that learning over encrypted data could be a reasonable strategy when the
sensitivity of data is high. An external contractor could then train a model on data that have been encrypted
by the data provider, and returns an encrypted learned model, and this without having an understanding at
any time neither of the data nor of the purpose of the model.

While this approach suffers from a certain number of limitations, the main one being the current high
computational cost of a single operation compared to the unencrypted approach, it is an active area of
research that already provided working implementations [112], and that will likely grow in the coming years.

Secure aggregation [113] is a different yet close technique to secure communications of information between
different parties, by suggesting ways to securely share information about models. It is particularly useful
when combined with federated learning.

21
4 From technical to policy solutions

Al is a rapidly growing technology, which is becoming the driving force of a new industry that takes advantage
of its power to build innovative perception and predictive systems at a relatively low cost. The impact of this
technology on EU citizens will be significant, because of the numerous areas in which Al is expected to be a
tool to assist or replace human decisions: health, justice, transportation, economy, job market, to name a few.

Technical advances to provide assurance that the technology is in line with human values have been part of
the research in Al, but they do yet provide strong guarantees on the reliability of such systems. At the same
time, the integration of Al components in products and services, and its use in sensitive contexts, requires an
intervention of regulatory bodies to avoid potential harms on EU citizens.

Standards and of certification procedures regarding the use of Al are fundamental components to build a
favourable ecosystem around this emerging technology, that will guarantee the alignment between Al
objectives and human values. They will allow:

— industrial actors to share common set of best practices that encourages interoperability and the right
integration of Al inside existing infrastructures;

— regulators to build efficient policies that protect citizens’ right while preserving economic
competitiveness;

— users to understand and trust the novelties and possible disruptions of Al.

In the following are discussed two approaches of ways to provide a regulatory framework for the use of Al,
through the angles of, respectively, the robustness and explainability. Several suggestions are made to include
technical mechanisms at the different stages of the conception of a product with an Al component, from the
conception, to the development, and to the maintenance of the product. These suggestions are a first step
and do not constitute a unique answer, and should be considered in conjunction within an appropriate legal
framework.

4.1 Certification of the robustness of Al systems

Regulations should ensure that the cybersecurity and safety of users and systems are taken into account
during the full lifecycle of an Al product. This means that secure software development practices, security
certifications, security audits and cybersecurity controls need to be implemented, extending the current
practices already in place in cybersecurity. Establishing new standards and certifications for Al will take
advantage of current legislations instruments proposed or already in place. Al systems are under the scope of
the Cybersecurity Act [114], that introduced an EU-wide cybersecurity certification framework for digital
products, services and processes.

Extending this framework for Al is not straightforward: The risks and threats for which a product can greatly
vary according the technical aspects of the systems, but also the domains of applications and context of uses.
Al is still a very active area of research, and tools are not yet available to properly guarantee the right
behaviour of Al-based systems. A certification scheme for Al in this context could be then based on two
pillars: first, a careful risk assessment of the impact of Al on its environment, as well as the threats posed by
potential adversaries and existing vulnerabilities. Second, an extensive testing of Al systems through their
transparency, the evaluation of their performances in edge cases, and their explainability.

4.1.1 Impact assessments of Al systems

Data Protection Impact Assessments (DPIA) have been introduced in the GDPR and are a key tool to assess
the risks involved in the usage of automated decision making or profiling [28] Data controllers can use them
to identify and implement the necessary measures to appropriately manage the risks identified. These
measures should implement the safeguards foreseen in the GDPR with respect to the explainability
requirements.

Bearing in mind that current trend in the application of increasingly complex machine learning models, data
controllers should pay particular attention to the inherent challenges that these models present in terms of
both explainability and robustness. In this regard, the Article 29 Data Protection Working Party [28] highlighted
the need for transparency on the algorithmic processing carried out, not only in terms of the specific
algorithm employed but also in terms of the data used by it.

22
Indeed, errors and biases in the data used by the algorithm can result in mistakes in the outcome of the
automatic processing (e.g., the decision taken), which can cause negative impact on individuals potentially
affecting their interests and rights. When machine learning is involved in the automated processing,
considering this dimension in the DPIA is of paramount importance, for the reasons further explained in
Section 3. Data controllers have to be aware of the limitations that such systems exhibit given their nature
and guided by the DPIA process, introduce appropriate measures to tackle them. For critical contexts where no
safe and secure technical solution exists, human supervision and final decision-making should remain the
default option.

This impact assessment shall not be restricted to systems handling personal data, but to any automated
system whose the scope of action may have a negative influence on any the interests and rights of
individuals. This include for instance previously excluded systems such as any physical systems moving
autonomously in the public space by means of computer vision.

4.1.2 Testing

Following the example of the Cybersecurity Act [114], here is a list of the objectives a certification scheme
should be designed to achieve:

— Identification of the vulnerabilities of systems, and the potential impacts. These impacts can be on citizen
and organisations, on an economic, social, and ecological levels;

— Consideration of the scope of the data and the potential edge cases where the system could fail;

— Demonstration of the performances on various datasets, including external datasets that have not been
used in the training phase.

As for secure systems, no strict guarantee cannot be made on the robustness of systems by malicious actors.
Nonetheless, Al systems also cannot be strictly verified that it does what it was designed for. The risks of
such failures should be appreciated with respect to the level of autonomy of the systems, and the impacts
associated with the consequences of such failures.

Another important aspect of Al systems concerns their evolution over time. Throughout their operational
stage, Al models, and this is one of the characteristics that add an extra value compared to traditional
systems, have to constantly take into account freshly acquired data. Beyond the risk of data poisoning already
mentioned, that means that after a few updates, a system can present a radically different behaviour
compared to the time it entered in production. This characteristic raises the question of a possible expiration
date for the certification.

Explainability of machine learning algorithms plays a key role in the auditing of algorithms, which is one of
the proposed safeguards [28] following the requirements of article 22 of the GDPR. The audit of machine
learning algorithms can help the data controller ensure that they are robust (i.e. unbiased and resilient against
edge cases and malicious input) and demonstrate compliance with the GDPR requirements. This audit could
be carried out by a third party and possibly evolve into a certification mechanism.

4.2 Standardization

Standardization is a powerful way of acting to reduce the risks linked with the use of Al in systems, through
the publication of a collection of materials to prevent and mitigate the risks of failures. Certification
procedures also operate according to standards. The recent good practices released by ENISA for the security
of various cyber-physical systems such as loT networks [115] or smart cars [116] constitutes a relevant
example of what could be achieved for Al. In this spirit, here is a non-exhaustive list of points that can support
the establishment of standards and good practices to increase the reliability of Al systems.

4.2.1 Known vulnerabilities

Establishing a taxonomy of known vulnerabilities of Al systems in different contexts with relevant references
from the scientific literature, along with the associated adversary tactics and techniques similar to the MITRE
attack framework®, would give engineers the opportunity to take into account design flaws at the conception
stage. This would come along with information existing tools and methods to fix those vulnerabilities, or to
the operating environment to set up to mitigate the risks. These tools include the strategies used at the

 

Shttps://attack.mitre.org/

23
training phase to strengthen the learned model and reduce the surface attack of models, but also the
different mechanisms to ensure a protection of the data in various threat scenarios. In addition to this, a
collection of use cases on real-world examples could be provided to illustrate the risks in machine learning
contexts.

Machine learning practitioners are essentially coming from statistics and mathematical modelling, and are not
well aware of standard cybersecurity procedures that might affect traditional software systems. In order to
disseminate a security culture, the redaction of guidelines could be contemplated, as it has previously been
done for web programming [117] or scientific computing [118]. In the same way the use of software design
patterns has provided a structured approach to computer programming, allowing software engineers to
integrate best practices, in particular concerning security aspects, a similar approach for Al would help
machine learning engineers to reduce the number of vulnerabilities in their system.

4.2.2 Systematic transparency

Transparency is a crucial element to hope to get an understanding of the robustness of a system, its safety,
and its compliance with regulations. This transparency is essential internally at the conception and operational
phases of the Al product, and also for auditing and certification.

Transparency means a traceability of the different stages of the machine learning processing chain, as
described in the previous section. It should also include how the assessment of the performances of the
system has been conducted, and in particular which tools have been used and which methodologies have
been followed. Here again, the establishment of good practices for the proper evaluation of Al systems may
be of relevance, in order to favour the use of state-of-the-art techniques such as statistical analysis, formal
verification, external validation, and so on.

Finally, machine learning models should follow an explainability-by-design approach to take into account from
the beginning of the cycle life of product the need to provide explanations to users and/or regulatory bodies.
This is particularly true when personal data are part of the training dataset, or if the system can have a
negative impact on fundamental rights of users. More generally, explaining the decision of a system is
intrinsically linked to its reliability. A sound explanation guarantees the correlations extracted by the algorithm
from the data are causal relations that have a sense in the considered system, and not spurious relationships.

4.2.3. Understandable explanation

The meaning of what the academic literature in Al refers to as interpretability or explainability of an Al model
is very different from the meaning of an explanation that is generally discussed in other social contexts
(see [55, 49, 53, 56] for the ongoing academic discussion). In essence, the fact that the output of an
algorithm is interpretable does not necessarily imply that this interpretation is sufficient as an explanation,
either considering the domain of application of the systems (e.g., a medical explanation in a health context) or
from a legal point of view.

According to the level of criticality of applications and the threats to the system, different levels of
requirements to be determined should be applied. Indeed, the relevance of an explanation is subject to the
targeted audience: explaining the decision to an end user, to a technical engineering team or to a certification
body requires different tools and approaches, and should be done considering both the technical limitations of
Al interpretability and legitimate expectations of stakeholders. To this end, the definition of recommendations
to connect technical interpretability methods and explanations would go one step further in the understanding
of Al systems.

24
5 Conclusion

Expectations about Al are high and there are good reasons for it given the latest advances in the field and the
growing number of success cases of its application in several domains. However, it is important to understand
the limits of the current generation of Al algorithms that are leading this revolution, which are still far away in
terms of capabilities from autonomous systems with human-level reasoning skills. Nevertheless, these
algorithms, led by the latest developments in the fields of deep learning and reinforcement learning, have
proven to be very effective in performing specific tasks, in some specific cases reaching superhuman
performance. They will likely not replace the human operator in the foreseeable future, but instead free
resources for complex tasks. Because of this, it is expected that these algorithms will become a key element
in digital information systems in many areas to achieve goals more effectively and efficiently.

On many aspects however, Al systems that are currently under development are far from achieving the
minimal requirements of safety and security that would be expected from autonomous systems. As much as
their performances, unthinkable a decade ago, are impressive, their implementation in real-world applications
could pave the way to major disappointments if it is not done within a controlled framework. The prime
importance of data in the development of machine learning models is to the detriment of the understanding
of the underlying mechanisms, exposing digital systems to various vulnerabilities. This considerably limits the
transparency of decision processes, and poses risks on the respect of the fundamental rights. Al systems may
also be subject to various risks regarding to their reliability, with the multiplication of edge cases not
considered by the algorithms, and, in adversarial contexts, to a partial or complete loss of control of systems
to the benefit of a malicious actor. Finally, it poses several risks in terms of data protection, with potential
issues concerning the confidentiality of data used to train the machine learning models. Broadly speaking,
various risks for the interest and rights of users have been taken into consideration, and appropriate
safeguards measures have to be implemented, based on current scientific knowledge.

As of now, several avenues for reflection could be considered to undertake the implementation of standards
in Al technologies, and of security and reliability certifications of Al components embedded in real systems.
These avenues include:

— developing a methodology to evaluate the impacts of Al systems on society built on the model of the
Data Protection Impact Assessments (DPIA) introduced in the GDPR, that would provide an assessment of
the risks involved in the usage of Al models to the users and organisations;

— introducing standardized tests to assess the robustness of Al models, in particular to determine their field
of action with respect to the data that have been used for the training, the type of mathematical model,
and the context of use, amongst others factors;

— raising awareness among Al practitioners through the publication of good practices regarding to known
vulnerabilities of Al models, and technical solutions to address them;

— promoting transparency in the conception of machine learning models, emphasizing the need of an
explainability-by-design approach for Al systems with potential negative impacts on fundamental rights
of users.

The importance of the establishment of good practices and threat-driven procedures to strengthen the trust in
Al systems is of paramount importance. This is all the more important with respect to the fact that Al is an
active scientific field, in which practices and techniques move fast. Building policies able to keep up this pace
and to stay relevant in the long term will be undeniably a determining factor for the success of the
integration of Al in all sectors of the society.

25
References

[1] M. Craglia, A. Annoni, P. Benczur, P. Bertoldi, B. Delipetrev, G. De Prato, C. Feijoo, E. Fernandez Macias,
E. Gomez Gutierrez, M. Iglesias Portela, H. Junklewitz, M. Lopez Cobo, B. Martens, S. Figueiredo Do Nascimento,
S. Nativi, A. Polvora, J. Sanchez Martin, S. Tolan, |. Tuomi, and L. Vesnic Alujevic, “Artificial Intelligence—a
European perspective,” 2018, JRC113826.

[2] E. Becic, N. Zych, and J. lvarsson, “Vehicle Automation Report HWY18MHO10,” National Transportation
Safety Board - Office of Highway Safety, Tech. Rep., 2019.

[3] G. Neff and P. Nagy, “Automation, algorithms and politics— Talking to bots: Symbiotic agency and the
case of Tay,” International Journal of Communication, 2016.

[4] European Parliament and the Council, “Regulation (EU) 2016/679 of the European Parliament and of
the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal
data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection
Regulation),” 2016, https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016RO0679from=EN.

[5] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané, “Concrete problems in Al
safety,” arXiv e-prints, p. arXiv:1606.06565, 2016.

[6] S. Myers West, M. Whittaker, and K. Crawford, “Discriminating systems. gender, race and power in Al,”
Al NOW, Tech. Rep., 2019.

[7] Algorithm Watch, “Atlas of automation. Automated decision making and participation in Germany,”
Algorithm Watch, Tech. Rep., 2019.

[8] The National Artificial Intelligence, “The National Artificial Intelligence Research and Development
Strategic Plan,” 2016.

[9] Executive Office and of the President and National Science and Technology Council, “Preparing for
The Future Of Artificial Intelligence,” 2016.

[10] Strategic and Council for Al and Technology, “Artificial Intelligence Technology Strategy,” 2017.
[11] Executive Office of the President, “Artificial Intelligence, Automation and the Economy,” 2016.

[12] Canada’s Vision and for Security and Prosperity in the Digital and Age, “National Cyber Security
Strategy,” Public Safety Canada, Tech. Rep., 2018.

[13] C. Villani, M. Schoenauer, Y. Bonnet, A-C. Comut, F. Levin, B. Rondepierre et al, “For a meaningful
artificial intelligence: Towards a French and European strategy,” 2018.

[14] Select Committee and on Artificial and Intelligence, “Al in the UK: ready, willing and able?” 2018.
[15] Nationale Strategie fur Kiinstliche Intelligenz, “Artificial Intelligence Strategy,” 2018.
[16] S. Sikkut, “Report of Estonia’s Al taskforce,” 2019.

[17] Office of the President of the Russian Federation, “Decree of the President of the Russian Federation
on the Development of Artificial Intelligence in the Russian Federation,” 2019, translation by CSET.

[18] European Commission, “Digital transformation monitor: USA-China-EU plans for Al: where do we
stand?” 2018, https://ec.europa.eu/growth/tools-databases/dem/monitor/sites/default/files/DTM\s\do6(A)|

[19] OECD, “OECD Al Policy Observatory - A platform for Al information, evidence, and policy options,”
2019.

[20] B. Barron, N. Chowdhury, K. Davidson, and K. Kleiner, “Annual Report of the CIFAR Pan-Canadian Al
Strategy,” 2019.

[21] 0. A. Osoba and W. Welser IV, “The Risks of Artificial Intelligence to Security and the Future of Work,”
Rand Corporation, Tech. Rep., 2017.

[22] U. Von der Leyen, “A Union that strives for more: My agenda for Europe. Political guidelines for the
next European Commission 2019-2024’, 2019, https://ec.europa.eu/commission/sites/beta-
political/files/political-quidelines-next-commission\s\do6(e)n.pdf

[23] European Commission, “Artificial intelligence for Europe,” 2018, COM(2018) 237 final.

26
[24]  ---— , ‘Coordinated Plan on Artificial Intelligence,” 2018, COM(2018) 795 final.

[25] European Commission High Level Expert Group on Artificial Intelligence, “A definition of Al: Main
capabilities and scientific disciplines,” 2019, https://ec.europa.eu/digital-single-market/en/high-level-expert-
group-artificial-intelligence.

[26] 9 ---— , “Ethics Guidelines for Trustworthy Al,” 2019, https://ec.europa.eu/digital-single-market/en/high-
level-expert-group-artificial-intelligence.

[27] European Parliament and the Council, “Regulation (EU) 2018/1807 of the European Parliament and of
the Council of 14 November 2018 on a framework for the free flow of non-personal data in the European
Union,”, 2018, https://eur-lex.europa.eu/legal-content/EN/TXT/?uri= CELEX:320 18R1807

[28] Article 29 Data Protection Working Party, “Guidelines on Automated individual decision-making and
Profiling for the purposes of Regulation 2016/679,” 2018, https://ec.europa.eu/newsroom/article2 9/item-
detail.cfm?item\s\do6(i)d=612053.

(29) ----— , “Guidelines on transparency under Regulation 2016/679,” 2018,
https://ec.europa.eu/newsroom/article29/item-detail.cfm?item\s\do6(i)d=622 227.

[30] M. E. Kaminski, “The right to explanation, explained,” Berkeley Tech. LJ, vol. 34, p. 189, 2019.

[31] M. E. Kaminski and G. Malgieri, “Algorithmic impact assessments under the GDPR: Producing multi-
layered explanations,” U of Colorado Law Legal Studies Research Paper, no. 19-28, 2019.

[32] U. Gasser and V. A. F. Almeida, “A layered model for Al governance,” /FEF Internet Computing, vol. 21,
no. 6, pp. 58-62, 2017.

[33] The Al Element, “From Data Governance to Al Governance,”, 2019
http://theaielement.libsyn.com/from-data-governance-to-ai-governance

[34] Beijing Academy of Artificial Intelligence, “Beijing Al Principles,”, 2019,
https://baip.baai.ac.cn/en?fbclid=IlwAR2HtIRKJxxy9Q 1Y953H-2pMHl\s\do6(b)Ir8pcslxho9 3BtZY-
FPH39W9v9B2eY

[35] T. Madiega, “EU guidelines on ethics in artificial intelligence: Context and implementation,”, 2019
https://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS\s\do6(B)RI(2019)640163

[36] Berkman Klein Center, “Ethics and Governance of Al at Berkman Klein: Report on Impact, 2017-
2019,”, 2019, https://web.archive.org/web/20191121162323/https://cyber.harvard.edu/story/2019-10/ethics-
and-governance-ai-berkman-klein-report-impact-2017-2019

[37] R. Richardson, J.M. Schultz, and V. M. Southerland, “Litigating Algorithms 2019 US Report: New
Challenges To Government Use Of Algorithmic Decision Systems,” 2019,
https://ainowinstitute.org/litigatingalgorithms-2019-us.html.

[38] S. J. Russell, P. Norvig, and E. Davis, Artificial intelligence: a modern approach. Prentice Hall, 2010.
[39] C.M. Bishop, Pattern recognition and machine learning. Springer, 2006.

[40] S. Shalev-Shwartz and S. Ben-David, Understanding machine learning: From theory to algorithms.
Cambridge University Press, 2014.

[41] |. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016.

[42] |. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
“Generative Adversarial Nets,” in Advances in Neural Information Processing Systems (NIPS), vol. 27, 2014

[43] M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfinkel, A. Dafoe, P. Scharre, T. Zeitzoff, and
B. Filar, “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” Future of
Humanity Institute, Tech. Rep., 2018.

[44] O. Kubovié, J. Janosik, and P. KoSinar, “Can artificial intelligence power future malware?” ESET, Tech.
Rep., 2018

[45] = ---— , “Machine learning era in cybersecurity: a step towards a safer world or the brink of chaos,”
ESET, Tech. Rep., 2019.

27
[46] S. J. Russell, D. Dewey, and M. Tegmark, “Research priorities for robust and beneficial artificial
intelligence,” Al Magazine, vol. 36, no. 4, p. 105, 2015.

[47] T. Gebru, J. Morgenstern, B. Vecchione, J. Wortman Vaughan, H. Wallach, |. Daume, Hal, and

K. Crawford, “Datasheets for Datasets,” in Proceedings of the sth Workshop on Fairness, Accountability, and
Transparency in Machine Learning, Stockholm, Sweden, PMLR, 2018, p. arXiv:1803.09010.

[48] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, |. D. Raji, and T. Gebru,
“Model cards for model reporting,” in Proceedings of the Conference on Fairness, Accountability, and
Transparency. ACM, 2019, pp. 220-229.

[49] A. Adadi and M. Berrada, “Peeking inside the black-box: A survey on Explainable Artificial Intelligence
(XAl),” [EEE Access, vol. 6, pp. 52138-52160, 2018.

[50] F.K. DoSilovié, M. Bréié, and N. Hlupié, “Explainable artificial intelligence: A survey,” in 4y5t

International convention on information and communication technology, electronics and microelectronics
(MIPRO). IEEE, 2018, pp. 0210-0215.

[51] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi, “A survey of methods for
explaining black box models,” ACM computing surveys (CSUR), vol. 51, no. 5, p.93, 2019.

[52] C. Rudin, “Stop explaining black box machine learning models for high stakes decisions and use
interpretable models instead,” Nature Machine Intelligence, vol. 1, no. 5, p. 206, 2019.

[53] Z. C. Lipton, “The mythos of model interpretability,” Communications of the ACM, vol. 61, no. 10, pp.
36-43, 2018.

[54] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu, “Interpretable machine learning:
definitions, methods, and applications,” arXiv e-prints, p. arXiv:1901.04592, Jan. 2019.

[55] F. Doshi-Velez and B. Kim, “Towards A Rigorous Science of Interpretable Machine Learning,” arXiv e-
prints, p. arXiv:1702.08608, 2017.

[56] T. Miller, “Explanation in artificial intelligence: Insights from the social sciences,” Artificial Intelligence,
vol. 267, pp. 1-38, 2019.

[57] J.Y. Halpern and J. Pearl, “Causes and explanations: A structural-model approach. part i: Causes,” The
British journal for the philosophy of science, vol. 56, no. 4, pp. 843-887, 2005.

[58] ---— , “Causes and explanations: A structural-model approach. part i: Causes,” The British journal for
the philosophy of science, vol. 56, no. 4, pp. 889-911, 2005.

[59] S. Wachter, B. Mittelstadt, and C. Russell, “Counterfactual Explanations without Opening the Black Box:
Automated Decisions and the GPDR,” Harv. JL & Tech., vol. 31, p. 841, 2017.

[60] S.H. Park and K. Han, “Methodologic guide for evaluating clinical performance and effect of artificial
intelligence technology for medical diagnosis and prediction,” Radiology, vol. 286, no. 3, pp. 800-809, 2018.

[61] N. Papernot, P. McDaniel, |. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical black-box attacks
against machine learning,” in Proceedings of the 2017 ACM on Asia conference on computer and
communications security. ACM, 2017, pp. 506-519.

[62] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural networks,” in Proceedings of
the IEEE Symposium on Security and Privacy (SP). |EEE, 2017.

[63] L. Huang, A. D. Joseph, B. Nelson, B. |. Rubinstein, and J. Tygar, “Adversarial machine learning,” in
Proceedings of the ath ACM workshop on Security and artificial intelligence. 2011, pp. 43-58.

[64] S. Huang, N. Papernot, |. Goodfellow, Y. Duan, and P. Abbeel, “Adversarial Attacks on Neural Network
Policies,” arXiv e-prints, p. arXiv:1702.02284, 2017.

[65] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector machines,” in

Proceedings of the 2gth International Conference on International Conference on Machine Learning, 2012, pp.
1467-1474.

28
[66] R. Elderman, L. J. J. Pater, A. S. Thie, M.M. Drugan, and M. M. Wiering, “Adversarial reinforcement

learning in a cyber security simulation,” in Proceedings of the gth International Conference on Agents and
Artificial Intelligence. SCITEPRESS - Science and Technology Publications, 2017.

[67] T. Everitt, V. Krakovna, L. Orseau, and S. Legg, “Reinforcement learning with a corrupted reward
channel,” in Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.
International Joint Conferences on Artificial Intelligence Organization, 2017.

[68] P. Kiourti, K. Wardega, S. Jha, and W. Li, “TrojDRL: Trojan Attacks on Deep Reinforcement Learning
Agents,” arXiv e-prints, p. arxXiv:1903.06638, 2019.

[69] A. Kurakin, |. Goodfellow, and S. Bengio, “Adversarial examples in the physical world,” arXiv e-prints, p.
arXiv:1607.02533, 2017.

[70] C. Szegedy, V. Vanhoucke, S. loffe, J. Shlens, and Z. Wojna, “Rethinking the inception architecture for
computer vision,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp.
2818-2826.

[71] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S$. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” international
Journal of Computer Vision (ICV), vol. 115, no. 3, pp. 211-252, 2015.

[72] C. Szegedy, W. Zaremba, |. Sutskever, J. Bruna, D. Erhan, |. Goodfellow, and R. Fergus, “Intriguing
properties of neural networks,” in Proceedings of International Conference on Learning Representations (ICLR),
2014.

[73] |. Goodfellow, P. McDaniel, and N. Papernot, “Making machine learning robust against adversarial
inputs,” Communications of the ACM, vol. 61, no. 7, pp. 56-66, 2018.

[74] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing robust adversarial examples,” in
Proceedings of the International Conference on Machine Learning (ICML), 2018, pp. 284-293.

[75] T. B. Brown, D. Mané, A. Roy, M. Abadi, and J. Gilmer, “Adversarial Patch,” arXiv e-prints, p.
arXiv:1712.09665, 2017.

[76] |. J. Goodfellow, J.Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” in
Proceedings of the International Conference on Learning Representations (ICLR), 2015.

[77] J. Kos, |. Fischer, and D. Song, “Adversarial examples for generative models,” in 2018 [FEE Security
and Privacy Workshops (SPW). IEEE, 2018, pp. 36-42.

[78] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep neural networks,” /EFF
Transactions on Evolutionary Computation, 2019.

[79] G. Elsayed, S. Shankar, B. Cheung, N. Papernot, A. Kurakin, |. Goodfellow, and J. Sohl-Dickstein,
“Adversarial examples that fool both computer vision and time-limited humans,” in Advances in Neural
Information Processing Systems, 2018, pp. 3910-3920.

[80] J.H. Metzen, M. C. Kumar, T. Brox, and V. Fischer, “Universal adversarial perturbations against
semantic image segmentation,” in 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017,
pp. 2774-2783.

[81] A. Rosenfeld, R. Zemel, and J. K. Tsotsos, “The Elephant in the Room,” arXiv e-prints, vol.
arXiv:1808.03305, 2018.

[82] N. Carlini, D. Wagner, U. of California, and Berkeley, “Audio Adversarial Examples: Targeted Attacks on
Speech-to-Text,” in Proceedings of IEEE Security and Privacy Workshops (SPW), 2018, pp. 1-7.

[83] B. Liang, H. Li, M. Su, P. Bian, X. Li, and W. Shi, “Deep text classification can be fooled,” in Proceedings
of the az7th International Joint Conference on Artificial intelligence. AAAI Press, 2018, pp. 4208-4215.

[84] M. Cheng, J.Yi, H.Zhang, P.-Y. Chen, and C-J. Hsieh, “Seq2Sick: Evaluating the Robustness of
Sequence-to-Sequence Models with Adversarial Examples,” arXiv e-prints, p. arXiv:1803.01128, 2018.

[85] K. Eykholt, |. Evtimov, E. Fernandes, B. Li, A.Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song, “Robust
physical-world attacks on deep learning visual classification,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 1625-1634.

29
[86] N. Morgulis, A. Kreines, S. Mendelowitz, and Y. Weisglass, “Fooling a Real Car with Adversarial Traffic
Signs,” arXiv e-prints, p. arXiv:1907.00374, 2019.

[87] C. Sitawarin, A. Nitin Bhagoji, A. Mosenia, M. Chiang, and P. Mittal, “DARTS: Deceiving Autonomous Cars
with Toxic Signs,” arXiv e-prints, p. arXiv:1802.06430, 2018.

[88] T.K.S. Lab, “Experimental security research of tesla autopilot,” Tencent Keen Security Lab, Tech. Rep.,
2019,
https://keenlab.tencent.com/en/whitepapers/Experimental\s\do6(S)ecurity\s\do6(R)esearch\s\do6(o)f\s\do6(T)esl
a\s\do6(A)utopilot.pdf

[89] M. Barreno, B. Nelson, R. Sears, A.D. Joseph, and J.D. Tygar, “Can machine learning be secure?” in
Proceedings of the 2006 ACM Symposium on Information, computer and communications security. ACM, 2006,
pp. 16-25.

[90] P. Madani and N. Vlajic, “Robustness of deep autoencoder in intrusion detection under adversarial
contamination,” in Proceedings of the 5 Annual Symposium and Bootcamp on Hot Topics in the Science of

Security, ser. HoTSOS ’18 New York, NY, USA: ACM, 2018, pp. 1:1-1:8.

[91] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “A General Framework for Adversarial Examples
with Objectives,” arXiv e-prints, p. arXxiv:1801.00349, 2017.

[92] N. Papernot, P. McDaniel, S.Jha, M. Fredrikson, Z.B. Celik, and A.Swami, “The limitations of deep
learning in adversarial settings,” in Proceedings of IEEE European Symposium on Security and Privacy
(EuroS&P). |EEE, 2016, pp. 372-387.

[93] H.Zhang, Y. Yu, J.Jiao, EP. Xing, L.E. Ghaoui, and M.|. Jordan, “Theoretically principled trade-off
between robustness and accuracy,” in Proceedings of the International Conference on Machine Learning
(ICML), 2019.

[94] K.H. Tae, Y.Roh, Y.H. Oh, H. Kim, and $.E. Whang, “Data Cleaning for Accurate, Fair, and Robust
Models: A Big Data-Al Integration Approach,” in Proceedings of the 3id International Workshop on Data
Management for End-to-End Machine Learning. ACM, 2019, p. 5.

[95] V.Zantedeschi, M.-l. Nicolae, and A. Rawat, “Efficient defenses against adversarial attacks,” in
Proceedings of the 10h ACM Workshop on Artificial Intelligence and Security, ser. AlSec ’17. New York, NY,
USA: ACM, 2017, pp. 39-49.

[96] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning models resistant to
adversarial attacks,” in International Conference on Learning Representations, 2018.

[97] N. Papernot, P.McDaniel, X.Wu, S.Jha, and A.Swami, “Distillation as a defense to adversarial
perturbations against deep neural networks,” in 2016 [EEE Symposium on Security and Privacy (SP). |EEE,
2016, pp. 582-597.

[98] N. Carlini and D. Wagner, “Defensive Distillation is Not Robust to Adversarial Examples,” arXiv e-prints,
p. arXiv:1607.04311, 2016.

[99] N. Papernot and P. McDaniel, “Extending Defensive Distillation,” arXiv e-prints, p. arXiv:1705.05264,
2017.

[100] D.Hendrycks and T. Dietterich, “Benchmarking neural network robustness to common corruptions and
perturbations,” Proceedings of international Conference on Learning Representations (ICLR), 2019.

[101] G Katz, C. Barrett, D.L. Dill, K. Julian, and M.J. Kochenderfer, “Reluplex: An efficient SMT solver for
verifying deep neural networks,” in International Conference on Computer Aided Verification. Springer, 2017,
pp. 97-117.

[102] X.Huang, M. Kwiatkowska, S.Wang, and M. Wu, “Safety verification of deep neural networks,” in
International Conference on Computer Aided Verification. Springer, 2017, pp. 3-29.

[103] D. Gopinath, G. Katz, C.S. Pasdreanu, and C. Barrett, “Deepsafe: A data-driven approach for assessing
robustness of neural networks,” in International Symposium on Automated Technology for Verification and
Analysis, 2018, pp. 3-19.

[104] — ENISA, “Pseudonymisation techniques and best practices,” ENISA, Tech. Rep., 2019.

30
[105] N. Carlini, C. Liu, U. Erlingsson, J.Kos, and D.Song, “The Secret Sharer: Evaluating and Testing
Unintended Memorization in Neural Networks,” arXiv e-prints, p. arXiv:1802.08232, 2018.

[106] M.Fredrikson, S.Jha, and T.Ristenpart, “Model inversion attacks that exploit confidence information

and basic countermeasures,” in Proceedings of the 2and ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2015, pp. 1322-1333.

[107] R.Shokri, M.Stronati, C.Song, and V.Shmatikov, “Membership inference attacks against machine
learning models,” in 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017, pp. 3-18.

[108] C.Dwork, F.McSherry, K. Nissim, and A.Smith, “Calibrating noise to sensitivity in private data
analysis,” in Theory of cryptography conference. Springer, 2006, pp. 265-284.

[109] C.Dwork, A. Roth et al, “The algorithmic foundations of differential privacy,” Foundations and Trends®
in Theoretical Computer Science, vol. 9, no. 3-4, pp. 211-407, 2014.

[110] ™M. Abadi, A. Chu, |. Goodfellow, H.B. McMahan, |. Mironov, K. Talwar, and L. Zhang, “Deep learning with
differential privacy,” in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 2016, pp. 308-318.

[111]  R. Bost, R.A. Popa, S. Tu, and S. Goldwasser, “Machine learning classification over encrypted data.” in
NDSS, vol. 4324, 2015, p. 4325.

[112] F.Boemer, Y. Lao, R. Cammarota, and C. Wierzynski, “nGraph-HE: a graph compiler for deep learning

on homomorphically encrypted data,” in Proceedings of the eth

Frontiers. ACM, 2019, pp. 3-13.

ACM International Conference on Computing

[113] K.Bonawitz, V.lvanov, B. Kreuter, A.Marcedone, H.B. McMahan, S. Patel, D.Ramage, A. Segal, and
K. Seth, “Practical secure aggregation for privacy-preserving machine learning,” in Proceedings of the 2017
ACM SIGSAC Conference on Computer and Communications Security ACM, 2017, pp. 1175-1191.

[114] European Parliament and the Council, “Regulation (EU) 2019/881 of the European Parliament and of
the Council of 17 April 2019 on ENISA (the European Union Agency for Cybersecurity) and on information and
communications technology cybersecurity certification and repealing Regulation (EU) no 526/2013
(Cybersecurity Act),” 2019 https://eur-lex.europa.eu/eli/reg/2019/88 1/oj

[115] — ENISA, “Good Practices For Security Of loT,” ENISA, Tech. Rep., 2019.
[116]  ----— , “Good Practices For Security Of Smart Cars,” ENISA, Tech. Rep., 2019.
[117] OWASP, “Application Security Verification Standard 4.0,” OWASP, Tech. Rep., 2019.

[118] G. Wilson, D.A. Aruliah, C. T. Brown, N. P. C. Hong, M. Davis, R. T. Guy, S.H.D. Haddock, K. D. Huff, |. M.
Mitchell, M.D. Plumbley, B. Waugh, E. P. White, and P. Wilson, “Best practices for scientific computing,” PLoS
Biology, vol. 12, no. 1, p.e1001745, 2014.

[119] M.T. Ribeiro, $.Singh, and C. Guestrin, “Why should | trust you?: Explaining the predictions of any
classifier,” in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining ACM, 2016, pp. 1135-1144.

31
List of boxes

Examples of Al decision-making SYStOINS ..ccccccceccee cece eect eee e ene een ne EEE E ESSE SEED DE ERED EE EE ESSE EE EE ESSE EES 11
Example of a counterfactual CxDLANation....cccccceccee ee ence eee eee ee eee enn E AE EEE SEED DE EE ESSE SEES ESSE ESSE EE EES 13
Example of post-hoc interpretability method in Computer VISION... cc cccccesseeeeeseeeeeeeeeeeeaeeeeeeeeeeesaaeees 13
Example of the evaluation of performances on a binary decision making SYStEIM......ccceeeseeeeeeeeeeeeaaeees 15
Example of an adversarial example in COMmpUter VISION .....ccce cece ee seeeeeeeeeeeeaeeeeesaeeeeeeeeeeaeaeeenaaeees 17
Example of augmented dataset for testing in COMPUter VISION ....cceccceeesseeeeeeeeeesaeeeeeaeeeesesaetenaanees 19
Example of a formal verification procedure in malware Analysis .....ccccessceeeeseeeeeseeeeeseeeeeseseeeesaaeees 19

32
List of figures

Figure 1: Al for Europe: ROAGMAD 2. ee eece cece e teen een een ne ene e nen ene ene nee nana 6

Figure 2: Explanation provided by the method introduced in [119], indicating which the area of the image that
has been found of importance for the decision of the Classifier. ....ccccccccesceeeeeseeeeeeeeeeeeaeeeeeeseteeeaaes 14

Figure 3: Paradigm change in the cybersecurity of systems because of the introduction of Al components..16

Figure 4: Illustration of an adversarial example using Basic Iterative Method [69]. The classifier used is
Inception v3 [70]. The image comes from the validation of the ImageNet dataset [71] (left) Original image,
correctly classified as a school bus. (middle) Perturbation added to the image, with a 10x amplification (right)
Adversarial example, wrongly classified with high CONfFIGENCE. ....cceccceeeseeeeeeeeeeeesaeeeeeeeeesanettesnanees 17

Figure 5: Illustration of the different alterations defined in [100], performed on an image of a boat from the
ImageNet dataset [71], to mimic different weather conditions or noise that may appear on the sensors. ....18

33
List of tables

Table 1: Data protection rights introduced in the GDPR linked to algorithmic processing of data (see [28, 31]
for more CetailS)...cscccecccccccveuveueeeeenecccueveveeueeeeenanscucuaveueeueeeeeneeucuaveueuutentetaneguguaueuvernennas

34
GETTING IN TOUCH WITH THE EU
In person

All over the European Union there are hundreds of Europe Direct information centres. You can find the address of the centre

nearest you at: https //europa.eu/european-union/contact_en

On the phone or by email

Europe Direct is a service that answers your questions about the European Union. You can contact this service:
- by freephone: 00 800 6 7 89 10 11 (certain operators may charge for these calls),

- at the following standard number: +32 22999696, or

- by electronic mail via: https: //europa.eu/european-union/contact en

FINDING INFORMATION ABOUT THE EU

Online

Information about the European Union in all the official languages of the EU is available on the Europa website at:

https //europa.eu/european-unionfindex_en

EU publications

You can download or order free and priced EU publications from EU Bookshop at: bttos://publications europa eu/en/oublications.
Multiple copies of free publications may be obtained by contacting Europe Direct or your local information centre (see
https //europa.eu/european-union/contact_en).
 

Publications Office
of the European Union

The European Commission’s
science and knowledge service

Joint Research Centre

JRC Mission

As the science and knowledge service
of the European Commission, the Joint
Research Centre’s mission is to support
EU policies with independent evidence
throughout the whole policy cycle.

De AD EU Science Hub

ol ec.europa.eu/jrc

| @EU_ScienceHub

 f | EU Science Hub - Joint Research Centre
oO EU Science, Research and Innovation

fisy EU Science Hub

doi:10.2760/57493

ISBN 978-92-76-14660-5

a
Zz
>
oS
Qo
ao
&
a
ra
ral
Zz
ey smart cities (miPr)

Article
Explainable Artificial Intelligence for Developing
Smart Cities Solutions

Dhavalkumar Thakker !*, Bhupesh Kumar Mishra !, Amr Abdullatif 1, Savodeep Mazumdar 2©
and Sydney Simpson ?
1 Department of Computer Science, University of Bradford, Bradford BD7 1DP, UK;
b.mishra@bradford.ac.uk (B.K.M.); a.r.a.abdullatif@bradford.ac.uk (A.A.)
2 Information School, University of Sheffield, Sheffield S10 2TN, UK; s.mazumdar@sheffield.ac.uk

3 City of Bradford Metropolitan District Council, Bradford BD1 1HX, UK; sydney.simpson@bradford.gov.uk
* Correspondence: d.thakker@bradford.ac.uk; Tel.: +44-1274234578

Received: 29 September 2020; Accepted: 29 October 2020; Published: 13 November 2020 g cn ress
Abstract: Traditional Artificial Intelligence (AI) technologies used in developing smart cities solutions,
Machine Learning (ML) and recently Deep Learning (DL), rely more on utilising best representative
training datasets and features engineering and less on the available domain expertise. We argue
that such an approach to solution development makes the outcome of solutions less explainable,
i.e, it is often not possible to explain the results of the model. There is a growing concern among
policymakers in cities with this lack of explainability of AI solutions, and this is considered a major
hindrance in the wider acceptability and trust in such Al-based solutions. In this work, we survey
the concept of ‘explainable deep learning’ as a subset of the ‘explainable AI’ problem and propose a
new solution using Semantic Web technologies, demonstrated with a smart cities flood monitoring
application in the context of a European Commission-funded project. Monitoring of gullies and
drainage in crucial geographical areas susceptible to flooding issues is an important aspect of any
flood monitoring solution. Typical solutions for this problem involve the use of cameras to capture
images showing the affected areas in real-time with different objects such as leaves, plastic bottles etc.,
and building a DL-based classifier to detect such objects and classify blockages based on the presence
and coverage of these objects in the images. In this work, we uniquely propose an Explainable AI
solution using DL and Semantic Web technologies to build a hybrid classifier. In this hybrid classifier,
the DL component detects object presence and coverage level and semantic rules designed with close
consultation with experts carry out the classification. By using the expert knowledge in the flooding
context, our hybrid classifier provides the flexibility on categorising the image using objects and their
coverage relationships. The experimental results demonstrated with a real-world use case showed
that this hybrid approach of image classification has on average 11% improvement (F-Measure) in
image classification performance compared to DL-only classifier. It also has the distinct advantage
of integrating experts’ knowledge on defining the decision-making rules to represent the complex
circumstances and using such knowledge to explain the results.

Keywords: explainable AI; multi-object; coverage detection; semantic rules; hybrid image classification;
flood monitoring

 

1. Introduction

In 2017 there were more than 250 smart cities projects in 178 cities worldwide. The prevalence of
building smart cities across the globe is largely driven by the use of Internet of Things (oT) and Big
data technologies, with smartness attributed to the use of Artificial Intelligence (AI) techniques and
tools [1]. In particular, Machine Learning (ML), and more recently Deep Learning (DL) techniques

Smart Cities 2020, 3, 1353-1382; doi:10.3390/smartcities3040065 www.mdpi.com/journal/smartcities
Smart Cities 2020, 3 1354

are utilised in solving various smart cities problems. A recent survey paper found that predictive
analytics was the most common technique and smart mobility and smart environment were the most
common areas addressed [2]. Image classification using ML is another popular area of application to
address smart cities challenges including managing traffic congestion [3] managing public safety [4],
footfall counting [5] and empowering self-driving and autonomous vehicles [6].

Over the years, numerous ML algorithms, such as decision tree, k-nearest-neighbours and
maximum-likelihood minimum distance, have been applied for image classification tasks [7]. With the
improvements in ML methodologies, including the advances in DL, the performance of intelligent
systems is matching or even bettering human intelligence on several complex problems [8]. Most ML
models are data-driven, where the accuracy of the model highly depends on the volume and variety
of datasets used for training. A highly efficient and accurate ML model is developed by applying
iterative training, evaluation and fine-tuning based on datasets. The nested non-linear structure of
these models highly non-transparent [9]. It is inherently difficult to understand which aspects of the
input data drive the decisions of the model, making the nature of these models appear as a black
box to the end-users or policymakers relying on the results from the model. Human understanding,
interpretation and explanation on decision making are crucial on complex tasks, such as many of
the smart city applications. The ability to explain the reasoning behind any conclusive decisions has
been the key feature of smart city applications as the fundamental tenant of democratic smart cities is
for the policymakers to explain decisions made by smart solution powering public services in their
cities. Singapore’s minister of foreign affairs and also the Minister-in-charge of the Smart Nation
Programme Office, in his talk at the Smart City Expo World Congress in Barcelona in November 2019
said: “Many people think of AI as a black box—into which you throw in data and ask a question.
We cannot approach Al like that. The solutions and answers it proposes have to be explainable in
human terms. Without that ability—to explain how a solution has been arrived at—you will get
pushback”. These words are a good reflection of policy makers and city administrators around the
world who are keen to utilise AI in building smart solutions for chronic city problems, or for improving
public service offerings to their citizens, but are wary of the black box nature of these solutions.

The loss of control over interpretability of decision making becoming a serious concern for high
impact problems [9]. These expectations from policymakers are in line with results of other surveys
done in the area of trust in autonomous systems [10] and where the end-users cite ‘explainability’ as
one of the prerequisites for trust in such systems [11]. End-users and decision-makers who use the
recommendations of the intelligent system require explanations to assure confidence and trust over the
system through direct intervention [12]. Explainable AI enhances the trust on decision making, as has
been the case for medical education, research and clinical decision making [13], detection of COVID-19
using chest X-ray, CT scan images [14], Multi-agent system through Blockchain Technology [15] or
Intrusion Detection Systems [16]. Clearly, for sensitive tasks involving critical infrastructures and
affecting human well-being and trust, it is crucial to limit the possibility of improper, non-explainable
and unsafe decisions and actions [17]. Before deploying an AI system, we see a strong need to track
and validate its behaviour, and thus establish guarantees that it will be reliable for long-term when
deployed in a real-world environment [18]. There is a good deal of research highlighting that the
explanations of decisions are equally important as the model’s accuracy [19].

This paper is from our experience of dealing with a similar issue while developing a flood
monitoring application for a large consortium of European cities as part of a European Union-funded
project—Smart Cities and Open Data REuse (SCORE, https://northsearegion.eu/score/) Real-time
monitoring of gully and drainage blockage is an important part of flood monitoring applications.
Building viable IoT sensors for detecting blockage is a complex task due to the limitations of deploying
such sensors in situ. Image classification with deep learning is a potential alternative solution. To address
these issues, we have built a novel image classification approach based on DL (Convolutional Neural
Network: CNN) with an IoT-enabled camera to monitor gullies and drainages [20]. This approach
utilises deep learning to develop an effective image classification model to classify blockage images
Smart Cities 2020, 3 1355

into three different class labels based on the severity. However, preliminary evaluation with experts
revealed that our model lacked transparency in terms of how objects were related to each other in
the scenarios where it was known that drainage and gully blockages were mainly caused by the
aggregation of multiple objects—and how the model was classifying drainage and gully images into
different classes. The experts wanted to know the reasoning behind the classification to help them
and us understand why the model was classifying an image in one of the three blockage categories
(No Blockage, Partially Blocked and Fully Blocked). However, as the inherent limitations of how
the CNN classifiers function, it was not possible to explain the crucial decisions made by the model.
To address such ‘explainability’ deficit of the CNN models in this work we uniquely propose the use of
Semantic Web technologies [21], in particular ontologies and reasoners [22]. Ontologies, also often
referred to as knowledge graphs, allows us to capture the relationships between concepts and entities
in a particular domain for better reasoning and explanation. For example, in the domain relevant to this
paper, it is possible to work with drainage maintenance teams in city councils to capture the concepts
of different ‘objects’ that are commonly found as obstructers when dealing with gully blockages, or the
inter-relationship and level of their coverage leading to the different level of blockages, in ontological
languages. In this paper, we demonstrate how such knowledge captured in ontologies can be used to
classify an image and also to explain the result—why and how the algorithm arrived to that decision.

Considering these, an approach that combines both ML in the form of CNN and expert knowledge
has been explored in this research. We propose a hybrid model that consists of machine learning
and semantic rules set out to analyse the inclusion of expert knowledge in image classification.
This approach can be utilised more transparently and effectively in the context of real-time applications.
In this hybrid model, the semantic rules are defined to represent the context of the presence of objects
in an image, whereas CNN been used to identify the object coverage from the image dataset. The use of
semantic rules provides the capability to use multi-criteria decision making on any instances whereas
CNN provides feature extraction in the form of objects coverage level within the image. The use of a
hybrid approach for image classification can fill the gap between ML implementation and expert’s
knowledge implementation on monitoring a real-time instance.

Section 2 of the paper surveys the literature in the area of the use of ML and Semantic web
technologies in addressing challenges of building smart cities solutions. Section 3 provides context to
this work and outlines the use case. Sections 4 and 5 provide details of a hybrid image classification
model consisting of Semantic Web technologies and the use of DL models. The experimental design
and results are discussed in Section 6. Finally, we conclude in Section 7 by summarising the advantages
of our unique approach and also outline the future work.

2. Literature Review

In general, a smart city is defined as a city that monitors and integrates critical infrastructure and
services through sensor and IoT devices [23]. The IloT devices capture data of the instances under
monitoring and those data are transferred and stored for further processing. Smart Cities application
integrates real-time data using IoT and the rational method to design and plan any decision making
more systematically. For decision making, human experts have been the core element of all analysis
and objectives [24]. Human experts often make the decision based on data coming to any application.
Decision making in the context of smart cities became more challenging because of the information
available and the involvement of multiple stakeholders [25].

A wide range of sensors and IoT based applications data is being stored and processed in the
digital infrastructure of the cities to support decision making [26]. With the change in tools and
techniques on the digital platform, a variety of data from apps and other real-time data sources are
combined to analyse and make a decision to address specific issues of the city [27]. There have been
qualitative and quantitative data for decision making to meet some objectives, which often appear
with conflict. Therefore, all smart city applications had to operate and adapt working routines in more
Smart Cities 2020, 3 1356

effective ways. Any changes must be dependent on the data being used for the application, and hence,
the decision making [28].

One of the major challenges of decision making in the smart city has been how to deal with
application data and how to apply human expert knowledge. Semantic Web technology in combination
with data analysis has been used for expert-based recommendation systems [29,30]. The semantic
techniques give the flexibility to apply human experts’ knowledge and control over the prediction
model. In different application-based system, the implementation of semantic representation enhanced
the performance levels as the semantic rules were developed by applying high-level information,
which was gathered from experts from the corresponding fields. We list several types of transparency
expected in smart cites models. Each may require a different sort of explanation, requiring different
measures of efficacy [18]:

(1) For a company or a service provider: to understand and explain how their system works, aiming
to identify the root cause of problems and see whether it is working well or not, and explain why.

(2) For end-users: human users need to trust AI systems in obtaining their needs, but what should
be the basis for this trust? In addition to providing end-users with knowledge on the system’s
prediction accuracy and other aspects of the performance, providing users with an effective
explanation for the AI system’s behaviour using semantic rules that are derived from the domain
experts can enhance their trust in the system.

(3) For society: it is important to consider the possible impact of AI in terms of increased inequality
(bias) and unethical behaviours. We believe it is not acceptable to deploy an AI system which
could make a negative impact on society.

Flood monitoring is one of the major concerns in most of the cities around the world. Over the
years, rainfall forecast and satellite images have been used for predicting and monitoring flooding
conditions. A wide range of examples also exists, primarily in the modelling of natural hazards and
emergencies [31] or the wider context of flooding [32]. Methods such as Liquid level monitoring [33],
the water level of gully pot monitoring [34,35] has been applied. Despite the improved access to
rainfall data, water level reading, satellite imagery and improved forecasting accuracy, real-time
monitoring to support decision making is still a challenge [36]. There are some limited examples of
the application of semantic technology for flood risk analysis by applying an expert’s knowledge [37].
However, the model had not applied any machine learning to analyse and extract features from the
existing flood-related data to support risk analysis. In this example, semantics were only used as a
descriptive mechanism using high-level information for recommendation system. Much research in
flood monitoring involves the use of imagery available in the public domain. However, these studies
are either too generic or rely on further contextual data to support classification tasks [38].

Objects analysis has been used for analysing image-based decision-making applications. In such
applications objects are identified and analysed individually based on extracted information [39-41].
This extracted information has been used to classify the images into corresponding classes. During the
object analysis, target objects were often defined and all other objects were treated as noise [42]. An
image can be classified into multiple classes depending on the types of objects present within the
image along with the context of the object. In multi-object scenarios, multi-class label approaches
are applied to classify such images [43,44]. In those approaches, one classifier is trained for each
object to be analysed the image. The classified outputs are combined with the probabilities of the
individual classes to classify the image. Object analysis along with semantic representation is also used
for scene recognition [45,46], where the detection of objects and semantic segmentation is applied to
recognise the scene. Apart from these approaches, ontology-based approaches are also used to retrieve
a specified [31] image from the library [47,48]. In the ontology-based methods, computer vision and
manual annotation are used to search the specified image from the library.
Smart Cities 2020, 3 1357

Machine learning algorithms have been widely used for image classification by analysing the
characteristic features of the images and organising them into corresponding categories or classes.
Explainability has been applied in recent years, which allowed control and understanding of a machine
learning model. Sameket et al. [8] presented why explainability had been an effective alternative for
detecting weaknesses in the model. Abdullatif et al. [49] showed that mining big data is challenging,
as this may depend on time. Traditional solutions do not exploit the intrinsic streaming nature of the
incoming data: a continuous, possibly unlimited flow of data that evolves during the time. Stream data
mining algorithms, including fuzzy methods, can incrementally adapt to the non-stationary changes
and data, by creating models that adapt their structure with new data. Moreover, these techniques can
track changes in the incoming data with the ability to remove the outliers from it.

Adding explainability to a model allows to verify the system, understand the weakness and detect
the biases of the model. Explainability helped to understand the models’ processes, how the model
makes any predictions and why it sometimes it fails to make the correct predictions. The use of an
explainable expert system allows extracting the detailed knowledge of the system, which gives the
flexibility of modifying the model with new insights. Explainability defines the justification behind
model predictions, inner workings and approaches of the model enhancement [9]. Model debugging
is one of the approaches of analysing and fixing any errors within machine learning. A visualisation
tool could be applied to understand the structure of the model. The visualisation could also help for
understanding the data that is responsible for the prediction, and hence, clues for further improvement
of the model.

Model refinement by incorporating expert knowledge through expert interaction has been
applied for the improvement of the model. However, to design an explainable model, two major
technical challenges need to be highlighted [19]. The first challenge was to accurately extract features
from noisy and sparse data into the model since it was challenging to address the relationships
between high-level and low-level features and their semantic meaning. The second challenge was to
generate easy and understandable explanation from the multilevel model structure. A knowledge-base
representation was applied to understand heterogeneous entities based on the embedded knowledge
base [50]. The knowledge graph focused on how to represent different types of properties over
heterogeneous entities.

The main goal of the explainable system is to link the human context-based reasoning with facts
to construct contextual explanatory models [51]. Human understanding is applied for the intellectual
understanding of the context apart from data. Humans can understand the context using very few
data and bring reasoning into decision making in a particular task. In other words, the explainable
system provides extensions to the existing system to apply human capabilities and understanding
to make any decision or action. Explainability is used in multi-domain operations to strengthen
confidence, knowledge representation and reasoning. Situational understanding is required to conclude
multi-domain scenarios. However, the situational understanding depends on data coming from the
machine and context considered by humans [52].

The effectiveness of the explainable system has been often restricted because the criteria are
ill-defined and difficult to interpret [20]. Interpretability of the deep learning networks can be
increased by applying learning via human-computer interactions and representation. Oftentimes, the
explainable system is aimed to provide human-readable and interpretable explanations of decision
making [53]. A twin system consists of a black-box built from the machine-learning method, and a
white-box method built from human knowledge has been used for the interpretation of chronic renal
disease since the black-box systems showed limited effectiveness on explaining the reason behind
the predictions or classifications [54]. An adversarial approach was used to modify the explainable
system, where modification of the system was based on the information of the relevant features [16].
Explanations had been provided for understanding the model’s reasoning, which was consistent with
expert knowledge. A hybrid human-in-the-loop approach was applied where machine learning was
Smart Cities 2020, 3 1358

improved using extracted domain expert knowledge [55]. Human-in-the-loop approaches iteratively
apply experts’ input on machine learning with reasoning.

A combination of the logic-based approach and probabilistic machine learning approach is
required to build context-adaptive systems [56]. Integration of these two approaches combines the
strength of each to make an effective explainable knowledge-based system. A user-centric explainable
decision support system was applied, where the system linked the human reasoning process with
intelligent explainable techniques [57]. The system had scaled up the machine learning model with
user understanding and observations of an event. An explainable framework was used for anomaly
detection. In the framework, the deep learning model detected the anomalous event with the confidence
in the description of the detected anomaly and the relevant factors, i.e., the framework had justified
the decisions [58]. Deep Learning or black-box models are not suitable for cognitive cities where
collaborative approaches have been used between the city and its citizens. Information and knowledge
sharing between human and machines are required in cognitive cities. Explainable intelligent systems
can adopt humans knowledge in such collaborative approaches [59].

Convolution Neural network (CNN) models had been applied in image classification tasks [42,60].
While building the CNN model, there had not been any single architecture for CNN development.
The CNN structure had been modelled by altering network parameters such as the number and types
of layers and activation function [61,62]. These parameters had been tuned iteratively to increase
model accuracy. A CNN model as an image classifier takes an unclassified image as an input and
classified the image into a class label based on the feature extracted. CNN is one of the approaches
that has also been applied for the multi-class classification [62,63]. In an application, either one CNN
model or multiple CNN models could be designed based on the task [64,65]. When multiple models
were developed, each model performed a specific task, and the output of each model was combined.

CNN achieved remarkably higher accuracy on many image analysis applications. However,
the network was heavily depended on the number of data that were used for the training [66].
There was a low number of training images in many application domains. Data augmentation is a
commonly-used approach in many machine learning models to virtually increase the training dataset
for the enhancement of the model accuracy along with for avoiding overfitting. Different augmentation
techniques, such as geometric transformations, feature space augmentation, colour space augmentation
and random erasing, were applied. Taylor and Nitschke [67] applied geometric transformation
augmentation, which changed the shape, size and orientation of the original image during the
augmentation. Image augmentation by pairing samples is another approach, where a new image
is formed from an image by overlapping another image randomly selected from the dataset [68].
Auto-augmentation techniques were applied which defined probabilities to allow a function for the
image augmentation in terms of translation, colour normalisation and rotation [69]. Different image
augmentation techniques have been used in different applications; however, in all those applications,
the common objective was to increase the training data set to enhance the model performance.

Semantic technologies have been one of the approaches to explore the real-time complex data
stream, analyse it and provide higher-level of understandings in Smart City environments [70-72].
With the extremely complex nature of IoT data for smart cities such as the variety of representations,
distribution, scales and densities, the semantic web offers a solution by providing a layer of abstraction,
standardisation and semantics to solve interoperability issues [73]. Furthermore, this facilitates the
fusion of a variety of IoT data with existing knowledge from standard data sources and linked
data [74,75]. One of the challenges of smart cities has been integrating a large number of applications.
The conventional applications might have been complex to communicate with multiple objects and
heterogeneous data. Semantic technologies have been applied to integrate multiple heterogeneous IoT
devices for data monitoring real-time events and reasoning to support intelligent systems [76-78], i.e.,
semantic technology has provided the flexibility to integrate heterogeneous data and functionalities
into a common platform.
Smart Cities 2020, 3 1359

At the heart of semantic web applications, ontologies play a key role. In simplistic terms, ontologies
are “partial, the simplified conceptualization of the world as it is assumed to exist” [79], and essentially
involves the formal definition of a domain as a set of concepts and their entity types, properties and
interrelationships [80]. The ontology-based semantic approach has improved the interoperability
between the applications. To integrated many applications in smart cities, ontology catalogue was
used [81]. Ontologies catalogue has allowed researchers to share and reuse domain knowledge.
The ontology-based semantic approach has improved the interoperability between the applications.
The ontological solution uses internal and external knowledge about environment, behaviour and
activities [82]. One of the key motivations for applying ontologies is to ensure heterogeneous sensors
can be integrated effectively during natural disasters and crises [83,84].

Several works have used semantics and ontologies within smart cities for a variety of functions,
e.g., to add value to data collected from sensor and social data streams [85], to combine sensor and
social data streams with machine learning techniques and to facilitate interoperability and information
exchange [83]. A range of projects has explored the use of the semantic web in specific domains,
such as energy and transport [28], mobility and autonomous vehicles [86], traffic jams and delay [87],
surveillance system [88], emergency management [89], environment [90], parking [91], energy [92,93],
water [94] and so on. Several projects and frameworks have also emerged, aiming to exploit semantics
to enable interoperability, e.g., OpenIoT [95], CityPulse [96] and VITAL [97].

Over the past few years, a variety of ontologies have also been developed for use within smart cities
environments for IoT, sensors, actuators and sensor observations, such as [28,98-100]. Over the past
many years, there has been a considerable effort in developing ontologies for defining sensor networks,
with particularly the SSN (Semantic Sensor Network) ontology being one of the most commonly
extended and adapted ones. The SSN ontology is a domain-independent model that covers sensors,
sensing and measurements, and also incorporates other models such as SensorML and Observations
and Measurements (O&M). Many studies have also extended the SSN ontology to adapt to their
specific needs and domains such as environment [101], smart buildings [102], energy monitoring [103]
and transport [104]. Ontologies applied within flooding to incorporate sensor data, although available,
are limited [105-108] and applied in specific scenarios, as noted ina recent systematic review of flooding
ontologies [109]. However, the application of deep learning and semantic web in disaster response
has been limited, primarily aimed at classification and identification of disaster-related information in
social media [110-112] or analysing remote sensing [113] and aerial imagery [114]. The use of semantic
technologies in smart cities has led to discovering new opportunities such as information discovery,
categorisation of events, complex event processing and reasoning for decision making, as the semantic
networks provide a powerful way of transforming knowledge into machine-readable content [115].

3. Flood Monitoring in Smart Cities

Flood monitoring has been one of the major issues in smart cities. Drainage and gully blockages
have been identified as the foremost reason for urban flooding; hence, monitoring of drainage and
gullies is an important aspect of flood monitoring systems. Blockage of drainage and gullies on the
streets and roads is a condition when external objects obstruct the normal flow of water. However,
building and installing electronic sensors to monitor them is complex and not always feasible. Therefore,
an alternative technique is required to monitor drain and gully blockages for the effective monitoring
of flooding instances. Real-time capturing of drainage and gully images using a smart camera,
as shown in Figure 1, and hence, analysing and classifying the image, can detect the potential flooding
threat. The effectiveness of the monitoring depends on the proficiency of the image classification task.
Therefore, an efficient image classification method is required for classifying drainage and gully images
to identify blockage level, and hence, the flooding alert. Drainage and gullies often get blocked due to
accumulation of objects at the street and roads.
Smart Cities 2020, 3 1360

Network

Connectivity
q | a
2

Smart Camera

  

Figure 1. Basic Flow of Flood Monitoring, with an example image of a drain blocked by leaves.

3.1. Major Objects and Their Significance

To learn about the list of major objects typically causing drainage and gully blockages, a workshop

with five experts, working in the domain of drainage and gully blockage and cleaning section in

the Bradford Metropolitan District Council, was organised. In discussion with the experts during

the workshop and after analysing more than 50 randomly selected images. Four major objects,

noe

namely “Leaves”, “Mud”, “Plastic Bag and Bottle” and “Water”, were identified as the most common

objects on monitoring drainage and gully blockage. These four major objects and their significance in

drain and gully blockage is summarised as follows:

i.

Leaves: Leaves were raised as one of the most prevalent problems when it comes to blockages.
Once leaves enter into the drainage, they become less of a problem, as they can pass through the
sewage system relatively easily. The real problem is when the leaves gather on top of a drainage
system and begin to form dams if they cannot pass through, as shown in Figure 2.

 

Figure 2. Sample image for object: Leaves.

Slit (Mud): Silt is solid, dust-like sediment that water, ice and wind transport and deposit. Silt is
made up of rock and mineral particles that are larger than clay but smaller than sand, as shown
in Figure 3. During the discussion, silt was discussed as a major problem for drainage and
gully blockage if they were not sufficiently cleaned regularly and were allowed to build up.
Furthermore, if silt accumulated for a longer period, it can be fertile enough for vegetation to
grow relatively easily, which can cause further problems with the drainage system.

 

Figure 3. Sample image for object: Mud.
Smart Cities 2020, 3 1361

iii, Plastic and Bottles: Plastic and bottles were identified as another major risk to drainage system
due to the capability of these objects being able to cover the drainage and restrict the water
flow into the sewage system, as shown in Figure 4. Further discussions revealed that bottles
by themselves are not an issue, but in combination with other litter or debris, raise the risk of
blockage. As discussed with experts, bottles would typically be pushed up against the entryways
to the drainage and gully, leaving the access way either blocked or restricted.

 

Figure 4. Sample image for Object: Plastic Bag and Bottle.

iv. Water: Finally, water was identified as one of the four major objects to be monitored while
deciding the drainage and gully blockage. The presence of water along with other objects and
their coverage, as shown in Figure 5, is the key factor in deciding the blockage level.

‘ ]

   

Figure 5. Sample image for object: Restricted water flow.

3.2. Convolutional Neural Network for Object Coverage Detection

In this flood monitoring application, the drainage and gully images show multiple objects.
The presence of these images causes blockages of drainage and gullies, and hence, urban flooding.
Detection of the objects with their coverage level within the image is crucial for detecting drainage and
gully blockages. An efficient image classification method is required for classifying drainage and gully
images. Considering the need for object coverage detection based on image features, CNN models
have been built. The details of the CNN models are presented in Section 5.4.1.

3.3. Semantics for Flood Monitoring

In our solution, semantic techniques enable understanding the characteristics of objects and the
context of these objects, with the use of explicit formal rules. Decision making on detection of drainage
and gully blockage is a contextual problem, as it depends on the presence of the types of objects and
their proportion. For example, the detection of water coverage in an image is not enough to classify the
blockage level in a flooding context. The presence of other objects in combination with accumulation
of water define the severity of drainage and gully blockage, i.e., the presence of other objects and their
relationship is crucial on decision making. The object coverage levels detected by CNN models do not
provide the explainability on classification. Expert knowledge adds the explainability of the system for
decision making. To bring control over decision making, semantic representation and formulation of
semantic rules are defined. With the knowledge elicitation process, involving interviewing experts,
Smart Cities 2020, 3 1362

the relationship among objects coverage level and the context of drainage and gully blockage instances
have been articulated with semantic representations. Semantics rules bring the knowledge from the
domain experts to the system on decision making to classify the image into a class label. The details on
semantic rules formulation are presented in Sections 5.5 and 5.6.

4. Hybrid Image Classification Models with Object Coverage Detectors and Semantic Rules

We propose a novel hybrid image classification model that classifies the drainage and gully
images into a class label. This hybrid approach is a combination of machine learning and semantic
techniques. In this hybrid approach, the machine learning algorithm is used to detect object coverage
proportion within the image, whereas the semantic technique has been used to define the relationship
among the objects based on the detected object coverage level. The classification process of this
proposed hybrid image classifier is described with the conceptual flow diagram, as shown in Figure 6.
In this hybrid image classifier, the classification method consists of three computational steps: “Object
coverage detection”, “Semantic representation and rule base formulation (indicated by ‘Rule Base’)”
and “Inference Engine”.

 

Figure 6. Conceptual flow diagram of Hybrid Image Classifier.

4.1. Object Coverage Detection

Drainage and gully may get blocked with litter materials such as leaves, mud, plastics and
bottles. The list of individual objects causing the blockage is considerable, and hence, identifying every
object’s coverage is not practically feasible within object coverage detection and image classification.
Therefore, major objects coverage detection is crucial to develop an efficient image classification model.
Information on the presence of the objects within an image is not sufficient to apply conclusive reasoning
on deciding the blockage level. To strengthen the classification decision, the level of coverages of
the detected object within the image is used. Coverage detectors are built using CNNs that classify
the image into one among four coverage levels namely: Zero, One, Two and Three. These coverage
detectors are iteratively modelled, trained and tested with sample images.

To detect the presence of objects within an image, the object coverage detector for each object was
applied. Four object coverage detectors (one for each object) are built using CNN, where each detector
has been modelled, trained and tested with sample images. Each object coverage detector detects the
presence of those objects within the image.

4.2. Semantic Representation and Rule Base Formulation

Semantic rules for the image classification are defined based on expert knowledge captured during
the workshop. During the workshop, experts were asked to categorise the sample images into three
class labels: “fully blocked”, “partially blocked” or “no blockage”. Experts were also asked to identify
the objects in the images and provide the reasoning on classifying the image into the corresponding
class labels. Five experts were used to classify single images and as a result, we observed experts
Smart Cities 2020, 3 1363

classifying images into different class labels. In such cases, the majority count approach was applied
for knowledge extraction, i.e., the image was classified into the class label with the majority vote.

4.3. Inferencing and Image Classification

Inferencing is applied to classify the image based on object coverage and semantic rules.
Inference engine selects appropriate rules from the rule-base. The selection of rules depends on
the detection of each object’s coverage level. The most appropriate rule is selected to classify an image
into the corresponding class label.

5. Methodology

In this section, preparation of training data for the image coverage detectors model, implementation
details using machine learning and the use of semantic rules for image classification is presented
in detail. Performance of the hybrid classification model in terms of accuracies on object coverage
detection and image classification are also analysed. Moreover, the classification results are compared
with the machine learning-based image classification model.

5.1. Data Construction

Preparing data sets for object detector was challenging due to the lack of public datasets on
drainage and gully blockages. To overcome this, images are collected from publicly available image
sources such as Google Image, YouTube videos and other public sources. These images are retrieved
using multiple keywords, such as ‘drainage block’, ‘rubbish and drain blockage’ and ‘drain-grate
and flooding’. It was observed that all the collected images were not useful considering our problem
domain, owing to some being noisy, unwanted and blurred images. To remove those unwanted,
noisy and blurred images from the dataset, manual supervision was applied for dataset preparation.
During the manual supervision, each image was analysed in terms of their size, quality and how
closely they are related to the problem domain. After data pre-processing, the image dataset for each
object type was prepared.

5.2. Image Augmentation

Convolutional Neural Network (CNN) model with higher accuracy, lower training and validation
loss are required. To build an effective CNN model, a larger training dataset is required [116]. However,
there has been a limitation on accessing a large number of images for model training. To achieve this,
image augmentation has proven to be a powerful technique to enrich the image dataset. In other words,
image augmentation has found to be an effective alternative to enhance model performance [67,117,118].
Supported by the reported improvement in model performance using image augmentation in the
literature, we have chosen to use image augmentation to build an improved CNN model for object
coverage detector accuracy.

The main objective of applying image augmentation is to increase the training image dataset.
Different image augmentation techniques such as geometric transformation, random erasing,
colour space transformations and feature space augmentation have been applied for image
augmentation. “ImageDataGenerator” class from Keras library has been used as an alternative
for image augmentation to improve the image classification accuracy [69,119]. Images are augmented
by changing augmentation parameters such as zoom, height shift, width shift, shear and brightness
by altering the values as shown in Table 1. Random values are set within the range for different
parameters to increase the diversity in image dataset. By applying image augmentation, the number of
training images was increased by approximately 10 times.
Smart Cities 2020, 3 1364

Table 1. Image Augmentation parameters and value range.

 

 

Parameter Value (Range)
Rotation Range 5-20
Width Shift Range 0.1-0.25
Height Shift Range 0.1-0.25
Shear Range 0.05-0.2
Zoom Range 0.05-0.15
Horizontal Flip True
Fill Mode Nearest
Data Format Channel Last
Brightness Range 0.05-1.5

 

5.3. Image Annotation and Coverage Level

One of the major issues on object coverage detection is to find the coverage proportion of each
object within the image. The object count [120,121] method has been applied as an approach to find
the density or proportion of object area within an image. However, the object count approach for small
objects appearing in a group has been a limiting factor for object count analysis [122]. In this work,
the object count method is not viewed as a feasible option, as leaves and plastic and bottles are small in
size and often appears as a group. Additionally, water and mud cannot be counted in discrete numbers.
The object count method also does not appropriately address the coverage area proportion. This can
be illustrated with the example of analysing coverage of leaves as shown in Figure 7a,b. In these
two figures, both the images have approximately 53 % leaves coverage of leaves. However, it can be
observed that there is a considerable difference in the number of leaves in each image. Furthermore,
it has also been visually observed in those figures that there is no uniform pattern on image coverages.
The size and shape of the objects vary from image to image as well. Therefore, object counts within the
image would not be a feasible option to categorise the image into different levels.

  

(b)

Figure 7. (a,b). Leave Coverage within the image, indicating the same coverage, but with a different

count of objects.

To overcome these complexities, image annotation techniques have been applied that mark the
object of interest within the images. Using the annotation tool “labellmg (https://github.com/tzutalin/
labellmg)”, images were manually annotated; an example of an annotated image is shown in Figure 8.
During the annotation, boxes (boundaries) are created by covering the objects in the image. There are
one or more than one boxes for individual objects within the image depending on the position and
orientation of the objects. Mostly, each object covers some proportion of the total area within the image.
However, there are overlapping boxes, mainly with the presence of water within the image.
Smart Cities 2020, 3 1365

File Edit View Help

 

Dupicats
RECON

 

 

 

Figure 8. Screenshot of “Image Annotation”.

The next challenge was to categorise the annotated image into different coverage levels. An analogy
of the object count method within the frame, used for density calculation, has been adopted to define
the coverage level [123]. The coverage area percentage was applied to categorise the images into
different levels. To find the coverage percentage of each object within the images, the area of each
box has been calculated for each object based on box coordinates. The areas of individual objects
are summed, and hence, the percentage of coverage of each object within the image was calculated.
Based on the coverage percentage, as listed in Table 2, annotated images have been categorised into
four coverage class levels. Example of coverage level corresponding coverage percentage of three
images, shown in Figure 9a-c, is listed in Table 3.

Table 2. Coverage level and coverage percentage.

 

 

Coverage Level Coverage Percentage
Zero Coverage Percentage < 5%
One 5% <= Coverage Percentage < 20%
Two 20% <= Coverage Percentage < 50%
Three Coverage Percentage >= 50%

 

   

(a) (b)

Figure 9. Sample Images of Different Coverage Level (a): Two, (b): One, (¢): Two.
 

 

Smart Cities 2020, 3 1366
Table 3. Example of Object Coverage Percentage and Corresponding Level.
Plastic
: Leaf Coverage $Bottle Coverage Mud Coverage Water Coverage
Figure Coverage Coverage Coverage
6 Level Coverage Level ° Level 6 Level
(%) (%) (%) (%)
9.a 40.25 Two 0 Zero 0 Zero 43.92 Two
9.b 14.19 One 0 Zero 15.02 One 42.95 Two
9.¢ 22.54 Two 5.25 One 0 Zero 8.40 One

 

5.4. Coverage Detector Implementation

Object coverage detectors have been modelled, trained and implemented to detect the coverage
level of each object within the image. Coverage detector model for each object, leaves, plastic and
bottle and mud and water, has been designed as a CNN model.

5.4.1. Convolutional Neural Network

CNN is a feed-forward neural network made up of layers with learnable parameters including
weights and biases. A CNN is typically composed of multiple layers interconnected to each other.
Different layers that have been applied in CNN models are summarised as below.

Convolutional layers: Convolution layer is the first layer in the CNN model and works as a
feature extractor to extract features, such as edges or textures from the input image. This layer uses
a convolution operation on the input image and passes the extracted output to the next layer in the
model. The three-dimensional input in terms of height, width and the number of channels are applied
to the convolutional layer to get the output as a feature map. In this work, the number of convolution
layers was adjusted based on the model training accuracy. After several iterations and adjustment of
model parameters and based on the best training accuracy, two convolution layers was set for leaves
and water coverage detector CNN models, whereas three convolutional layers were set for plastic and
bottles and mud coverage detector CNN models.

Pooling layers: a pooling layer is a down-sampling layer, which is commonly applied after
the convolution layer and takes feature maps as its input. The main purpose of applying pooling
layers is to minimise the spatial resolution of the feature maps. Max pooling was implemented to
progressively reduce the spatial size of the feature maps, and hence, reduce the number of parameters
and computation in the network.

Flatten Layer: a flatten layer transforms the entire pooled feature map matrix into a single
column. The transformed values are then fed to the next layer in the network for further processing.
Flatten layers are applied to increase the training accuracy.

Dense Layer (a fully connected layer): several convolutional pooling layers and flatten layers are
applied in combination to extract features. The dense layer that appears after these layers infers the
feature representations and perform the reasoning. In this work, the dense layer classifies the input
image into four levels. Softmax layer has been used to calculate the final probabilities of each class.

Activation function: the activation function decides whether or not a node in the network should
be activated. The activation function introduces non-linearity to the network, which analyses whether
the information that a node is receiving should be stored or ignored. ReLU function was applied as the
activation for the CNN models.

Four CNN models are designed by altering the number of layers. The CNN model architecture,
for the leaves coverage detector, is shown in Figure 10. Other coverage detectors have differed only in
terms of the number of convolution and pooling layers that were used.
Smart Cities 2020, 3 1367

Feature Feature Feature Feature Hidden Hidden
Inputs maps maps maps maps units units Outputs
3@i64x64 64@62K62 64@31x31 64@29x29 b64eqpl4x14 12544 4096 4

 

a

     

      

Convolution
3x3 kernel

lution Max-pooling Flatten Fully Fully
ernel 2x2 kernel connected connected

  

Figure 10. Convolutional Neural Network (CNN) model (Leaves).

5.4.2. Model Regularisation and Parameter Selection

One of the most common problems in CNN training is overfitting. Regularisation has been
applied as one of the approaches for improvement in CNN training accuracy [124]. Regularisation
modifies the model’s learning parameters such that the performance of the model improves during
models training. In this work, different parameters have been applied for the model regularisation:

L2 (lambda): L2 is also known as weight decay. It has been applied so that the weights concentrate
toward zero. Lambda regularisation parameter value is set as 0.001.

Dropout: dropout randomly removes the output of some of the hidden nodes during the training
and only updated the remaining nodes. We have defined 0.25 to 0.5 as the probability of dropout
during the training. After analysing different ranges, a dropout probability of 0.4 was set for the model,
as it shows the best training accuracy.

Image Augmentation: High number of training image dataset has been generated using data
augmentation, as discussed in Section 4.2.

Padding: padding is mainly applied to add columns and rows with zeroes to keep the spatial
sizes the same after convolution. Padding has been applied such that the output has the same length
as the original input.

5.5. Semantic Representation

Semantic representation of the blockage instances was created after the analysis of an individual
expert’s view on object identification, classification and reasoning. Structured knowledge is generated
to represent the conceptual model on blockages of drainage and gully. The relationship between objects
coverage level and classification instances: “Fully Blocked”, “Partial Blocked” and “No Blockages”
instances have been presented with semantic representation, as shown in Figure 11. The concept map
reflects the top-level concept of drainage and gully blockages. This representation shows the major
components of an image in a certain combination, which defines the instances of blockages.

is_of
has

Objects Gull
can_be
has

Object 1

,
. can_cause —#| Fully_Blocked

Object 2 -_

 

can_be

    

—
No_ Blockage
Partially Blocked

Figure 11. Concept Map with the top-level concept related to Blockage.
Smart Cities 2020, 3 1368

“Fully Blocked”, “Partially Blocked” and “No Blockage” instances are formulated based on
objects and their coverage within the image. Using the experts’ reasoning in image classification into
corresponding class labels, the semantic rule-base has been created. The drainage and gully blockages’
instances have been converted into the OWL (Web Ontology Language) ontology as shown in Figure 12.

© Partially_Block
ed

 
    
  
   
    

© Drainage_Classi
fication

aN

  
  
 

oa

© Object aw \
® owl:Thing b ~~

 

 

| © Fully_Blocked

\ j

\
— NS
© Drain_Gully o gully_camera_2
9
\
© No Blockage

if
® Piastic_Bottle

 

   
   

           
 
    

 
 

® Mud

 

Figure 12. OWL ontology of drain and gully blockage.

5.6. Rule-Based Formulation

Experts have highlighted that an image is classified as “Fully Blocked” when there are many
objects that cover most of the image portion with the sign of severe restriction of water flow through
the drainage system, i.e., water cannot enter the drainage system due to objects that caused a blockage.
Similarly, an image is classified into the “Partially Blocked” label when the water is hindered from
entering the drainage system but is still able to enter to some degree. In a drainage system with this
classification, there are fewer objects that block it, or the combination of objects are not as severe.
An image is classified with the “No Blockage” category when water can freely flow into the drainage
system and no obstruction-causing objects are detected. Considering that the flooding condition,
the experts have also highlighted that the drains and gullies do not always cope with the amount
of water, but also the location and rain duration are among the parameters that define the flooding
scenarios. In this hybrid image classification model, the location and rain duration parameter is
ignored, since with image analysis these parameters cannot be detected. Mutually exclusive semantic
rules are defined to classify the image instances based on object coverage detection using experts
knowledge of image classification. SWRL (Semantic Web Rule Language) rules have been created
for all the possible instances depending on the presence of object coverage and their combinations;
Smart Cities 2020, 3 1369

an example of one of such rules are presented in Figure 13. We discuss the rules in more detail in
Section 6.2.

Name

IR1

Comment
Partial_Blocked
Status

\Ok
Image(?p1) “ has_coverage(?p1, ?c1)* has_object(?c1, water) “ has_coverage_level(?c1, one) “
has_coverage(?p1, ?¢2) “ has_object(?c2, plastic_bottle) * has_coverage_level(?c2, one) ->
Partially_Blocked(?p1)

 

 

 

 

Figure 13. Sample of SWRL rule.

6. Experimental Design and Result Analysis

The simulation was performed on a machine with Intel(R) Core(TM) i7-8750 HCPU @2.20 GHz
processor with 15.5 GB (usable) of RAM running on Windows-10 64-bit operating system. For CNN
model training, the experimental setup for image size, the number of epochs and batch size have
been set by altering their values in different iterations. Input image sizes 32, 64, 96 and 128 have been
applied in different iterations, the number of epochs caries from 100 to 500, whereas the batch size
varies from 8 to 32. The models with the best training and validation accuracy with the corresponding
setup have been used for object coverage detection.

6.1. Object Coverage Detection Training

For the evaluation of models’ training, training accuracy, training loss, validation accuracy and
validation loss have been analysed iteratively for each object coverage detectors. The models with
the best training accuracy, training loss, validation accuracy and validation loss, as listed in Table 4,
have been used for the object coverage detection. It was observed that these training performance
parameters are not the same for each object coverage detectors. In other words, there has been a
variance in the performance level of object coverage detectors such as training accuracy ranges from
0.9626 to 0.9983 and validation accuracy ranges from 0.7727 to 0.8955. The training accuracy and loss
and validation accuracy and loss are analysed plots over 300 iterations are presented in Figure 14a—d.
These plots showed that the model’s accuracies improve significantly, by up to 60 iterations, following
which the accuracy and loss performance appears to stabilise. Therefore, the training of the models is
limited to 300 iterations only.

Table 4. Training and Validation Performance of Coverage Detector.

 

 

Object wo wos tas Validation

Detector Training Loss Training Accuracy Validation Loss Accuracy
Leaves 0.2081 0.9633 1.4371 0.8421
Mud 0.0335 0.9880 1.1784 0.7717
Plastic & Bottle 0.1250 0.9626 1.5632 0.7976

Water 0.1208 0.9983 0.9052 0.8955

 
Smart Cities 2020, 3 1370

 

a

yi

 

 

 

(b)

W

TA TR v4

 

 

 

 

 

0 50 100 150 200 250
(d)

Figure 14. Training and Validation plot of (a) Leaves, (b) Mud, (c) Plastic and Bottles and (d) Water.

 

6.2. Analysis of Semantic Rules Implementation

Class labels generated by the inference engine on the test images are compared with the class
label defined by experts for those test images. These test images belong to all three categories with
the presence of objects in many ways; some images have only one of the listed objects, whereas other
images have more than one listed object. Figure 15 shows the examples of implementation of semantic
rules for the image classification. The following examples illustrate how the matched rules are applied
based on the object coverage detection and hence the classification decision. Analysis of rule selection is
presented for three sample images of the category “Fully Blocked”, which has been correctly classified
as “Fully Blocked” and others two images which are incorrectly classified as “Partially Blocked” and
“No Blockage”. In the scenarios, when no rule is selected the default rule “No Blockage” is used for
defining the class.

In the first image (Figure 15a), the object coverage detector detects Mud level three, Leaf level one,
Plastic and Bottle Level zero and Water Level zero. The combination of these selects the following rule
and correctly classified the image.

Rule: Image(?p1) * has_coverage(?p1, ?c1) * has_object(?cl, leaves) * has_coverage_level(?cl, one)
“has_object(?c2, mud) * has_coverage_level(?c2, three) -> Fully_Blocked(?p1)

In the second image (Figure 15b), the object coverage detector detects Mud level one, Leaf level
zero, Plastic $ Bottle Level zero and Water Level zero and selects the rule:

Rule: Image(?p1) * has_coverage(?p1, ?c1) * has_object(?c1, water) * has_coverage_level(?c1,
zero) ~ has_object(?cl1, plastic_bottle) * has_coverage_level(?c1, zero) * has_object(?c2, mud) *
has_coverage_level(?c2, one) -> Partially_Blocked(?p1)

Similarly, for the third image (Figure 15c), the object coverage detector detects Mud level zero,
Leaf level zero, Plastic $ Bottle Level zero and Water Level zero and selects the rule:

Rule: Image(?p1) * has_coverage(?p1, ?c1) * has_object(?cl, water) “ has_coverage_level(?c1, zero) *
has_object(?c1, leaves) * has_coverage_level(?c1, zero) * has_object(?cl, leaves) * has_coverage_level(?c1,
zero) “has_object(?c2, mud) * has_coverage_level(?c2, zero) -> No_Blockage(?p1)
Smart Cities 2020, 3 1371

Selection of the rule depends on the detection of the coverage level by the CNN model.
The inference engine selects the appropriate rule form the rule base for the image classification.
Semantic rule implementation has both correct and incorrect classification of all three instances of
drainage and gully blockage.

   

(b)
Figure 15. (a-c). Sample of Image Classification (a) FB as FB (b) FB as PB (c) FB as NB. FB: “Fully
Blocked”, PB: “Partially Blocked” and NB: “No Blockage”.

6.3. Hybrid Class Performance Analysis

In general, the image classifier models’ accuracies have been a one-step evaluation [60,64,65,125].
The model performance is evaluated in terms of correctly classifying the test image into corresponding
class labels. Those models were built using machine learning algorithms. Therefore, the models’
performance was evaluated in terms of model test accuracy. In this proposed hybrid image classifier,
the performance of the model was analysed in two stages. First, the accuracy of the object coverage
detector is analysed, and second, the accuracy of the hybrid image classifier by applying the semantic

rules is analysed.

6.3.1. Accuracy of the Object Coverage Detector

Accuracy of the detection of objects’ coverage level within an image is crucial for the
implementation of the semantic rules for image classification as the accuracy of semantic rules
depends on the accuracy of objects coverage level detection. Individual object coverage detectors have
been tested with 21 sample annotated images. The result is summarised with the combined confusion
matrix of object coverage detectors in Table 5.

Table 5. Combined confusion matrix of coverage detectors.

 

 

Object/Level Zero One Two Three
Zero 75% 25% 0% 0%
Leaves One 20% 80% 0% 0%
Two 0% 0% 60% 40%

Three 0% 0% 33.14% 66.34%

; Zero 71 A% 14.28% 7.15% 7.15%
Plastic One 0% 50% 0% 50%
Bottles Two 0% 20% 80% 0%

Three 0% 0% 33.33% 66.67%
Zero 92.3% 7.7% 0% 0%

Mud One 33.33% 33.33% 0% 33.34%
Two 50% 0% 50% 0%
Three 25% 25% 0% 50%
Water Zero 75% 0% 25% 0%
One 50% 50% 0% 0%

Two 33.33% 0% 50% 16.67%

Three 0% 0% 33.33% 66.67%

 
Smart Cities 2020, 3 1372

From the confusion matrix, it is clear that detector models do not have uniform accuracy.
The accuracies of level Zero and level Three was found to be relatively higher in comparison to the
accuracies of level One and level Two.

Level Zero has no or very low coverage of the representative object within the image, to generate
characteristic features of that object during the model training, which could lead to higher accuracy
for level Zero detection. For level Three coverage detection, there was more than 50% coverage of
the representative object, which reflects the significant characteristic features representation of the
object. Because of this, the modes have higher accuracy than level One and level Two during the model
training. For level One and Two, coverage of representative objects is less, i.e., there are other objects or
image background features that were extracted during the model training, and hence, there is a lower
range of test accuracy. The analysis showed that the object detection accuracy ranged from 61.9 % to
76.2% and the aggregated accuracy of the object coverage detectors was 69.04%, as plotted in Figure 16.

Coverage Detector Test Accuracy

90
80 76.2
70 61.9
60
50
40
30
20
10

0

Leaves Plastic & Mud Water Overall
Bottles

Figure 16. Individual object coverage test accuracy and overall coverage detectors accuracy.
6.3.2. Accuracy of the Hybrid Image Classifier

In the next stage of the image classifier, 26 test images are classified into corresponding class labels
using semantic rules. These test images are selected such that they reflect the best match scenarios
for drainage and gully blockage that can cause flooding. The classification accuracy of the proposed
hybrid classifier has been compared with machine learning-based classifier based on deep learning.
The performance has been evaluated on the accuracy on correctly classifying the Fully Blocked (FB)
image as FB, the Partially Blocked (PB) image as PB and No Blockage (NB) images as NB, along with
wrong classifications such as FB images classified as PB, PB images classified as NB and so on, as shown
in Figure 17. The overall accuracy of the machine learning-based classifier and hybrid classifier has
also been compared, also shown in Figure 17.

Figures 18-20 present a further sensitivity analysis in terms of precision, recall and F-scores for
individual instances of our classifier. We observe that the hybrid classifier outperforms the machine
learning classifier.

Analysing Figure 17, it can be observed that the hybrid image classifier improved performance
compared to that of machine learning-based classifier. The hybrid classifier has better accuracy on
all the true positive classification, that is, classifying “Fully Blocked” as “Fully Blocked”, “Partially
Blocked” as “Partially Blocked” and “No Blockage” as “No Blockage”. The overall accuracy of the
hybrid classifier is 69.23%, which is an improvement of about 2% accuracy compared to the machine
learning-based classifier. It has been reported in the literature that machine learning-based classifier
Smart Cities 2020, 3 1373

accuracy depends on the number of training dataset used for CNN model training [126]. Since there
have been fewer training datasets available for CNN model training, the machine learning-based model
has the limitation of achieving higher accuracy. Apart from adjusting the model's training parameter,
there has been no other explainable reasoning or control for model training to enhance accuracy.
Sensitivity analyses in terms of precision, recall and F-score for “Fully Blocked”, “Partially Blocked”
and “No Blockage” have shown improved performance on the classification for each instance when a
hybrid classifier has been applied.

Comparision Plot of Classifier Performacne

80
70

60
50
40
30
20
10

0

&

@ Machine Learning Based Classifier

@ Hybrid Classifier (Machine Laerning + Semantic)

Figure 17. Image classification accuracies comparison between “Machine Learning-Based Classifier”
and “Hybrid Classifier (Machine Learning + Semantic). FB: “Fully Blocked”, PB: “Partially Blocked”
and NB: “No Blockage”.

Precision-Plot

0.8
0.6
0.4
0
& sa 2 s s
Ss s AS soe 2
ra e se gf e
¥ e& as Pd &
s &

@ Machine Learning Based Classifier

@ Hybrid Classifier (Machine Laerning + Semantic)

Figure 18. Precision Plot of “Machine Learning-Based Classifier” and “Hybrid Classifier (Machine
Learning + Semantic). FB: “Fully Blocked”, PB: “Partially Blocked” and NB: “No Blockage.
Smart Cities 2020, 3 1374

Recall Plot

0.8
0.7

0.6
0.5
0.4
0.3
0.2
0.1

0

Recall FB Recall PB Recall NB Macro-Recall Weighted-Recall

M@ Machine Learning Based Classifier

@ Hybrid Classifier (Machine Laerning + Semantic)

Figure 19. Recall Plot of “Machine Learning-Based Classifier” and “Hybrid Classifier (Machine
Learning + Semantic). FB: “Fully Blocked”, PB: “Partially Blocked” and NB: “No Blockage.

F1-Score Plot

0.8
0.7

0.6
0.5
0.4
0.3
0.2
0.1

0

F1-Score FB F1-Score PB Fi-Score NB Macro-FiScore Weighted-F1
Score

Machine Learning Based Classifier @ Hybrid Classifier (Machine Laerning + Semantic)

Figure 20. F-Score plot of “Machine Learning-Based Classifier” and “Hybrid Classifier (Machine
Learning + Semantic). FB: “Fully Blocked”, PB: “Partially Blocked” and NB: “No Blockage.

In the hybrid model, the accuracy analysis of a hybrid classifier is a two-stage accuracy analysis.
The overall accuracy of the classifier depends on the accuracy of the object coverage detectors and
implementation of the semantic rules. In this work, the context of the object presence and their coverage
levels have been important to define the class level. Use of semantic rules provides the flexibility to
apply expert knowledge for context analysis. Expert knowledge in terms of semantic rules reflects the
context of the image, which helps to correctly classify an image into the corresponding class label.

In the literature, the explainable AI presents some challenges in terms of it being used as a
selective decision making that focuses on explanations and background knowledge [127], a large
amount of information [128] and using case-specific decision making [129]. Moreover, we have faced
other challenges as well in this work which might be useful to other researchers working on utilising
Explainable AI for developing Smart Cities’ solutions. Availability of domain knowledge in the form
Smart Cities 2020, 3 1375

of ontologies or knowledge graphs: in our case, we needed a domain-specific ontology that reflects
the flood case scenarios. We had to create this ontology from scratch in our work, as the existing
ontologies do not cover the case we have worked on. Access to domain experts: if no open-domain
knowledge about the problem areas under investigation exists then domain knowledge in the form
of ontologies have to be created with close consultation with experts. This is also often an iterative
process. Skillset in knowledge engineering: knowledge engineering is a complex field, and capturing
tacit knowledge is a complex task requiring specialist skillsets to build Explainable AI-based systems.

7. Conclusions and Future Work

In this work, we have made a case for ‘Explainable Al’ with a hybrid image classification model
consisting of ontological representation of the domain including rules captured with the help of domain
experts and a DL-based classifier. Such a hybrid model is applied in a real-world use case involving
flood monitoring application in the context of a Smart Cities EU project. The hybrid model is based
on machine learning and semantic technology, which classifies drainage and gully images into either
of the following three blockage instances: “Fully Blocked”, “Partially Blocked” or “No Blockage”.
We captured expert input as semantic rules and used them in our hybrid approach to identify four
major objects primarily responsible for drainage and gully blockages. A set of images were classified
into a classification label based on the knowledge elicitation process with experts in this project.
Semantic web rule language was applied to define relationships between various objects’ coverage
level within the image. In this hybrid image classification task, the classification process was divided
into two phases. In the first phase, we applied object overage detection, in which object detectors were
modelled and trained using CNN. To build our CNN models, publicly available images were manually
annotated and then used for training. The annotated images were classified into four coverage levels
based on the coverage percentage of each object within the image. In the second phase, semantic rules
were applied based on the coverage of the object within the image to classify the image instance. The
accuracy of our proposed hybrid image classification model was analysed in two stages. The accuracy
for both object coverage detectors and the hybrid classifier based on inferencing semantic rules have
been analysed. The accuracy analysis demonstrated an improvement in the accuracy of the hybrid
classifier in comparison to the machine-based image classifier.

The major advantage of the hybrid image classifier is that it gives the flexibility to incorporate
experts’ knowledge in the classification process. In the machine learning-based classifier,
the classification performance depends only on the training of the model, whereas the hybrid
classifier not only shows the improvement in classification accuracy but also adopts the expert’s
knowledge. The analysis showed that some rules correctly predict the class label as “Partially Blocked”,
whereas some rules predict incorrect class labels, for example classifying a “Partially Blocked” image
into a “No Blockage” image. The hybrid model transparently showed which rules perform well and
which rules need to be revised.

For our future work, the accuracy of both the object coverage detectors and hybrid classifier will
be enhanced. The accuracy of coverage detectors will be improved by adding a higher number of
application-focused images for training. Besides these four object coverage detectors, other object
coverage detectors, such as grate, can also be included. The rules will be updated by revisiting rules
that have been identified as the main contributors to low accuracy. The rules will be re-evaluated and
rewritten to improve the overall classification accuracy. Future work will also explore the possibility of
including additional contextual information, such as weather and rainfall (higher rainfall may increase
surface run-off), seasonality (autumn involves increased leaves) or the occurrence of local events
(local street events may increase litter), which could further help improve our approach. Fuzzy rules
can be applied for the knowledge representation and rules generations when the domain knowledge
has some kind of vagueness.
Smart Cities 2020, 3 1376

Author Contributions: D.T.: conceptualisation, methodology, software, validation, formal analysis, investigation,
writing—review and editing, supervision, project administration, funding acquisition. B.K.M.: conceptualisation,
methodology, software, validation, formal analysis, investigation, writing—review and editing, visualisation. A.A.:
conceptualisation, methodology, software, validation, formal analysis, investigation, writing—review and editing.
$.S.: validation, supervision, project administration, funding acquisition. $.M.: formal analysis, investigation,
writing—review and editing. All authors have read and agreed to the published version of the manuscript.

Funding: This research was funded by Interreg EC grant as part of Smart Cities and Open Data REuse
(SCORE) project.

Acknowledgments: The research presented in this paper is funded by the European Commission Interreg project
Smart Cities and Open Data REuse (SCORE). Authors would like to thank the five experts from Bradford City
Council who took part in the knowledge elicitation and evaluation process. We would also like to thank Aberdeen
City Council representatives in the SCORE project for their input to identify the generality of the application.

Conflicts of Interest: The authors declare no conflict of interest.

References

1. Souza, J.T.; Francisco, A.C.; Piekarski, C.M.; Prado, G.F. Data Mining and Machine Learning to Promote
Smart Cities: A Systematic Review from 2000 to 2018. Sustainability 2019, 11, 1077. [CrossRef]

2. | Chakraborty, P.; Adu-Gyamfi, Y.O.; Poddar, S.; Ahsani, V.; Sharma, A.; Sarkar, S. Traffic Congestion Detection
from Camera Images using Deep Convolution Neural Networks. Transp. Res. Rec. J. Transp. Res. Board
2018, 2672, 222-231. [CrossRef]

3. Yuan, Z.; Zhou, X.; Yang, T. Hetero-ConvLSTM. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, London, UK, 19-23 August 2018; Association for
Computing Machinery (ACM): New York, NY, USA, 2018; pp. 984-992.

4, Shukla, U.; Verma, S.; Verma, A.K. An Algorithmic Approach for Real Time People Counting with Moving
Background. J. Comput. Theor. Nanosci. 2020, 17, 499-504. [CrossRef]

5. Chen, Q.; Wang, W.; Wu, F; De, S.; Wang, R.; Zhang, B.; Huang, X. A Survey on an Emerging Area:
Deep Learning for Smart City Data. IEEE Trans. Emerg. Top. Comput. Intell. 2019, 3, 392-410. [CrossRef]

6. | Simhambhatla, R.; Okiah, K.; Kuchkula, S.; Slater, R. Self-driving cars: Evaluation of deep learning techniques
for object detection in different driving conditions. SMU Data Sci. Rev. 2019, 2, 23.

7.  Foody, G.M.; Mathur, A. A relative evaluation of multiclass image classification by support vector machines.
IEEE Trans. Geosci. Remote Sens. 2004, 42, 1335-1343. [CrossRef]

8 Samek, W.; Wiegand, T.; Miiller, K.R. Explainable Artificial Intelligence: Understanding, Visualizing and
Interpreting Deep Learning Models. arXiv 2017, arXiv:1708.08296.

9. Choo, J.; Liu, $. Visual Analytics for Explainable Deep Learning. IEEE Eng. Med. Biol. Mag. 2018, 38, 84-92.
[CrossRef]

10. Shahrdar, S.; Menezes, L.; Nojoumian, M. A Survey on Trust in Autonomous Systems. In Advances in
Intelligent Systems and Computing; Springer Science and Business Media LLC: Cham, Switzerland, 2018;
pp. 368-386.

11. Winikoff, M. Towards Trusting Autonomous Systems. In Lecture Notes in Computer Science; Springer Science
and Business Media LLC: Cham, Switzerland, 2018; pp. 3-20.

12. Al Ridhawi, 1; Otoum, S.; Aloqaily, M.; Boukerche, A. Generalizing AI: Challenges and Opportunities for
Plug and Play AI Solutions. IEEE Netw. 2020, 1-8. [CrossRef]

13. Holzinger, A.; Langs, G.; Denk, H.; Zatloukal, K.; Miiller, H. Causability and explainability of artificial
intelligence in medicine. Wiley Interdiscip. Rev. Data Min. Knowl. Discov. 2019, 9, 1312. [CrossRef]

14. Hossain, M.S.; Muhammad, G.; Guizani, N. Explainable AI and Mass Surveillance System-Based Healthcare
Framework to Combat COVID-I9 Like Pandemics. IEEE Netw. 2020, 34, 126-132. [CrossRef]

15. Calvaresi, D.; Mualla, Y; Najjar, A.; Galland, S.; Schumacher, M. Explainable Multi-Agent Systems
Through Blockchain Technology. In Biometric Recognition; Springer Science and Business Media LLC:
Cham, Switzerland, 2019; pp. 41-58.

16. Marino, D.L.; Wickramasinghe, C.S.; Manic, M. An Adversarial Approach for Explainable AI in Intrusion
Detection Systems. In Proceedings of the IECON 2018—44th Annual Conference of the IEEE Industrial
Electronics Society, Washington, DC, USA, 21-23 October 2018; pp. 3237-3243.
Smart Cities 2020, 3 1377

17.

18.

19.

20.

21.

23.

24.

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.

39.

Abdullatif, A.; Masulli, F; Rovetta, S. Tracking Time Evolving Data Streams for Short-Term Traffic Forecasting.
Data Sci. Eng. 2017, 2, 210-223. [CrossRef]

Fong, R.C.; Vedaldi, A. Interpretable Explanations of Black Boxes by Meaningful Perturbation. In Proceedings
of the 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22-29 October 2017;
pp. 3449-3457.

Gao, J.; Wang, X.; Wang, Y,; Xie, X. Explainable Recommendation through Attentive Multi-View Learning.
In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, Honolulu, HI, USA,
27 January-1 February 2019; Volume 3, pp. 3622-3629.

Papernot, N.; McDaniel, P. Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep
Learning. arXiv 2018, arXiv:1803.04765.

Berners-Lee, T.; Hendler, J.; Lassila, O. The Semantic Web. Sci. Am. 2001, 284, 34-43. [CrossRef]
Dhavalkumar, T.; Fan, ¥.-T.; Dimoklis, D. User Interaction with Linked Data: An Exploratory Search
Approach. Int, J. Distrib. Syst. Technol. [DST 2016, 7, 79-91.

Escolar, S.; Villanueva, FJ.; Santofimia, M.J.; Villa, D.; Del Toro, X.; Lopez, J.C. A Multiple-Attribute Decision
Making-based approach for smart city rankings design. Technol. Forecast. Soc. Chang. 2019, 142, 42-55.
[CrossRef]

Tobey, M.B.; Binder, R.B.; Chang, S.; Yoshida, T.; Yamagata, Y.; Yang, P.P.-J. Urban Systems Design:
A Conceptual Framework for Planning Smart Communities. Smart Cities 2019, 2, 522-537. [CrossRef]
Hoang, G.T.T.; Dupont, L.; Camargo, M. Application of Decision-Making Methods in Smart City Projects:
A Systematic Literature Review. Smart Cities 2019, 2, 433-452. [CrossRef]

Gupta, K.; Hall, R.P. Understanding the What, Why, and How of Becoming a Smart City: Experiences from
Kakinada and Kanpur. Smart Cities 2020, 3, 232-247. [CrossRef]

Browne, N.J.W. Regarding Smart Cities in China, the North and Emerging Economies—One Size Does Not
Fit All. Smart Cities 2020, 3, 186-201. [CrossRef]

Komninos, N.; Bratsas, C.; Kakderi, C.; Tsarchopoulos, P. Smart City Ontologies: Improving the effectiveness
of smart city applications. J. Smart Cities 2016, 1, 31-46. [CrossRef]

Subramaniyaswamy, V.; Manogaran, G.; Logesh, R.; Vijayakumar, V.; Chilamkurti, N.; Malathi, D.;
Senthilselvan, N. An ontology-driven personalized food recommendation in loT-based healthcare system.
J. Supercomput. 2019, 75, 3184-3216. [CrossRef]

Alkahtani, M.; Choudhary, A.; De, A.; Harding, J.A.; Harding, J. A decision support system based on ontology
and data mining to improve design using warranty data. Comput. Ind. Eng. 2019, 128, 1027-1039. [CrossRef]
Sermet, Y.; Demir, I. Towards an information centric flood ontology for information management and
communication. Earth Sci. Inform. 2019, 12,541-551. [CrossRef]

Wu, Z.; Shen, Y.; Wang, H.; Wu, M. An ontology-based framework for heterogeneous data management and
its application for urban flood disasters. Earth Sci. Inform. 2020, 13, 377-390. [CrossRef]

Lin, C.-H.; Wang, S.; Chia, C.-; Wu, C.-M.; Huang, C.-M. Temperature Variation Tolerance High Resolution
Real-time Liquid Level Monitoring System. In Proceedings of the 2018 IEEE 8th International Conference on
Consumer Electronics—Berlin (ICCE-Berlin), Berlin, Germany, 2-5 September 2018; pp. 1-6.

See, C.H.; Horoshenkov, K.V.; Abd-Alhmeed, R.; Hu, Y.E; Tait, S. A Low Power Wireless Sensor Network for
Gully Pot Monitoring in Urban Catchments. IEEE Sens. J. 2011, 12, 1545-1553. [CrossRef]

Atojoko, A.; Jan, N.; Elmgri, F; Abd-Alhameed, R.A.; See, C.H.; Noras, J.M. Energy efficient gully pot
monitoring system using radio frequency identification (RFID). In Proceedings of the 2013 Loughborough
Antennas & Propagation Conference (LAPC), Loughborough, UK, 11-12 November 2013; pp. 333-336.
Sunkpho, J.; Ootamakorn, C. Real-time flood monitoring and warning system. Songklanakarin J. Sci. Technol.
2011, 33, 227-235.

Scheuer, S.; Haase, D.; Meyer, V. Towards a flood risk assessment ontology—Knowledge integration into a
multi-criteria risk assessment approach. Comput. Environ. Urban Syst. 2013, 37, 82-94. [CrossRef]

Bischke, B.; Bhardwaj, P., Gautam, A.; Helber, P.; Borth, D.; Dengel, A. Detection of Flooding Events in Social
Multimedia and Satellite Imagery Using Deep Neural Networks. In Proceedings of the Working Notes
Proceedings of the MediaEval 2017, Dublin, Ireland, 13-15 September 2017.

Tamaazousti, Y.; Le Borgne, H.; Hudelot, C. Diverse Concept-Level Features for Multi-Object Classification.
In Proceedings of the 2016 ACM on Internet Measurement Conference—IMC ‘16, Santa Monica, CA, USA,
14-16 November 2016; Association for Computing Machinery (ACM): New York, NY, USA, 2016; pp. 63-70.
Smart Cities 2020, 3 1378

40.

41.

43.

45.

46.

47.

48.

49.

50.

51.

52.

53.

54.

55.

56.

57.

58.

59.

60.

Ginsca, A.; Popescu, A.; Le Borgne, H.; Ballas, N.; Vo, P.; Kanellos, I. Large-Scale Image Mining with Flickr
Groups. In Lecture Notes in Computer Science; Springer Science and Business Media LLC: Cham, Switzerland,
2015; Volume 8935, pp. 318-334.

Torresani, L.; Szummer, M.; FitzGibbon, A. Efficient Object Category Recognition Using Classemes. In Static
Analysis; Springer Science and Business Media LLC: Cham, Switzerland, 2010; pp. 776-789.

Xiong, Z.; Zheng, J.; Song, D.; Zhong, S.; Huang, Q. Passenger Flow Prediction of Urban Rail Transit Based
on Deep Learning Methods. Smart Cities 2019, 2, 371-387. [CrossRef]

Boutell, M.R.; Luo, J.; Shen, X.; Brown, C.M. Learning multi-label scene classification. Pattern Recogn.
2004, 37, 1757-1771. [CrossRef]

Cheng, G.; Han, J.; Zhou, P.; Guo, L. Multi-class geospatial object detection and geographic image classification
based on collection of part detectors. ISPRS J. Photogramm. Remote Sens. 2014, 98, 119-132. [CrossRef]

Li, L.-J.; Su, H.; Lim, Y.; Fei-Fei, L. Objects as Attributes for Scene Classification. In Lecture Notes in Computer
Science; Springer: Berlin/Heidelberg, Germany, 2012; pp. 57-69.

Teichmann, M.; Weber, M.; Zollner, M.; Cipolla, R.; Urtasun, R. MultiNet: Real-time Joint Semantic Reasoning
for Autonomous Driving. In Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV), Changshu,
China, 26-30 June 2018; pp. 1013-1020.

Manzoor, U.; Balubaid, M.A.; Zafar, B., Umar, H.; Khan, M.S. Semantic Image Retrieval: An Ontology Based
Approach. Int, J. Adv. Res. Artif. Intell. 2015, 4. [CrossRef]

Jiang, S.; Huang, T.; Gao, W. An Ontology-based Approach to Retrieve Digitized Art Images. In Proceedings
of the IEEE/WIC/ACM International Conference on Web Intelligence (WI’04), Beijing, China, 20-24 September
2004; pp. 131-137.

Abdullatif, A.; Masulli, F.; Rovetta, S. Clustering of nonstationary data streams: A survey of fuzzy partitional
methods. Wiley Interdiscip. Rev. Data Min. Knowl. Discov. 2018, 8, e1258. [CrossRef]

Ai, Q.; Azizi, V.; Chen, X.; Zhang, Y. Learning Heterogeneous Knowledge Base Embeddings for Explainable
Recommendation. Algorithms 2018, 11,137. [CrossRef]

Holzinger, A. From Machine Learning to Explainable AI. In Proceedings of the 2018 World Symposium on
Digital Intelligence for Systems and Machines (DISA), Kosice, Slovakia, 23-25 August 2018; pp. 55-66.
Preece, A.; Braines, D.; Cerutti, F; Pham, T. Explainable AI for Intelligence Augmentation in Multi-Domain
Operations. arXiv 2019, arXiv:1910.07563v1.

Spinner, T.; Schlegel, U.; Schafer, H.; El-Assady, M. explAIner: A Visual Analytics Framework for Interactive
and Explainable Machine Learning. IEEE Trans. Vis. Comput. Graph. 2019, 26, 1. [CrossRef]
Vasquez-Morales, G.R.; Martinez-Monterrubio, $.M.; Moreno-Ger, P.; Recio-Garcia, J.A. Explainable Prediction
of Chronic Renal Disease in the Colombian Population Using Neural Networks and Case-Based Reasoning.
IEEE Access 2019, 7, 152900-152910. [CrossRef]

Holzinger, A.; Biemann, C.; Pattichis, M.; Currin, A. What do we need to build explainable AI systems for
the medical domain? arXiv 2017, arXiv:1712.09923.

Holzinger, A.; Kieseberg, P.; Weippl, E.R.; Tjoa, AM. Current Advances, Trends and Challenges of Machine
Learning and Knowledge Extraction: From Machine Learning to Explainable AI. In Intelligent Tutoring
Systems; Springer Science and Business Media LLC: Cham, Switzerland, 2018; pp. 1-8.

Wang, D.; Yang, Q.; Abdul, A.; Lim, B.Y. Designing Theory-Driven User-Centric Explainable AI. In Proceedings
of the 2019 CHI Conference on Human Factors in Computing Systems—CHI ‘19, Glasgow, UK, 4-9 May 2019;
Association for Computing Machinery (ACM); pp. 1-15.

Amarasinghe, K.; Kenney, K.; Manic, M. Toward Explainable Deep Neural Network Based Anomaly
Detection. In Proceedings of the 2018 11th International Conference on Human System Interaction (HSI),
Gdansk, Poland, 4-6 July 2018; pp. 311-317.

Alonso, J.M.; Mencar, C. Building Cognitive Cities with Explainable Artificial Intelligent Systems. 2018.
In Proceedings of the First International Workshop on Comprehensibility and Explanation in Al and ML
2017 co-Located with 16th International Conference of the Italian Association for Artificial Intelligence
(AFIA 2017), Bari, Italy, 16-17 November 2017; Published on CEUR-WS: 11-Mar-2018. Available online:
http://ceur-ws.org/Vol-2071/CExAIIA_2017_paper_1.pdf (accessed on 1 November 2020).

Lee, H.; Kwon, H. Going Deeper With Contextual CNN for Hyperspectral Image Classification. IEEE Trans.
Image Process. 2017, 26, 4843-4855. [CrossRef]
Smart Cities 2020, 3 1379

61.

62.

63.

65.

66.

67.

68.

69.

70.

71.

72.

73.

74.

75.

76.

78.
79.
80.
81.
82.

83.

Gebrehiwot, A.; Hashemi-Beni, L.; Thompson, G.; Kordjamshidi, P.; Langan, T.E. Deep Convolutional Neural
Network for Flood Extent Mapping Using Unmanned Aerial Vehicles Data. Sensors 2019, 19, 1486. [CrossRef]
Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive
Review. Neural Comput. 2017, 29, 2352-2449. [CrossRef] [PubMed]

Murugan, P. Implementation of Deep Convolutional Neural Network in Multi-class Categorical Image
Classification. arXiv 2018, arXiv:1801.01397.

Chen, Y.; Jiang, H.; Li, C.; Jia, X.; Ghamisi, P. Deep Feature Extraction and Classification of Hyperspectral
Images Based on Convolutional Neural Networks. IEEE Trans. Geosci. Remote Sens. 2016, 54, 6232-6251.
[CrossRef]

Zhang, C.; Sargent, L; Pan, X.; Li, H.; Gardiner, A.; Hare, J.; Atkinson, P.M. An object-based convolutional
neural network (OCNN) for urban land use classification. Remote Sens. Environ. 2018, 216, 57-70. [CrossRef]
Shorten, C.; Khoshgoftaar, T.M. A survey on Image Data Augmentation for Deep Learning. J. Big Data
2019, 6, 60. [CrossRef]

Taylor, L.; Nitschke, G. Improving Deep Learning with Generic Data Augmentation. In Proceedings of the
2018 IEEE Symposium Series on Computational Intelligence (SSCI), Bangalore, India, 18-21 November 2018;
Volume 1708, pp. 1542-1547.

Inoue, H. Data Augmentation by Pairing Samples for Images Classification. arXiv 2018, arXiv:1801.02929.
Cubuk, E.D.; Zoph, B.; Mane, D.; Vasudevan, V.; Le, Q.V. AutoAugment: Learning Augmentation Strategies
From Data. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Long Beach, CA, USA, 15-20 June 2019; pp. 113-123.

D’‘Aniello, G.; Gaeta, M.; Orciuoli, F. An approach based on semantic stream reasoning to support decision
processes in smart cities. Telemat. Inform. 2018, 35, 68-81. [CrossRef]

Gyrard, A.; Serrano, M. A Unified Semantic Engine for Internet of Things and Smart Cities: From Sensor
Data to End-Users Applications. In Proceedings of the 2015 IEEE International Conference on Data Science
and Data Intensive Systems, Sydney, NSW, Australia, 11-13 December 2015; pp. 718-725.

Ali, S.; Wang, G.; Fatima, K.; Liu, P. Semantic Knowledge Based Graph Model in Smart Cities.
In Communications in Computer and Information Science; Springer Science and Business Media LLC: Cham,
Switzerland, 2019; pp. 268-278.

Zhang, N.; Chen, J.; Chen, X.; Chen, J. Semantic Framework of Internet of Things for Smart Cities: Case Studies.
Sensors 2016, 16, 1501. [CrossRef]

Bizer, C.; Heath, T.; Berners-Lee, T. Linked data: The story so far. In Semantic Services, Interoperability and Web
Applications: Emerging Concepts; IGI Global: Hershey, PA, USA, 2011; pp. 205-227.

Abid, T,; Laouar, M.R. Using Semantic Web and Linked Data for Integrating and Publishing Data in Smart
Cities. In Proceedings of the 7th International Conference on Software Engineering and New Technologies,
Hammamet, Tunisie, 26-28 December 2018; pp. 1-4, ISBN 978-1-4503-6101-9.

Petrolo, R.; Loscri, V.; Mitton, N. Towards a smart city based on cloud of things, a survey on the smart city
vision and paradigms. Trans. Emerg. Telecommun. Technol. 2015, 28, e2931. [CrossRef]

Kamilaris, A.; Gao, F; Prenafeta-Boldu, F.X.; Ali, M.I. Agri-loT: A semantic framework for Internet of
Things-enabled smart farming applications. In Proceedings of the 2016 IEEE 3rd World Forum on Internet of
Things (WF-IoT)}, Reston, VA, USA, 12-14 December 2016; pp. 442-447.

Guo, K.; Lu, ¥.; Gao, H.; Cao, R. Artificial Intelligence-Based Semantic Internet of Things in a User-Centric
Smart City. Sensors 2018, 18, 1341. [CrossRef] [PubMed]

Jacob, E.K. Ontologies and the Semantic Web. Bull. Am. Soc. Inf. Sci. Technol. 2005, 29, 19-22. [CrossRef]
Keeling, M.; Dirks, S. A Vision of Smarter Cities; IBM Institute for Business Value: Cambridge, MA, USA, 2009.
Gyrard, A.; Zimmermann, A.; Sheth, A. Building loT-Based Applications for Smart Cities: How Can Ontology
Catalogs Help? IEEE Internet Things J. 2018, 5, 3978-3990. [CrossRef]

Saba, D.; Sahli, Y.; Abanda, F.H.; Maouedj, R.; Tidjar, B. Development of new ontological solution for an
energy intelligent management in Adrar city. Sust. Comput. Inform. Syst. 2019, 21, 189-203. [CrossRef]
Costin, A.; Eastman, C. Need for Interoperability to Enable Seamless Information Exchanges in Smart and
Sustainable Urban Systems. f. Comput. Civ. Eng. 2019, 33, 04019008. [CrossRef]

Rueda, C.; Galbraith, N.; Morris, R.A.; Bermudez, L.E.; Arko, R.A.; Graybeal, J. The MMI device ontology:
Enabling sensor integration. In Proceedings of the AGU Fall Meeting Abstracts, San Francisco, CA, USA,
13-17 December 2010; p. 8.
Smart Cities 2020, 3 1380

85.

86.

87.

88.

89.

90.

91.

92.

93.

94.

95.

96.

97.

98.

99.

100.

101.

102.

103.

104.

105.

Vakali, A.; Anthopoulos, L.; Kréo, S$. Smart Cities Data Streams Integration: Experimenting with Internet
of Things and social data flows. In Proceedings of the 4th International Conference on Web Intelligence,
Mining and Semantics (WIMS14), Thessaloniki, Greece, 24 June 2014; pp. 1-5. [CrossRef]

Viktorovi¢, M.; Yang, D.; De Vries, B.; Baken, N. Semantic web technologies as enablers for truly connected
mobility within smart cities. Proc. Comput. Sci. 2019, 151, 31-36. [CrossRef]

Balakrishna, S.; Thirumaran, M. Semantic Interoperable Traffic Management Framework for loT Smart City
Applications. EAI Endorsed Trans. Internet Things 2018, 4, 1-18. [CrossRef]

Calavia, L.; Baladrén, C.; Aguiar, J.M.; Carro, B.; Sanchez-Esguevillas, A. A Semantic Autonomous Video
Surveillance System for Dense Camera Networks in Smart Cities. Sensors 2012, 12, 10407-10429. [CrossRef]
De Nicola, A.; Melchiori, M.; Villani, M.L. Creative design of emergency management scenarios driven by
semantics: An application to smart cities. Inf. Syst. 2019, 81, 21-48. [CrossRef]

Zheng, Y.; Chen, X.; Jin, Q.; Chen, ¥.; Qu, X.; Liu, X.; Chang, E.; Ma, W.Y.; Rui, Y.; Sun, W. A Cloud-Based
Knowledge Discovery System for Monitoring Fine-Grained Air Quality. MSR-TR-2014-40 Tech. Rep. 2014.
Available online: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/UAir20Demo.pdf
(accessed on 1 November 2020).

Bellini, P.; Benigni, M.; Billero, R.; Nesi, P.; Rauch, N. Km4City ontology building vs data harvesting and
cleaning for smart-city services. J. Vis. Lang. Comput. 2014, 25, 827-839. [CrossRef]

Shang, J.; Zheng, Y; Tong, W.; Chang, E.; Yu, Y. Inferring gas consumption and pollution emission of
vehicles throughout a city. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining—KDD ‘14, New York, NY, USA, 24-27 August 2014; pp. 1027-1036. [CrossRef]
Choi, C.; Esposito, C.; Wang, H.; Liu, Z.; Choi, J. Intelligent Power Equipment Management Based on
Distributed Context-Aware Inference in Smart Cities. IEEE Commun. Mag. 2018, 56, 212-217. [CrossRef]
Howell, S.; Rezgui, Y.; Beach, T-H. Integrating building and urban semantics to empower smart water
solutions. Autom. Constr. 2017, 81, 434-448. [CrossRef]

Soldatos, J.; Kefalakis, N.; Hauswirth, M.; Serrano, M.; Calbimonte, J.-P.; Riahi, M.; Aberer, K.; Jayaraman, P.P.;
Zaslavsky, A.; Zarko, LP, et al. OpenloT: Open Source Internet-of-Things in the Cloud. In The Semantic Web;
Springer Science and Business Media LLC: Cham, Switzerland, 2015; pp. 13-25.

Barnaghi, P.; Tonjes, R.; Holler, J.; Hauswirth, M.; Sheth, A.; Anantharam, P. CityPulse: Real-Time Iot
Stream Processing and Large-Scale Data Analytics for Smart City Applications. In Europen Semantic Web
Conference (ESWC). 2014. Available online: http://www.ict-citypulse.eu/doe/CityPulse_ExtendedAbstract_
ESWC_EU.pdf (accessed on 1 November 2020).

Petrolo, R.; Loscri, V.; Mitton, N. Towards a Cloud of Things Smart City. IEEE COMSOC MMTC E Lett.
2014, 9, 44-48.

Lefrancois, J.; Ghariani, T.; Zimmermann, A. The SEAS Knowledge Model; Technical Report, ITEA2 12004
Smart Energy Aware Systems; ITEA: Eindhoven, The Netherlands, 2017.

Seydoux, N.; Drira, K.; Hernandez, N.; Monteil, T. loT-O, a Core-Domain IoT Ontology to Represent
Connected Devices Networks. In Pattern Recognition and Computer Vision; Springer Science and Business
Media LLC: Cham, Switzerland, 2016; pp. 561-576.

Janowicz, K.; Haller, A.; Cox, S.; Le Phuoc, D.; Lefrancois, M. SOSA: A Lightweight Ontology for Sensors,
Observations, Samples, and Actuators. SSRN Electron. J. 2018, 56, 1-10. [CrossRef]

Llaves, A.; Corcho, O.; Taylor, P.; Taylor, K. Enabling RDF Stream Processing for Sensor Data Management in
the Environmental Domain. Int. J. Semantic Web Inf. Syst. 2016, 12, 1-21. [CrossRef]

Ploennigs, J.; Schumann, A.; Lécué, F. Adapting semantic sensor networks for smart building diagnosis.
In International Semantic Web Conference; Springer: Cham, Switzerland, 2014; pp. 308-323.

Dey, S.; Jaiswal, D.; Dasgupta, R.; Mukherjee, A. Organization and management of Semantic Sensor
information using SSN ontology: An energy meter use case. In Proceedings of the 2015 9th International
Conference on Sensing Technology (ICST), Auckland, New Zealand, 8-10 December 2015; pp. 468-473.
Fernandez, S.; Ito, T. Using SSN Ontology for Automatic Traffic Light Settings on Inteligent Transportation
Systems. In Proceedings of the 2016 IEEE International Conference on Agents (ICA), Matsue, Japan,
28-30 September 2016; pp. 106-107.

Agresta, A.; Fattoruso, G.; Pollino, M.; Pasanisi, F.; Tebano, C.; De Vito, S.; Di Francia, G. An Ontology
Framework for Flooding Forecasting. In Proceedings of the Lecture Notes in Computer Science; Springer Science
and Business Media LLC: Cham, Switzerland, 2014; Volume 8582, pp. 417-428.
Smart Cities 2020, 3 1381

106.

107.

108.

109.

110.

111.

112.

113.

114.

115.

116.

117.

118.

119.

120.

121.

122.

123.

124.

125.

Wang, C.; Chen, N.; Wang, W.; Chen, Z. A Hydrological Sensor Web Ontology Based on the SSN Ontology:
A Case Study for a Flood. ISPRS Int. J. Geo Inform. 2017, 7, 2. [CrossRef]

Ding, Y.; Zhu, Q.; Lin, H. An integrated virtual geographic environmental simulation framework: A case
study of flood disaster simulation. Geo Spat. Inf. Sci. 2014, 17, 190-200. [CrossRef]

Sun, J.; De Sousa, G.; Roussey, C.; Chanet, J.P.; Pinet, F; Hou, K.M. Intelligent Flood Adaptive Context-aware
System: How Wireless Sensors Adapt their Configuration based on Environmental Phenomenon Events.
Sens. Transduc. 2016, 206, 68.

Sinha, P.K.; Dutta, B. A Systematic Analysis of Flood Ontologies: A Parametric Approach. Knowl. Organ.
2020, 47, 138-159. [CrossRef]

Ning, H.; Li, Z.; Hodgson, M.E.; Wang, C. Prototyping a Social Media Flooding Photo Screening System
Based on Deep Learning. ISPRS Int. J. Geo Inform. 2020, 9, 104. [CrossRef]

Sit, M.A.; Koylu, C.; Demizr, I. Identifying disaster-related tweets and their semantic, spatial and temporal
context using deep learning, natural language processing and spatial analysis: A case study of Hurricane
Irma. Int. J. Digit. Earth 2019, 12, 1205-1229. [CrossRef]

Burel, G.; Saif, H.; Alani, H. Semantic Wide and Deep Learning for Detecting Crisis-Information Categories
on Social Media. In Proceedings of the Lecture Notes in Computer Science; Springer Science and Business Media
LLC: Cham, Switzerland, 2017; Volume 10587, pp. 138-155.

Sublime, J.; Kalinicheva, E. Automatic post-disaster damage mapping using deep-learning techniques for
change detection: Case Study of the Tohoku Tsunami. Remote Sens. 2019, 11, 1123. [CrossRef]

Cavaliere, D.; Saggese, A.; Senatore, S.; Vento, M.; Loia, V. Empowering UAV scene perception by semantic
spatio-temporal features. In Proceedings of the 2018 IEEE International Conference on Environmental
Engineering, Milan, Italy, 12-14 March 2018; pp. 1-6.

Cretu, L.-G. Smart Cities Design using Event-driven Paradigm and Semantic Web. Inform. Econ. 2012, 16,
57-67.

Abdulnabi, A.H.; Wang, G.; Lu, J.; Jia, K. Multi-Task CNN Model for Attribute Prediction. IEEE Trans. Multimedia
2015, 17, 1949-1959. [CrossRef]

Perez, L.; Wang, J. The Effectiveness of Data Augmentation in Image Classification using Deep Learning.
arXiv 2017, arXiv:1712.04621.

Chen, Y.; Lin, Z.; Zhao, X.; Wang, G.; Gu, Y. Deep Learning-Based Classification of Hyperspectral Data.
IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 2094-2107. [CrossRef]

Ahmed, T.U.; Hossain, S.; Hossain, M.S.; Islam, R.U.; Andersson, K. Facial Expression Recognition using
Convolutional Neural Network with Data Augmentation. In Proceedings of the 2019 Joint 8th International
Conference on Informatics, Electronics & Vision (ICIEV) and 2019 3rd International Conference on Imaging,
Vision & Pattern Recognition (icIVPR), Spokane, WA, USA, 30 May-2 June 2019; pp. 336-341.

Ma, R.; Li, L.; Huang, W.; Tian, Q. On pixel count based crowd density estimation for visual surveillance.
In Proceedings of the IEEE Conference on Cybernetics and Intelligent Systems, Singapore, 1-3 December 2004;
Volume 1, pp. 170-173.

Vandoni, J.; Aldea, E.; Le Hegarat-Mascle, S$. Active learning for high-density crowd count regression.
In Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based
Surveillance (AVSS), Lecce, Italy, 29 August-1 September 2017; pp. 1-6.

Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV,
USA, 27-30 June 2016; pp. 779-788.

Zhang, C.; Li, H.; Wang, X.; Yang, X. Cross-scene crowd counting via deep convolutional neural networks.
In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston,
MA, USA, 7-12 June 2015; pp. 833-841.

Ide, H.; Kurita, T. Improvement of learning for CNN with ReLU activation by sparse regularization.
In Proceedings of the 2017 International Joint Conference on Neural Networks (IJCNN), Anchorage, AK,
USA, 14-19 May 2017; pp. 2684-2691.

Hao, W.; Bie, R.; Guo, J.; Meng, X.; Wang, S. Optimized CNN Based Image Recognition Through Target
Region Selection. Optik 2018, 156, 772-777. [CrossRef]
Smart Cities 2020, 3 1382

126.

127.

128.

129.

Gomez-Rios, A.; Tabik, S.; Luengo, J.; Shihavuddin, A.; Krawczyk, B.; Herrera, F. Towards highly accurate
coral texture images classification using deep convolutional neural networks and data augmentation.
Expert Syst. Appl. 2019, 118, 315-328. [CrossRef]

Arrieta, A.B.; Diaz-Rodriguez, N.; Del Ser, J.; Bennetot, A.; Tabik, S.; Barbado, A.; Garcia, S.; Gil-Lopez, S.;
Molina, D.; Benjamins, R.; et al. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities
and challenges toward responsible AI. Inf. Fusion 2020, 58, 82-115. [CrossRef]

Wu, Y; Zhang, Z.; Kou, G.; Zhang, H.; Chao, X.; Li, C.-C.; Dong, Y.; Herrera, F. Distributed linguistic
representations in decision making: Taxonomy, key elements and applications, and challenges in data science
and explainable artificial intelligence. Inf. Fusion 2021, 65, 165-178. [CrossRef]

Alzetta, F., Giorgini, P.; Najjar, A.; Schumacher, M.; Calvaresi, D. In-Time Explainability in Multi-Agent
Systems: Challenges, Opportunities, and Roadmap. In Lecture Notes in Computer Science; Springer Science
and Business Media LLC: Cham, Switzerland, 2020; pp. 39-53.

Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional
affiliations.

i) © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
BY

(CC BY) license (http://creativecommons.org/licenses/by/4.0/).
A Bibliometric Analysis of the Explainable Artificial Intelligence Research
Field

Jose M. Alonso?
Ciro Castiello!

Corrado Mencar!

 

Abstract

This paper presents the results of a bibliometric study of the recent research on eXplainable Artificial
Intelligence (XAI) systems. We took a global look at the contributions of scholars in XAI as well as in the
subfields of AI that are mostly involved in the development of XAI systems. It is worthy to remark that we
found out that about one third of contributions in XAI come from the fuzzy logic community. Accordingly,
we went in depth with the actual connections of fuzzy logic contributions with AI to promote and improve
XAI systems in the broad sense. Finally, we outlined new research directions aimed at strengthening
the integration of different fields of AI, including fuzzy logic, toward the common objective of making AI
accessible to people.

Key words: Interpretability, Understandability, Comprehensibility, Explainable AI, Interpretable Fuzzy
Systems

 

1. Introduction

In the era of the Internet of Things and Big Data, data scientists are required to extract valuable
knowledge from the given data. They first analyze, cure and pre-process data; then, they apply Artificial
Intelligence (AI) techniques to automatically extract knowledge from data [20]. Getting AI into widespread
real-world usage requires to think carefully of many important issues. Among them, we would like to
highlight (1) Ethics, (2) Law and (3) Technology.

Recently, ACM issued a Statement on “Algorithmic Transparency and Accountability”, which establishes
a set of principles, consistent with the ACM Code of Ethics, to support the benefits of algorithmic decision-
making while addressing ethical and legal concerns [1]. Among such principles, Explanation is of relevance

 

Email addresses: josemaria.alonso.moral@usc.es (Jose M. Alonso), ciro.castiello@uniba. it (Ciro Castiello),
corrado.mencar@uniba.it (Corrado Mencar)
!Department of Informatics, University of Bari “Aldo Moro”, Bari, Italy
2Centro Singular de Investigacion en Tecnoloxias da Informacion (CiTIUS), Universidade de Santiago de Compostela,
Campus Vida, E-15782, Santiago de Compostela, Galicia, Spain

Research Centre in Information Technologies (CiTIUS}, University of Santiago de Compostela (USC) (May 2018).

Final version published in proceedings of the 17th International Conference on Information Processing and Management of
Uncertainty in Knowledge-Based Systems (IPMU}, 2018.

Please cite this article in press as:

J.M. Alonso, C. Castiello, C. Mencar, “A bibliometric analysis of the explainable artificial intelligence research field”, Infor-
mation Sciences, 2018, DOI:10.1007/978-3-319-91473-2_1

Available online at http://dx.doi.org/10.1007/978-3-319-91473-2_1
for this study. According to ACM: “Systems and institutions that use algorithmic decision-making are
encouraged to produce explanations regarding both the procedures followed by the algorithm and the specific
decisions that are made. This is particularly important in public policy contexts.”

In addition, a new European General Data Protection Regulation (GDPR) is expected to take effect
in 2018 [11]. It takes care of the protection of natural people when personal data have to be processed
and freely moved. Moreover, it emphasizes the “right to explanation” of European citizens: “[...] decision-
making based on such processing, including profiling, should be allowed [...] In any case, such processing
should be subject to suitable safeguards, which should include [...] the right to obtain human intervention,
to express his or her point of view, to obtain an explanation of the decision reached after such assessment
and to challenge the decision.”

Regarding technological issues, the theme of explainability in AI is also remarked in the last challenge
stated by the USA Defense Advanced Research Projects Agency (DARPA) [13]: “Even though current AI
systems offer many benefits in many applications, their effectiveness is limited by a lack of explanation
ability when interacting with humans.”

Accordingly, non-expert users, i.e., users without a strong background on AJ, require a new generation of
explainable AI (XAI) systems. Such systems are expected to naturally interact with humans by providing
comprehensible explanations of decisions that are automatically made. XAI systems can be also considered
as an important step forward toward Collaborative Intelligence [7] which promises a fully accepted integration
of AI in our society.

In this paper, we report the results of a bibliometric study of the recent research on XAI systems. We
are interested in assessing the contributions of AI scholars in XAI, as well as in the subfields of AI that are
mostly involved in the development of XAI systems. More specifically, we are interested in the role of the
fuzzy logic community in the progress of XAI, exploring the connections of fuzzy logic contributions with
AI to promote and improve systems explainability. While moving along this way, we hope to outline new
research directions aimed at strengthening the integration of different fields of Al, including fuzzy logic,
toward the common objective of making AI accessible to people.

The rest of the manuscript is organized as follows. Section 2 introduces material and methods. Section 3
presents our bibliometric analysis focused on XAI. Section 4 introduces additional details while focusing
on Interpretable Fuzzy Systems (IFS) only. Finally, Section 5 remarks the main points of the study and
pinpoints future work.

2. Material and Methods

2.1. Bibliometric Techniques

Scientometrics is informally defined as the discipline that studies the quantitative features and charac-
teristics of science and scientific research, technology and innovation. Within Scientometrics, Bibliometrics
copes with the statistical analysis of books, articles, or other kinds of publications [6].

Usually, bibliographical data are treated by statistical mathematical methods and results are visualized
in form of tables and graphs. For example, Vargas-Quesada et al. [25] proposed a methodology for creating
visual representations of scientific domains. They focused on illustrating interactions among authors and pa-
pers through citations and co-citations. Later, other authors generalized the idea and developed alternative
methods and tools (e.g., [5, 24]) to create maps of linked items (scientific publications, scientific journals,
researchers, research organizations, countries, or keywords).

Different types of links between pairs of items can be considered. As an example, let us briefly introduce
the concept of item co-citation. Given a set of items, all potential links among pairs of items can be
characterized by the standardized co-citation measure [22] as follows:

Co 9

MCN, = —=4
JC > CF

(1)

 

http: //eur-lex. europa. eu/legal-content/en/TXT/?uri=CELEX\%3432016R0679
2
where Cc means co-citation, c stands for citation, i and 7 are two different items.

The link values (MCN;,;) define the adjacency matrix of a graph which can be analyzed and visual-
ized with social network analysis (SNA) techniques [26]. These techniques have been already applied to
multiple fields of research, such as software development (e.g., debugging multi-agent systems [23]), scien-
tometrics (e.g., analyzing large scientific domains [18]), or fuzzy modeling (e.g., analyzing fuzzy rule-bases
with fingrams [19]).

There are many metrics designed to assess the importance of a node in a bibliographical graph (e.g.,
centrality degree, closeness, betweenness or page rank) [25]. In addition, there are many different methods
for graph visualization [4]. Among them, force-directed algorithms are the most widely used in information
science [16]. Their purpose is to locate the nodes of a graph in a 2D or 3D space, so that all the edges are
approximately of equal length and there are as few crossing edges as possible, trying to obtain the most
aesthetically pleasing view. There are also many clustering techniques aimed at discovering communities
(or bunches of highly related nodes) in accordance with the importance of each single node and how it is
connected to the others [21].

2.2. Bibliographic Repositories

Bibliographic data can be read from different sources such as Web of Science (WoS) or Scopus. WoS
appears not to be adequate for assessing publications and citations in Informatics. In addition, some other
sources may be too large (e.g., Google Scholar) or too specialized (e.g., ACM DL, IEEEXplore, etc.).
Therefore, in this work we focus on Scopus which also offers advanced search functionalities useful to select
meaningful sets of items which can be considered as a ground to build our bibliometric analysis. Anyway,
the selection of Scopus as a bibliographical source comes without any loss of generalization. We performed
a preliminary study on data collected from WoS: the main trends and the general conclusions remained
unchanged (only slight minor variations were detected).

Finally, it should be highlighted that data collected from Scopus have been cured in order to remove
spurious information that would have hampered the subsequent steps of our analysis.

2.8. Bibliographic Analysis Tools

We used a couple of tools to analyze the results of search queries from Scopus:

Bibliometrix [3] - An R package for performing comprehensive quantitative research in Scientometrics
and Bibliometrics. It allows importing bibliographic data from several sources (including Scopus and
WoS). In addition, it evaluates co-citation as well as other kinds of measures, such as coupling, scientific
collaboration and co-word analyses.

VOS viewer [24] - A software tool for constructing and visualizing bibliometric networks which can
be related to citation, co-citation, bibliographic coupling, co-authorship or co-occurrence of words
relations. Some clustering methods to identify related groups or communities are also provided.

3. A Global Overview on XAI

On October 20th, 2017, we ran the following query through the “Advanced Search” tool provided by
Scopus:

Qi = TITLE (‘‘*interpretab*’’) OR TITLE (‘ ‘*comprehensib*’ ’ )
OR TITLE (‘‘*understandab*’’) OR TITLE (‘‘*explainab*’?)

OR TITLE (‘‘*self-explanat*’’?) OR KEY (‘‘*interpretab*’ ’)

OR KEY (‘‘*comprehensib*’’) OR KEY (‘‘*understandab*’ ’)

OR KEY (‘‘*explainab*’’) OR KEY (‘‘*self-explanat*’’)

 

Informatics Research Evaluation (Draft), An Informatics Europe Report. http://www. informatics-europe.org/
working-groups/research-evaluation. html
eo so eu} is
09 py et tag 09) a

 

05 FNP a an oe
a

 

 

Ott eat et ealey

Computer Sciences
Mathematics
Engineering

Medicine

Social Sciences

Arts and Humanities
Psychology

Decision Sciences
Biochemistry, Genetics
and Molecular Biology
Physics and Astronomy

 

Figure 1: Histogram of XAI publications and their distribution by subject areas.

As a result, we found out 5735 documents. It is worthy to note this query is intentionally very general
in order to broaden the global picture of the research field under examination. We identified only 5 general
terms and their variants represented by the * symbol. We required at least one of these terms to be present
in the title of the retrieved document or in the associated keywords (provided by authors or automatically
indexed).

Fig. 1 depicts the number of XAI publications since 1960 (top picture) and the distribution of publications
in the top-10 ranking of subject fields (bottom picture). The number of publications started to grow
significantly since 2000. Accordingly, we decided to focus our analysis only on the years ranging from 2000
to 2017.

XAI represents a multidisciplinary research field, as witnessed by the variety of subject areas. Anyway,
three of them (Computer Sciences, Mathematics, and Engineering) collect most of the publications. There-
fore, we are going to pay attention only to publications in these research areas. In this way, the final number
of publications to analyze is 3737. We downloaded from Scopus all the related bibliographical information
in form of csv and bib files.

Table 1 presents the Top-5 rankings of authors (columns 2-4) and countries (columns 5 and 6) with
respect to h-index, total number of citations (TC) and publications (NP). Herrera stands as the leading
author in terms of h-index, TC and NP. USA is by far the leading country in terms of both TC and NP.

The leading publications are listed in Table 2 in terms of TC and average citations per year (ACY).
Guillaume [12] reviewed methods for automatically designing IFS. This is the most cited publication being

4
Table 1: Top-5 Ranking of XAI Authors and Countries in terms of h-index, Total Citations (TC) and Number of Publications
(NP).

 

Rank. Authors (h-index) Authors (TC) Authors (NP) Countries (TC) Countries (NP)

 

1 Herrera (22) Herrera (2287) Herrera (45) USA (6737) USA (604)

2 Aleala (13) Alcala (1216) Mencar (29) Spain (4628) China (323)

3 Mendling (13) Baesens (846) Piattini (29) Germany (2242) Spain (318)

4 Alonso (12) Mendling (717) Alonso (26) UK (1703) Germany (270)
5 Piattini (11) Guillaume (707) — Alcala (25) China (1692) UK (175)

 

Table 2: Top-5 Ranking of XAI Publications in terms of Total Citations (TC) and Average Citations per Year (ACY).
TC Rank. Publication (Authors, Year, Source, TC) [Ref]

1 S. Guillaume, 2001, IEEE T Fuzzy Syst, 435 [12]

2 V. Aleven and K. Koedinger, 2002, Cognitive Sci, 418 [2]
3 Y. Jin, 2000, IEEE T Fuzzy Syst, 333 [15]
4
5

 

 

S. Garcia et al., 2009, Soft Comput, 308 [10]
H. Ishibuchi and Y. Nojima, 2007, Int J Approx Reason, 266 [14]

ACY Rank. Publication (Authors, Year, Source, ACY) [Ref]

 

L. Martinez and F. Herrera, 2012, Inform Sciences, 39.8 [17]
S. Garcia et al., 2009, Soft Comput, 38.5 [10]

M.J. Gacto et al., 2011, Inform Sciences, 33.3 [9]

V. Aleven and K. Koedinger, 2002, Cognitive Sci, 27.9 [2]
S. Guillaume, 2001, IEEE T Fuzzy Syst, 27.2 [12]

ok WN

 

= Lect. Notes Comput. Sci. = Lect. Notes Comput. Sci.

= IEEE Int. Conf. Fuzzy Syst. = IEEE Int. Conf. Fuzzy Syst.
CEUR-WS Proc. IEEE Trans. Fuzzy Syst.

= Expert Sys. Appl. = Soft Comput.

= Proc. SPIE Int. Soc. Opt. Eng. “° = Inf. Sci.

= ACM Int. Conf. Proc. Ser. F >

= Expert Sys. Appl.

= Fuzzy Sets Syst.

= Appl. Soft Comput. J.

= Stud. Comput. Intell.

= Commun. Comput. Info. Sci.

= Neurocomputing

= Soft Comput.

= Commun. Comput. Info. Sci.
= Inf. Sci.

 

Figure 2: Pie charts with the leading sources in XAI (left) and IFS (right).

also the fifth as for ACY. Aleven and Koedinger [2] described how to improve students’ learning with a
computer-based approach endowed with self-explanation. This is the second most cited publication, ranked
fourth in terms of ACY. Jin [15] authored the third most cited publication, presenting a fuzzy modeling
approach designed to improve the interpretability of high-dimensional systems. This publication is out of
the Top-5 in terms of ACY. Garcia et al. [10] reviewed statistical techniques to get a good interpretability-
accuracy trade-off in genetics-based machine learning. This is the fourth most cited paper and the second
one in terms of ACY. The fifth publication in terms of TC (out of the ACY Top-5) comes from Ishibuchi and
Nojima [14] who applied a multi-objective genetics-based machine learning approach to build fuzzy systems
with a good interpretability-accuracy trade-off. The scenario is completed by Martinez and Herrera [17]
(first paper in terms of ACY), who proposed a linguistic model for solving decision-making problems, and
Gacto et al. [9] (third paper in terms of ACY), who reviewed interpretability indexes for assessing IFS.
Notice that Herrera co-authored 3 of the Top-5 publications in terms of ACY: this emphasizes his leading
role in the XAI research field (see Table 1).

The leading sources in XAI are depicted in the pie chart on the left of Fig. 2. Most papers are published
in conference proceedings. Nevertheless, the Top-5 papers (see Table 2) appear in well-recognized journals.

Fig. 3 shows a graph with the most popular author keywords in the publications under study. Each
node is associated to a keyword and its size is proportional to the number of documents where the keyword

5
appears. Interpretability is the main keyword since it is associated to the larger node. Understandability
and classification are the second and the third main keywords. Links between nodes relate keywords which
usually appear together in the same documents.

This graph gives a global overview about the main topics of interest in the XAI research field, with
groups of closely related nodes painted in the same color. On the one hand, interpretability is closer to
topics usually addressed in the fuzzy logic community (e.g., fuzzy modeling or rule selection). On the
other hand, understandability is surrounded by keywords related to software engineering. The gap between
the main keywords is filled by other relevant nodes such as comprehensibility or self-explanation. Moreover,
interpretability and comprehensibility are related to a group of keywords including popular topics in AI (e.g.,
classification, data mining or knowledge discovery). A community of keywords is partially disconnected from
the rest of the graph {e.g., semantic web, ontology, and so on), and some single nodes lie away from others
(e.g., interpretability logic or image interpretability). That is due to their relatedness to some specific research
lines. Notice that NITRS stands for National Imagery Interpretability Rating Scale which is a subjective
scale for rating the quality of images.

When we turn to consider author co-citation, we look for pairs of authors being cited by the same
publications. Fig. 4 shows the co-citation map obtained by the VOS viewer (the minimum number of total
citations by author is set to 50). Size of nodes is proportional to the number of citations, while link weights

 

The graph was generated by the VOS viewer employing the suggested default parameters for layout visualization and
clustering of nodes. Other clustering approaches may be applied, but choosing the best approach is out of the scope of this

paper.

omputer-integpretable clinica
knowledge 4 fepresentation
self-explanations computer-intefpretable guideli
© knowledgediscavery . semantic web

timeseries Cecision tree ¢

ontglogy
ere visualization

sparsity
softwi i i

interpretable mode Bago owas Speineenne

ature extraction

_Beretic@lgorithm, e
rule extraction measuifement refactoring
La bpmn
e. compr' sibility *
genetic algorithms .. mw e maintainability

multi-objectiv@evolutioggry a

° interpretability woven © UNderstandability

rule s@jection ~=t8'Self-explanation mettics soft metrics
e ‘ae “ee s
© “fuzzy modelingaccuracy readability  .  “~
fuzzy system
prediction
interpretability logic
niirs
image quality
ay

Figure 3: Map of author keywords in XAI.
6
nae

ch 2 Oe
chap, 2 wosmon.

= 5 ™ kay j simop, h.a
+ &

wang, d.

  

michalewic

“pelegam.

Figure 4: Map of author co-citation in XAI.

Table 3: Top-5 Ranking of IFS Authors and Countries.
Rank. Authors (h-index) Authors (TC) Authors (NP) Countries (TC) Countries (NP)

 

 

1 Herrera (20) Herrera (1900) Herrera (42) Spain (3255) Spain (177)
2 Aleala (13) Alcala (1216) Mencar (29) Italy (850) China (116)
3 Alonso (12) Guillaume (707) — Alonso (26) France (766) Italy (81)

4 Magdalena (11) Ishibuchi (646) Alcala (25) UK (749) UK (68)

5 Mencar (10) Nojima (568) Fanelli (24) Poland (552) India (42)

 

come from the co-citation index defined by eq. (1). Most nodes are concentrated in the left-hand side of the
map. Again, Herrera stands out as the main node.

4. Detailed Analysis on Interpretable Fuzzy Systems

We replicated the previous analysis with a modified query:

Q2 = Q1 AND ‘‘fuzz*??

By adding “fuzz*” to Q1 we focus our search on publications in the XAI field that are related to fuzzy
sets and systems. Hereafter, we refer to this field of research as IFS. In addition, we filtered the collected
results by adopting the same constraints imposed in the previous section: (1) years range [2000-2017] and
(2) subject areas [Computer Sciences, Mathematics and Engineering]. As a result, we got 1054 documents,
consisting in about 28% of the whole set of documents previously analyzed.

The Top-5 rankings of authors and countries is detailed in Table 3 concerning IFS. Most authors in
Table 3 are present also in Table 1, thus certifying the relevance of the fuzzy community in the context of
XAI. However, USA (the leading country in Table 1) is now out of the Top-5. Moreover, European countries
take up the Top-5 in terms of TC. China and India appear only when looking at NP. These data reflect the
relevance of European scholars in the fuzzy community and their outstanding leadership in IFS.

Table 4: Top-5 Ranking of IFS Publications.
TC Ranking Publication (Authors, Year, Source, TC) [Ref]

 

 

 

 

1 S. Guillaume, 2001, IEEE T Fuzzy Syst, 435 [12]

2 Y. Jin, 2000, IEEE T Fuzzy Syst, 333 [15]

3 H. Ishibuchi and Y. Nojima, 2007, Int J Approx Reason, 266 [14]

4 M.J. Gacto et al., 2011, Inform Sciences, 200 [9]

5 L. Martinez and F. Herrera, 2012, Inform Sciences, 199 [17]
ACY Ranking Publication (Authors, Year, Source, ACY) [Ref]

1 L. Martinez and F. Herrera, 2012, Inform Sciences, 39.8) [17]

2 M. Fazzolari et al., 2013, IEEE T Fuzzy Syst, 34 [8]

3 M.J. Gacto et al., 2011, Inform Sciences, 33.3 [9]

4 S. Guillaume, 2001, IEEE T Fuzzy Syst, 27.2 [12]

5 H. Ishibuchi and Y. Nojima, 2007, Int J Approx Reason, 26.6 [14]

 
system identification Fuzzysets
¥ ipnsa fuzzy models. oe wed cla =

computinggwith words

2 we
fuzzy rule-based classifiers

neuro-fuzzy systems fem
. > ocmultiobjectiveroptimization
forecasting learning
clusterin e . :
fuzzy inference E eranular@aripung genetic algorithms
fi | fuzzy systems
x hi ing A y it tabil t d
Interpretabill ecuracy trai
fuzzy set mac ine learning p tye y ad
vattern ‘ rule selection
® inter ili
‘ — Yeceutacy fuzzy rule-baedsystems
; . slutionag@palz e
ecisian trees |
identities “fuzeyllosic (fuzzy modeling multi-objectiv@evolutionary a
identification © Iti-obiec enetic aleor
i miulti-objectivgjeenetic algori a:
decisign tree fuzzy elustering  pport vege machines - - accurgcy-intetpyetability trad
interpretable model a 5
cl sification
* rule mining
@ understanda bility fuzzy system

feature selection
.

comprehensibility
=

e
support vegor machine fuzzy classification systems fuzzy identificatian

creditgcorin|
& fuzzy association rules

Figure 5: Map of author keywords in IFS.

Table 4 lists the leading publications in IFS and reflects once again the relevance of the fuzzy community
in the context of XAI. All the papers in the TC ranking already appeared in Table 2: the current Top-3 is
included in the TC ranking related to XAI, while [17] and [9] appear in the Top-3 of the ACY ranking of
Table 2. Actually, only the work authored by Fazzolari et al. [8] (a review of multi-objective evolutionary
fuzzy systems devoted, among other things, to find a good interpretability /accuracy trade-off) is a new entry
with respect to Table 2.

Looking carefully at the map of author keywords in Fig. 5, we miss some of the important topics
highlighted in Fig. 3. For example, comprehensibility and understandability seem to play a much more
prominent role in XAI than in IFS. It could be argued that Fig. 5 may be read as a zoom produced in
a specific area of Fig. 3, namely the one related to the interpretability node. This suggests that many
important issues in XAJ are still to be addressed by IFS scholars.

Finally, Fig. 6 shows the map of author co-citation in IFS. Once again, the current map looks like a
zoom of the left-hand side of Fig. 4.

5. Concluding Remarks

The results reported in the previous sections allow a number of considerations. First, there is a strong
community of scholars in fuzzy logic addressing their study to the theme of XAI, with special emphasis on
interpretability. In fact, interpretability studies in fuzzy logic started from pioneering works in 1999 and
about one third of the selected papers belong to the fuzzy logic mainstream. As a result, many of the most
influential authors and papers in XAI refer to the fuzzy community. However, if we compare Fig. 3 with
Fig. 5 we observe that, within the fuzzy community, the main notions of interpretability, comprehensibility,
understandability and erplainability are not clearly distinct as in XAI. Rather, interpretability has a major
role while the other keywords are either treated as synonyms or distinctly used in very specialized studies
only.

Furthermore, a deeper analysis of the co-citation graph let us observe a neat separation between authors
in the fuzzy community and authors in XAI not related to fuzzy logic. This can be appreciated in Fig. 7

8
antonelli,

 
     
     

cannone, r
F gr
Fe All|) ji
eee,
: "
castiello, r 4 ‘ es
gongalez,

   
 
 

ee

 

© fuitkGwski, ° i ey es .
- 4 5 ek
eran. rity. uillaume, — setnes, ——
= eo. lozano m goreia, S
charegrnot babyske, ls © luenge, j
S. on NEP. ches y.@ ventura, 5
ques “be We. .
ebewgge |  Gvandewalle i?
“za lave e
zaden, | pullermele uid: rrankie
Yaser, -rkacpipryk,
«
manger, baesg@ns, b

Figure 6: Map of author co-citation in IFS.

martinez,

 

pang m.

 

erm]

rutkowski,

zalasinski

Figure 7: Map of author co-citation in XAI (zoom in the left hand side of Fig. 4).

where a zoom of the left-hand side of Fig. 4 is provided: authors related to fuzzy logic appear to be aggregated
in two compact clusters (the yellow and the green ones in Fig. 7) demonstrating also a tight interconnection
of the related research activities. On the other hand, authors in XAI not related to fuzzy logic appear to be
loosely distributed, as a sign of a more scattered collaboration.

This analysis suggests at least two lines of development. Firstly, there is a need to clarify and distinguish

9
the notions of interpretability, comprehensibility, understandability and erplainability to provide a common
terminological ground inside the varied XAI context. This could also shed light on refined conceptualizations
where fuzzy logic could significantly contribute. Moreover, an opportunity emerges to tighten the connec-
tions of studies between fuzzy and non-fuzzy worlds of XAI, which now appear unnecessarily separated.
We strongly believe that cross-fertilization between these communities is needed to successfully face the
challenges posed by XAI.

Acknowledgements

This work was supported by RYC-2016-19802 (Ramén y Cajal contract), and two MINECO projects
TIN2017-84796-C2-1-R (BIGBISC) and TIN2014-56633-C3-3-R (ABS4SOW). All of them funded by the
Spanish “Ministerio de Economia y Competitividad”. Financial support from the Xunta de Galicia (Centro
singular de investigacién de Galicia accreditation 2016-2019) and the European Union (European Regional
Development Fund - ERDF), is gratefully acknowledged.

References

{1

ACM US Public Policy Council, A., 2017. Statement on Algorithmic Transparency and Accountability. URL: http:

//waw.acm. org/binaries/content/assets/public-policy/2017{\_}usacm{\_}statement{\_}algorithms. pdf.

[2] Aleven, V., Koedinger, K., 2002. An effective metacognitive strategy: Learning by doing and explaining with a computer-

based cognitive tutor. Cognitive Sci 26, 147-179. doi:10.1016/S0364-0213(02)00061-7.

[3] Aria, M., Cuccurullo, C., 2017. bibliometrix: An R-tool for comprehensive science mapping analysis. J Informetr 11,

959-975. doi:10.1016/j .joi.2017.08.007.

di Battista, G., Eades, P., Tamassia, R., Tollis, I., 1998. Graph drawing: algorithms for the visualization of graphs.

Prentice Hall, Upper Saddle River, N.J.

Cobo, M., Lépez-Herrera, A., Herrera-Viedma, E., Herrera, F., 2011. Science mapping software tools: Review, analysis,

and cooperative study among tools. J Assoc Inf Sci Tech 62, 1382-1402.

[6] De Bellis, N., 2009. Bibliometrics and citation analysis: from the science citation index to cybermetrics. Scarecrow Press.

[7] Epstein, S.L., 2015. Wanted: Collaborative intelligence. Artif Intell 221, 36-45. doi:10.1016/j.artint.2014.12.006.

[8] Fazzolari, M., Alcal4, R., Nojima, Y., Ishibuchi, H., Herrera, F., 2013. A review of the application of multiobjective

evolutionary fuzzy systems: Current status and further directions. IEEE T Fuzzy Syst 21, 45-65. doi:10.1109/TFUZZ.

2012. 2201338.

Gacto, M., Alcala, R., Herrera, F., 2011. Interpretability of linguistic fuzzy rule-based systems: An overview of inter-

pretability measures. Inform Sciences 181, 4340-4360. doi:10.1016/j.ins.2011.02.021.

[10] Garcia, $., Fernandez, A., Luengo, J., Herrera, F., 2009. A study of statistical techniques and performance mea-
sures for genetics-based machine learning: Accuracy and interpretability. Soft Comput 13, 959-977. doi:10.1007/
800500-008-0392-y.

[11] Goodman, B., Flaxman, 8., 2016. European union regulations on algorithmic decision-making and a “right to explanation” ,
in: ICML Workshop on Human Interpretability in Machine Learning (WHI}, New York, NY. pp. 1-9.

[12] Guillaume, S., 2001. Designing fuzzy inference systems from data: An interpretability-oriented review. IEEE T Fuzzy
Syst 9, 426-443. doi:10.1109/91.928739.

[13] Gunning, D., 2016. Explainable Artificial Intelligence (XAI). Technical Report. Defense Advanced Research Projects
Agency. Arlington, USA. DARPA-BAA-16-53.

[14] Ishibuchi, H., Nojima, Y., 2007. Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy
genetics-based machine learning. Int J Approx Reason 44, 4-31. doi:10.1016/j.ijar.2006.01.004.

[15] Jin, Y., 2000. Fuzzy modeling of high-dimensional systems: Complexity reduction and interpretability improvement. IEEE
T Fuzzy Syst 8, 212-221. doi:10.1109/91.842154.

[16] Kobourov, 8.G., 2012. Force-directed drawing algorithms, in: Tamassia, R. (Ed.), Handbook of Graph Drawing and
Visualization. CRC Press. chapter 12.

[17] Martinez, L., Herrera, F., 2012. An overview on the 2-tuple linguistic model for computing with words in decision making:
Extensions, applications and challenges. Inform Sciences 207, 1-18. doi:10.1016/j.ins.2012.04.025.

[18] Moya-Anegén, F., Vargas-Quesada, B., Herrero-Solana, V., Chinchilla-Rodriguez, Z., Corera-Alvarez, E., Murioz-
Fernandez, F.J., 2004. A new technique for building maps of large scientific domains based on the cocitation of classes
and categories. Scientometrics 61, 129-145.

[19] Pancho, D., Alonso, J., Cordén, O., Quirin, A., Magdalena, L., 2013. Fingrams: Visual representations of fuzzy rule-based
inference for expert analysis of comprehensibility. IEEE T Fuzzy Syst 21, 1133-1149. doi:10.1109/TFUZZ. 2013. 2245130.

[20] Philip Chen, C., Zhang, C., 2014. Data-intensive applications, challenges, techniques and technologies: a survey on big
data. Inform Sciences 275, 314-347.

[21] Porter, M., Onnela, J., Mucha, P., 2009. Communities in networks. Not Am Math Soc 56, 1082-1166.

[22] Salton, G., Bergmark, D., 1979. A citation study of computer science literature. IEEE T Prof Commun 22, 146-158.

10

[4

[5

[9
[23] Serrano, E., Quirin, A., Botia, J., Cordén, O., 2010. Debugging complex software systems by means of pathfinder networks.
Inform Sciences 180, 561-583.

[24] Van Eck, N., Waltman, L., 2010. Software survey: Vosviewer, a computer program for bibliometric mapping. Scientometrics
84, 523-538.

[25] Vargas-Quesada, B., Moya-Anegon, F., 2007. Visualizing the Structure of Science. Springer-Verlag.

[26] Wasserman, S., Faust, K., 1994. Social Network Analysis: Methods And Applications (Structural Analysis in the Social
Sciences). Cambridge University Press.

11
44 Al MAGAZINE

Deep Learning and Security

DARPA’s Explainable
Artificial Intelligence Program

David Gunning, David W. Aha

Mi Dramatic success in machine learning
has led to a new wave of Al applications (for
example, transportation, security, medicine,
finance, defense) that offer tremendous
benefits but cannot explain their decisions
and actions to human users. DARPA’s
explainable artificial intelligence (XAD)
program endeavors to create AI systems
whose learned models and decisions
can be understood and appropriately
trusted by end users. Realizing this goal
requires methods for learning more
explainable models, designing effective
explanation interfaces, and understanding
the psychologic requirements for effective
explanations. The XAI developer teams are
addressing the first two challenges by
creating ML techniques and developing
principles, strategies, and human-computer
interaction techniques for generating effec-
tive explanations. Another XAI team is
addressing the third challenge by summa-
rizing, extending, and applying psychologic
theories of explanation to help the XAI
evaluator define a suitable evaluation
framework, which the developer teams
will use to test their systems. The XAI
teams completed the first of this 4-year
program in May 2018. In a series of
ongoing evaluations, the developer
teams are assessing how well their XAM
systems’ explanations improve user un-
derstanding, user trust, and user task
performance.

to produce Al systems that perceive, learn, decide, and

act on their own. However, they will be unable to
explain their decisions and actions to human users. This lack
is especially important for the Department of Defense, whose
challenges require developing more intelligent, autonomous,
and symbiotic systems. Explainable AI will be essential if
users are to understand, appropriately trust, and effectively
manage these artificially intelligent partners. To address this,
DARPA launched its explainable artificial intelligence (XAI)
program in May 2017. DARPA defines explainable AI as AI
systems that can explain their rationale to a human user,
characterize their strengths and weaknesses, and convey an
understanding of how they will behave in the future. Naming
this program explainable AI (rather than interpretable,
comprehensible, or transparent AI, for example) reflects
DARPA’s objective to create more human-understandable AI
systems through the use of effective explanations. It also
reflects the XAI team’s interest in the human psychology of
explanation, which draws on the vast body of research and
expertise in the social sciences.

A dvances in machine learning (ML) techniques promise

Copyright © 2019, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602
Early AI systems were predominantly logical and
symbolic; they performed some form of logical in-
ference and could provide a trace of their inference
steps, which became the basis for explanation. There
was substantial work on making these systems more
explainable, but they fell short of user needs for
comprehension (for example, simply summarizing
the inner workings of a system does not yield a suf-
ficient explanation) and proved too brittle against
real-world complexities.

Recent AI success is due largely to new ML tech-
niques that construct models in their internal repre-
sentations. These include support vector machines,
random forests, probabilistic graphical models, re-
inforcement learning (RL), and deep learning (DL)
neural networks. Although these models exhibit high
performance, they are opaque. As their use has in-
creased, so has research on explainability from the
perspectives of ML (Chakraborty et al. 2017; Ras et al.
2018) and cognitive psychology (Miller 2017). Simi-
larly, many XAl-related workshops have been held re-
cently on ML (for example, the International Conference
on Machine Learning, the Conference on Neural In-
formation Processing Systems), AI (for example, the In-
ternational Joint Conference on Artificial Intelligence),
and HCI (for example, the Conference on Human-
Computer Interaction, Intelligent User Interfaces) con-
ferences, as have special topic meetings related to XAI.

There seems to be an inherent tension between ML
performance (for example, predictive accuracy) and
explainability; often the highest-performing methods
(for example, DL) are the least explainable, and the
most explainable (for example, decision trees) are the
least accurate. Figure 1 illustrates this with a notional
graph of the performance-explainability trade-off for
various ML techniques.

When DARPA formulated the XAI program, it envi-
sioned three broad strategies to improve explainability,
while maintaining a high level of learning performance,
based on promising research at the time (figure 2): deep
explanation, interpretable models, and model induction.

Deep explanation refers to modified or hybrid DL
techniques that learn more explainable features or
representations or that include explanation genera-
tion facilities. Several design choices might produce
more explainable representations (for example, training
data selection, architectural layers, loss functions,
regularization, optimization techniques, training
sequences). Researchers have used deconvolutional
networks to visualize convolutional network layers, and
techniques existed for associating semantic concepts
with deep network nodes. Approaches for generating
image captions could be extended to train a second deep
network that generates explanations without explicitly
identifying the original network’s semantic features.

Interpretable models are ML techniques that learn
more structured, interpretable, or causal models. Early
examples included Bayesian rule lists (Letham et al.
2015), Bayesian program learning, learning models of
causal relationships, and use of stochastic grammars
to learn more interpretable structure.

Deep Learning and Security

Model induction refers to techniques that experi-
ment with any given ML model— such as a black
box— to infer an approximate explainable model. For
example, the model-agnostic explanation system of
Ribeiro et al. (2016) inferred explanations by ob-
serving and analyzing the input-output behavior of a
black box model.

DARPA used these strategies to categorize a portfolio
of new ML techniques and provide future practitioners
with a wider range of design options covering the
performance-explainability trade space.

XAI Concept and Approach

The XAI program’s goal is to create a suite of new or
modified ML techniques that produce explainable
models that, when combined with effective explanation
techniques, enable end users to understand, appropri-
ately trust, and effectively manage the emerging gen-
eration of AI systems. The target of XAI is an end user
who depends on decisions or recommendations pro-
duced by an AI system, or actions taken by it, and
therefore needs to understand the system’s rationale.
For example, an intelligence analyst who receives rec-
ommendations from a big data analytics system needs
to understand why it recommended certain activity for
further investigation. Similarly, an operator who tasks
an autonomous vehicle to drive a route needs to un-
derstand the system’s decision-making model to ap-
propriately use it in future missions. Figure 3 illustrates
the XAI concept: provide users with explanations that
enable them to understand the system’s overall
strengths and weaknesses, convey an understanding of
how it will behave in future or different situations, and
perhaps permit users to correct the system’s mistakes.

This user-centered concept poses interrelated re-
search challenges: (1) how to produce more explain-
able models, (2) how to design explanation interfaces,
and (3) how to understand the psychologic require-
ments for effective explanations. The first two chal-
lenges are being addressed by the 11 XAI research
teams, which are developing new ML techniques to
produce explainable models, and new principles,
strategies, and HCI techniques (for example, visuali-
zation, language understanding, language generation)
to generate effective explanations. The third challenge
is the focus of another XAI research team that is
summarizing, extending, and applying psychologic
theories of explanation.

The XAI program addresses two operationally rel-
evant challenge problem areas (figure 4): data analytics
(classification of events of interest in heterogeneous
multimedia data) and autonomy (decision policies
for autonomous systems). These areas represent
two important ML problem categories (supervised
learning and RL) and Department of Defense in-
terests (intelligence analysis and autonomous systems).

The data analytics challenge was motivated by a
common problem: intelligence analysts are presented
with decisions and recommendations from big data

SUMMER 2019 45
Deep Learning and Security

 

 

  
     
       
   
 

  
   

  

Learning Techniques

Performance vs.
Explainability

Graphical
Models

   

Learning

  

Bayesian
Belief Nets

SRL

CREs HBNs

  
  

AOGs

 
  

Statistical ML

Models

    
 
 

  

Decision

 

    

Trees

 

SVMs

 

Explainability

 

 

46

analytics algorithms and must decide which to report as
supporting evidence in their analyses and which to
pursue further. These algorithms often produce false
alarms that must be pruned and are subject to concept
drift. Furthermore, these algorithms often make recom-
mendations that the analyst must assess to determine
whether the evidence supports or contradicts their hy-
potheses. Effective explanations will help confront these
issues.

The autonomy challenge was motivated by the
need to effectively manage AI partners. For example,
the Department of Defense seeks semiautonomous
systems to augment warfighter capabilities. Operators
will need to understand how these behave so they can
determine how and when to best use them in future
missions. Effective explanations will better enable such
determinations.

For both challenge problem areas, it is critical to
measure explanation effectiveness. While it would be
convenient if a learned model’s explainability could be
measured automatically, an XAI system’s explanation
effectiveness must be assessed according to how its
explanations aid human users. This requires human-
in-the-loop psychologic experiments to measure the
user’s satisfaction, mental model, task performance,
and appropriate trust. DARPA formulated an initial
explanation evaluation framework that includes po-
tential measures of explanation effectiveness (figure 5).
Exploring and refining this framework is an important
part of the XAI program’s research agenda.

The XAI program’s goal, concept, strategies, chal-
lenges, and evaluation framework are described in
the program’s 2016 broad agency announcement.
Figure 6 displays the XAI program’s schedule, which

AI MAGAZINE

Figure 1. Learning Performance Versus Explainability Trade-Off for Several Categories of Learning Techniques.

consists of two phases. Phase 1 (18 months) com-
menced in May 2017 and includes initial technology
demonstrations of XAI systems. Phase 2 (30 months)
includes a sequence of evaluations against challenge
problems selected by the system developers and the
XAI evaluator. The first formal evaluations of XAI
systems took place during the fall of 2018. This article
describes the developer teams’ progress leading up to
these evaluations, whose results were presented at an XAT
program meeting during the winter of 2019.

XAI Program
Development and Progress

Figure 7 summarizes the 11 XAI Technical Area 1 (TA1)
developer teams and the TA2 team [from the Florida
Institute for Human and Machine Cognition (IHMC)]
that is developing the psychologic model of explanation.
Three TA1 teams are pursuing both challenge problem
areas (autonomy and data analytics), three are working on
only the former, and five are working on only the latter.
Per the strategies described in figure 2, the TA1 teams are
investigating a diverse range of techniques for developing
explainable models and explanation interfaces.

Naturalistic
Decision-Making Foundations of XAI

The objective of the IHMC team (which includes
researchers from MacroCognition and Michigan
Technological University) is to develop and evaluate
psychologically plausible models of explanation and
develop actionable concepts, methods, measures, and
metrics for explanatory reasoning. The IHMC team is
Deep Learning and Security

 

New
Approach

   
 
    

Create a suite of
machine learning
techniques that
produce more
explainable models,
while maintaining a
high level of
learning
performance

  

Deep
Learning

  

Learning Techniques

Graphical
Models

Bayesian
Belief Nets

 
  
 
    

o.U°#
o @

Ensemble @Q °

Methods

  
 

Explainability

 

 

 

 

x

ain \
DENG AR

LEN DENA
Input Hidden Output
units units units

 

 

 

 

 

  

 

 

 

 

 

S|...
ap " ]
Experiment

 

 

 

 

 

 

 

Deep Explanation
Modified deep learning
techniques to learn
explainable features

 

 

 

Interpretable Models

Techniques to learn more
structured, interpretable,
causal models

 

Model Induction

Techniques to infer an
explainable model from any
model as a black box

 

 

 

 

 

Figure 2. Strategies for Developing Explainable Models.

investigating the nature of explanation itself. What
must happen for a person to be satisfied with an attempt
to explain (1) the workings of a complex system and (2)
why it acted the way it did in a given situation? To
address such questions, the team has formulated a
naturalistic model of human explanatory reasoning and
is providing guidance to the performer teams on
methods for evaluating the effectiveness of their XAI
systems’ explanations. The team reviewed the relevant
literatures in philosophy of science and specializations
within psychology, from which criteria were synthe-
sized for assessing the “goodness” of explanations. The
team is also collecting and analyzing a corpus of cases in
which individuals create or receive explanations of the
workings of complex systems.

This team developed measures for explanation
goodness, a user’s mental model (for example, cor-
rectness, completeness), and user task performance.
From this, the user can make reasonably accurate
judgments about when to trust (or doubt) the system.
To gain this insight, the user must explore the decision-
making processes and performance of an XAI system,
especially for boundary cases, including ways in which
deep neural networks (DNNs) can be spoofed. This

methodology was described in a series of essays
(Hoffman and Klein 2017; Hoffman et al. 2017; Klein
2018; Hoffman et al. 2018).

Figure 8 illustrates IHMC’s model of the XAI ex-
planation process, highlighting measurement categories
for assessing explanation effectiveness. The user re-
ceives a recommendation or decision from an XAT
system, along with an explanation that could be tested
for goodness (versus preestablished criteria) and user
satisfaction. The explanation contributes to the user’s
mental model of the AI system, which could be tested
for accuracy and comprehension. The AI system’s
recommendations and the user’s mental model may
enable, or decrease, user task performance, which
could also be measured. These processes contribute to
the user’s appropriate, or inappropriate, trust of the AI
system. The XAI evaluator is using this model to test
the developer teams’ XAI systems.

Evaluation

The XAI program’s independent government evalua-
tor is the Naval Research Laboratory. For Phase 1, the
laboratory (with IHMC’s help) prepared an evaluation
framework for the TA1 teams to use as a template for

SUMMER 2019 47

Performance vs.
Explainability

 
Deep Learning and Security

 

 

e Why did you do that?

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

  

 

    

 

e Why not something else?
Learning This is a cat e When do you succeed?
Process (p = 0.93) ¢ When do you fail?
: Input Hidden Output e When can | trust you?
Training Learned Output User with * How do | correct an error?
Data Function a Task
Tomorrow ¢ | understand why
N This is a cat ¢ | understand why not
ew It has fur, ,
Learning +4 whiskers, claws Ho e | know when you'll succeed
It has this feature ¢ | know when you'll fail
Process Vye PEE rs ads * | know when to trust you
Explainable Explanation — User with * | know why you erred
Model Interface a Task

 

 

 

 

Figure 3. The XAI Concept.

designing and implementing their Phase 1 evaluation
experiments, where they will select a test problem or
problems in the challenge problem areas of data an-
alytics or autonomy; apply their new ML techniques
to learn an explainable model for their problems;
evaluate the performance of their learned ML model
(table 1); combine their learned model with their
explanation interface to create their explainable
learning system; conduct experiments in which users
perform specified tasks using the explainable learning
system; and measure explanation effectiveness by
employing IHMC’s model of the explanation process
(figure 8) and explanation effectiveness measurement
categories (table 1).

The evaluations will include the following experi-
mental conditions: (1) without explanation: the XAI
system is used to perform a task without providing
explanations to the user; (2) with explanation: the
XAI system is used to perform a task and generates
explanations for every recommendation or decision it
makes and every action it takes; (3) partial explana-
tion: the XAI system is used to perform a task and
generates only partial or ablated explanations (to as-
sess various explanation features); and (4) control: a
baseline state-of-the-art nonexplainable system is used
to perform a task.

Explainable Learning Systems

Table 2 summarizes the TAl teams’ technical ap-
proaches and Phase 1 test problems.

48 Al MAGAZINE

Deeply Explainable AI

The University of California, Berkeley (UCB) team
(including researchers from Boston University, the
University of Amsterdam, and Kitware) is developing
an Al system that is human understandable by virtue
of explicit structural interpretation (Hu et al. 2017),
provides post hoc (Park et al. 2018) and introspective
(Ramanishka et al. 2017) explanations, has predictive
behavior, and allows for appropriate trust (Huang
et al. 2018). The key challenges of deeply explain-
able AI (DEXAD) are to generate accurate explanations
of model behavior and select those that are most useful
to a user. UCB is addressing the former by creating
implicit or explicit explanation models: they can im-
plicitly present complex latent representations in un-
derstandable ways or build explicit structures that are
inherently understandable. These DEXAI models create
a repertoire of possible explanatory actions. Because
these actions are generated without any user model,
they are called reflexive. For the second challenge, UCB
proposes rational explanations that use a model of the
user’s beliefs when deciding which explanatory actions
to select. UCB is also developing an explanation interface
based on these innovations informed by iterative design
principles.

UCB is addressing both challenge problem areas.
For autonomy, DEXAI will be demonstrated in ve-
hicle control (using the Berkeley Deep Drive data set
and the CARLA simulator) (Kim and Canny 2017)
and strategy game scenarios (StarCraft II). For data
Deep Learning and Security

 

 

Explain
decisions

4

Use the
explanation

 

 

 

 

 

 

 

 

 

Multimedia Data

 

An analyst is

Data looking for
° . . items of
Anal Ics Explainable | Explanation ; ;
yt Model Interface interest n
Classification ratined ia
Learning Task data sets

 

 

Classifies items of
interest in large data set

Explains why/why not
for recommended items

Analyst decides which
items to report, pursue

 

 

 

 

An operator is

 

 

 

 

 

©ArduPikot.org

 

ArduPilot & SITL Simulation

  

directin
Autonom g
y Explainable | Explanation autonomous
Reinforcement Model Interface rem
Learning Task xe ish a
% missions

Eas

 

 

Learns decision policies
for simulated missions

 

 

Explains behavior in an
after-action review

 

Operator decides which
future tasks to delegate

 

 

 

 

Figure 4. XAI Challenge Problem Areas.

analytics, DEXAI will be demonstrated using visual
question answering (VQA) and filtering tasks (for
example, using large-scale data sets such as VQA-X
and ACT-X for VQA tasks and activity recognition
tasks, respectively), xView, and Distinct Describable
Moments (Hendricks et al. 2018).

Causal Models to Explain Learning

The goal of the Charles River Analytics (CRA) team
(including researchers from the University of Massa-
chusetts and Brown University) is to generate and
present causal explanations of ML operation, through
its causal models to explain learning (CAMEL) ap-
proach. CAMEL explanations are presented to a user
as narratives in an interactive, intuitive interface.
CAMEL includes a causal probabilistic programming
framework that combines representations and learning
methods from causal modeling (Marazopoulou et al.
2015) with probabilistic programming languages
(Pfeffer 2016) to describe complex and rich phenomena.
CAMEL can be used to describe what an ML system did,
how specific data characteristics influenced its outcome,

and how changing these factors would affect this out-
come. Generative probabilistic models, represented in
a probabilistic programming language, naturally ex-
press causal relationships; they are well suited for this
task of explaining ML systems.

CAMEL probes the internal representation of an ML
system to discover how it represents user-defined,
natural domain concepts. It then builds a causal
model of their effect on the ML system’s operation by
conducting experiments in which the domain con-
cepts are systematically included or removed. CRA has
applied this approach to DNNs for classification and
RL.

Once learned, it uses causal probabilistic models to
infer explanations of the system’s predictions or ac-
tions. Because inferences can be large and complex and
can contain many interacting components, CAMEL
composes them into explanatory narratives that walk
the user through the interactions of the major concepts
and their influence on the ML system’s output. The
CAMEL explanation interface, based on cognitive
systems engineering design principles and established

SUMMER 2019 49
Deep Learning and Security

 

Explanation Framework

 

Task

Recommendation,
Decision or
Action

    
   
  

 

 

 

 

 

Explainable | Explanation Decision
Model Interface
The user
makes a
XAI System Explanation decision
The system takes The system provides based on the
input from the current —_an explanation to the explanation

task and makes a
recommendation,
decision, or action

user that justifies its
recommendation,
decision, or action

Measure of Explanation

Effectiveness

 

User Satisfaction

 

¢ Clarity of the explanation (user rating)
¢ Utility of the explanation (user rating)

 

Mental Model

 

e Understanding individual decisions
e Understanding the overall model

¢ Strength/weakness assessment

¢ ‘What will it do’ prediction

¢ ‘How do | intervene’ prediction

 

Task Performance

 

¢ Does the explanation improve the
user’s decision, task performance?

¢ Artificial decision tasks introduced to
diagnose the user’s understanding

 

 

 

Trust Assessment

 

 

 

¢ Appropriate future use and trust

 

Correctability (Extra Credit)

 

¢ Identifying errors
¢ Correcting errors
¢ Continuous training

 

 

 

 

Figure 5. Evaluating Explanation Effectiveness.

HCI techniques, allows users to understand and in-
teract with the explanatory narratives, engendering
trust in automation and enabling effective user-system
teamwork.

CRA is addressing both challenge problem areas.
For data analytics, CAMEL has been demonstrated
using pedestrian detection (using the INRIA pedestrian
data set) (Harradon et al. 2018), and CRA is working
toward activity recognition tasks (using ActivityNet). For
autonomy, CAMEL has been demonstrated on the Atari
game Amidar, and CRA is working toward demon-
strating it on StarCraft IT.

Learning and

Communicating

Explainable Representations

for Analytics and Autonomy

The University of California, Los Angeles (UCLA) team
(including researchers from Oregon State University and
Michigan State University) is developing interpretable
models that combine representational paradigms,

SO AI MAGAZINE

including interpretable DNNs, compositional graphical
models such as and-or graphs, and models that produce
explanations at three levels (that is, compositionality,
causality, and utility).

UCLA’s system includes a performer that executes
tasks on multimodal input data and an explainer that
explains its perception, cognitive reasoning, and de-
cisions to a user. The performer outputs interpretable
representations in a spatial, temporal, and causal
parse graph (STC-PG) for three-dimensional scene
perception (for analytics) and task planning (for au-
tonomy). STC-PGs are compositional, probabilistic,
attributed, interpretable, and grounded on DNN
features from images and videos. The explainer out-
puts an explanatory parse graph in a dialogue process
(She and Chai 2017), localizes the relevant subgraph
in the STC-PG, and infers the user’s intent.

The system represents explanations at three levels:
(1) concept compositions, represented by parse graph
fragments that depict how information is aggregated
from its constituents and contexts, how decisions are
made at nodes under uncertainty, and the decision’s
Deep Learning and Security

 

 

FY2017,

FY2019

FY2021

 

Fm

faayDun] sa. [auc] ser [ocr] ov] oxc [oan | es [oe] are [av] on J ov [auc] ser [ocr [vou)oce] an

a Puna] arn Poor] ron] 1a Jour] ser [ocr] nov occ [ran] ea [oan] are | wav] on J oc [auc] ser [ocr [vou)occ] mn] rea Joan] rm

aa

 

PHASE 1
Technology Demonstrations

PHASE 2
Government Evaluations

 

 

Prepare for

Evaluator (NRL) Eval 1

Define Evaluation Framework

Analyze Analyze
H Results Prepare for Eval 2

Results Prepare for Eval 3 3

Analyze Results;
Accept Software
Libraries/Toolkits

 

 

 

 

Explainable Learning
Systems
(11 Developer Teams)

Develop and Demonstrate Explainable
Learning Systerns

Refine and Test Explainable
1 Learning Systerns

Refine and Test Explainable
Learning Systerns 3

Deliver Software
Libraries and

 

 

Psychological Model
of Explanation
(1 Team)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Toolkits
[UT TT Tt tt tt LT I I | | I [TTT TI I T {TT J J
Fl F Pp 7 Ps Deliver Final
Summarize Current Psychological Develop Theoretical Model of Refine and Test Model of Explanation to Support Model of
Theories of Explanation Explanation Systern Development and Evaluation 9
Explanation
| | | Firfal

 

 

 

 

 

Kickoff Progress Report Tech Demos

Eval 1 Results Eval 2 Results

Eval 3 Results

 

 

 

Figure 6. XAI Program Schedule.

confidence levels; (2) causal and counterfactual rea-
soning, realized by extracting causal diagrams from STC-
PGs to predict what will happen if certain alternative
actions had been taken; and (3) utility explanations,
which explain why the system made certain decisions.

UCLA is addressing both XAI challenge problem
areas using a common framework of representation
and inference. For data analytics, UCLA demonstrated
their system using a network of video cameras for scene
understanding and event analysis. For autonomy,
UCLA demonstrated it in scenarios using robots exe-
cuting tasks in physics-realistic virtual reality
platforms and autonomous vehicle driving game
engines.

Explanation-Informed Acceptance
Testing of Deep Adaptive Programs

Oregon State University (OSU) is developing tools for
explaining learned agents that perform sequential
decision making and is identifying best principles for
designing explanation user interfaces. OSU’s explain-
able agent model employs explainable deep adaptive
programs (xDAPs), which combine adaptive programs,
deep RL, and explainability. With xDAPs, program-
mers can create agents by writing programs that in-
clude choice points, which represent decisions that
are automatically optimized via deep RL through
simulator interaction. For each choice point, deep RL
attaches a trained deep decision neural network
(dNN), which can yield high performance but is in-
herently unexplainable.

After initial xDAP training, xACT trains an explana-
tion neural network (Qi and Li 2017) for each dNN.
These provide a sparse set of explanation features (x-
features) that encode properties of a dNN’s decision
logic. Such x-features, which are neural networks, are
not initially human interpretable. To address this,

xACT enables domain experts to attach interpretable
descriptions to x-features, and xDAP programmers to
annotate environment reward types and other con-
cepts, which are automatically embedded into the
dNNs as “annotation concepts” during learning.

The dNN decisions can be explained via the de-
scriptions of relevant x-features and annotation con-
cepts, which can be further understood via neural
network saliency visualization tools. OSU is investi-
gating the utility of saliency computations for
explaining sequential decision making.

OSU’s explanation user interface allows users to
navigate thousands of learned agent decisions and
obtain visual and natural language (NL) explanations. Its
design is based on information foraging theory (IFT),
which allows a user to efficiently drill down to the most
useful explanatory information at any moment. The
assessment of rationales for learned decisions may more
efficiently identify flaws in the agent’s decision
making and improve user trust.

OSU is addressing the autonomy challenge prob-
lem area and has demonstrated xACT in scenarios
using a custom-built real-time strategy game engine.
Pilot studies have informed the explanation user in-
terface design by characterizing how users navigate
Al-agent game play and tend to explain game de-
cisions (Dodge et al. 2018).

Common Ground
Learning and Explanation

The Palo Alto Research Center (PARC) team (including
researchers from Carnegie Mellon University, the Army
Cyber Institute, the University of Edinburgh, and the
University of Michigan) is developing an interactive
sensemaking system that can explain the learned ca-
pabilities of an XAI system that controls a simulated
unmanned aerial system.

SUMMER 2019 51

 

 
Deep Learning and Security

 

 

Training Data

 

 

 

 
    

 

 

 

 

 

 

Psychological Model of Explanation

 

 

 

 

 

       
 

Explanation

 

Explanation
Quality

 

User
Performance
User Satisfaction

User’s Mental
Model

 

 

 
  

Better
Performance

 

 

Trust or Mistrust

 

User
Comprehension

 

 

 

 

  
     
 

Appropriate
Trust

  

Appropriate Use

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Leatning Explainable | Explanation
Process Model Interface
CP Area Performer Explainable Model Explanation Interface
UCB Deep Learning Reflexive and Rational
Both CRA Causal Model Induction Narrative Generation
UCLA Pattern Theory+ 3-Level Explanation
OSU Adaptive Programs Acceptance Testing
Autonomy PARC Cognitive Modeling Interactive Training
CMU Explainable RL XRL Interaction
SRI Deep Learning Show-and-Tell Explanations
Raytheon BBN Deep Learning Argumentation and Pedagogy
Analytics UTD Probabilistic Logic Decision Diagrams
TAMU Mimic Learning Interactive Visualization
Rutgers Model Induction Bayesian Teaching

 

 

 

IHMC

 

Figure 7.

XAI Research Teams.

 

 

An XAI system’s explanations should communicate
what information it uses to make decisions, whether it
understands how things work, and its goals. To
address this, PARC’s common ground learning and
explanation (COGLE) and its users establish com-
mon ground about what terms to use in explana-
tions and their meaning. This is enabled by PARC’s
introspective discourse model, which interleaves
learning and explaining processes.

Performing tasks in the natural world is challeng-
ing for autonomous systems, requiring experience to
create enough knowledge for reliable high perfor-
mance. COGLE employs K-models that encode an AI
agent’s domain-specific task knowledge. K-models orga-
nize this knowledge into levels of elements, where higher
(lower) levels model actions with longer range (local)
effects. They support a competency-based framework
that informs and guides the learning and testing of XAI
systems.

COGLE’s multilayer architecture partitions its in-
formation processing into sensemaking, cognitive
modeling, and learning. The learning layer employs
capacity constrained recurrent and hierarchical DNNs
to produce abstractions and compositions over the
states and actions of unmanned aerial systems to sup-
port an understanding of generalized patterns. It
combines learned abstractions to create hierarchical,
transparent policies that match those learned by the

52 AI MAGAZINE

system. The cognitive layer bridges human-usable
symbolic representations to the abstractions, composi-
tions, and generalized patterns.

COGLE’s explanation interfaces support performance
review, risk assessment, and training. The first provides a
map that traces an unmanned aerial systems’ mission
actions and divides the action or decision (flight) path
into explainable segments. The second interface’s
tools enable users to examine and assess the sys-
tem’s competencies and make predictions about
mission performance.

COGLE will be demonstrated in ArduPilot’s Soft-
ware-in-the-Loop Simulator and a discretized abstract
simulation test bed. It will be evaluated by drone op-
erators and analysts. Competency-based evaluation will
help PARC to determine how best to develop appro-
priate domain understandable models.

Explainable Reinforcement Learning

Carnegie Mellon University is creating a new disci-
pline of explainable RL to enable dynamic human-
machine interaction and adaptation for maximum
team performance. This effort has two goals: to de-
velop new methods for learning inherently explain-
able RL policies and to develop strategies that can
explain existing black-box policies. For the former,
Carnegie Mellon is developing methods to improve
model learning for RL agents to capture the benefits
Deep Learning and Security

 

EXPLAINABLE ARTIFICIAL INTELLIGENCE

System

 

 

receives :
Explanation

is assessed by

may initially

 

 

 

  
  

Trust or
Mistrust

   

gives way to

 

revises

can engender

 

XAI Process

 
  
   

User-System
Task
Performance

User’s Mental
Model

 

   
   

    

    
  

  

is assessed by is assessed by

   
  
   

Appropriate
Use

 

Figure 8. Initial Model of the Explanation Process and Explanation Effectiveness Measurement Categories.

of model-based approaches (ability to visualize plans
in the internal model space), while integrating
the benefits of model-free approaches (simplicity
and higher ultimate performance). These include
methods that incrementally add states and actions to
world models after discovering relevant latent in-
formation, learn models via end-to-end training
of complex model-based optimal control policies,
learn general DL models that directly integrate
and exploit rigid body physics (Belbute-Peres and
Kolter 2017), and learn understandable predictive
state representations using recurrent architectures
(Hefny et al. 2018).

Carnegie Mellon is also developing methods that
can explain the actions and plans of black-box RL
agents, observed either online or from system logs.
This involves answering questions such as, why did
an agent choose a particular action? or, what train-
ing data were most responsible for this choice? To
achieve this, Carnegie Mellon developed techniques
that generate NL descriptions of agents from behavior
logs and detect outliers or anomalies. Carnegie Mellon
also developed improvements over traditional in-
fluence function methods in DL, allowing its XRL
system to precisely identify the portions of a train-
ing set that most influence a policy’s outcome.

Carnegie Mellon is addressing the autonomy chal-
lenge problem area and has demonstrated XRL in
several scenarios, including OpenAIl Gym, Atari
games, autonomous vehicle simulation, mobile service
robots, and self-improving educational software and
games.

Deep Attention-Based
Representations for Explanation/
Explainable Generative Adversarial Networks

SRI International’s team (including researchers from
the University of Toronto, the University of Guelph,
and the University of California, San Diego) is de-
veloping an explainable ML framework for multi-
modal data analytics that generates show-and-tell
explanations with justifications of decisions accom-
panied by visualizations of input data used to generate
inferences.

The deep attention-based representations for
explanationxplainable generative adversarial networks
(DARE/X-GANS) system employs DNN architectures
inspired by attentional models in visual neuroscience.
It identifies, retrieves, and presents evidence to a user as
part of an explanation. The attentional mechanisms
provide a user with a means for system probing and
collaboration.

SUMMER 2019 53

     

  
  
Deep Learning and Security

 

Measure Description

ML Model performance

Various measures (on a per-challenge problem area
basis)

Accuracy/performance of the ML model in its given domain (to
understand whether performance improved or degraded relative to
state-of-the-art nonexplainable baselines)

Explanation Effectiveness

Explanation goodness Features of explanations assessed against criteria for explanation

goodness

Explanation satisfaction User’s subjective rating of explanation completeness, usefulness,

accuracy, and satisfaction

Mental model understanding User’s understanding of the system and the ability to predict the system’s

decisions/behavior in new situations

User task performance Success of the user performing the tasks for which the system is designed

to support

 

Appropriate Trust and Reliance

User’s ability to know when to, and when not to, trust the system’s

recommendations and decisions

 

Table 1. Measurement Categories.

DARE/X-GANS uses generative adversarial net-
works (GANs), which learn to understand data by
creating it, while learning representations with explan-
atory power. GANs are made explainable by using
interpretable decoders that map unsupervised clus-
ters onto parts-based representations. This involves
generating visual evidence, given text queries, using
text-to-parts generation (Vicol et al. 2018), the parts
being interpretable features such as human poses or
bounding boxes. This evidence is then used to
search the queried visual data.

The system presents explanations of its answers
based on visual concepts extracted from the mul-
timodal data inputs and knowledge base queries.
Given explanatory questions, it provides justifica-
tions, visual evidence used for decisions, and vi-
sualizations of the system’s inner workings. This
show-and-tell explanation interface ensures highly
intuitive explanations, made possible by atten-
tional modules that localize evidence used for
each visual task. Initial studies demonstrate that
such explanations substantially improve user task
performance.

SRI is addressing the data analytics challenge
problem area and has demonstrated DARE/X-GANs
using VQA and multimodal QA tasks with image and
video data sets.

Explainable Question Answering System

The Raytheon BBN Technologies team (including
researchers from the Georgia Institute of Technology,
MIT, and the University of Texas, Austin) is devel-
oping a system that answers unrestricted NL questions
posed by users about multimedia data and provides

54 AI MAGAZINE

interactive, explorable explanations of why it derived
an answer.

The explainable question answering system
(EQUAS) learns explainable DNN models in which
internal structures (for example, individual neurons)
have been aligned to semantic concepts (for example,
wheels and handlebars) (Zhou et al. 2015). This allows
neural activations within the network, during a de-
cision process, to be translated to NL explanations (for
example, “this object is a bicycle because it has two
wheels and handlebars”). EQUAS also uses neural
visualization techniques to highlight input regions
associated with neurons that most influenced its de-
cisions. To express case-based explanations, EQUAS
retains indexes and retrieves cases from its training
data that support its choices. Rejected alternatives are
recognized and ruled out using contrastive language,
visualization, and examples. Four explanation mo-
dalities map to key elements of argument construc-
tion and interactive pedagogy: didactic statements,
visualizations, cases, and rejections of alternative
choices.

The EQUAS explanation interface allows users to
explore the explanation space populated by these
explanation modes. It enables iterative and guided
collaborative interaction, allowing users to drill down
into the supporting evidence from each explanation
category.

Raytheon BBN is addressing the analytics chal-
lenge problem area and has demonstrated initial
EQUAS capabilities on VQA tasks for images, ex-
ploring how different explanation modalities en-
able users to understand and predict the behavior of
the underlying VQA system.
Deep Learning and Security

 

 

 

aK vn) Explainable Model Explanation Interface Challenge Problem
UC Berkeley e Post hoc explanations by e Reflexive explanations (arise e Autonomy: vehicle control (BDD-
training additional DL models from the model} X, CARLA), strategy games

(StarCraft ID

e Explicit introspective e Rational explanations (come e Analytics: visual QA and filtering
explanations (NMNs) from reasoning about user’s tasks (VQA-X, ACT-X, xView,
beliefs) DiDeMo, etc.)

e Reinforcement learning
(informative rollouts, explicit
modular agent)

Charles River Experiment with the learned Interactive visualization based on e Autonomy: Atari, StarCraft II
Analytics model to team an explainable, the generation of temporal, * Analytics: pedestrian detection
causal, probabilistic spatial narratives from the we was
rogramming model causal, probabilistic models ee peo
pros! 8 a (ActivityNet)

UCLA e Interpretable representations: e Three-level explanation concept « Autonomy: robot executing daily
STC-AOG (spatial, temporal, compositions, causal and tasks in physics-realistic VR
causal models), STC-PG (scene counterfactual reasoning, utility platform autonomous vehicle
and event interpretations in explanation driving (GTAS game engine)
analytics), STC-PG+ (task plans
in autonomy)

e Theory of mind representations * Explanation representations: e Analytics: network of video
(user’s beliefs, user's mental X-AOG (explanation model), cameras for scene understanding
model of agent) X-PG (explanatory parse graph and event analysis

as dialogue), X-Utility (priority
and loss for explanations)

Oregon State xDAPs, combination of adaptive Provides a visual and NL Autonomy: real-time strategy sames
programs, deep learning, and explanation interlace for based on custom-designed game
explainabilty acceptance testing by test pilots engine designed to support

based on IFT explanation; StarCraft II

PARC Three-layer architecture: learning ¢ Interactive visualization of Autonomy: MAVSim wrapper over
layer (DNNs), cognitive layer States, actions, policies, values. ArduPilot simulation environment
on cae iuch * Module for test pilots to refine

P y and train the system
Carnegie Mellon A new scientific discipline for XRL « Interactive explanations of Autonomy: OpenAl Gym, autonomy
University XRL with work on new dynamic systems in the electrical grid, mobile service
algorithms and representations co: : robots, self-improving educational
e Human-machine interaction
: software
to improve performance
SRI Multiple DL techniques: attention- « DNN visualization Analytics: VQA (Visual Gnome,

based mechanisms,
compositional NMNs, GANs

Flickr30), MovieQA

Query evidence that explains
DNN decisions

Generate NL justifications

Raytheon BBN Semantic labeling of DNN « Comprehensive strategy based Analytics: VQA for images and video
neurons on argumentation theory
e DNN audit trail construction « NL generation

Gradient-weighted class DNN visualization

activation mapping

UTD TPLMs Enables users to explore and Analytics: infer activities in
correct the underlying model multimodal data (video and text),
as well as add background wet lab (biology), and Textually
knowledge Annotated Cooking Scenes data
sets

 

(continued on following page)

 

SUMMER 2019 55
Deep Learning and Security

 

 

 

a Kerrnal

Explainable Model

Texas ASM ¢ Mimic learning framework
combines DL models for
prediction and shallow models
for explanations.

Interpretable learning
algorithms extract knowledge
from DNNs for relevant
explanations

Rutgers Select the optimal trading
examples to explain model
decisions based on Bayesian

Teaching examples

Explanation Interface

Interactive visualization over
multiple news, using heat maps
and topic modeling clusters to
show predictive features

Example-based explanation of the Analytics: image processing, text
full model, user-selected
substructure, user submitted

Challenge Problem

Analytics: multiple tasks using data
from Twitter, Facebook, ImageNet,
and news websites

corpora, VQA, movie events

 

56

Table 2. Summary of Explainable Learning System Developer Approaches and Selected Phase 1 Test Problems.

Tractable Probabilistic Logic Models:
A New, Deep Explainable Representation

The University of Texas at Dallas (UTD) team (in-
cluding researchers from UCLA, Texas A&M, and the
Indian Institute of Technology, Delhi) is developing a
unified approach to XAI using tractable probabilistic
logic models (TPLMs).

TPLMs are a family of representations that in-
clude (for example) decision trees, binary decision
diagrams, cutset networks, sentential decision dia-
grams, first-order arithmetic circuits, and tractable
Markov logic (Gogate and Domingos 2016). The
UTD system extends TPLMs to generate explana-
tions of query results; handle continuous variables,
complex constraints, and unseen entities; compactly
represent complex objects such as parse trees, lists, and
shapes; and enable efficient representation and rea-
soning about time.

For scalable inference, the system uses novel al-
gorithms to answer complex explanation queries
using techniques including lifted inference, varia-
tional inference, and their combination. For fast and
increased learning accuracy, it uses discriminative
techniques, deriving algorithms that compose NNs
and support vector machines with TPLMs, using in-
terpretability as a bias to learn more interpretable
models. These approaches are then extended to handle
real-world situations.

The UTD explanation interface displays in-
terpretable representations with multiple related
explanations. Its interactive component allows
users to debug a model and suggest alternative
explanations.

UTD is addressing the analytics challenge prob-
lem area and has demonstrated its system for rec-
ognizing human activities in multimodal data
(video and text), such as the Textually Annotated
Cooking Scenes data set.

AI MAGAZINE

Transforming Deep Learning to
Harness the Interpretability of Shallow
Models: An Interactive End-to-End System

The Texas A&M University (TAMU) team (including
researchers from Washington State University) is
developing an interpretable DL framework that uses
mimic learning to leverage explainable shallow
models and facilitates domain interpretation with
visualization and interaction. Mimic learning
bridges the gap between deep and shallow models
and enables interpretability. The system also mines
informative patterns from raw data to enhance in-
terpretability and learning performance.

The system’s interpretable learning algorithms ex-
tract knowledge from DNNs for relevant explanations. Its
DL module connects to a pattern-generation module
by leveraging the interpretability of the shallow
models. The learning system’s output is displayed to
users with visualization including coordinated and
integrated views.

The TAMU system handles image (Du et al. 2018) and
text (Gao et al. 2017) data and is being applied to the XAI
analytics challenge problem area. It provides effective
interpretations of detected inaccuracies from diverse
sources while maintaining a competitive detection per-
formance. The TAMU system combines model-level (that
is, model transparency) and instance-level (that is, in-
stance explanation) interpretability to generate expla-
nations that are more easily comprehended by users. This
system has been deployed on multiple tasks using data
from Twitter, Facebook, ImageNet, CIFAR-10, online
health care forums, and news websites.

Model Explanation by
Optimal Selection of Teaching Examples

Rutgers University is extending Bayesian teaching to
enable automatic explanation by selecting the data
subset that is most representative of a model’s in-
ference. Rutgers’ approach allows for explanation of the
inferences of any probabilistic generative and discrimina-
tive model, as well as influential DL models (Yang
and Shafto 2017).

Rutgers is also developing a formal theory of human-
machine cooperation and supporting interactive
guided explanation of complex compositional models.
Common among these is a core approach of building
from models of human learning to foster explainability
and carefully controlled behavioral experiments to
quantify explainability.

Explanation by Bayesian teaching inputs a data set,
a probabilistic model, and an inference method and
returns a small subset of examples that best explains
the model’s inference. Experiments with unfamiliar
images show that explanations of inferences about
categories of (and specific) images increase the accu-
racy of people’s reasoning about a model (Vong et al.
2018). Experiments with familiar image categories
show that explanations allow users to accurately
calibrate trust in model predictions.

Explanation of complex models is facilitated by
interactive guided explanations. By exploiting com-
positionality and cooperative modifications of ML
models, Rutgers provides a generic approach to
fostering understanding via guided exploration. In-
teraction occurs through an interface that exposes
model structure and explains each component with
aspects of the data. The Rutgers approach has been
demonstrated to facilitate understanding of large text
corpora, as assessed by a human’s ability to accurately
summarize the corpus after short, guided explanations.

Rutgers is addressing the data analytics challenge
problem area and has demonstrated its approach on
images, text, combinations of these (for example,
VQA), and structured simulations involving temporal
causal structure.

Conclusions and Future Work

DARPA’s XAI program is developing and evaluating a
wide variety of new ML techniques: modified DL
techniques that learn explainable features; methods
that learn more structured, interpretable, causal
models; and model induction techniques that infer an
explainable model from any black-box model. One year
into the XAI program, initial technology demonstra-
tions and results indicate that these three broad strate-
gies merit further investigation and will provide future
developers with design options covering the perfor-
manice versus explainability trade space. The developer
teams’ XAI systems are being evaluated to assess the
value of explanations that they provide, localizing the
contributions of specific techniques within this
trade space.

Acknowledgments

The authors thank the XAI development teams,
specifically their principle investigators, for their
innovative research and contributions to this article:

Deep Learning and Security

Trevor Darrell (UCB), Brian Ruttenberg and Avi Pfeffer
(CRA), Song-Chun Zhu (UCLA), Alan Fern (OSU),
Mark Stefik (PARC), Zico Kolter (Carnegie Mellon),
Mohamed Amer and Giedrius Burachas (SRI Interna-
tional), Bill Ferguson (Raytheon BBN), Vibhav Gogate
(UTD), Xia (Ben) Hu (TAMU), Patrick Shafto (Rutgers),
and Robert Hoffman (IHMC). The authors owe a
special thanks to Marisa Carrera for her exceptional
technical support to the XAI program and her ex-
tensive editing skills.

References

Belbute-Peres, F., and Kolter, J. Z. 2017. A Modular Differ-
entiable Rigid Body Physics Engine. Paper presented at the
Neural Information Processing Systems Deep Reinforcement
Learning Symposium. Long Beach, CA, December 7.

Chakraborty, S.; Tomsett, R.; Raghavendra, R.; Harborne, D.;
Alzantot, M.; Cerutti, F.; and Srivastava, M.; et al. 2017.
Interpretability of Deep Learning Models: A Survey of Re-
sults. Presented at the IEEE Smart World Congress 2017
Workshop: DAIS 2017 — Workshop on Distributed Ana-
lytics Infrastructure and Algorithms for Multi-Organiza-
tion Federations, San Francisco, CA, August 4-8. doi.org/10.
1109/UIC-ATC.2017.8397411

Dodge, J.; Penney, S.; Hilderbrand, C.; Anderson, A.; and
Burnett, M. 2018. How the Experts Do It: Assessing and
Explaining Agent Behaviors in Real-Time Strategy Games. In
Proceedings of the 2018 CHI Conference on Human Factors in
Computing Systems. New York: Association for Computing
Machinery. doi.org/10.1145/3173574.3174136

Du, M.; Liu, N.; Song, Q.; and Hu, X. 2018. Towards Ex-
planation of DNN-Based Prediction and Guided Feature In-
version. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 1358-67.
New York: Association for Computing Machinery. doi.org/
10.1145/3219819.3220099

Gao, J.; Liu, N.; Lawley, M.; and Hu, X. 2017. An In-
terpretable Classification Framework for Information Ex-
traction from Online Healthcare Forums. Journal of
Healthcare Engineering: 2460174. doi.org/10.1155/2017/
2460174

Gogate, V., and Domingos, P. 2016. Probabilistic Theorem
Proving. Communications of the ACM 59(7): 107-15. doi.org/10.
1145/2936726

Harradon, M.; Druce, J.; and Ruttenberg, B. 2018. Causal
Learning and Explanation of Deep Neural Networks via
Autoencoded Activations. arXiv preprint. arXiv:1802.00541v1
[cs.Al]. Ithaca, NY: Comell University Library.

Hefny, A.; Marinho, Z.; Sun, W.; Srinivasa, S.; and Gordon, G.
2018. Recurrent Predictive State Policy Networks. In Pro-
ceedings of the 35th International Conference on Machine
Learning, 1954-63. International Machine Learning Society.
Hendricks, L. A.; Hu, R.; Darrell, T.; and Akata, Z. 2018.
Grounding Visual Explanations. Presented at the European
Conference of Computer Vision (ECCV). Munich, Germany;
September 8-14. doi.org/10.1007/978-3-030-01216-8_17
Hoffman, R.; Miller, T.; Mueller, S. T.; Klein, G.; and Clancey,
W. J. 2018. Explaining Explanation, Part 4: A Deep Dive on
Deep Nets. IEEE Intelligent Systems 33(3): 87-95. doi.org/10.
1109/MIS.2018.033001421

Hoffman, R. R.; and Klein, G. 2017. Explaining Explanation,
Part 1: Theoretical Foundations. IEEE Intelligent Systems 32(3):
68-73. doi.org/10.1109/MIS.2017.54

SUMMER 2019 57
Deep Learning and Security

Hoffman, R. R., Mueller, S. T.; and Klein, G. 2017. Explaining
Explanation, Part 2: Empirical Foundations. IEEE Intelligent
Systems 32(4): 78-86. doi.org/10.1109/MIS.2017.3121544

Hu, R.; Andreas, J.; Rohrbach, M.; Darrell, T.; and Saenko, K.
2017. Learning to Reason: End-to-End Module Networks for
Visual Question Answering. In Proceedings of the IEEE In-
ternational Conference on Computer Vision, 804-13. New York:
IEEE. doi.org/10.1109/ICCV.2017.93

Huang, S. H.; Bhatia, K.; Abbeel, P.; and Dragan, A. 2018.
Establishing Appropriate Trust via Critical States. Pre-
sented at the 13th Annual ACM/IEEE International
Conference on Human-Robot [Interaction Workshop on
Explainable Robot Behavior. Madrid, Spain; October 1-5.
doi.org/10.1109/IROS.2018.8593649

Kim, J., and Canny, J. 2017. Interpretable Learning for
Self-Driving Cars by Visualizing Causal Attention. In
Proceedings of the International Conference on Computer Vi-
sion, 2942-50. New York: IEEE. doi.org/10.1109/ICCV.
2017.320

Klein, G. 2018. Explaining Explanation, Part 3: The Causal
Landscape. IEEE Intelligent Systems 33(2): 83-88. doi.org/10.
1109/MIS.2018.022441353

Letham, B.; Rudin, C.; McCormick, T. H.; and Madigan, D.
2015. Interpretable Classifiers Using Rules and Bayesian
Analysis: Building a Better Stroke Prediction Model. Annals of
Applied Statistics 9(3): 1350-71. doi.org/10.1214/15-
AOAS848

Marazopoulou, K.; Maier, M.; and Jensen, D. 2015. Learning
the Structure of Causal Models with Relational and Temporal
Dependence. In Proceedings of the Thirty-First Conference on
Uncertainty in Artificial Intelligence, 572-81. Association for
Uncertainty in Artificial Intelligence.

Miller, T. 2017. Explanation in Artificial Intelligence: Insights
from the Social Sciences. arXiv preprint. arXiv:1706.07269v1
[cs.Al]. Ithaca, NY: Cornell University Library.

Park, D. H.; Hendricks, L. A.; Akata, Z.; Rohrbach, A.; Schiele,
B.; Darrell, T.; and Rohrbach, M. 2018. Multimodal Expla-
nations: Justifying Decisions and Pointing to the Evidence.
In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. New York: IEEE. doi.org/10.1109/CVPR.
2018.00915

Pfeffer, A. 2016. Practical Probabilistic Programming.
Greenwich, CT: Manning Publications.

Qi, Z., and Li, F. 2017. Learning Explainable Embeddings for
Deep Networks. Paper presented at the NIPS Workshop on
Interpreting, Explaining and Visualizing Deep Learning.
Long Beach, CA, December 9.

Ramanishka, V.; Das, A.; Zhang, J.; and Saenko, K. 2017. Top-
Down Visual Saliency Guided by Captions. In Proceedings of the
30th IEEE Conference on Computer Vision and Pattern Recognition,
7206-15. New York, IEEE.

Ras, G.; van Gerven, M.; and Haselager, P. 2018 Explanation
Methods in Deep Learning: Users, Values, Concerns and
Challenges. arXiv preprint. arXiv:1803.07517v2_ [cs.AI].
Ithaca, NY: Cornell University Library.

Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. “Why Should
I Trust You?”: Explaining the Predictions of Any Classifier. In
Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 1135-44. New York:
Association for Computing Machinery. doi.org/10.1145/
2939672.2939778

She, L., and Chai, J. Y. 2017. Interactive Learning for Ac-
quisition of Grounded Verb Semantics towards Human-Ro-
bot Communication. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics, vol. 1,

58 AI MAGAZINE

1634-44. Stroudsburg, PA: Association for Computation
Linguistics. doi.org/10.18653/v1/P17-1150

Vicol, P.; Tapaswi, M.; Castrejon, L.; and Fidler, S.
2018.MovieGraphs: Towards Understanding Human-Cen-
tric Situations from Videos. In IEEE Conference on Computer
Vision and Pattern Recognition. New York: IEEE. doi.org/10.
1109/CVPR.2018.00895

Vong, W.-K.; Sojitra, R.; Reyes, A.; Yang, S. C.-H.; and Shafto,
P. 2018. Bayesian Teaching of Image Categories. Paper pre-
sented at the 40th Annual Meeting of the Cognitive Science
Society (CogSci). Madison, WI, July 25-28.

Yang, S. C.-H., and Shafto, P. 2017. Explainable Artificial In-
telligence via Bayesian Teaching. Paper presented at the 31st
Conference on Neural Information Processing Systems
Workshop on Teaching Machines, Robots and Humans. Long
Beach, CA, December 9.

Zhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba, A.
2015. Object Detectors Emerge in Deep Scene CNNs. Paper
presented at the International Conference on Learning Rep-
resentations. San Diego, CA, May 7-9.

David Gunning is a program manager in DARPA’s In-
formation Innovation Office, as an Intergovernmental Per-
sonnel Act from the Pacific Northwest National Labs.
Gunning has more than 30 years of experience in developing
AI technology. In prior DARPA tours he managed the PAL
project that produced Siri and the CPOF project that the US
Army adopted as its C2 system for use in Iraq and Afgha-
nistan. Gunning was also a program director at PARC, a
senior research manager at Vulcan, senior vice president at
SET Corporation, vice president of Cycorp, and a senior
scientist at the Air Force Research Labs. Gunning holds an MS
in computer science from Stanford University and an MS in
cognitive psychology from the University of Dayton.

David W. Aha is acting director of NRL’s Navy Center for
Applied Research in AI in Washington, D.C. His interests
include goal reasoning, XAI, case-based reasoning, and
machine learning, among other topics. He has coorganized
many events on these topics (for example, the IJCAI-18 XAI
Workshop), launched the UCI Repository for ML Databases,
served as an AAAI Councilor, and leads DARPA XAI’s eval-
uation team.
1909.12072v1 [cs.AI] 26 Sep 2019

arXiv

Towards Explainable Artificial Intelligence

Wojciech Samek! and Klaus-Robert Miiller?*+

| Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany
wojciech.samek@hhi.fraunhofer.de
2 Technische Universitat Berlin, 10587 Berlin, Germany
3 Korea University, Anam-dong, Seongbuk-gu, Seoul 02841, Korea
* Max Planck Institute for Informatics, Saarbriicken 66123, Germany
klaus-robert.mueller@tu-berlin.de

Abstract. In recent years, machine learning (ML) has become a key
enabling technology for the sciences and industry. Especially through
improvements in methodology, the availability of large databases and in-
creased computational power, today’s ML algorithms are able to achieve
excellent performance (at times even exceeding the human level) on an
increasing number of complex tasks. Deep learning models are at the
forefront of this development. However, due to their nested non-linear
structure, these powerful models have been generally considered “black
boxes”, not providing any information about what exactly makes them
arrive at their predictions. Since in many applications, e.g., in the med-
ical domain, such lack of transparency may be not acceptable, the de-
velopment of methods for visualizing, explaining and interpreting deep
learning models has recently attracted increasing attention. This intro-
ductory paper presents recent developments and applications in this field
and makes a plea for a wider use of explainable learning algorithms in
practice.

Keywords: Explainable Artificial Intelligence - Model Transparency -
Deep Learning - Neural Networks - Interpretability

1 Introduction

Today’s artificial intelligence (AI) systems based on machine learning excel in
many fields. They not only outperform humans in complex visual tasks [16,53]
or strategic games [56,83,61], but also became an indispensable part of our every
day lives, e.g., as intelligent cell phone cameras which can recognize and track
faces [71], as online services which can analyze and translate written texts [11]
or as consumer devices which can understand speech and generate human-like
answers [90]. Moreover, machine learning and artificial intelligence have become

The final authenticated publication is available online at

https: //doi.org/10.1007/978-3-030-28954-6_1. In: W. Samek et al. (Eds.)
Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture
Notes in Computer Science, vol. 11700, pp. 5-22. Springer, Cham (2019).
2 W. Samek and K.-R. Miiller

indispensable tools in the sciences for tasks such as prediction, simulation or ex-
ploration [78,15,89,92]. These immense successes of AI systems mainly became
possible through improvements in deep learning methodology [48,47], the avail
ability of large databases [17,34] and computational gains obtained with powerful
GPU cards [52].

Despite the revolutionary character of this technology, challenges still exist
which slow down or even hinder the prevailance of AI in some applications.
Examplar challenges are (1) the large complexity and high energy demands of
current deep learning models [29], which hinder their deployment in resource
restricted environments and devices, (2) the lack of robustness to adversarial
attacks [55], which pose a severe security risk in application such as autonomous
driving’, and (3) the lack of transparency and explainability [76,32,18], which
reduces the trust in and the verifiability of the decisions made by an AI system.

This paper focuses on the last challenge. It presents recent developments in
the field of explainable artificial intelligence and aims to foster awareness for the
advantages—and at times—also for the necessity of transparent decision making
in practice. The historic second Go match between Lee Sedol and AlphaGo [82]
nicely demonstrates the power of today’s AI technology, and hints at its enor-
mous potential for generating new knowledge from data when being accessible
for human interpretation. In this match AlphaGo played a move, which was
classified as “not a human move” by a renowned Go expert, but which was the
deciding move for AlphaGo to win the game. AlphaGo did not explain the move,
but the later play unveiled the intention behind its decision. With explainable AI
it may be possible to also identify such novel patterns and strategies in domains
like health, drug development or material sciences, moreover, the explanations
will ideally let us comprehend the reasoning of the system and understand why
the system has decided e.g. to classify a patient in a specific manner or asso-
ciate certain properties with a new drug or material. This opens up innumerable
possibilities for future research and may lead to new scientific insights.

The remainder of the paper is organized as follows. Section 2 discusses the
need for transparency and trust in AI. Section 3 comments on the different types
of explanations and their respective information content and use in practice.
Recent techniques of explainable AI are briefly summarized in Section 4, includ-
ing methods which rely on simple surrogate functions, frame explanation as an
optimization problem, access the model’s gradient or make use of the model’s
internal structure. The question of how to objectively evaluate the quality of
explanations is addressed in Section 5. The paper concludes in Section 6 with a
discussion on general challenges in the field of explainable AI.

° The authors of [24] showed that deep models can be easily fooled by physical-world
attacks. For instance, by putting specific stickers on a stop sign one can achieve that
the stop sign is not recognized by the system anymore.
1. Towards Explainable Artificial Intelligence 3
2 Need for Transparency and Trust in Al

Black box AI systems have spread to many of today’s applications. For machine
learning models used, e.g., in consumer electronics or online translation services,
transparency and explainability are not a key requirement as long as the overall
performance of these systems is good enough. But even if these systems fail,
e.g., the cell phone camera does not recognize a person or the translation service
produces grammatically wrong sentences, the consequences are rather unspec-
tacular. Thus, the requirements for transparency and trust are rather low for
these types of AI systems. In safety critical applications the situation is very
different. Here, the intransparency of ML techniques may be a limiting or even
disqualifying factor. Especially if single wrong decisions can result in danger to
life and health of humans (e.g., autonomous driving, medical domain) or signifi-
cant monetary losses (e.g., algorithmic trading), relying on a data-driven system
whose reasoning is incomprehensible may not be an option. This intransparency
is one reason why the adoption of machine learning to domains such as health is
more cautious than the usage of these models in the consumer, e-commerce or
entertainment industry.

In the following we discuss why the ability to explain the decision making of
an AI system helps to establish trust and is of utmost importance, not only in
medical or safety critical applications. We refer the reader to [91] for a discussion
of the challenges of transparency.

2.1 Explanations Help to Find “Clever Hans” Predictors

Clever Hans was a horse that could supposedly count and that was considered a
scientific sensation in the years around 1900. As it turned out later, Hans did not
master the math but in about 90 percent of the cases, he was able to derive the
correct answer from the questioner’s reaction. Analogous behaviours have been
recently observed in state-of-the-art AI systems [46]. Also here the algorithms
have learned to use some spurious correlates in the training and test data and
similarly to Hans predict right for the ‘wrong’ reason.

For instance, the authors of [44,46] showed that the winning method of the
PASCAL VOC competition [23] was often not detecting the object of interest,
but was utilizing correlations or context in the data to correctly classify an im-
age. It recognized boats by the presence of water and trains by the presence
of rails in the image, moreover, it recognized horses by the presence of a copy-
right watermark®. The occurrence of the copyright tags in horse images is a
clear artifact in the dataset, which had gone unnoticed to the organizers and
participants of the challenge for many years. It can be assumed that nobody
has systematically checked the thousands images in the dataset for this kind
of artifacts (but even if someone did, such artifacts may be easily overlooked).
Many other examples of “Clever Hans” predictors have been described in the

© The PASCAL VOC images have been automatically crawled from flickr and espe-
cially the horse images were very often copyrighted with a watermark.
4 W. Samek and K.-R. Miiller

literature. For instance, [73] show that current deep neural networks are dis-
tinguishing the classes “Wolf” and “Husky” mainly by the presence of snow in
the image. The authors of [46] demonstrate that deep models overfit to padding
artifacts when classifying airplanes, whereas [63] show that a model which was
trained to distinguish between 1000 categories, has not learned dumbbells as
an independent concept, but associates a dumbbell with the arm which lifts it.
Such “Clever Hans” predictors perform well on their respective test sets, but will
certainly fail if deployed to the real-world, where sailing boats may lie on a boat
trailer, both wolves and huskies can be found in non-snow regions and horses do
not have a copyright sign on them. However, if the AI system is a black box, it
is very difficult to unmask such predictors. Explainability helps to detect these
types of biases in the model or the data, moreover, it helps to understand the
weaknesses of the AI system (even if it is not a “Clever Hans” predictor). In the
extreme case, explanations allow to detect the classifier’s misbehaviour (e.g., the
focus on the copyright tag) from a single test image”. Since understanding the
weaknesses of a system is the first step towards improving it, explanations are
likely to become integral part of the training and validation process of future AI
models.

2.2. Explanations Foster Trust and Verifiability

The ability to verify decisions of an AI system is very important to foster trust,
both in situations where the AI system has a supportive role (e.g., medical diag-
nosis) and in situations where it practically takes the decisions (e.g., autonomous
driving). In the former case, explanations provide extra information, which, e.g.,
help the medical expert to gain a comprehensive picture of the patient in order
to take the best therapy decision. Similarly to a radiologist, who writes a detailed
report explaining his findings, a supportive AI system should in detail explain
its decisions rather than only providing the diagnosis to the medical expert. In
cases where the AI system itself is deciding, it is even more critical to be able to
comprehend the reasoning of the system in order to verify that it is not behav-
ing like Clever Hans, but solves the problem in a robust and safe manner. Such
verifications are required to build the necessary trust in every new technology.

There is also a social dimension of explanations. Explaining the rationale
behind one’s decisions is an important part of human interactions [30]. Explana-
tions help to build trust in a relationship between humans, and should therefore
be also part of human-machine interactions [3]. Explanations are not only an in-
evitable part of human learning and education (e.g., teacher explains solution to
student), but also foster the acceptance of difficult decisions and are important
for informed consent {e.g., doctor explaining therapy to patient). Thus, even if
not providing additional information for verifying the decision, e.g., because the
patient may have no medical knowledge, receiving explanations usually make us
feel better as it integrates us into the decision-making process. An AI system
which interacts with humans should therefore be explainable.

” Traditional methods to evaluate classifier performance require large test. datasets.
1. Towards Explainable Artificial Intelligence 5

2.3. Explanations are a Prerequisite for New Insights

AI systems have the potential to discover patterns in data, which are not acces-
sible to the human expert. In the case of the Go game, these patterns can be
new playing strategies [82]. In the case of scientific data, they can be unknown
associations between genes and diseases [51], chemical compounds and material
properties [68] or brain activations and cognitive states [49]. In the sciences,
identifying these patterns, i.e., explaining and interpreting what features the AI
system uses for predicting, is often more important than the prediction itself, be-
cause it unveils information about the biological, chemical or neural mechanisms
and may lead to new scientific insights.

This necessity to explain and interpret the results has led to a strong domi-
nance of linear models in scientific communities in the past (e.g. [42,67]). Linear
models are intrinsically interpretable and thus easily allow to extract the learned
patterns. Only recently, it became possible to apply more powerful models such
as deep neural networks without sacrificing interpretability. These explainable
non-linear models have already attracted attention in domains such as neuro-
science [87,89,20], health [33,14,40], autonomous driving [31], drug design [70]
and physics [78,72] and it can be expected that they will play a pivotal role in
future scientific research.

2.4 Explanations are Part of the Legislation

The infiltration of AI systems into our daily lives poses a new challenge for the
legislation. Legal and ethical questions regarding the responsibility of AI systems
and their level of autonomy have recently received increased attention [21,27].
But also anti-discrimination and fairness aspects have been widely discussed in
the context of AI [28,19]. The EU’s General Data Protection Regulation (GDPR)
has even added the right to explanation to the policy in Articles 13, 14 and 22,
highlighting the importance of human-understandable interpretations derived
from machine decisions. For instance, if a person is being rejected for a loan by
the AI system of a bank, in principle, he or she has the right to know why the
system has decided in this way, e.g., in order to make sure that the decision is
compatible with the anti-discrimination law or other regulations. Although it is
not yet clear how these legal requirements will be implemented in practice, one
can be sure that transparency aspects will gain in importance as AI decisions
will more and more affect our daily lives.

3 Different Facets of an Explanation

Recently proposed explanation techniques provide valuable information about
the learned representations and the decision-making of an AI system. These
explanations may differ in their information content, their recipient and their
purpose. In the following we describe the different types of explanations and
comment on their usefulness in practice.
6 W. Samek and K.-R. Miiller

3.1 Recipient

Different recipients may require explanations with different level of detail and
with different information content. For instance, for users of AI technology it may
be sufficient to obtain coarse explanations, which are easy to interpret, whereas
AI researchers and developers would certainly prefer explanations, which give
them deeper insights into the functioning of the model.

In the case of image classification such simple explanations could coarsely
highlight image regions, which are regarded most relevant for the model. Several
preprocessing steps, e.g., smoothing, filtering or contrast normalization, could be
applied to further improve the visualization quality. Although discarding some
information, such coarse explanations could help the ordinary user to foster
trust in AI technology. On the other hand AI researchers and developers, who
aim to improve the model, may require all the available information, including
negative evidence, about the AI’s decision in the highest resolution (e.g., pixel-
wise explanations), because only this complete information gives detailed insights
into the (mal)functioning of the model.

One can easily identify further groups of recipients, which are interested in
different types of explanations. For instance, when applying AI to the medical
domain these groups could be patients, doctors and institutions. An AI system
which analyzes patient data could provide simple explanations to the patients,
e.g., indicating too high blood sugar, while providing more elaborate explana-
tions to the medical personal, e.g., unusual relation between different blood
parameters. Furthermore, institutions such as hospitals or the FDA might be
less interested in understanding the AI’s decisions for individual patients, but
would rather prefer to obtain global or aggregated explanations, i.e., patterns
which the AI system has learned after analyzing many patients.

3.2 Information Content

Different types of explanation provide insights into different aspects of the model,
ranging from information about the learned representations to the identification
of distinct prediction strategies and the assessment of overall model behaviour.
Depending on the recipient of the explanations and his or her intent, it may be
advantageous to focus on one particular type of explanation. In the following we
briefly describe four different types of explanations.

1. Explaining learned representations: This type of explanation aims to
foster the understanding of the learned representations, e.g., neurons of
a deep neural network. Recent work [12,38] investigates the role of single
neurons or group of neurons in encoding certain concepts. Other methods
[84,93,64,65] aim to interpret what the model has learned by building proto-
types that are representative of the abstract learned concept. These meth-
ods, e.g., explain what the model has learned about the category “car” by
generating a prototypical image of a car. Building such a prototype can
be formulated within the activation maximization framework and has been
1. Towards Explainable Artificial Intelligence 7

shown to be an effective tool for studying the internal representation of a
deep neural network.

2. Explaining individual predictions: Other types of explanations provide
information about individual predictions, e.g., heatmaps visualizing which
pixels have been most relevant for the model to arrive at its decision [60]
or heatmaps highlighting the most sensitive parts of an input [84]. Such
explanations help to verify the predictions and establish trust in the correct
functioning on the system. Layer-wise Relevance Propagation (LRP) [9,58]
provides a general framework for explaining individual predictions, i.e., it is
applicable to various ML models, including neural networks [9], LSTMs [7],
Fisher Vector classifiers [44] and Support Vector Machines [35]. Section 4
gives an overview over recently proposed methods for computing individual
explanations.

3. Explaining model behaviour: This type of explanations go beyond the
analysis of individual predictions towards a more general understanding of
model behaviour, e.g., identification of distinct prediction strategies. The
spectral relevance analysis (SpRAy) approach of [46] computes such meta
explanations by clustering individual heatmaps. Each cluster then represents
a particular prediction strategy learned by the model. For instance, the au-
thors of [46] identify four clusters when classifying “horse” images with the
Fisher Vector classifier [77] trained on the PASCAL VOC 2007 dataset [22],
namely (1) detect the horse and rider, 2) detect a copyright tag in portrait
oriented images, 3) detect wooden hurdles and other contextual elements of
horseback riding, and 4) detect a copyright tag in landscape oriented images.
Such explanations are useful for obtaining a global overview over the learned
strategies and detecting “Clever Hans” predictors [46].

4. Explaining with representative examples: Another class of methods
interpret classifiers by identifying representative training examples [41,37].
This type of explanations can be useful for obtaining a better understanding
of the training dataset and how it influences the model. Furthermore, these
representative examples can potentially help to identify biases in the data
and make the model more robust to variations of the training dataset.

3.3 Role

Besides the recipient and information content it is also important to consider the
purpose of an explanation. Here we can distinguish two aspects, namely (1) the
intent of the explanation method (what specific question does the explanation
answer) and (2) our intent (what do we want to use the explanation for).
Explanations are relative and it makes a huge difference whether their intent
is to explain the prediction as is (even if it is incorrect), whether they aim to
visualize what the model “thinks” about a specific class (e.g., the true class) or
whether they explain the prediction relative to another alternative (“why is this
image classified as car and not as truck”). Methods such as LRP allow to answer
all these different questions, moreover, they also allow to adjust the amount of
positive and negative evidence in the explanations, i.e., visualize what speaks
8 W. Samek and K.-R. Miiller

for (positive evidence) and against (negative evidence) the prediction. Such fine-
grained explanations foster the understanding of the classifier and the problem
at hand.

Furthermore, there may be different goals for using the explanations beyond
visualization and verification of the prediction. For instance, explanations can
be potentially used to improve the model, e.g., by regularization [74]. Also since
explanations provide information about the (relevant parts of the) model, they
can be potentially used for model compression and pruning. Many other uses
(certification of the model, legal use) of explanations can be thought of, but the
details remain future work.

4 Methods of Explainable AI

This section gives an overview over different approaches to explainable AI, start-
ing with techniques which are model-agnostic and rely on a simple surrogate
function to explain the predictions. Then, we discuss methods which compute
explanations by testing the model’s response to local perturbations (e.g., by uti-
lizing gradient information or by optimization). Subsequently, we present very
efficient propagation-based explanation techniques which leverage the model’s
internal structure. Finally, we consider methods which go beyond individual ex-
planations towards a meta-explanation of model behaviour.

This section is not meant to be a complete survey of explanation methods,
but it rather summarizes the most important developments in this field. Some
approaches to explainable AI, e.g., methods which find influencial examples [37],
are not discussed in this section.

4.1 Explaining with Surrogates

Simple classifiers such as linear models or shallow decision trees are intrinsically
interpretable, so that explaining its predictions becomes a trivial task. Complex
classifiers such as deep neural networks or recurrent models on the other hand
contain several layers of non-linear transformations, which largely complicates
the task of finding what exactly makes them arrive at their predictions.

One approach to explain the predictions of complex models is to locally
approximate them with a simple surrogate function, which is interpretable. A
popular technique falling into this category is Local Interpretable Model-agnostic
Explanations (LIME) [73]. This method samples in the neighborhood of the
input of interest, evaluates the neural network at these points, and tries to fit
the surrogate function such that it approximates the function of interest. If the
input domain of the surrogate function is human-interpretable, then LIME can
even explain decisions of a model which uses non-interpretable features. Since
LIME is model agnostic, it can be applied to any classifier, even without knowing
its internals, e.g., architecture or weights of a neural network classifier. One
major drawback of LIME is its high computational complexity, e.g., for state-of
the-art models such as GoogleNet it requires several minutes for computing the
explanation of a single prediction [45].
1. Towards Explainable Artificial Intelligence 9

Similar to LIME which builds a model for locally approximating the function
of interest, the SmoothGrad method [85] samples the neighborhood of the input
to approximate the gradient. Also SmoothGrad does not leverage the internals
of the model, however, it needs access to the gradients. Thus, it can also be
regarded as a gradient-based explanation method.

4.2 Explaining with Local Perturbations

Another class of methods construct explanations by analyzing the model’s re-
sponse to local changes. This includes methods which utilize the gradient infor-
mation as well as perturbation- and optimization-based approaches.

Explanation methods relying on the gradient of the function of interest [2]
have a long history in machine learning. One example is the so-called Sensitivity
Analysis (SA) [62,10,84]. Although being widely used as explanation methods,
SA technically explains the change in prediction instead of the prediction itself.
Furthermore, SA has been shown to suffer from fundamental problems such as
gradient shattering and explanation discontinuities, and is therefore considered
suboptimal for explanation of today’s AI models [60]. Variants of Sensitivity
Analysis exist which tackle some of these problems by locally averaging the
gradients [85] or integrating them along a specific path [88].

Perturbation-based explanation methods [94,97,25] explicitly test the model’s
response to more general local perturbations. While the occlusion method of [94]
measures the importance of input dimensions by masking parts of the input, the
Prediction Difference Analysis (PDA) approach of [97] uses conditional sampling
within the pixel neighborhood of an analyzed feature to effectively remove infor-
mation. Both methods are model-agnostic, i.e., can be applied to any classifier,
but are computationally not very efficient, because the function of interest (e.g.,
neural network) needs to be evaluated for all perturbations.

The meaningful perturbation method of [25,26] is another model-agnostic
technique to explaining with local perturbations. It regards explanation as a
meta prediction task and applies optimization to synthesize the maximally in-
formative explanations. The idea to formulate explanation as an optimization
problem is also used by other methods. For instance, the methods [84,93,64]
aim to interpret what the model has learned by building prototypes that are
representative of the learned concept. These prototypes are computed within
the activation maximization framework by searching for an input pattern that
produces a maximum desired model response. Conceptually, activation maxi-
mization [64] is similar to the meaningful perturbation approach of [25]. While
the latter finds a minimum perturbation of the data that makes f(z) low, activa-
tion maximization finds a minimum perturbation of the gray image that makes
f(x) high. The costs of optimization can make these methods computationally
very demanding.
10 W. Samek and K.-R. Miiller

4.3 Propagation-Based Approaches (Leveraging Structure)

Propagation-based approaches to explanation are not oblivious to the model
which they explain, but rather integrate the internal structure of the model into
the explanation process.

Layer-wise Relevance Propagation (LRP) [9,58] is a propagation-based ex-
planation framework, which is applicable to general neural network structures,
including deep neural networks [13], LSTMs [7,5], and Fisher Vector classifiers
[44]. LRP explains individual decisions of a model by propagating the prediction
from the output to the input using local redistribution rules. The propagation
process can be theoretically embedded in the deep Taylor decomposition frame-
work [59]. More recently, LRP was extended to a wider set of machine learning
models, e.g., in clustering [36] or anomaly detection [35], by first transforming
the model into a neural network (‘neuralization’) and then applying LRP to
explain its predictions. The leveraging of the model structure together with the
use of appropriate (theoretically-motivated) propagation rules, enables LRP to
deliver good explanations at very low computational cost (one forward and one
backward pass). Furthermore, the generality of the LRP framework allows also to
express other recently proposed explanation techniques, e.g., [81,95]. Since LRP
does not rely on gradients, it does not suffer from problems such as gradient
shattering and explanation discontinuities [60].

Other popular explanation methods leveraging the model’s internal structure
are Deconvolution [94] and Guided Backprogagation [86]. In contrast to LRP,
these methods do not explain the prediction in the sense “how much did the
input feature contribute to the prediction”, but rather identify patterns in input
space, that relate to the analyzed network output.

Many other explanation methods have been proposed in the literature which
fall into the “leveraging structure” category. Some of these methods use heuris-
tics to guide the redistribution process [79], others incorporate an optimization
step into the propagation process [39]. The iNNvestigate toolbox [1] provides
an efficient implementation for many of these propagation-based explanation
methods.

4.4 Meta-Explanations

Finally, individual explanations can be aggregated and analyzed to identify gen-
eral patterns of classifier behavior. A recently proposed method, spectral rel-
evance analysis (SpRAy) [46], computes such meta explanations by clustering
individual heatmaps. This approach allows to investigate the predictions strate-
gies of the classifier on the whole dataset in a (semi-)automated manner and to
systematically find weak points in models or training datasets.

Another type of meta-explanation aims to better understand the learned rep-
resentations and to provide interpretations in terms of human-friendly concepts.
For instance, the network dissection approach of [12,96] evaluates the semantics
of hidden units, i.e., quantify what concepts these neurons encode. Other recent
work [38] provides explanations in terms of user-defined concepts and tests to
which degree these concepts are important for the prediction.
1. Towards Explainable Artificial Intelligence 11

5 Evaluating Quality of Explanations

The objective assessment of the quality of explanations is an active field of re-
search. Many efforts have been made to define quality measures for heatmaps
which explain individual predictions of an AI model. This section gives an
overview over the proposed approaches.

A popular measure for heatmap quality is based on perturbation analysis
[9,75,6]. The assumption of this evaluation metric is that the perturbation of
relevant (according to the heatmap) input variables should lead to a steeper
decline of the prediction score than the perturbation of input dimensions which
are of lesser importance. Thus, the average decline of the prediction score after
several rounds of perturbation (starting from the most relevant input variables)
defines an objective measure of heatmap quality. If the explanation identifies the
truly relevant input variables, then the decline should be large. The authors of
[75] recommend to use untargeted perturbations (e.g., uniform noise) to allow fair
comparison of different explanation methods. Although being very popular, it is
clear that perturbation analysis can not be the only criterion to evaluate expla-
nation quality, because one could easily design explanations techniques which
would directly optimize this criterion. Examples are occlusion methods which
were used in [94,50], however, they have been shown to be inferior (according to
other quality criteria) to explanation techniques such as LRP [8].

Other studies use the ‘pointing game” [95] to evaluate the quality of a
heatmap. The goal of this game is to evaluate the discriminativeness of the
explanations for localizing target objects, i.e., it is compared if the most rele-
vant point of the heatmap lies on the object of designated category. Thus, these
measures assume that the AI model will focus most attention on the object of
interest when classifying it, therefore this should be reflected in the explanation.
However, this assumption may not always be true, e.g., “Clever Hans” predic-
tors [46] may rather focus on context than of the object itself, irrespectively of
the explanation method used. Thus, their explanations would be evaluated as
poor quality according to this measure although they truly visualize the model’s
prediction strategy.

Task specific evaluation schemes have also been proposed in the literature.
For example, [69] use the subject-verb agreement task to evaluate explanations
of a NLP model. Here the model predicts a verb’s number and the explana-
tions verify if the most relevant word is indeed the correct subject or a noun
with the predicted number. Other approaches to evaluation rely on human judg-
ment [73,66]. Such evaluation schemes relatively quickly become impractical if
evaluating a larger number of explanations.

A recent study [8] proposes to objectively evaluate explanation for sequential
data using ground truth information in a toy task. The idea of this evaluation
metric is to add or subtract two numbers within an input sequence and measure
the correlation between the relevances assigned to the elements of the sequence
and the two input numbers. If the model is able to accurately perform the ad-
dition and subtraction task, then it must focus on these two numbers (other
12 W. Samek and K.-R. Miiller

numbers in the sequence are random) and this must be reflected in the explana-
tion.

An alternative and indirect way to evaluate the quality of explanations is to
use them for solving other tasks. The authors of [6] build document-level rep-
resentations from word-level explanations. The performance of these document-
level representations (e.g., in a classification task) reflect the quality of the word-
level explanations. Another work [4] uses explanation for reinforcement learning.
Many other functionally-grounded evaluations [18] could be conceived such as
using explanations for compressing or pruning the neural network or training
student models in a teacher-student scenario.

Lastly, another promising approach to evaluate explanations is based on the
fulfillment of a certain axioms [80,88,54,60,57]. Axioms are properties of an ex-
planation that are considered to be necessary and should therefore be fulfilled.
Proposed axioms include relevance conservation [60], explanation continuity [60],
sensitivity [88] and implementation invariance [88]. In contrast to the other qual-
ity measures discussed in this section, the fulfillment or non-fulfillment of certain
axioms can be often shown analytically, ie., does not require empirical evalua-
tions.

6 Challenges and Open Questions

Although significant progress has been made in the field of explainable AI in the
last years, challenges still exist both on the methods and theory side as well as
regarding the way explanations are used in practice. Researchers have already
started working on some of these challenges, e.g., the objective evaluation of
explanation quality or the use of explanations beyond visualization. Other open
questions, especially those concerning the theory, are more fundamental and
more time will be required to give satisfactory answers to them.

Explanation methods allow us to gain insights into the functioning of the
AI model. Yet, these methods are still limited in several ways. First, heatmaps
computed with today’s explanation methods visualize “first-order” information,
i.e., they show which input features have been identified as being relevant for the
prediction. However, the relation between these features, e.g., whether they are
important on their own or only whether they occur together, remains unclear.
Understanding these relations is important in many applications, e.g., in the
neurosciences such higher-order explanations could help us to identify groups of
brain regions which act together when solving a specific task (brain networks)
rather than just identifying important single voxels.

Another limitation is the low abstraction level of explanations. Heatmaps
show that particular pixels are important without relating these relevance values
to more abstract concepts such as the objects or the scene displayed in the
image. Humans need to interpret the explanations to make sense them and
to understand the model’s behaviour. This interpretation step can be difficult
and erroneous. Meta-explanations which aggregate evidence from these low-level
heatmaps and explain the model’s behaviour on a more abstract, more human
1. Towards Explainable Artificial Intelligence 13

understandable level, are desirable. Recently, first approaches to aggregate low-
level explanations [46] and quantify the semantics of neural representations [12]
have been proposed. The construction of more advanced meta-explanations is a
rewarding topic for future research.

Since the recipient of explanations is ultimately the human user, the use
of explanations in human-machine interaction is an important future research
topic. Some works (e.g., [43]) have already started to investigate human factors in
explainable AI. Constructing explanations with the right user focus, i-e., asking
the right questions in the right way, is a prerequisite to successful human-machine
interaction. However, the optimization of explanations for optimal human usage
is still a challenge which needs further study.

A theory of explainable AI, with a formal and universally agreed definition of
what explanations are, is lacking. Some works made a first step towards this goal
by developing mathematically well-founded explanation methods. For instance,
the authors of [59] approach the explanation problem by integrating it into
the theoretical framework of Taylor decomposition. The axiomatic approaches
[88,54,60] constitute another promising direction towards the goal of developing
a general theory of explainable AI.

Finally, the use of explanations beyond visualization is a wide open
challenge. Future work will show how to integrate explanations into a larger
optimization process in order to, e.g., improve the model’s performance or
reduce its complexity.

Acknowledgements. This work was supported by the German Ministry
for Education and Research as Berlin Big Data Centre (011S14013A), Berlin
Center for Machine Learning (0118180371) and TraMeExCo (011S18056A).
Partial funding by DFG is acknowledged (EXC 2046/1, project-ID: 390685689).
This work was also supported by the Institute for Information & Communi-
cations Technology Planning & Evaluation (IITP) grant funded by the Korea
government (No. 2017-0-00451, No. 2017-0-01779).

References

1. Alber, M., Lapuschkin, S., Seegerer, P., Hagele, M., Schiitt, K.T., Montavon, G.,
Samek, W., Miller, K.R., Daéhne, S., Kindermans, P.J.: iNNvestigate neural net-
works!. Journal of Machine Learning Research 20(93), 1-8 (2019)

2. Ancona, M., Ceolini, E., Oztireli, C., Gross, M.: Gradient-based attribution meth-
ods. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning.
Lecture Notes in Computer Science 11700, Springer (2019)

3. Antunes, P., Herskovic, V., Ochoa, S.F., Pino, J.A.: Structuring dimensions for col-
laborative systems evaluation. ACM Computing Surveys (CSUR) 44(2), 8 (2012)

4. Arjona-Medina, J.A., Gillhofer, M., Widrich, M., Unterthiner, T., Hochreiter,
S.. RUDDER: Return Decomposition for Delayed Rewards. arXiv preprint
arXiv:1806.07857 (2018)

5. Arras, L., Arjona-Medina, J., Gillhofer, M., Widrich, M., Montavon, G., Miiller,
K.R., Hochreiter, $., Samek, W.: Explaining and interpreting LSTMs with LRP. In:
10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

W. Samek and K.-R. Miiller

Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture
Notes in Computer Science 11700, pp. 211238. Springer (2019)

. Arras, L., Horn, F., Montavon, G., Miller, K.R., Samek, W.: ” What is relevant

in a text document?”: An interpretable machine learning approach. PLoS ONE
12(8), e0181142 (2017)

. Arras, L., Montavon, G., Miller, K.R., Samek, W.: Explaining recurrent neural

network predictions in sentiment analysis. In: EMNLP’17 Workshop on Computa-
tional Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA).
pp. 159-168 (2017)

. Arras, L., Osman, A., Miller, K.R., Samek, W.: Evaluating recurrent neural net-

work explanations. In: ACL’19 Workshop on BlackboxNLP: Analyzing and Inter-
preting Neural Networks for NLP, pp. 113-126 (2019)

. Bach, S., Binder, A., Montavon, G., Klauschen, F., Miiller, K.R., Samek, W.: On

pixel-wise explanations for non-linear classifier decisions by layer-wise relevance
propagation. PLoS ONE 10(7), e0130140 (2015)

Baehrens, D., Schroeter, T., Harmeling, S., Kawanabe, M., Hansen, K., Miiller,
K.R.: How to explain individual classification decisions. Journal of Machine Learn-
ing Research 11, 1803-1831 (2010)

Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning
to align and translate. In: International Conference on Learning Representations
(ICLR). (2015)

Bau, D., Zhou, B., Khosla, A., Oliva, A., Torralba, A.: Network dissection: Quanti-
fying interpretability of deep visual representations. In: IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR). pp. 6541-6549 (2017)

Binder, A., Bach, 8., Montavon, G., Miiller, K.R., Samek, W.: Layer-wise relevance
propagation for deep neural network architectures. In: Information Science and
Applications (ICISA), pp. 913-922 (2016)

Binder, A., Bockmayr, M., Hagele, M., Wienert, S., Heim, D., Hellweg, K., Sten-
zinger, A., Parlow, L., Budczies, J., Goeppert, B., et al.: Towards computational
fluorescence microscopy: Machine learning-based integrated prediction of morpho-
logical and molecular tumor profiles. arXiv preprint arXiv:1805.11178 (2018)
Chmiela, S., Sauceda, H.E., Miiller, K.R., Tkatchenko, A.: Towards exact molecular
dynamics simulations with machine-learned force fields. Nature Communications
9(1), 3887 (2018)

Ciregan, D., Meier, U., Masci, J., Schmidhuber, J.: A committee of neural networks
for traffic sign classification. In: International Joint Conference on Neural Networks
(IJCNN). pp. 1918-1921 (2011)

Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 248-255 (2009)

Doshi- Velez, F., Kim, B.: Towards a rigorous science of interpretable machine learn-
ing. arXiv preprint arXiv:1702.08608 (2017)

Doshi-Velez, F., Kortz, M., Budish, R., Bavitz, C., Gershman, 8., O’Brien, D.,
Schieber, $., Waldo, J., Weinberger, D., Wood, A.: Accountability of AI under the
law: The role of explanation. arXiv preprint arXiv:1711.01134 (2017)

Eitel, F., Soehler, E., Bellmann-Strobl, J., Brandt, A.U., Ruprecht, K., Giess, R.M.,
Kuchling, J., Asseyer, S., Weygandt, M., Haynes, J.D., et al.: Uncovering convo-
lutional neural network decisions for diagnosing multiple sclerosis on conventional
tri using layer-wise relevance propagation. arXiv preprint arXiv:1904.08771 (2019)
European Commission’s High-Level Expert Group: Draft ethics guidelines for
trustworthy AI. European Commission (2019)
22.

23.

24.

25.

26.

27.

28.

29.

30.

31.

32.

34.

35.

36.

37.

38.

1. Towards Explainable Artificial Intelligence 15

Everingham, M., Eslami, $.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman,
A.: The PASCAL Visual Object Classes Challenge: A Retrospective. International
Journal of Computer Vision 111(1), 98-136 (2015)

Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The Pascal
visual object classes (VOC) challenge. International Journal of Computer Vision
88(2), 303-338 (2010)

Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., Prakash,
A., Kohno, T., Song, D.: Robust physical-world attacks on deep learning models.
arXiv preprint arXiv:1707.08945 (2017)

Fong, R.C., Vedaldi, A.: Interpretable explanations of black boxes by meaningful
perturbation. In: IEEE International Conference on Computer Vision (CVPR). pp.
3429-3437 (2017)

Fong, R., Vedaldi, A.: Explanations for attributing deep neural network predic-
tions. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning.
Lecture Notes in Computer Science 11700, pp. 149167. Springer (2019)
Goodman, B., Flaxman, S.: European union regulations on algorithmic decision-
making and a “right to explanation”. AI Magazine 38(3), 50-57 (2017)

Hajian, $., Bonchi, F., Castillo, C.: Algorithmic bias: From discrimination discovery
to fairness-aware data mining. In: 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. pp. 2125-2126 (2016)

Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for
efficient neural network. In: Advances in Neural Information Processing Systems
(NIPS). pp. 1135-1143 (2015)

Heath, R.L., Bryant, J.: Human communication theory and research: Concepts,
contexts, and challenges. Routledge (2013)

Hofmarcher, M., Unterthiner, T., Arjona-Medina, J., Klambauer, G., Hochreiter,
S., Nessler, B.: Visual scene understanding for autonomous driving using semantic
segmentation. In: Explainable AI: Interpreting, Explaining and Visualizing Deep
Learning. Lecture Notes in Computer Science 11700, pp. 285296. Springer (2019)
Holzinger, A., Langs, G., Denk, H., Zatloukal, K., Mtiller, H.: Causability and
explainabilty of artificial intelligence in medicine. Wiley Interdisciplinary Reviews:
Data Mining and Knowledge Discovery p. e1312 (2019)

Horst, F., Lapuschkin, $., Samek, W., Miiller, K.R., Schdllhorn, W.I.: Explaining
the unique nature of individual gait patterns with deep learning. Scientific Reports
9, 2391 (2019)

Karpathy, A., Toderici, G., Shetty, $., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-
scale video classification with convolutional neural networks. In: IEEE conference
on Computer Vision and Pattern Recognition (CVPR). pp. 1725-1732 (2014)
Kauffmann, J., Miller, K.R., Montavon, G.: Towards explaining anomalies: A deep
Taylor decomposition of one-class models. arXiv preprint arXiv:1805.06230 (2018)
Kauffmann, J., Esders, M., Montavon, G., Samek, W., Miller, K.R.: From cluster-
ing to cluster explanations via neural networks. arXiv preprint arXiv:1906.07633
(2019)

Khanna, R., Kim, B., Ghosh, J., Koyejo, O.: Interpreting black box predictions
using fisher kernels. arXiv preprint arXiv:1810.10118 (2018)

Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., Sayres, R.:
Interpretability beyond feature attribution: Quantitative testing with concept acti-
vation vectors (TCAV). In: International Conference on Machine Learning (ICML).
pp. 2673-2682, (2018)
39.

40.

41.

42.

43.

44.

45.

46.

47.

48.

49.

50.

51.

52.

53.

54.

55.

56.

57.

W. Samek and K.-R. Miiller

Kindermans, P.J., Schtitt, K.T., Alber, M., Miiller, K.R., Erhan, D., Kim, B.,
Dahne, S.: Learning how to explain neural networks: Patternnet and patternattri-
bution. In: International Conference on Learning Representations (ICLR). (2018)
Klauschen, F., Miiller, K.R., Binder, A., Bockmayr, M., Hagele, M., Seegerer, P.,
Wienert, S., Pruneri, G., de Maria, S., Badve, S., et al.: Scoring of tumor-infiltrating
lymphocytes: From visual estimation to machine learning. Seminars in Cancer
Biology 52(2), 151-157 (2018)

Koh, P.W., Liang, P.: Understanding black-box predictions via influence functions.
In: International Conference on Machine Learning (ICML). pp. 1885-1894 (2017)
Kriegeskorte, N., Goebel, R., Bandettini, P.: Information-based functional brain
mapping. Proceedings of the National Academy of Sciences 103(10), 3863-3868
(2006)

Lage, I., Chen, E., He, J., Narayanan, M., Kim, B., Gershman, $., Doshi-Velez,
F.: An evaluation of the human-interpretability of explanation. arXiv preprint
arXiv:1902.00006 (2019)

Lapuschkin, $., Binder, A., Montavon, G., Miiller, K.R., Samek, W.: Analyzing
classifiers: Fisher vectors and deep neural networks. In: IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR). pp. 2912-2920 (2016)
Lapuschkin, 8.: Opening the Machine Learning Black Box with Layer-wise Rele-
vance Propagation. Ph.D. thesis, Technische Universitat Berlin (2019)
Lapuschkin, $., Waldchen, S., Binder, A., Montavon, G., Samek, W., Miiller, K.R.:
Unmasking clever hans predictors and assessing what machines really learn. Nature
Communications 10, 1096 (2019)

LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436-444
(2015)

LeCun, Y.A., Bottou, L., Orr, G.B., Miller, K.R.: Efficient backprop. In: Neural
networks: Tricks of the trade, pp. 9-48. Springer (2012)

Lemm, 8., Blankertz, B., Dickhaus, T., Miiller, K.R.: Introduction to machine
learning for brain imaging. Neuroimage 56(2), 387-399 (2011)

Li, J., Monroe, W., Jurafsky, D.: Understanding Neural Networks through Repre-
sentation Erasure. arXiv preprint arXiv:1612.08220 (2016)

Libbrecht, M.W., Noble, W.S.: Machine learning applications in genetics and ge-
nomics. Nature Reviews Genetics 16(6), 321 (2015)

Lindholm, E., Nickolls, J., Oberman, 8., Montrym, J.: Nvidia tesla: A unified
graphics and computing architecture. IEEE Micro 28(2), 39-55 (2008)

Lu, C., Tang, X.: Surpassing human-level face verification performance on LFW
with GaussianFace. In: 29th AAAI Conference on Artificial Intelligence. pp. 3811-
3819 (2015)

Lundberg, S.M., Lee, S.L: A unified approach to interpreting model predictions.
In: Advances in Neural Information Processing Systems (NIPS). pp. 4765-4774
(2017)

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning
models resistant to adversarial attacks. In: International Conference on Learning
Representations (ICLR). (2018)

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. Nature 518(7540), 529-533 (2015)
Montavon, G.: Gradient-based vs. propagation-based explanations: An axiomatic
comparison. In: Explainable AI: Interpreting, Explaining and Visualizing Deep
Learning. Lecture Notes in Computer Science 11700, pp. 253265. Springer (2019)
58.

59.

60.

61.

62.

63.

64.

65.

66.

67.

68.

69.

70.

71.

72.

73.

1. Towards Explainable Artificial Intelligence 17

Montavon, G., Binder, A., Lapuschkin, S., Samek, W., Miiller, K.R.: Layer-wise
relevance propagation: An overview. In: Explainable AI: Interpreting, Explaining
and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp.
193-209. Springer (2019)

Montavon, G., Lapuschkin, 8., Binder, A., Samek, W., Miller, K.R.: Explaining
nonlinear classification decisions with deep Taylor decomposition. Pattern Recog-
nition 65, 211-222 (2017)

Montavon, G., Samek, W., Miller, K.R.: Methods for interpreting and understand-
ing deep neural networks. Digital Signal Processing 73, 1-15 (2018)

Moravéik, M., Schmid, M., Burch, N., Lisy, V., Morrill, D., Bard, N., et al.:
Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science
356(6337), 508-513 (2017)

Morch, N., Kjems, U., Hansen, L.K., Svarer, C., Law, I., Lautrup, B., Strother, S.,
Rehm, K.: Visualization of neural networks using saliency maps. In: International
Conference on Neural Networks (ICNN). vol. 4, pp. 2085-2090 (1995)
Mordvintsev, A., Olah, C., Tyka, M.: Inceptionism: Going deeper into neural net-
works (2015)

Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., Clune, J.: Synthesizing the
preferred inputs for neurons in neural networks via deep generator networks. In:
Advances in Neural Information Processing Systems (NIPS). pp. 3387-3395 (2016)
Nguyen, A., Yosinski, J., Clune, J.: Understanding neural networks via feature
visualization: A survey. In: Explainable AI: Interpreting, Explaining and Visualiz-
ing Deep Learning. Lecture Notes in Computer Science 11700, pp. 5576. Springer
2019

Coven, D.: Comparing Automatic and Human Evaluation of Local Explanations
for Text Classification. In: Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language Technologies (NAACL-
HLT). pp. 1069-1078 (2018)

Phinyomark, A., Petri, G., Ibafiez-Marcelo, E., Osis, S.T., Ferber, R.: Analysis of
big data in gait biomechanics: Current trends and future directions. Journal of
Medical and Biological Engineering 38(2), 244-260 (2018)

Pilania, G., Wang, C., Jiang, X., Rajasekaran, S., Ramprasad, R.: Accelerating
materials property predictions using machine learning. Scientific Reports 3, 2810
2013

poeta N., Roth, B., Schiitze, H.: Evaluating neural network explanation methods
using hybrid documents and morphosyntactic agreement. In: 56th Annual Meeting
of the Association for Computational Linguistics (ACL). pp. 340-350 (2018)
Preuer, K., Klambauer, G., Rippmann, F., Hochreiter, $., Unterthiner, T.: Inter-
pretable deep learning in drug discovery. In: Explainable AI: Interpreting, Explain-
ing and Visualizing Deep Learning. Lecture Notes in Computer Science 11700,
Springer (2019)

Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified,
real-time object detection. In: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 779-788 (2016)

Reyes, E., Estévez, P.A., Reyes, I., Cabrera-Vives, G., Huijse, P., Carrasco, R.,
Forster, F.: Enhanced rotational invariant convolutional neural network for super-
novae detection. In: International Joint Conference on Neural Networks (IJCNN).
pp. 1-8 (2018)

Ribeiro, M.T., Singh, 8., Guestrin, C.: Why should i trust you?: Explaining the
predictions of any classifier. In: ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. pp. 1135-1144 (2016)
74.

75.

76.

77.

78.

79.

80.

81.

82.

83.

84.

85.

86.

87.

88.

89.

90.

91.

W. Samek and K.-R. Miiller

Ross, A.S., Hughes, M.C., Doshi-Velez, F.: Right for the right reasons: Training
differentiable models by constraining their explanations. In: 26th International
Joint Conferences on Artificial Intelligence (IJCAI). pp. 2662-2670 (2017)

Samek, W., Binder, A., Montavon, G., Lapuschkin, S., Miller, K.R.: Evaluating
the visualization of what a deep neural network has learned. IEEE Transactions
on Neural Networks and Learning Systems 28(11), 2660-2673 (2017)

Samek, W., Wiegand, T., Miiller, K.R.: Explainable artificial intelligence: Under-
standing, visualizing and interpreting deep learning models. ITU Journal: ICT
Discoveries - Special Issue 1 - The Impact of Artificial Intelligence (AI) on Com-
munication Networks and Services 1(1), 39-48 (2018)

Sanchez, J., Perronnin, F., Mensink, T., Verbeek, J.J.: Image classification with
the Fisher vector: Theory and practice. International Journal of Computer Vision
105(3), 222-245 (2013)

Schutt, K.T., Arbabzadah, F., Chmiela, S., Miiller, K.R., Tkatchenko, A.:
Quantum-chemical insights from deep tensor neural networks. Nature Commu-
nications 8, 13890 (2017)

Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-
cam: Visual explanations from deep networks via gradient-based localization. In:
IEEE International Conference on Computer Vision (CVPR). pp. 618-626 (2017)
Shapley, L.S.: A value for n-person games. Contributions to the Theory of Games
2(28), 307-317 (1953)

Shrikumar, A., Greenside, P., Kundaje, A.: Learning Important Features Through
Propagating Activation Differences. arXiv preprint arXiv:1704.02685 (2017)
Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G.,
et al.: Mastering the game of Go with deep neural networks and tree search. Nature
529(7587), 484-489 (2016)

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I, Huang, A., Guez, A.,
Hubert, T., Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of Go without
human knowledge. Nature 550(7676), 354-359 (2017)

Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks:
Visualising image classification models and saliency maps. In: ICLR Workshop.
(2014)

Smilkov, D., Thorat, N., Kim, B., Viégas, F., Wattenberg, M.: Smoothgrad: re-
moving noise by adding noise. arXiv preprint arXiv:1706.03825 (2017)
Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.: Striving for simplic-
ity: The all convolutional net. In: ICLR Workshop. (2015)

Sturm, I., Lapuschkin, $., Samek, W., Miiller, K.R.: Interpretable deep neural
networks for single-trial eeg classification. Journal of Neuroscience Methods 274,
141-145 (2016)

Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In:
International Conference on Machine Learning (ICML). pp. 3319-3328 (2017)
Thomas, A.W., Heekeren, H.R., Miller, K.R., Samek, W.: Analyzing neuroimag-
ing data through recurrent deep learning models. arXiv preprint arXiv:1810.09945
(2018)

Van Den Oord, A., Dieleman, $., Zen, H., Simonyan, K., Vinyals, O., Graves, A.,
Kalchbrenner, N., Senior, A.W., Kavukcuoglu, K.: Wavenet: A generative model
for raw audio. SSW 125 (2016)

Weller, A.: Transparency: Motivations and Challenges. In: Explainable AI: Inter-
preting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer
Science 11700, Springer (2019)
92.

93.

94.

95.

96.

97.

1. Towards Explainable Artificial Intelligence 19

Wu, D., Wang, L., Zhang, P.: Solving statistical mechanics using variational au-
toregressive networks. Physical Review Letters 122(8), 080602 (2019)

Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H.: Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579 (2015)
Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.
In: European Conference Computer Vision (ECCV). pp. 818-833 (2014)

Zhang, J., Lin, Z.L., Brandt, J., Shen, X., Sclaroff, S.: Top-down neural attention
by excitation backprop. In: European Conference on Computer Vision (ECCV).
pp. 543-559 (2016)

Zhou, B., Bau, D., Oliva, A., Torralba, A.: Comparing the interpretability of deep
networks via network dissection. In: Explainable AI: Interpreting, Explaining and
Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp. 243252.
Springer (2019)

Zintgraf, L.M., Cohen, T.S., Adel, T., Welling, M.: Visualizing deep neural network
decisions: Prediction difference analysis. In: International Conference on Learning
Representations (ICLR). (2017)
The Pragmatic Turn in Explainable Artificial Intelligence (XAI)

Minds and Machines, 29(3), 441-459
DOI: 10.1007/s11023-019-09502-w

Please quote the printed version

Andrés Paez
Universidad de los Andes
apaez(@uniandes.edu.co

ABSTRACT

In this paper I argue that the search for explainable models and interpretable decisions
in AI must be reformulated in terms of the broader project of offering a pragmatic and
naturalistic account of understanding in AI. Intuitively, the purpose of providing an
explanation of a model or a decision is to make it understandable to its stakeholders.
But without a previous grasp of what it means to say that an agent understands a model
or a decision, the explanatory strategies will lack a well-defined goal. Aside from
providing a clearer objective for XAI, focusing on understanding also allows us to relax
the factivity condition on explanation, which is impossible to fulfill in many machine
learning models, and to focus instead on the pragmatic conditions that determine the
best fit between a model and the methods and devices deployed to understand it. After
an examination of the different types of understanding discussed in the philosophical
and psychological literature, I conclude that interpretative or approximation models not
only provide the best way to achieve the objectual understanding of a machine learning
model, but are also a necessary condition to achieve post-hoc interpretability. This
conclusion is partly based on the shortcomings of the purely functionalist approach to
post-hoc interpretability that seems to be predominant in most recent literature.

1. Introduction

The main goal of Explainable Artificial Intelligence (XAJ) has been variously described as a
search for explainability, transparency and interpretability, for ways of validating the
decision process of an opaque AI system and generating trust in the model and its predictive
performance.! All of these goals remain underspecified in the literature and there are

numerous proposals about which attributes make models interpretable. Instead of analyzing

 

' For a survey of recent characterizations of the goals of XAI, see Lipton (2016), Doshi-Velez & Kim
(2017), Samek et al. (2017), and Gilpin et al. (2019).
these goals and proposals piecemeal, the main contention of this paper is that the search for
explainable, interpretable, trustworthy models and decisions* in AI must be reformulated in
terms of the broader project of offering an account of understanding in AI. Intuitively, the
purpose of providing an explanation or an interpretation of a model or a decision is to make
it understandable or comprehensible to its stakeholders. But without a previous grasp of what
it means to say that a human agent understands a decision or a model, the explanatory or
interpretative strategies will lack a well-defined theoretical and practical goal. This paper
provides a characterization of the theoretical goal of XAI by offering an analysis of human
understanding in the context of machine learning in general, and of black box models in
particular.

In recent years, there has been an increased interest in the notion of understanding
among epistemologists (Pritchard 2014, Grimm 2018) and philosophers of science (de Regt
et al. 2009). The interest in this notion has several sources. In epistemology, several authors
realized that the conceptual analysis of understanding differs significantly from the
traditional analysis of knowledge. In particular, unlike knowledge, understanding need not
be factive: not all the information on the basis of which a phenomenon is understood must
be true. Understanding is also an epistemic achievement that some authors regard as more
valuable than mere knowledge. It also seems to be immune to Gettier cases, it is transparent
to the epistemic agent, and it has internalist conditions of success. In sum, understanding and
knowledge seem to be entirely different concepts and it is implausible to conceive the former
simply as a species of the latter.*

In the philosophy of science, the first philosophers of explanation (Hempel 1965;
Salmon 1984) regarded the understanding provided by a scientific explanation as a pragmatic
and psychological by-product that falls beyond the ken of a proper philosophical theory. In

their view, once we have developed an adequate account of explanation, any remaining

 

?T will use decision as the general term to encompass outputs from AI models, such as predictions,
categorizations, action selection, etc.

3 Needless to say, each of these differences has been the subject of great philosophical controversy. I
am simply reporting some of the reasons that have been stated in the literature to motivate the analysis
of understanding as an independent concept.
questions regarding the notion of understanding can be addressed from a psychological
perspective. A recent interest in the role of models, simulations, and idealizations in science,
and a closer examination of actual scientific practice, has revealed that scientific
understanding can be achieved without the use of traditionally-defined scientific
explanations, and that the simple possession of explanatory knowledge is often not sufficient
for the working scientist’s understanding of a phenomenon. Scientific understanding thus
seems to be a topic worth investigating in its own right.

There are many aspects of this literature that are germane to XAI. Here I will only
focus on two main issues. The first one regards the relationship between explanation and
understanding in the context of opaque machine learning models. While many authors defend
the idea that there is no understanding without explanation, the impossibility of finding
explanations, in the traditional sense of the term, for black box machine learning models
should lead us to question the inseparability of these two concepts in the context of AI. The
literature suggests alternative paths to achieve understanding, and it is worth investigating
how these paths can be fruitfully adapted to understand opaque models and decisions.

The second issue regards the nature of understanding itself. Are understanding the
decision of a model and understanding the model that produced that decision two states that
demand different accounts or can they be reduced to the same underlying cognitive processes
and abilities? I will argue that post-hoc interpretability and model transparency correspond
to different levels of the same type of understanding. There is, however, a different kind of
understanding that stems from the functional or instrumental analysis of machine learning
models. I will argue that functional understanding falls short in many respects of the stated
goals of XAI.

It should be noted that the notion of opacity in machine learning is itself in need of
further specification. There are many types of machine learning models that are purposely
designed as black boxes (e.g. deep neural networks and Support Vector Machines). Other
methods, such as rule lists, linear regressions, simple naive Bayes classifiers, and decision
trees are often interpretable, but not always. “Sufficiently high dimensional models, unwieldy
tule lists, and deep decision trees could all be considered less transparent than comparatively

compact neural networks” (Lipton 2016, p. 5). Other relatively simple models will be opaque
only to certain users who lack the required background knowledge to understand them. To
simplify the object of analysis, in this paper I will only focus on the extreme case of models
that are unambiguously designed as black boxes. Most of the results of this analysis can then
be naturally extended to models and methods that are opaque only in certain cases or to
specific stakeholders.

Finally, given the variety of purposes of black box machine learning models, and the
differences in background knowledge and interests of their stakeholders, there is no reason
to believe that a single interpretative strategy will be equally successful in all cases.
Designing interpretative models and tools will inevitably require taking into account the
psychological and pragmatic aspects of explanation. The study of the cognitive aspects of
interpretative models is in its infancy. It follows from the general outlook that I present in
this paper that this area of research should receive more attention in the coming years.

The essay is organized as follows. The next section examines the question of whether
there are independent theoretical reasons to appeal to the notion of understanding in XAI or
whether it will be sufficient, as the traditionalists claim, to develop the best possible account
of explanation in AI and let understanding naturally emerge from it. I will argue that the
connection between explanation and understanding in AI is not comparable to that same
connection in the natural and social sciences. The disconnection arises from the impossibility,
in most cases, to offer an explanation that fulfills the factivity condition. This will lead, in
section 3, to a discussion about alternative paths to understanding that are not based on
traditional explanations. I show how these alternative paths are exemplified in some recent
attempts to find adequate methods and devices to understand opaque models and decisions.
In section 4, I analyze the types of understanding that emerge from these different avenues
to understanding. This will require introducing a distinction between understanding-why,
which prima facie is the type of understanding involved in post-hoc interpretability, and
objectual understanding, which requires grasping the inner workings of a complex system
such as an AI model. This section also addresses the functional understanding of AI systems.
Using evidence from psychology, it will be possible to offer a nuanced analysis of the

interconnections between these three possible ways of characterizing understanding in AI.
2. Why not settle for AI-explanations?

A great number of philosophers of science have argued that understanding is inextricably
linked to explanation. For Salmon, a defender of the ontic conception of explanation,
“understanding results from our ability to fashion scientific explanations” (1984, p. 259). In
more recent times, Strevens has staunchly defended the idea that “explanation is essentially
involved in scientific understanding” (2013, p. 510). Perhaps the strongest claim in this
direction is made by Khalifa, who defends the reductionist thesis that “any philosophically
relevant ideas about scientific understanding can be captured by philosophical ideas about
the epistemology of scientific explanation without loss” (2012, p. 17). In the context of XAI,
this thesis* implies that understanding an AI model or decision is simply a question of finding
an adequate explanation for it. But the implication holds only if scientific explanations and
Al-explanations share a sufficient number of essential characteristics to be considered two
species of the same genus. If they are, our task will be to find in Al-explanations the same
features that enable scientific explanations to generate understanding. However, in this
section I will argue that explanations in the present stage of AI are incommensurable with
the types of explanations discussed in the philosophy of science.

My first task will be to clarify what I mean by an Al-explanation. The notion of
explanation in what is often referred to as “Good Old-Fashioned AI’ (GOFAD), that is, in
symbolic, logic-based AI models, differs significantly from the present task of explaining
opaque machine learning models. The function of an explanation in a logic-based system,
either monotonic or nonmonotonic, is to support the addition of an input to a belief set or a
database. For example, an update request “insert (g)” can be achieved by finding some
formula consistent with the database such that the union of the set of ground facts in the
database and the formula yields ¢ as a logical consequence. In previous work (Paez 2009) I
argued that this abductive task is at odds with the way in which explanation has historically

been understood in the philosophy of science. I refer the reader to the paper for the relevant

 

4 Here I will not evaluate the merits of this thesis in the philosophy of science. For a discussion, see
the collection edited by De Regt, Leonelli and Eigner (2009).
arguments. My purpose here is to defend the same conclusion for the notion of explanation
as it is being used in the field of computational intelligence in recent times.

An important difference between explanation in logic-based models and in current
machine learning models is that the explanandum is entirely different. In the former, as just
mentioned, the goal is to justify an input. In the latter, it is to explain an output, generally a
decision, or to provide explanatory information about the workings of the model that
generated that output. The exp/anandum of an Al-explanation as it is currently conceived is
thus similar to the outcome of a scientific experiment, or to the structure of a physical or
social system. A natural scientist and the stakeholder of a machine learning model would
thus be searching for explanations for similar objects. But that is as far as the similarities go.
In what follows I will present three fundamental reasons why it is misguided to make our
understanding of machine learning models dependent on establishing an account of AI-
explanations, even if we were to accept the claim that scientific understanding depends on
devising bona fide scientific explanations.

The first reason has to do with truth. An essential feature of explanations in science
is their factivity (Hempel 1965), i.e., both the exp/anans and the explanandum must be true.°
If one denies the factivity of explanation, the claim goes, one cannot avoid the conclusion
that the Ptolemaic theory, the phlogiston theory, or the caloric theory, provided bona fide
scientific explanations. An explanation of x must reveal, depending on which theory of
explanation one adopts, either the true causal structure of x or the natural laws that determine
x or its relationship with factors that make x more or less probable.° All objectivist theories
of explanation assume that researchers have epistemic access either to the inner workings of

x or to the complete’ causal or probabilistic context that determines the properties of the

 

* More precisely, the explanans-statement and the explanandum-statement must be true. If one holds,
following Lewis (1986) and Woodward (2003), that the relata of the explanation relation are
particulars, i.e., things or events, the claim amounts to saying that the things or events occurring in
both the explanans and the explanandum position exist or occur.

° This list is not meant to be exhaustive and it excludes pragmatic theories of explanation such as the
ones defended by Achinstein (1983) and van Fraassen (1980). I have argued elsewhere (Paez 2006)
that these theories offer an account of explanation that lacks any sort of objectivity.

7 Salmon’s (1971) reference class rule, for example, requires the probabilistic (causal) context of a
single event to be complete to avoid any epistemic relativity.
explanandum. Without such epistemic access it would be impossible to reach true
explanatory information about x.

This kind of epistemic access is blocked in the case of opaque AI models. A general
knowledge of the structure of a deep neural network will be insufficient to explain, in this
traditional sense, either a specific decision or the actual computation that was made to
generate it. Many types of black box models, like deep neural networks, are stochastic (non-
deterministic). Randomness is introduced in data selection, training, and evaluation to help
the learning algorithm be more robust and accurate.* Examining the training set and all the
weights, biases and structure of the network will not allow us to understand its specific
decisions, and its predictive failures and successes cannot be traced back to particular causal
paths in its hidden layers. To be sure, it is possible to give a true explanation of the general
design and purpose of a black box model, but such an explanation will not be sufficient to
explain specific decisions or to generate trust in the model.

One of the main virtues of replacing explanation by understanding as the focus of
analysis in XAI is that the factivity condition need not be satisfied. According to so-called
moderate factivists (Kvanvig 2009b; Mizrahi 2012; Carter & Gordon 2016), not all the
information on the basis of which something is understood must be true, only the central
propositions. Other philosophers go even further and reject the factivity condition
altogether.? Elgin’s (2007) discussion of the role of models and idealizations allows that our
understanding of some aspects of reality may be literally false. Far from being an unfortunate
expedient, idealizations and models are an essential and ineliminable component of our
scientific understanding of the world; she calls them “felicitous falsehoods” (2004, p. 116).
In section 3 I will explore Elgin’s idea in the context of our understanding of opaque models.
I will argue that although the methods and artifacts used to understand an intelligent system
and its decisions are not, and perhaps cannot be, entirely faithful to the model, this does not
tell against them. On the contrary, they can afford indirect epistemic access to matters of fact

that are otherwise humanly impossible to discern.

 

51 am grateful to an anonymous reviewer for pointing this out.
° The relaxation of the factivity condition is often defended in the context of objectual understanding,
but it remains controversial in the case of understanding why. I return to this distinction in section 4.
A second reason to shift our focus from explanation to understanding is the
importance of taking into account the specific context, background knowledge, and interests
of end-users and stakeholders of opaque models.’° In any field it is possible to establish a
distinction between different levels of expertise and different levels of understanding
depending on the depth of a person’s knowledge of a phenomenon. In the sciences, it is
expected that the novice will become an expert by acquiring the required knowledge and
skills. More importantly, scientific experts will be able to master the best possible
explanations of the phenomena within their field of study. This situation is not replicated in
the case of machine learning. The medical doctor or the parole officer who makes use of a
black box model is not supposed to acquire the level of expertise of a computer scientist, and
their respective level of understanding of any explanatory model of the opaque system will
remain incomparable. This seems to be an element that has not always been kept in mind in
XAI. Many AI researchers build explanatory models for themselves, rather than for the
intended users, a phenomenon that Miller et al. (2017) refer to as “the inmates running the
asylum” (p. 36). The alternative they propose, and which I fully endorse, is to incorporate
results from psychology and philosophy to XAI.!! It is necessary to explore a naturalistic
approach to the way in which context and background knowledge mold an agent’s
understanding of an interpretative model. Existing theories of how people formulate
questions and how they select and evaluate answers should also inform the discussion (Miller
2019).

A third advantage of focusing on the pragmatic elements of interpretative models is
that we can obtain a better grasp of the relationship between explanation and trust. When
using machine learning in high-stakes contexts such as medical diagnosis or parole decisions
it is necessary to trust the individual decisions generated by the model. Several authors have
argued that post-hoc interpretability, i.c., an explanation of the decision, is a necessary

condition for trust (Kim 2015, Ribeiro et al. 2016). Additionally, of course, the system must

 

© De Graaf and Malle (2017) have also emphasized the importance of these pragmatic factors: “The
success of an explanation therefore depends on several critical audience factors—assumptions,
knowledge, and interests that an audience has when decoding the explanation” (p. 19).

'! See also De Graaf & Malle (2017), Miller (2019), and Mittelstadt et al. (2019).
have avery high score on an evaluation metric based on decisions and ground truths. Suppose
that an opaque model has consistently shown a high degree of predictive accuracy and a user
has been given a clear post-hoc explanation of its behavior. The user has the best possible
understanding of the system, taking into account, of course, the epistemic limitations
mentioned above. But predictive reliability and a post-hoc explanation are not sufficient to
generate trust. Trust does not depend exclusively on epistemic factors; it also depends on the
interests, goals, resources, and degree of risk aversion of the stakeholders. Trust involves a
decision to accept an output and act upon it. Different agents bound by different contextual
factors will make different decisions on the basis of the same information. I will leave open
the question of whether classical decision theory can provide an adequate analysis of trust in
AI systems.'* But the important lesson to draw from the multidimensional character of trust
is that there is no simple correlation between explanation and trust, and that an adequate
analysis of trust requires taking into account contextual factors that can foster or hinder it.
The reasons I have presented in this section recommend abandoning the traditional
“explanationist” path according to which understanding can only be obtained via an
explanation in any of the guises it has adopted in the philosophy of science. The next section

will offer alternative ways to achieve understanding.

3. Alternative Paths to Understanding

Abandoning the necessary connection between explanation and understanding opens up
several avenues of research that can lead to understanding the workings and decisions of
opaque models. Implicit causal knowledge, analogical reasoning, and exemplars are obvious
alternative paths to understanding. But so are models, idealizations, simulations, and thought
experiments, which play important roles in scientific understanding despite being literally
false representations of their objects. In a similar vein, the methods and devices used to make
black box models understandable need not be propositionally-stated explanations and they

need not be truthful representations of the models. I will begin by presenting a few examples

 

” See Falcone & Castelfranchi (2001) for a critique of the use of decision theory to understand trust
in virtual environments.
of how understanding can be achieved in the natural sciences without the use of explanations
before moving to a discussion of how similar devices can be used, and have been used, in
understanding AI models.

Many philosophers, beginning with Aristotle and continuing with the defenders of
causal explanations, have argued that understanding-why is simply knowledge of causes
(Salmon 1984; Lewis 1986; Greco 2010; Grimm 2006, 2014).!3 Naturally, causal
explanations are the prime providers of knowledge of causes. But causal knowledge does not
come exclusively from explanation. As Lipton points out, “much of empirical inquiry
consists in activities-physical and intellectual-that generate causal information, activities
such as observation, experimentation, manipulation, and inference. And these activities are
distinct from the activity of giving and receiving explanations” (2009, p. 44). To be sure, the
causal information generated by these activities can be given a propositional representation
and can thus be transformed into explicit causal explanations. But Lipton argues that in many
cases such activities generate causal information that remains as tacit knowledge, allowing
us to perform epistemic and practical tasks. Such tacit causal knowledge comes primarily
from images and physical models. An orrery or a video, for example, can provide better
understanding of retrograde planetary motion than an explanation stated in propositional
form. A subject might even be able to understand retrograde motion without being able to
articulate such an explanation.

Direct manipulation or tinkering of a causal system is an even more obvious source
of implicit causal knowledge. Adjusting a lever, a button or an input variable and observing
its effects on other parts of a system is a way of beginning to understand how the system
works. Manipulation also provides modal information about the possible states of a system.
In fact, the ability to manipulate a system into new desired states should be seen as a sign of
understanding. In other words, understanding requires the ability to think counterfactually
(de Regt & Dicks 2005; Wilkenfeld 2013).

 

3 Philosophers of science are much more inclined to accept this view than epistemologists, who have
fiercely resisted it. See, for example, Zagzebski (2001), Kvanvig (2003), Elgin (2004), and Pritchard
(2014). I do not have space to discuss the issue here, but from the text it should be clear that I side
with the epistemologists.

10
Causal information, implicit or explicit, is not the only source of understanding.
Consider analogical reasoning. Darwin (1860/1903) used an analogy between the domestic
selection of animals and natural selection to argue for the latter. Although it is incomparable
in many respects, artificial selection illuminates how the mechanism would work in a larger
class (Lipton 2009, p. 51). Exemplification is another important avenue towards
understanding. The examples in a logic textbook can show a student how the rules of natural
deduction work. Her initial understanding of the rules will be tied to the examples, but it will
gradually drift away as her ability to use the rules in new situations improves. When an item
serves as an example, “it functions as a symbol that makes reference to some of the properties,
patterns, or relations it instantiates” (Elgin 2017, p. 184). It can only display some of these
features, downplaying or ignoring others. As the complexity of the item increases, the
decision to emphasize or underscore some salient features over others will be determined by
pragmatic reasons, such as the intended audience and use of the example.

The use of non-propositional representations such as diagrams, graphs, and maps
present another clear case of understanding without explanation. A subway map is never a
faithful representation of the real train network. It alters the distance between stations and
the exact location of the tunnels in order to make the network easy to understand, but it must
include the correct number of lines, stations and intersections to be useful at all. It must be
sufficiently accurate without being too accurate.

Finally, models and idealizations play a similar role in science (Potochnik 2017).
They simplify complex phenomena and sometimes the same phenomenon is represented by
multiple, seemingly incongruous models. They afford epistemic access to features of the
object that are otherwise difficult or impossible to discern. Models are not supposed to
accurately represent the facts, but they must be objective. Models have to denote in some
sense the facts they model. They “are representations of the things that they denote” (Elgin
2008, p. 77). The general relation between scientific models and their objects is a thorny issue
that deserves a more detailed discussion than the one I can provide here, but one important
aspect that must be noted is that the adequate level of “fit” between a model and its object is
a pragmatic question. Many models are, in Elgin’s apt phrase, “true enough” of the

phenomenon they denote:

11
This may be because the models are approximately true, or because they diverge
from truth in irrelevant respects, or because the range of cases for which they are
not true is a range of cases we do not care about, as for example when the model
is inaccurate at the limit. Where a model is true enough, we do not go wrong if
we think of the phenomena as displaying the features exemplified in the model.
Obviously, whether such a representation is true enough is a contextual question.
A representation that is true enough for some purposes, or in some respects is not

true enough for or in others (2008, p. 85).

Applications of all of the approaches mentioned above can be found in the XAI
literature. It is important to bear in mind that many authors in the field refer to these
alternative paths to understanding as “explanations,” a usage that threatens to trivialize the
term. If whatever makes an opaque model or its decisions better understood is called an
explanation, the term ceases to have any definitive meaning. My argument throughout the
paper has only focused on the notion of explanation as it has been traditionally understood
in the philosophy of science and epistemology (e.g., causal models, covering-law models,
probabilistic approaches, etc.). It is in this sense that there are alternative sources of
understanding.

It is customary to distinguish between two different goals in XAI: understanding a
decision, often called post-hoc interpretability, and understanding how the model functions,
i.c., making the model transparent (Lipton 2016; Lepri et al. 2017; Mittelstadt et al. 2019).
Exemplifications, analogies, and causal manipulation are often used in the former, while the
use of models is more common in the latter. I will present some examples of the use of these
techniques, and in the next section I will examine the kind of understanding they provide.
The ultimate question I will try to answer is whether transparency and post-hoc
interpretability aim at different types of understanding.

The attempts to make a model transparent can focus on the model as a whole
(simulatability), on its parameters (decomposability), or on its algorithms (algorithmic
transparency) (Lipton 2016). A complete understanding of the model would thus allow a user

to repeat (simulate) the computation process with a full understanding of the algorithm and

12
an intuitive grasp of every part of the model. Each of these aspects presents its own
challenges, but my interest here is in the use of interpretative devices to provide an overall
understanding of opaque models, 1.c., models that are not designed to be fully understood.
The most common way to make a black-box model as understandable as possible is through
the use of proxy or interpretative models (Guidotti et al. 2018). Many of these models provide
coarse approximations of how the system behaves over a restricted domain. The two most
widely used classes of models are linear or gradient-based approximations, and decision trees
(Mittelstadt et al. 2019). For the interpretative model to be useful, a user must know “over
which domain a model is reliable and accurate, where it breaks down, and where its behavior
is uncertain. If the recipient of a local approximation does not understand its limitations, at
best it is not comprehensible, and at worst misleading” (Mittelstadt et al. 2019, p. 281).
Oversimplified or misleading models also incur the risk of being perceived as deceitful,
thereby undermining the user’s trust in the original model. Thus, the first desideratum of
interpretative models is that they must be as faithful to the original model as possible and
absolutely transparent about their limitations.

Mittelstadt et al. (2019) argue that XAI should not focus on developing interpretative
models because they are akin to scientific models, and therefore very different from “the
types of scientific and ‘everyday’ explanations considered in philosophy, cognitive science,
and psychology” (p. 279). My view is exactly the opposite. Since the notion of explanation
discussed in the philosophy of science is inapplicable in the context of opaque machine
learning models, and since I do not want to settle for a purely subjective sense of explanation,
XAI should adopt any other methods and devices that provide objective understanding.
Scientific models, suitably adapted to the intended users, offer an indirect’* path towards an
objective understanding of a phenomenon. We should therefore see the parallel between
scientific models and interpretative models in a positive light.

The fidelity desideratum for interpretative models has to be balanced against the

desideratum of comprehensibility. There are very few empirical studies about which kinds

 

4 A direct understanding of a phenomenon would be factive, based on a literal description of the
explanatory elements involved. It is in this sense that models offer an indirect path towards objective
understanding.

13
of interpretative models are easier to understand. Huysmans et al. (2011), for example,
present evidence that single-hit decision tables perform better than binary decision trees,
propositional rules, and oblique rules in terms of accuracy, response time, and answer
confidence for a set of problem-solving tasks involving credit scoring. This study is of limited
use because it was done with extremely simple representation formats and the only
participants were 51 graduate business students.!> It is necessary to undertake similar studies
that also include linear regressions, simple naive Bayes classifiers, and random forests. !°
These interpretative models also have to be tested on a more diverse population with different
levels of expertise (Doshi-Velez & Kim 2017). These types of empirical studies are essential
for the purposes of XAI, and they have to be complemented with psychological studies of
the formal and contextual factors that enhance understanding. As noted by Pazzani (2000),
there is little understanding of the factors that foster or hinder interpretability in these cases,
and of whether users prefer, for example, visualizations over textual representations.

The appropriateness of an interpretative model thus depends on three factors:
obtaining the right fit between the interpretative model and the black box model in terms of
accuracy and reliability, providing sufficient information about its limitations, and achieving
an acceptable degree of comprehensibility for the intended user. While there may be some
identifiable, permanent features of interpretative models that facilitate understanding, the
choice of the best proxy method or artifact will also depend on who the intended users of the

original system are. Their background knowledge, their levels of expertise, and the time

 

In Allahyari and Lavesson (2011), 100 non-expert users were asked to compare the
understandability of decision trees and rule lists. The former method was deemed more
understandable. Freitas (2014) examines the pros and cons of decision trees, classification rules,
decision tables, nearest neighbors, and Bayesian network classifiers with respect to their
interpretability, and discusses how to improve the comprehensibility of classification models in
general. More recently, Ftimkranz et al. (2018) performed an experiment with 390 participants to
question the idea that the likeliness that a user will accept a logical model such as rule sets as an
explanation for a decision is determined by the simplicity of the model. Lage et al. (2019) also explore
the complexities of rule sets to find features that make them more interpretable, while Piltaver et al.
(2016) undertake a similar analysis in the case of classification trees. Another important aspect of this
empirical line of research is the study of cognitive biases in the understanding of interpretable models.
Khiegr et al. (2018) study the possible effects of biases on symbolic machine learning models.

'© As noted in the Introduction, none of these methods is intrinsically interpretable.

14
available to them to understand the proxy model can vary widely. This last aspect has been
entirely neglected in the literature; not a single method reviewed by Guidotti et al. (2018)
presents real experiments about the time required to understand an interpretative model.

Turning very briefly to post-hoc interpretability, we find in the literature several
interpretative devices to understand a decision. In many cases, a sensitivity analysis provides
a local, feature-specific, linear approximation of the model’s response. The result of the
analysis consists of a list, a table, or a graphical representation of the main features that
influenced a decision and their relative importance. Often, such devices allow a certain
degree of causal manipulation that brings out feature interactions. This is the basis of the
LIME model proposed by Ribeiro et al. (2016), a technique to offer functional explanations
of the decisions of any machine learning classifier. To understand the behavior of the
underlying model, the input is perturbed to see how the decisions change without worrying
about the actual computation that produced it. The user can ask counterfactual questions
about local changes and see the results in an intuitive way. Saliency maps offer a similar
functional understanding of the model. A network is repeatedly tested with portions of the
input occluded to create a map showing which parts of the data actually have influence on
the network output (Zeiler & Fergus 2014; Lapuschkin et al. 2019).

Caruana et al. (1999) argue that analogies and exemplars (prototypes) are a useful
heuristic device. A model can report, for every new decision, other examples in the training
set that the model considers to be most similar. The authors seek to use this method in clinical
contexts, where doctors often refer to case-studies to justify a course of action. The basic
assumption made by case-based methods, such as é-nearest neighbor, is that similar inputs
correlate with similar outputs. The methods look for the case in the training set, the prototype,
that is most similar in terms of input features to the case under consideration.

Another commonly used method, especially in interactions with autonomous agents,
is to provide natural language explanations of a decision (McAuley & Leskovec 2013;
Krening et al. 2016). These explanations state information about the most important features
in a decision and come closer than any other method to the causal explanations used in
science and everyday life. The difference, once again, is that these “explanations” are not

factive, regardless of how plausible they appear. Eshan et al. (2018) even suggest that textual

15
explanations can be rationalizations: “AI rationalization is based on the observation that there
are times when humans may not have full conscious access to reasons for their behavior and
consequently may not give explanations that literally reveal how a decision was made. In
these situations, it is more likely that humans create plausible explanations on the spot when
pressed. However, we accept human-generated rationalizations as providing some lay insight
into the mind of the other” (p. 81).

A common feature of many post-hoc interpretations is that they are model-agnostic.
They do not even attempt to open the black box and they offer only a functional approach to
the problem of explaining a decision. The cognitive achievement reached by the use of these
devices seems to differ in great measure from the understanding provided by an interpretative
model. In the last section of the paper I will tackle the question of whether it is possible to

characterize different types of understanding in AI.

4. Types of Understanding in AI

On the basis of the methods described in the previous section, it is tempting to divide the
understanding they provide into two different types. The first one would be associated with
post-hoc interpretability. This type is often called understanding-why, and in this case its
object will be a specific decision of a model. In contrast, transparency seems to generate an
objectual understanding of a model. The distinction between these two types of
understanding has been widely discussed in epistemology. The question I will examine in the
beginning of this section is whether this epistemological distinction can be defended in the
present context.

Epistemologists establish a distinction between understanding why something is the
case, and understanding an object, a system or a body of knowledge (Kvanvig, 2003). It
seems straightforward to say that the goal of transparency in machine learning can be
understood in terms of objectual understanding. Consider the various ways in which this type
of understanding has been described: According to Zagzebski, understanding “involves
grasping relations of parts to other parts and perhaps the relation of parts to a whole” (2009,
p. 144). For Grimm, the target of objectual understanding is a “system or structure ... that
has parts or elements that depend upon one another in various ways” (2011, p. 86). And

16
Greco characterizes it as “knowledge of a system of dependence relations” (2012, p. 123).
The interpretative models that we considered in the previous section all provide the kind of
understanding described by these authors.

Understanding why p, on the other hand, is not equivalent to simply Anowing why p.
Suppose the only thing a person knows about global warming is that it is caused, to a large
extent, by an increase in the concentration of greenhouse gases. This is a claim the person
has heard repeatedly in serious media outlets and scientific TV shows, but he has never
stopped to think about the causal mechanisms involved. The person knows why the earth is
warming, but this information is insufficient to understand why it is warming. The person
lacks, for example, the ability to answer a wide range of questions of the type what-if-things-
had-been-different (Woodward 2003, p. 221). What would happen to global temperatures if
all human activity were to cease? What would be the effect on global warming of a massive
volcanic eruption similar in scale to the eruption of Krakatoa in 1883? These are the kind of
counterfactual scenarios commonly studied in climate research and modelling, which the
common person is unable to understand. A complete understanding of global warming also
involves the ability to make probability estimates of future scenarios based on current data.

Notice that the ability to answer counterfactual questions and to make predictions
depends to a large extent on an objectual understanding of the larger body of knowledge to
which the specific object of understanding belongs. Without a basic understanding of the
structure, chemistry, and behavior of the earth’s atmosphere, for example, a person will not
be able to answer counterfactual questions or deliver probability estimates about global
warming. It follows, as Grimm (2011) convincingly argues, that understanding-why is a
variety of objectual understanding, but at a local level, and that there is no genuine distinction
between the two types of understanding. The implication for machine learning is that
understanding a decision requires some degree of objectual understanding of the model.

Mittelstadt et al. (2019) seem to reach a similar conclusion:

[A]t the moment, XAI generally avoids the challenges of testing and validating
approximation models, or fully characterizing their domain. If these elements are

well understood by the individual, models can offer more information than an

17
explanation of a single decision or event. Over the domain for which the model
accurately maps onto the phenomena we are interested in, it can be used to answer
“what if? questions, for example “What would the outcome be if the data looked
like this instead?” and to search for contrastive explanations, for example “How

could I alter the data to get outcome X?” (p. 282).1”

It is true that some of the post-hoc interpretability devices described in the previous
section allow stakeholders to manipulate the parameters and observe the different decisions
generated thereby. But this is not genuine counterfactual reasoning. By tinkering with the
parameters, the stakeholders can only form functional generalizations with a very weak
inductive base. True counterfactual reasoning is purely theoretical, based on knowledge
about how the model works. Thus, if we take the ability to think counterfactually about a
phenomenon as a sign that the agent understands it, as suggested by de Regt and Dicks
(2005), understanding the decisions of a model requires some degree of objectual
understanding.

There is, nonetheless, an important difference between the two types of understanding
under consideration. Virtually all epistemologists regard understanding-why as factive, while
allowing that objectual understanding might not be entirely so. Pritchard, for example, gives
the following example to show that understanding-why is factive: “Suppose that I believe
that my house has burned down because of an act of vandalism, when it was in fact caused
by faulty wiring. Do I understand why my house burned down? Clearly not” (2008, p. 8). In
other words, according to Pritchard, without a true causal explanation there can be no
understanding-why. But changing the example can debilitate the intuitions that support this

conclusion. Suppose an engineer is investigating the collapse of a bridge and uses Newtonian

 

7 A terminological clarification is in order. Mittelstadt et al. (2019) and other researchers in XAI use
the phrase “contrastive explanations” to refer to counterfactuals. But these are two very different
things. In philosophy, an explanation is contrastive if it answers the question “Why p rather than g?”
instead of just “Why p?” In either case the explanation provided must be factual. To turn it into a
counterfactual situation, the question must be changed to: “What changes in the world would have
brought about g instead of p?” And the answer will be a hypothetical or counterfactual statement, not
an explanation.

18
physics as the basis for his analysis. Strictly speaking, the explanation is based on a false
theory, but it can hardly be argued that the engineer is a priori barred from understanding
why the bridge collapsed. Or suppose an economist successfully explains a sudden rise in
inflation using a macroeconomic model that, again, cannot be literally true (Reiss 2012). It
thus seems that the factivity of understanding-why can only be defended in simple scenarios
where a complete analysis of the relevant causal variables can be provided, but as soon as
the context requires the use of theoretical tools such as idealizations and models, it becomes
highly doubtful.

Machine learning is precisely this kind of context. The use of arbitrary black-box
functions to make decisions in machine learning makes it impossible to reach the causal
knowledge necessary to provide a true causal explanation. The functions may be extremely
complex and have an internal state composed of millions of interdependent values. Machine
learning is the kind of context in which one can say that, in principle, it is impossible to
satisfy the factivity condition for understanding-why.

We thus have an argument to the effect that understanding-why and objectual
understanding in machine learning cannot be entirely independent of each other, but rather,
that the former is a localized variety of the latter. And we have an argument against the claim
that understanding-why is always factive, which was supposed to be the most important
property that distinguished both types of understanding. So even if prima facie the devices
and methods used to provide transparency and post-hoc interpretability are different, it is safe
to say, on the one hand, that understanding-why and objectual understanding are two different
species of the same genus, and on the other, that there is no essential difference between them
in terms of truth.

There is, however, a third way of characterizing understanding in AI. Psychologists
distinguish between the functional and the mechanistic understanding of an event. The former
“relies on an appreciation for functions, goals, and purpose” while the latter “relies on an
appreciation of parts, processes, and proximate causal mechanisms” (Lombrozo &
Wilkenfeld forthcoming, p. 1). For example, an alarm clock beeps because the circuit
connecting the buzzer to a power source has been completed (mechanical understanding) and

because its owner has set it to wake her up at a specific time (functional understanding).

19
Lombrozo and Wilkenfeld argue that a subject can have a functional understanding of an
event while being insensitive to mechanistic information. Lombrozo and Gwynne (2014)
have shown that properties that are understood functionally, as opposed to mechanistically,
are more likely to be generalized on the basis of shared functions. This means that a
functional, as opposed to a mechanistic understanding of the relation between an input and
an output will make it easier for a user to inductively conclude that similar inputs produce
similar decisions. There is also evidence that functional reasoning may be psychologically
privileged in the sense that it is often favored and seems to be less cognitively demanding
than mechanistic reasoning. Humans are “promiscuously teleological,” to use Kelemen’s
(1999) apt description. Finally, Lombrozo and Wilkenfeld also argue that functional and
mechanistic understanding differ with regard to normative considerations. A functional
understanding of a property of an object tells us what it is supposed to do, while
understanding the mechanism that causes that property lacks this normative element.
Functional understanding thus seems to be a different kind of understanding altogether,
compared to objectual understanding and understanding-why. It involves different content,
it supports different functions, and it has a distinctive phenomenology.

If we take the decision of an opaque model as our object of understanding, a
mechanistic understanding of it is equivalent to the local objectual understanding of the
model, as I have argued above. Its functional understanding, on the other hand, would focus
on the purpose of the model and the relation between its features and decisions. Functional
reasoning about black box models allows for a more mechanism-independent form of
reasoning. Aiming at this type of understanding will be appealing to those who want to offer
model-neutral interpretability devices and focus only on covariations between inputs and
decisions.

However, it seems to me that aiming for functional understanding in XAI is, to a
certain extent, to give up on the project of explaining why an AI model does what it does. It
is to embrace the black box and trust it as one trusts a reliable oracle without understanding
its mysterious ways. Less metaphorically, reliability by itself cannot usher trust because of
the dataset shift problem (Quinonero-Candela et al. 2009). To have confidence that the model

is really capturing the correct patterns in the target domain, and not just patterns valid in past

20
data that will not be valid in future data, it is necessary to have a global or at least a local
objectual understanding of the model.'* Unfortunately, most solutions to the dataset shift
problem focus only on accuracy ignoring model comprehensibility issues (Freitas 2014).
Furthermore, methods designed to enhance the functional understanding of a model are also
more likely to be tailored to user preferences and expectations, and thus prone to
oversimplification and bias. Although the understanding and trust sought by XAI should
always take into account a model’s stakeholders, it should not pursue these goals by offering
misleadingly simple functional explanations that can derive in unjustified or dangerous
actions (Gilpin et al. 2019). Finally, a purely functional understanding of a model would also
impede legal accountability and public responsibility for the decisions of the model. Guilt
for an unexpected decision with harmful or detrimental consequences to the user cannot be
decided if the only information available is the previous predictive accuracy of the model. It
is necessary to understand why the model produced the unexpected result, that is, to have a
local objectual understanding of it.

In sum, in this section I have argued that both transparency and post-hoc
interpretability should be seen as more or less encompassing varieties of objectual
understanding, and that the kind of understanding provided by the functional approach to a
model offers an understanding of a different and more limited kind. In my view, it is the

former kind that should interest researchers in XAI.

5. Conclusion
In this paper I have argued that the term ‘explanation’, as it is currently used in XAI, has no
definitive meaning and shares none of the properties that have been traditionally attributed

to explanations in epistemology and the philosophy of science. My suggestion has been to

 

'8 To be sure, there are many scenarios where both the owner and the user (but not the developer) of
the model will be satisfied with its accurate decisions without feeling the need to have an objectual
understand of it. Think of the books recommended by Amazon or the movies suggested by Netflix
using the simple rule: “If you liked x, you might like y.” As I argued in section 2, the relation between
understanding and trust is always mediated by the interests, goals, resources, and degree of risk
aversion of stakeholders. In these cases, the cost-benefit relation makes it unnecessary to make the
additional effort of looking for mechanisms.

21
shift our focus from a blind search for explanatory devices and methods whose success is
uncertain, to the study of the mental state that X AI researchers are aiming at, namely, an
objective understanding of opaque machine learning models and their decisions. I have
argued that the use of interpretative models is the best avenue available to obtain
understanding, both in terms of transparency (understanding how the model works) and post-
hoc interpretability (understanding a decision of the model). The current approaches to the
latter rely on a purely functional understanding of models; however, leaving the black box
entirely untouched seems to belie the purpose of XAI. It must be admitted that interpretative
models can provide false assurances of comprehensibility. The task ahead for XAI is thus to
fulfill the double desiderata of finding the right fit between the interpretative and the black
box model, and to design interpretative models and devices that are easily understood by the
intended users. This latter task must be guided by an empirical investigation of the features
of interpretative models that make them easier to understand to users with different
backgrounds and levels of expertise. One of the possible areas of research is the comparative
study of the complexity of rule sets, decision tables and trees, nearest neighbors, and
Bayesian network classifiers with respect to their interpretability. XAI can also benefit from
interdisciplinary work with designers to create user-friendly, accessible, and engaging
interpretative tools and interfaces, in the same spirit as the legal design movement. Finally,
an important aspect of this empirical line of research is the study of cognitive biases in the
interpretation of models, especially in the context of autonomous systems with human-like

interfaces.

References

Achinstein, P. (1983). The nature of explanation. New York: Oxford University Press.

Allahyan, H., & Lavesson, N. (2011). User-oriented assessment of classification model
understandability. Proceedings of the llth Scandinavian Conference on Artificial
Intelligence. Amsterdam: IOS Press.

Carter, J. A., & Gordon, E. C. (2016). Objectual understanding, factivity and belief. In: M.
Grajner & P. Schmechtig (Eds.), Epistemic reasons, norms and goals (pp. 423-442).
Berlin: De Gruyter.

22
Caruana, R., Kangarloo, H., Dionisio, J. D. N., Sinha, U., & Johnson, D. (1999). Case-based
explanations of non-case-based learning methods. In Proceedings of the AMIA
Symposium (p. 212). American Medical Informatics Association.

Castelfranchi, C., & Tan, Y-H. (Eds.). (2001). Trust and deception in virtual societies (pp.
157-168). Dordrecht: Kluwer Academic Publisher.

Darwin, C. (1860/1903). Letter to Henslow, May 1860. In F. Darwin (Ed.), More letters of
Charles Darwin, vol. I, New York: D. Appleton.

De Graaf, M. M., & Malle, B. F. (2017). How people explain action (and Autonomous
Intelligent Systems should too). In AAAI Fall Symposium on Artificial Intelligence for
Human-Robot Interaction (pp. 19-26). Palo Alto: The AAAI Press.

de Regt, H. W., & Dieks, D. (2005). A contextual approach to scientific understanding.
Synthese, 144, 137-170.

de Regt, H. W., Leonelli, S.. & Eigner, K. (Eds.). (2009). Scientific understanding:
Philosophical perspectives. Pittsburgh: University of Pittsburgh Press.

Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine
learning. arXiv preprint arXiv: 1702.08608.

Ehsan, U., Harrison, B., Chan, L., & Riedl, M. O. (2018). Rationalization: A neural machine
translation approach to generating natural language explanations. In Proceedings of the
2018 AAAIACM Conference on AI, Ethics, and Society (pp. 81-87). New York: ACM.

Elgin, C. Z. (2004). True enough. Philosophical Issues, 14, 113-131

Elgin, C. Z. (2007). Understanding and the facts. Philosophical Studies,132, 33-42.

Elgin, C. Z. (2008). Exemplification, idealization, and scientific understanding. In M. Suarez
(Ed.), Fictions in science: Philosophical essays on modelling and idealization (pp. 77-
90). London: Routledge.

Elgin, C. Z. (2017). True enough. Cambridge: MIT Press.

Falcone R., & Castelfranchi, C. (2001). Social trust: A cognitive approach. In C.
Castelfranchi, & Tan, Y.-H. (Eds), Trust and deception in virtual societies (pp. 55-90).
Springer: Dordrecht.

23
Freitas, A. A. (2014). Comprehensible classification models: a position paper. ACM SIGKDD
explorations newsletter, 15(1), 1-10.

Fiirnkranz, J., Kliegr, T., & Paulheim, H. (2018). On cognitive preferences and the
plausibility of rule-based models. arXiv preprint arXiv: 1803.01316.

Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L. (2019). Explaining
explanation. An overview of interpretability of machine learning. arXiv preprint
arXiv: 1806. 00069v3.

Greco, J. (2010). Achieving knowledge. Cambridge: Cambridge University Press.

Greco, J. (2012). Intellectual virtues and their place in philosophy. In C. Jager & W. Loffler
(Eds.), Epistemology: Contexts, values, disagreement: Proceedings of the 34"
International Wittgenstein Symposium (pp. 117-130). Heusenstamm: Ontos.

Grimm, S. R. (2006). Is understanding a species of knowledge? British Journal for the
Philosophy of Science, 57, 515-535.

Grimm, S. R. (2011). Understanding. In 8. Bernecker & D. Pritchard (Eds.), The Routledge
companion to epistemology (pp. 84-94). New York: Routledge.

Grimm, 8. R. (2014). Understanding as knowledge of causes. In A. Fairweather (Ed.), Virtue
epistemology naturalized: Bridges between virtue epistemology and philosophy of
science, Dordrecht: Springer.

Grimm, S. R. (Ed.). (2018). Making sense of the world: New essays on the philosophy of
understanding. New York: Oxford University Press.

Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A
survey of methods for explaining black box models. ACM Computing Surveys (CSUR),
51(5), Article 93.

Hempel, C. G. (1965). Aspects of scientific explanation. New York: The Free Press.

Huysmans, J., Dejaeger, K., Mues, C., Vanthienen, J., & B. Baesens (2011). An empirical
evaluation of the comprehensibility of decision table, tree and rule based predictive
models. Decision Support Systems, 51(1), 141-154.

Kelemen, D. (1999). Functions, goals, and intentions: Children’s teleological reasoning

about objects. Trends in Cognitive Science, 12, 461-468.

24
Khalifa, K. (2012). Inaugurating understanding or repackaging explanation. Philosophy of
Science, 79, 15-37.

Kim, B. (2015). /nteractive and interpretable machine learning models for human machine
collaboration. PhD thesis, Massachusetts Institute of Technology.

Kliegr, T., Bahnik, S., & Fiirnkranz, J. (2018). A review of possible effects of cognitive
biases on interpretation of rule-based machine learning models. arXiv preprint
arXiv: 1804.02969.

Krening, S., Harrison, B., Feigh, K., Isbell, C., Riedl, M., & Thomaz, A. (2016). Learning
from explanations using sentiment and advice in RL. EEE Transactions on Cognitive
and Developmental Systems, 9(1), 44-55.

Kvanvig, J. (2003). The value of knowledge and the pursuit of understanding. New York:
Cambridge University Press.

Kvanvig, J. (2009). Response to critics. In A. Haddock, A. Millar, & D. Pritchard (Eds.),
Epistemic value (pp. 339-351). New York: Oxford University Press.

Lage, I., Chen, E., He, J., Narayanan, M., Kim, B., Gershman, §., & Doshi-Velez, F. (2019).
An Evaluation of the Human-Interpretability of Explanation. arXiv preprint
arXiv: 1902.00006.

Lapuschkin, $., Waldchen, S., Binder, A., Montavon, G., Samek, W., & Miiller, K. R. (2019).
Unmasking Clever Hans predictors and assessing what machines really learn. Nature
communications, 10(1), 1096.

Lepri, B., Oliver, N., Letouzé, E., Pentland, A., & Vinck, P. (2017). Fair, transparent, and
accountable algorithmic decision-making processes: The premise, the proposed
solutions, and the open challenges. Philosophy & Technology, 31, 611-627.

Lewis, D. K. (1986). Causal explanation. In Philosophical papers, vol. I (pp. 214-240). New
York: Oxford University Press.

Lipton, P. (2009). Understanding without explanation. In H. W. de Regt, S. Leonelli, & K.
Eigner (Eds.), Scientific understanding: Philosophical perspectives (pp. 43-63).
Pittsburgh: University of Pittsburgh Press.

Lipton, Z. C. (2016). The mythos of model interpretability. arXiv preprint arXiv: 1606.03490.

25
Lombrozo, T. & Gwynne, N. Z. (2014). Explanation and inference: Mechanistic and
functional explanations guide property generalization. Frontiers in Human
Neuroscience, §, 700.

Lombrozo, T., & Wilkenfeld, D. A. (forthcoming). Mechanistic vs. functional understanding.
In S. R. Grimm (Ed.), Varieties of understanding: New perspectives from philosophy,
psychology, and theology. New York: Oxford University Press.

McAuley, J., & Leskovec, J. (2013). Hidden factors and hidden topics: understanding rating
dimensions with review text. In Proceedings of the 7th ACM conference on
recommender systems (pp. 165-172). New York: ACM.

Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences.
Artificial Intelligence, 267, 1-18.

Miller, T., Howe, P., & Sonenberg, L. (2017). Explainable AI: Beware of inmates running
the asylum. In Proceedings of the LICAI-17 Workshop on Explainable AI (XAD) (pp. 36-
42). Accessed March 10, 2019 http:/Avww.intelligentrobots.org/files/IJCAI2017/IJCAI-
17_XAI_ WS Proceedings.pdf

Mittelstadt, B., Russell, C., & Wachter, S. (2019). Explaining explanations in AI. In
Proceedings of the Conference on Fairness, Accountability, and Transparency (pp. 279-
288). New York: ACM.

Mizrahi, M. (2012). Idealizations and scientific understanding. Philosophical Studies, 160,
237-252.

Paez, A. (2006). Explanations in K. An analysis of explanation as a beliefrevision operation.
Oberhausen: Athena Verlag.

Paez, A. (2009). Artificial explanations: The epistemological interpretation of explanation in
AI. Synthese, 170, 131-146.

Pazzani, M. (2000). Knowledge discovery from data? [EEF Intelligent Systems, 15(2), 10-
13.

Piltaver, R., LuStrek, M., Gams, M., & Martinéi¢-Ipsic¢, 8. (2016). What makes classification
trees comprehensible? Expert Systems with Applications: An International Journal,
62(C), 333-346.

26
Potochnik, A. (2017). [dealization and the aims of science. Chicago: University of Chicago
Press.

Pritchard, D. (2008). Knowing the answer, Understanding and epistemic value. Grazer
Philosophische Studien, 77, 325-339.

Pritchard, D. (2014). Knowledge and understanding. In A. Fairweather (Ed.), Virtue scientia:
Bridges between virtue epistemology and philosophy of science (pp. 315-328).
Dordrecht: Springer.

Quinonero-Candela, J., Sugiyama, M., Schwaighofer, A., & Lawrence, N. D. (Eds.). (2009).
Dataset shift in machine learning. Cambridge: MIT Press.

Reiss, J. (2012). The explanation paradox. Journal of Economic Methodology, 19, 43-62.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why should I trust you?”: Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (pp. 1135-1144). New York:
ACM.

Salmon, W. C. (1971). Statistical explanation. In W. C. Salmon (Ed.), Statistical explanation
and statistical relevance. Pittsburgh: Pittsburgh University Press.

Salmon, W. C. (1984). Scientific explanation and the causal structure of the world.
Princeton: Princeton University Press.

Samek, W., Wiegand, T., & Miller, K. R. (2017). Explainable artificial intelligence:
Understanding, visualizing and interpreting deep learning models. arXiv preprint
arXiv: 1708.08296.

Strevens, M. (2013). No understanding without explanation. Studies in the History and
Philosophy of Science, 44, 510-515.

van Fraassen, B. (1980). The scientific image. Oxford: Clarendon Press.

Wilkenfeld, D. (2013). Understanding as representation manipulability. Synthese, 190, 997-
1016.

Woodward, J. (2003). Making things happen. A theory of causal explanation. New York:
Oxford University Press.

27
Zagzebski, L. (2001). Recovering understanding. In M. Steup (Ed.), Knowledge, truth, and
duty: Essays on epistemic justification, responsibility, and virtue. New York: Oxford
University Press.

Zagzebski, L. (2009). On epistemology. Belmont: Wadsworth.

Zeiler, M. D., & Fergus, R. (2014). Visualizing and understanding convolutional networks.
In 13” European Conference on Computer Vision ECCV 2014 (pp. 818-833). Cham:
Springer.

28
Artificial Intelligence In Medicine 133 (2022) 102423

 

  

ELSEVIER

Contents lists available at ScienceDirect

Artificial Intelligence In Medicine

journal homepage: www.elsevier.com/locate/artmed

 

 

Research paper

A manifesto on explainability for artificial intelligence in medicine a

Carlo Combi**, Beatrice Amico*, Riccardo Bellazzi’, Andreas Holzinger‘, Jason H. Moore“,

Marinka Zitnik®, John H. Holmes‘

® University of Verona, Verona, Italy

> University of Pavia, Pavia, Italy

© Medical University Graz, Graz, Austria

4 Cedars-Sinai Medical Center, West Hollywood, CA, USA

© Harvard Medical School and Broad Institute of MIT & Harvard, MA, USA

f University of Pennsylvania Perelman School of Medicine Philadelphia, PA, USA

 

 

 

Check for
updates

 

d

 

ARTICLE INFO ABSTRACT

 

Keywords:

Artificial intelligence
Explainability

Explainable artificial intelligence
Interpretability

Interpretable artificial intelligence

The rapid increase of interest in, and use of, artificial intelligence (AI) in computer applications has raised a
parallel concern about its ability (or lack thereof) to provide understandable, or explainable, output to users.
This concer is especially legitimate in biomedical contexts, where patient safety is of paramount importance.
This position paper brings together seven researchers working in the field with different roles and perspectives,
to explore in depth the concept of explainable AI, or XAI, offering a functional definition and conceptual

framework or model that can be used when considering XAI. This is followed by a series of desiderata for
attaining explainability in AI, each of which touches upon a key domain in biomedicine.

 

1. Introduction

There is considerable discussion in the biomedical informatics and
computer science communities about the “un-explainable” nature of
artificial intelligence (AD, in that much is made of so-called “black-
box” algorithms and systems that leave users, and even developers,
in the dark as to how results were obtained. As a result, there is
growing skepticism about the potential limits of AI, even in the face
of burgeoning interest that at times reflects over-optimism about it.
At the same time, there is a growing community of researchers who
are working to address this skepticism through their work in making
Al explainable, and thus useful and potentially usable to those who
employ AI in their work. This is especially welcome in the domain of
biomedicine, where explainable AI is critically important for clinicians
in their daily practice.

As AI (including Machine Learning) becomes increasingly ubiqui-
tous, there are growing concerns and questions, such as:

+ How does an AI algorithm work — what is it doing?

* Does an AI system work as well as an expert?

* Does an AI system do what a user would do, were she in the same
situation?

* Why cannot the system tell a user how it arrived at a conclusion
or made a decision?

* Corresponding author.
E-mail address: carlo.combi@univr.it (C. Combi).

https://doi.org/10.1016/j.artmed.2022.102423

These concerns are of urgent importance and need to be addressed
with scientific and engineering rigor in a variety of biomedical do-
mains, including clinical decision support systems, patient monitoring,
public health surveillance, and biomedical research. However, we in
the informatics community are uniquely positioned to take leadership
roles in developing and implementing strategies for improving the
explainability of AI systems.

The primary goal of this paper is to present a compelling case for the
need to address gaps in the explainability of AI software and the results
presented to users. We hope to meet this goal by means of a rigorously
developed conceptual model for thinking about explainable AI, or XAI,
through a thorough exposition of the work to date and identification of
gaps in research and application of XAI, and a proposition for how these
gaps could be addressed. Even though many definitions and concepts
we will introduce and discuss are general and may be applicable to
many different domains, in the following we will focus on XAI in
Medicine and Health. Indeed, these domains have special requirements
that make XAI quite idiosyncratic and worthy of particular attention.

We have structured this paper as follows: after an introduction
to the problem of explainability, in Section 2 we discuss some back-
ground on how informatics and computer science describe the problem,
approaches to explainability, and applications of XAI to a variety of
key clinical domains; Section 3 contains a proposal for a conceptual
framework and foundational definition of XAI; Section 4 presents a set

Received 2 March 2022; Received in revised form 4 October 2022; Accepted 4 October 2022

Available online 9 October 2022
0933-3657/© 2022 Published by Elsevier B.V.
C. Combi et al.

of desiderata that would be important to address XAI moving forward;
finally, Section 5 sketches some conclusions and future directions.

2. Background

In this section, we will briefly introduce the main aspects that have
been discussed about XAI in general, in the areas of Computer Science
and Artificial Intelligence. Then, we will move to the main specific
issues of XAI in Medicine, ending with some non-exhaustive examples
of XAI approaches in clinical domains.

2.1. A research field’s description of the current landscape of AI

The concept of explainability has a long story in AI. Indeed, since
the first proposals of the so-called “expert systems”, there was the
need of having an explanation of why and how some conclusions were
reached by the system in a complex decision-support task. Such a
requirement was, and remains, extremely important in medicine, as
physicians needed to understand why the system was proposing, for
example, a specific diagnosis or treatment regimen. The need of having
some explanation about the output, an Al-based system provides, has
recently been exacerbated by the adoption of machine learning (ML)
approaches, where the reasoning task is often performed by “black-
box” systems that do not allow one to understand clearly why a specific
result has been reached [1].

In principle, explainability is related to understanding, i.e., having a
mental model of what we are observing. With a slightly different termi-
nology, we may say that explaining/interpreting consists of providing
causes of observed phenomena in a comprehensible manner through
a linguistic description of its logical and causal relationships [1,2]. In
the context of XAI, we need to understand the conclusions of a system
that is reasoning on some data to reach some result. Such systems in
medicine are often related to a decision-support task, where data may
be incomplete, uncertain, ambiguous, or missing. Moreover, such data
have a high complexity and heterogeneity, being expressed as often
interrelated and intertwined data in various formats such as structured,
semi-structured, or unstructured alphanumeric data, movies, images,
sounds, waveform signals, and so on.

Methods proposed to support explainability are often divided into
ante-hoc and post-hoc approaches. Ante-hoc approaches are related to
systems that allow one to directly understand their mechanisms in
providing a result such as a conclusion (e.g., a diagnosis) or a recom-
mendation (e.g., a treatment option). Decision trees, rule-based models,
and linear approximations are, for example, commonly considered to be
implicitly explainable. Post-hoc approaches try to provide some expla-
nation to the results reached by ML models, such as those based on deep
neural networks, random forests, support vector machines, and many
others. Post-hoc approaches are, in principle, applicable to different
kinds of AI systems. The difference between these two approaches is
that post-hoc approaches are not considered when designing a system,
but deal with the extraction of explanatory information from an already
existing system, which is usually based on ML “black-box” models. As
we will see in this section, the distinction between post-hoc and ante-
hoc approaches is sometimes subtle and has to be informed by further
considerations.

Explainability is thus an inherently multifaceted concept, which still
needs some more effort to have a precise characterization, also from the
terminological point of view [1]. Let us now consider some dimensions
of analysis that have been recently discussed in the literature.

The content of explanation: What is being explained? Independently from
being either post-hoc or ante-hoc, XAI systems have to be specified

Artificial Intelligence In Medicine 133 (2022) 102423

and developed with respect to the subject of the provided explanation.
Indeed, sometimes it is the reasoning mechanism itself that has to be
explained. In this case, explanation focuses on the mechanics of the
path that allowed the system to reach a specific result. Both generic and
specific medical knowledge could be used to this regard. On the other
side, explanatory information could be provided without any reference
to the reasoning approach of the system, but focusing on deriving
some form of association/relationship (causality) between data and
corresponding results.

The stakeholders of explanation: Who needs explainability? Any kind of
explanation needs to be tailored according to its recipients. It was
recently highlighted that many possible stakeholders may be closely
related to any XAI system [1]. In the medical and healthcare set-
tings, among the possible stakeholders we consider a broad commu-
nity of users, including clinicians, technicians, nurses, general prac-
titioners, administrative staff, different kinds of students, healthcare
policy makers, medical informaticians, and patients. The background
knowledge of such stakeholders is often deeply different and often
requires different user-centric solutions and techniques for a successful
explanation.

The goal for explanation: Why is explainability required? Considering
different stakeholders is not sufficient. We have to consider not only
who is the recipient of the explanation, but also why the explanation is
required. Indeed, the same stakeholder may have different motivations
and requirements with respect to XAI systems. As an example, a physi-
cian may have different desiderata that include, variously, education
and experience, fairness, ethics, satisfaction, trust, or controllability, while
developers would consider system acceptance, possibly in addition to
those required by a physician. Often such desiderata are not completely
disjoint and may co-exist in a single XAI-system [1]. According to
different desiderata, stakeholders could be looking for an answer to
different questions related to explainability [2]: Why did the algorithm
do that? Can I trust these results? How can I correct an error? Are data
meaningful with respect to the required task?

The moment, the duration and the frequency of explanation: When, how
long, and how frequently. A further, under-evaluated, issue is related to
when and how frequently an explanation is requested of the system.
Indeed, while naive and occasional users often require frequent expla-
nations at any stage of use of the supported AI system, experienced
users who are supposed to use the system in the daily clinical routines,
may require less frequent explanations, possibly focusing on rare or
unexpected situations. The level of detail and thus the duration of the
explanation may also be different, according to the specific needs of
different stakeholders in different contexts, with different goals.

The modalities of explanation: How is explainability represented? Dif-
ferent choices are possible when deciding how to explain. A first
option is to support perceptive interpretability [3]. This concept refers
to interpretations that can be humanly perceived, (1) through the
highlighting (often visual) of important input features with respect
to a given output (saliency), (2) through the observation of the stim-
ulation of neurons or groups of neurons (signal interpretability), and
(3) through the composition of logical statements or sentences that
can explain, even indicating causality (verbal interpretability). Often,
perceptive interpretability is founded on an abstraction of the task at
hand, which focuses on the most important aspects that explain the
reached solution. Systems based on perceptive interpretability work
with different techniques with respect to the ones used for the given
task. For example, a fuzzy rule-based system may be coupled with
an artificial neural network (ANN) system in diagnosing electrocardio-
graphic (ECG) signals [4]. As for perceptive interpretability through
visual and graphical systems, a widely acknowledged distinction exists
between directly understandable data, which are visualized through
one or two dimensional representations, and multi-dimensional repre-
sentations, which are not directly understandable [2]. A second option
C. Combi et al.

is to consider interpretability by mathematical structures. In this case,
either simple mathematical models are used, or different data-oriented
approaches are used to highlight hidden features of data, such as data
clustering, perturbations, data dependencies. Systems which support
interpretability via mathematical structures consider outputs (which
are ultimately perceptive) that require deeper cognitive processes and
background knowledge, before being interpretable [3].

Further distinctions about the modalities of explanations supported
by different XAI systems consider model-agnostic approaches and model-
specific approaches. While the first approaches attempt to provide ex-
planatory information only by observing input/output associations,
model-specific approaches consider also specific features of the model
under explanation [1]. A last aspect to consider for XAI systems
is their scope. Indeed, some contributions focus on single predic-
tions/classifications of the supported system (i.e., a single pair of
inputs/output). Such systems have a local scope [5,6], in comparison
with other approaches that have a global scope, which are designed to
explain the overall reasoning mechanism of the model. Moving closer
to applications in medicine, some aspects of AI have been identified
that make XAI systems in medicine challenging but worthy of rigor-
ous investigation. Factors as risk and responsibilities, accountability,
and trustworthiness, even though already considered in non-medical
domains, become here prominent and multifaceted. As an example,
while explainability is a strong requirement in the clinical domain, as
for acceptance, accountability, and legal compliance, a certain level of
opaqueness can be acceptable for some clinical users, provided that
some functional understanding of the model is supported, disregarding
a possible low-level algorithmic understanding [2,3,7].

As XAI in medicine is in an early stage of investigation, some further
issues have to be faced. Among them, the evaluation of XAI systems
with actual end-users will help understand, represent, and satisfy user
requirements [1]. Causability is the term proposed in [2] to explicitly
highlight the need of measurements for the quality of explanations. In
this direction, explanation interfaces have to make the results obtained
through the explainable model both usable and useful to the considered
stakeholder. Causability is thus a measure for the usability of such a
human-Al interface.

All the previous arguments we discussed lead to a further, re-
cently highlighted consideration [1]. Researching and developing XAI
in medicine is an interdisciplinary task, which requires the active
participation of different stakeholders, to cover different perspectives.
Methodologies for the design of XAI systems in medicine would require
skills from different scientific domains, such as Al, medical informatics,
software engineering, medicine, healthcare, and cognitive sciences.

2.2. Applications

We find many examples in the literature of research activities that
are devoted to exploring XAI in medical domains. Here we report some
recent examples regarding different techniques.

ML algorithms such as neural networks are inherently non-
explainable and are typically referred to as “black-box” models. How-
ever, there are some examples where neural network models can
be shown to produce explanatory descriptions to support the inter-
pretability of the output. In one study, the authors proposed a modular
framework, CEFEs (CNN Explainability Framework for ECG signals), a
post-hoc tri-modular evaluation structure that provides local interpre-
tations and explanations from convolutional neural networks [8]. The
evaluation of the model’s capacity is performed through quantitative
interpretability, where the metrics represent the features learned by
the model. In addition, the visualization of the features allows visually
correlating the features. Pennisi et al. employed a novel lung-lobe seg-
mentation network to identify CT scans of COVID-19 patients and au-
tomatically categorize specific lesions [9]. They integrate the pipeline
into a web application to support radiologists in the investigation of
this disease.

Artificial Intelligence In Medicine 133 (2022) 102423

In recent years, ensemble learning has achieved excellent results in-
corporating explainability. Yeboah et al. present an ensemble
clustering-based XAI model for traumatic brain injury (TBI) prognostic
and diagnostic analysis [10]. The goal is to identify patient subgroups
and key phenotypes that delineate these subgroups using tomography
data, exploring the features’ relevance. In another example, the authors
proposed an auxiliary decision support system that combined ensemble
learning with case-based reasoning (CBR) to help physicians improve
the accuracy of breast cancer recurrence prediction [11]. They use
extreme gradient boosting (XGBoost) to predict the risk of breast cancer
recurrence, and then use CBR to explain the reason for the prediction.
Of note, they conducted a survey of 32 oncologists to assess the utility
of the system as perceived by users, measuring the evaluation of the
system through a questionnaire, leading to a positive assessment by
the users of the system.

There are different examples of the usage of systems that exploit
the explanations through rules-based systems extracted from medical
data. They generated explanations in a human-understandable format,
increasing the trust to believe the results given by the support system.
El-Sappagh, et al., proposed a system of fuzzy IF-THEN rules [12].
It integrates reasoning with fuzzy reasoning over an ontology. They
proposed and implemented a new semantically interpretable fuzzy
rule-based system framework for diabetes diagnosis that is able to
provide accurate decision support as a result. Kavya et al. developed
an Allergy Diagnosis Support System (ADSS) [13]. They applied several
ML algorithms and then selected the best-performing algorithm using
k-fold cross-validation. In terms of the XAI method, they developed
a rule-based approach by building a random forest. Each path in a
tree is represented as an IF-THEN rule, and these rules are stored in
a rule base for expert assessment. Additionally, the authors developed
a mobile application, which can assist junior clinicians in confirming
the diagnostic predictions.

Although the user represents a central aspect in the approaches
we have just seen, the creation of an explainable system to use in a
particular context requires a multi-disciplinary collaboration, involving
collaboration with the stakeholders. Schoonderwoerd et al. presented
a case study of an application of a human-centered design approach
for Al-generated explanations [14]. The approach consisted of three
components:

(i) Domain analysis to define the concept and context of explana-
tions;
(ii) Requirements elicitation and assessment to derive the use cases
and explanation requirements; and
(iii) The consequential multi-modal interaction design and evalua-
tion to create a library of design patterns for explanations.

They apply this system in the context of child health. Dragoni, et al.
proposed an XAI system based on logical reasoning that supports the
monitoring of users’ behaviors and persuades them to follow healthy
lifestyles [15]. In this case, the authors first assessed the usability of
the application with questionnaires filled out by the user. Second, they
validated the correctness of the explanation generated by the system.
Finally, the last evaluation included an effectiveness analysis of the
generated explanations.

3. Towards a foundational definition of XAI in medicine

We propose a conceptual framework for XAI that captures the
intersection of four characteristics that are typical of any information
system, statistical model, or software application. These characteristics
are Interpretability, Understandability, Usability, and Usefulness, respec-
tively. Interpretability is the degree to which a user can intuit the cause
of a decision and thus the ability of a user to predict a system’s re-
sults [16]. Understandability is the degree to which a user can ascertain
how the system works, and leads directly to user confidence in the
system’s output. Usability is the ease with which a user can learn
C. Combi et al.

Artificial Intelligence In Medicine 133 (2022) 102423

Interpretability

     
 

Usefulness

 

\
| Understandability
|

Explainability

Usability

Fig. 1. The Venn diagram of explainability as intersection of usability, usefulness, interpretability, and understandability.

to operate, prepare inputs for, and interpret outputs of a system or
component. Usability thus asks the question “Can one use the system
easily?”. Usefulness, on the other hand, asks the question “Will one use
the system because it meets a user’s needs?”, and is seen as the practical
worth or applicability of a system. A system is unlikely to be useful if
it is not usable, however. As a result, usability is generally a first-order
requirement of any information system or software application [17].

However, when it comes to AI, we are not talking about any in-
formation system. Rather, AI systems and applications typically realize
some kind of reasoning task, to support some kind of decision-making,
such as proposing a clinical diagnosis or controlling a task in an en-
gineering operation, or to derive new knowledge/information in some
specific context, as mining hidden patterns in patients’ clinical histories.
Perhaps unique to AI applications, we need two additional dimensions
in order to realize an ability to provide user confidence that the decision
was correct, but even more so, the ability for a user to ascertain how
the system works. Thus, we propose that understandability is one such
dimension, and furthermore that it is, in our framework, complemen-
tary to usability. That is, usability is enhanced via understandability:
an AI application that is understandable is more likely to be usable.

The first characteristic of AI systems that we consider to be central
to our framework is interpretability, which we construe as the degree to
which a user can intuit the cause of a decision; in addition, it is the
degree to which a human can consistently predict a model’s results,
based on her experience with the application. Just as understandability
and usability are complementary, we propose that interpretability and
usefulness are complementary as well. For example, a user of an AI
application is more likely to find it useful, something that would meet
her needs for a given purpose, if the result or decision made by the
application is interpretable in the face of a real-life contingency.

This framework is illustrated as a Venn diagram in which these
four characteristics overlap various points of articulation, but most
importantly in the center, where all are needed when considering
explainability, as is shown in Fig. 1.

Where these four characteristics intersect is that smallest, yet rich-
est, segment of the Venn diagram, explainability. Due to the inter-
sectionality of the four characteristics just described, explainability
is a complex concept. It is not merely a characteristic of the model,

but rather something that emerges from the intersection of the four
characteristics we addressed here. As a result, we maintain that it
is best to describe explainability in a multidimensional way through
addressing a series of seven questions through the lens of others who
have worked extensively in this domain.

The proposed foundational definition of XAI does not explicitly
contain any specific reference to the medical and health domains.
Indeed, the concepts introduced here are general and can be applied
to any domain. However, we would stress here that, to the best of our
knowledge, the definition of explainability as the intersection of four
different characteristics is both original and particularly well suited for
medicine and health AI.

As for the novelty of our definition, we identify here two different
aspects: (i) from one general side, we explicitly distinguish the concepts
of interpretability, understandability, and explainability. Such distinction
is not clearly discussed in the existing literature, where, for example,
interpretable and explainable are often taken as synonyms (see, for exam-
ple, [1,3,18,19]). On the other hand, we explicitly introduced usability
and usefulness as first principles of explainability. Such user-oriented
aspects of explainability, even though considered and highlighted in
the considered literature, have not been discussed as main component
of a complex concept as complex as that of explainability.

The highlighted novelty of our foundational definition is also the
leverage for making it especially well-suited for medicine and health-
care. Indeed, in our view, Medicine and healthcare are characterized by
some specific features, which need to be considered as central for XAI.
The first feature consists in the presence of distributed, heterogeneous
decision-making tasks and a second can be defined as knowledge-intensive
domain. The presence of distributed, heterogeneous decision-making tasks
and of the corresponding XAI systems justify the presence of usability
and usefulness in the definition of explainability. Indeed, usability and
usefulness have to be evaluated according to different users and tasks.
They are not absolute concepts and need to be assessed “on the field”.
The usability of systems that have to be adopted by specialized physi-
cians in some intensive clinical setting requires it to be evaluated by the
pertinent clinical stakeholders, while, for example, the usability of XAI
systems supporting the communication and shared decision-making
among clinicians, general practitioners, and patients (e.g., in a web
C. Combi et al.

app supporting the mental health monitoring of home patients) should
be suitably assessed according to different explainability requirements,
corresponding to different background knowledge and roles of the
involved stakeholders.

Moreover, such knowledge-intensive and decision-intensive tasks re-
quire one to distinguish between interpretability and understandability.
Indeed, while the concept of interpretability is related to the capability
of predicting a system’s result, even without being aware of the “inter-
nal” structure and functioning of the system, understandability refers
to the capability of being aware of how the system works. In many
intensive decision-based tasks, such as the prompt reaction to some
unexpected change in an ICU patient’s condition, the interpretability
of an Al-based system may emerge as an indispensable feature. Indeed,
the clinician has to be able to recognize how recorded vital signs are
related to the alarms triggered by an Al-based system. It is worthwhile
to stress that interpretability does not mean that the Al-based system is
not important or useful as the user is able to predict the system’s result.
Indeed, the capability of predicting the system’s result, does not mean
that a human can process all the required data in an acceptable way,
according to the requirements either related to the number of patients
to consider or to the real-time results.

On the other hand, interpretability alone is often not sufficient to
attain a necessary level of explainability. Understandability requires
that the stakeholders have to be able to understand how the Al-based
system works. In many medical and healthcare Al-based systems it
may be important to have a deep understanding of the system internal
behavior, in a way comprehensible to the specific clinical stakeholder.
Let us continue with the example of an Al-based system for patient
monitoring in ICU. While the Al-based system supporting real-time
monitoring requires some kind of interpretability, the same Al-based
system in the reporting and data analytics part could require more
explicitly some kind of understandability. Indeed, when doing off-line
data analysis it may be important to understand how the system is able
to derive even unexpected results. As these results have to be related
to existing and evolving medical knowledge, a deep comprehension
of system technicalities and behaviors would also support a suitable
elicitation of new medical knowledge.

4. Questions, propositions, and desiderata in the quest to attain
XAI in medicine

After the proposal of our foundational definition of XAI in Medicine,
supported by some simple examples in clinical domains, let us now
move to more concrete issues that are necessary to consider in the
practical development and use of (explainable) Al-based systems in
medicine and healthcare. In the following we will touch on several
different issues. After considering the design of XAI systems in Medicine
(What are the requirements for XAI? How can we evaluate the good-
ness of the provided explanation?), we will introduce some further
motivation supporting the distinction between understandability and
explainability (If an AI system’s output is understandable, is it auto-
matically explainable?). Then, we will deal with the importance of
modeling the considered medical domains (What is the role of domain
understanding in achieving XAI in medical applications?). We will then
continue with some more abstract aspects, as they relate to the evo-
lution from data to wisdom through explainability (Can explainability
draw us closer to wisdom?), to (Can an AI system that is not explainable
be trustworthy?) and that connecting explainability and trustworthi-
ness (Can an AI system that is not explainable be trustworthy?). We
will end this section by answering the (usually hidden) question: Is XAI
in medicine always required?

The questions we will deal with in this section complement the
foundational definition we proposed in the previous section and apply
such definition with respect to real-world aspects of XAI in clinical
contexts.

Artificial Intelligence In Medicine 133 (2022) 102423

4.1. What are the requirements for XAI? How can we evaluate the goodness
of the provided explanation?

Proposition: There are tangible, instantiable, user-centered re-
quirements that must be met in order to achieve an XAI system;
more specifically, there is the need to measure, interpret, and
understand usability vs. usefulness, and interpretability vs. under-
standability, and how those two relate to each other in the context
of use and users, particularly in the context of AI in medicine.
Similar to any information system, systems that employ AI can and
should be developed and evaluated using state-of-the-art methods that
can be extended to the domain of explainability. While validation and
verification have been part of the canon for evaluating AI systems
for several decades, these focus on operability and the accuracy of
knowledge representation and inference. However, neither validation
nor verification have fully taken into account the explainability or
interpretability of the results from a user’s perspective. Proposed here
are desiderata in two broad domains of requirements for XAI that
would serve to further the development of AI systems that help users to
understand how such systems reach conclusions or offer advice. These
domains are linking the cognitive to the explainable, and the evaluation
of explainability.

+ Linking the cognitive to the explainable: the role of theory. Knowl-
edge elicitation has long been the central purpose of knowledge
engineering, but it focuses on developing a knowledge base that
does not address the needs of users as they interact with an
AI system. This lacuna is especially evident with regard to the
user interface. It is argued here, and supported in the literature,
that qualitative inquiry driven by theoretical frameworks is needed
to develop user-centered interfaces for ML in healthcare appli-
cations [20]. Theory-driven user interface design that takes into
account the cognitive and behavioral aspects of users is founda-
tional to achieving true explainability. This extends traditional
principles of user interface design to include aspects of what
influences user interpretation. Such aspects include attitudes and
beliefs that may bias interpretability and subsequently influence
users’ confidence and understanding of the system and its results.
In their recent survey of models for achieving explainability,
Markus et al. provide a framework for choosing the type of
explainable interface between model-, attribution-, or example-
based explanations [19]. They advocate for methods for achieving
explainability that are sensitive to the requirements of the prob-
lem domain, and that these should drive the choice of approach,
rather than enforcing a single paradigm of explainability. In a
word, they call for an agile approach to attaining and evaluating
explainability, which is very much in line with the best prac-
tices of information system development in general. One agile
approach to attaining explainability in AI systems turns to fuzzy
set theory and its application to fuzzy reasoning systems. Such
systems provide a plausible paradigm for modeling explainability,
since natural language is one defining characteristic of fuzzy sys-
tems. Alonso Moral et al. argue for this paradigm, showing how
user-centered explainability is connected to fuzzy modeling [21].
Finally, any effort to establish explainability needs to be linked
to the cognitive aspects of human inference. It is arguable that
there is no more urgent need for this in medical decision making.
An example of this kind of cognition is seen in the principle of
ex adiuvantibus, which is the inference leading to a conclusion,
such as the cause of a diseases, that is based on evidence that
the disease responded to a treatment. As an example, one might
infer that a migraine headache was caused by exposure to a
specific allergen because an antihistamine was shown to prevent
the headache. Such causal inferences many or may not be correct
in practice, but they are made frequently in clinical practices, and
in fact this type of reasoning is at the heart of allopathic medicine.
C. Combi et al.

: A user-oriented perspective of explainability. The growing research
community in XAI has already developed a number of highly
successful XAI methods [22]. Explainability in this context high-
lights technically decision-relevant parts of machine representa-
tions and machine models. For example, parts that contributed
to model accuracy during training or to a particular prediction
are visualized by a heatmap, a good and proven example being
the very well known Layer Wise Relevance Propagation (LRP)
method [23]. However, this visualization does not refer to a
human model. For this purpose, the concept of causability was
introduced, which is defined as the measurable extent to which
an explanation reaches a certain level of causal understanding
for a human end-user [2]. Since this concept refers to a human
model, it can be used very well to design and evaluate future
human-Al interfaces [7]. These future Human—AI interfaces must
provide a successful mapping between Explainability and caus-
ability and foster contextual understanding and allow the expert
to ask questions and counterfactuals (“what-if questions) [24].
At the same time such question—answer interfaces can make use
of a human-in-the-loop, who can bring human experience and
conceptual knowledge to AI processes — something that the best
AI algorithms available still lack. An example that is important
for medical AI is the classification of entities into several classes,
where typically, taking into account the uncertainty about the
membership of the classes, entities are classified as “yes”, “no”,

or “maybe”. However, in doing so, it is desirable — especially in
medical problems — to indicate the propensity or probability of
a classification to belong to a single yes or no category. Neural
networks have proven their high performance in crisp classifica-
tion, however, as we know, the solution is not comprehensible
and therefore difficult or impossible for a human expert (e.g. a
physician) to interpret and understand. Rule-based systems are in
principle explainable, however they are based on formal inference
structures and also have problems with interpretability due to
their high complexity. We must emphasize that even human
experts sometimes cannot explain, but construct mental models
of the problem and use these models to select the best possible
solution. Hudec et al. propose a classification by aggregation
functions of mixed behavior through the variability of ordinal
sums of conjunctive and disjunctive functions [25]. In this way,
domain experts should assign only the most relevant observations
regarding the considered attributes. Consequently, the variability
of the functions provides room for ML to learn the best possible
option from the data. Such a solution is tractable, reproducible
and explainable to domain experts.

Evaluating explainability. Ultimately, explainability is in the eye

of the beholder, i.e., the user. As such it is incumbent on those

who aim to develop XAI systems to account for their usability,
but also their usefulness. Usability can be measured using modifi-

cations of such instruments as the System Usability Scale (SUS). A

detailed retrospective examination of the SUS is provided in [26].

Modification to this scale would need to account for the in-

terpretability of the system, including both inputs and outputs.

Another approach to usability assessment is one that focuses on

causality [2,27]. This approach allows users and developers to

trace inferential pathways and evaluate them for plausibility. As
such, not only can inferential errors be identified rapidly, the
reasoning behind them can, as well. Using this scale, a deep
assessment of usability can be obtained throughout the system
development life cycle. Yet another approach to assessing usabil-
ity focuses on user-centered reporting of results, such that users
provide important input on and influence over what is reported
by the system. This was shown to be an effective way to ensure
that random forest results were reported in a way that users found
them to be interpretable [28]. However, none of these approaches
to evaluating explainability address the issue of usefulness. While

Artificial Intelligence In Medicine 133 (2022) 102423

a system may be usable, it is not necessarily useful, meaning
that the system addresses some important task, telling a user
something they did not already know or infer from available facts
or knowledge. To assess usefulness, one needs to turn to long-
term, post-hoc qualitative and quantitative evaluation of how,
when, and why the system is being used and in what contexts
does it fit (or fail to fit) workflows. Another consideration for
usefulness is whether or not a system is used in practice to replace
another. This is especially important in busy clinical settings,
where AI systems might be used to augment medical decision
making. However, if a system is not useful, practitioners will
not use it, even though it might be very usable, or they will
use the system but develop workarounds to make it more useful,
sometimes with consequences that are potentially catastrophic to
patients. For this type of evaluation, the frameworks mentioned
above can inform the development of strategies and methods for
observing the use of AI systems in these contexts in real-time,
and the framework-driven analysis of data obtained during this
endeavor.

4.2. If an AI system’s output is understandable, is it automatically explain-
able?

Proposition: Understanding the output from an AI system is
foundational to explainability, but it is only one requirement that
has to be merged with usability, usefulness, and interpretability
to compose explainability. A central goal of ML is to build a model
which summarizes linear and/or nonlinear patterns in a dataset. Good
models are useful for making predictions in new data and thus have
the quality of generalizability, which in turn makes them useful. Most
ML models, such as those derived from neural networks or gradient
boosting, have an underlying mathematical foundation. For example,
a neural network model can be written as a summation of products
of weights and inputs from data and hidden layer nodes. Thus, our
knowledge of the mathematical foundation of a model makes it in-
herently understandable in that we know the function that relates
the data inputs to the outcome being predicted. Our understanding
can be improved by conducting experiments on the model by, for
example, perturbing inputs and/or model components to observe their
effects on model quality metrics. We can even decompose the model
into linear and nonlinear components using these kinds of perturba-
tion experiments when combined with entropy-based measures from
information theory, for example. In this way, it is possible to gain a
good understanding of a model. But does understanding translate to
explainability?

As previously described, characteristics of XAI include usability,
usefulness, interpretability, and understandability. Knowing the math-
ematical basis of a model does not necessarily make it useful. For
example, a neural network model might do a good job of predicting
30-day hospital readmissions following surgery. Further, the model
might generalize well to clinical data from other hospitals. The model
is understandable because the mathematical basis is known and can
be described. Although the model is predictive and understandable, it
might not be useful for reducing readmissions if the features include
patient demographics such as gender and zip code which cannot be
changed to improve the outcome. As another example, consider a neu-
ral network model relating gene expression features to risk of disease,
where the predictive features include a number of housekeeping genes
required for the maintenance and function of all cells. The model might
be understandable and useful, but it might not be interpretable. In other
words, it may be difficult for the domain expert to come up with an
explanation for why this set of genes contributes to disease risk when
they impact every cell in the body. This in turn would limit the ability
of a pharmacologist to develop a therapeutic intervention.

Understanding an ML model is thus a first step towards XAI. While
complementary, usability, usefulness, interpretability, and understand-
ability can be synergistic. For example, a domain-specific knowledge
C. Combi et al.

graph can make a model more understandable and more interpretable
by informing the user of biological relationships among the features
[29]. Further, biomedical ontologies can facilitate both understanding
and interpretation because the feature relationships have been de-
scribed through a synthesis of multiple knowledge sources that capture
their semantic meaning [30].

4.3. What is the role of domain understanding in achieving XAI in medical
applications?

Proposition: XAlI-based systems need to start from modeling
the biomedical and clinical domain in order to obtain a true
understanding of the context in which these systems will be used.
As stated by several authors, a key aspect of building biomedical (and
in particular clinical) Al-based systems is to understand the context.
For example, understanding the context of clinical decisions means to
model the patients’ careflow: identify the key actors of care and the
decision-makers, explicitly define the timing of decisions, and clarify
the data collection phases and their critical elements, including the
potential sources of missing data. Only by deeply analyzing all these
aspects it will be possible to design a successful Al-based system and to
properly identify the explainability components. The real importance
of an AI system in medicine is to support the planning and delivery
of medical treatment more than just perform diagnostic labeling [31].
XAI is essential to achieving this goal, in addition to the strategies to
induce trust in Al-supported decisions.

To this end, there is the need for integrating stakeholders and users
into entire AI development life-cycle. Following the approach proposed
by Bellazzi and Zupan in [32], a potential strategy is to apply in the
design of Al-based systems the same conceptual model proposed for
data mining models by the Cross Industry Standard Process for Data
Mining (CRISP-DM) process model. CRISP-DM has six phases that are
helpful to obtain explainable systems “by design”:

. Business understanding;
. Data understanding;

. Data preparation;

. Modeling;

. Evaluation;

6. Deployment.

ok WN

While data preparation, modeling, and evaluation are now reported in
all ML textbooks, very often little attention is given to business under-
standing, data understanding, and finally deployment. All of those are
related to understanding the biomedical context, modeling the process
and clearly expressing the goals. Data needs to be modeled; as well, it
should be understood who and when data are collected, which is often
related to the nature of missing data. Finally, having clearly in mind
the deployment scenario is a key driver for designing XAI approaches.
In this phase all actors involved in decision making should be involved,
resorting to different instruments, from formal questionnaires to qual-
itative interviews. Several other development methodologies could be
suitably adopted/extended/adapted when designing and implementing
XAI systems in medicine, where the different stakeholders and the
application domain are explicitly dealt with. As an example, well
established methodologies as CommonKADS, supporting the design
of knowledge-intensive systems coupled with UML notations, as well
as methodologies dealing with the design of ontology- and/or data-
based reasoning/analytics systems could provide suitable techniques
for domain understanding and modeling [33-36].

As also reported by the EU white paper [37], AI systems and their
decisions should be explained in a manner that is adapted to the
appropriate stakeholder.

Among the specific features of medicine and health, we have to
consider when designing XAI medical applications, we distinguish here:

Artificial Intelligence In Medicine 133 (2022) 102423

+ The heterogeneous nature of medical data. Medical data consists
of images, movies, biosignals, and structured and unstructured
alphanumeric data from electronic medical records. All of this
data needs to be suitably integrated into and consistently and
appropriately managed by XAI medical applications. Even though
explainability has been considered for these different kinds of
information systems (see, for example [38,39]), further research
efforts will have to deal with the elicitation of both visual and
textual knowledge from such kinds of data, often left partially im-
plicit by skilled physicians [40]. As an example, while radiology
is mainly based on images, which are visually analyzed by radi-
ologists even by the support of computerized devices, and related
natural language reports, oncology deals mainly with knowledge
represented in a textual way, often highly structured (as in the
case of chemotherapy guidelines), while cardiology has a lot of
information and related knowledge expressed through biosignals
(e.g., the electrocardiogram) and movies (e.g., echocardiograms).
The presence of highly specialized knowledge in different clinical
and healthcare domains. Specific domains as cardiology, oncology,
neurology, healthcare policy, and so on, have their own vocabu-
lary, specific shared knowledge about diagnosis, treatments, and
so on [41,42]. XAI systems have, thus, to deal with jargon, ab-
breviations and terminological heterogeneity, idiosyncratic usage
habits, and different kinds of knowledge, as previously stressed,
especially when they have to support the exchange of shared
information [43].

The presence of many different specialized processes, requiring the
coordination of different stakeholders. Explainability in medicine
and health is often related to the results of prediction and/or
classification tasks towards diagnosis and/or therapy effects and
so on. Besides this “static” part, clinical tasks as monitoring,
diagnosis, therapy, and prognosis are merged in a “dynamic”
context, composed of complex medical or healthcare processes
and pathways. In such processes, different healthcare actors, as
clinicians, epidemiologists, nurses, and technicians, are involved
with different roles. XAI systems cannot avoid facing these inter-
twined aspects, related to knowledge, information, processes, and
actors, to suitably support specific clinical activities [44].

4.4. Can explainability draw us closer to wisdom?

Proposition: Explainability is a requirement to completing the
data-information-knowledge-wisdom spectrum. Understandability
is an essential prerequisite for the transition from information to knowl-
edge and provides a path to the realization of knowledge as wisdom.
Explainability can, on the one hand, promote trust on the part of end
users (compare with the previous section), and, on the other hand,
promote understanding and, in turn, trust on the part of developers of
algorithms, and finally also provide new insights. Trust is of eminent
importance and is often underestimated and in order to bring AI into
the real world, it must be trustworthy [45]. To be trustworthy, any AI
must comply with applicable rules and regulations, adhere to ethical
principles [46], follow legal issues [47] and be implemented in a secure
and robust manner. This is particularly required by the EU High-Level
Expert Group on AI.*

In classical philosophy since ancient Greece, explanations have
always been central, as the word philosophy itself means “love of wis-
dom”. A good example is the deductive-nomological model of Hempel
and Oppenheim (1948) [48] which is based on a formal structure of
scientific explanation of a causal relationship using natural language.
The model consists of two parts, the proposition to be explained (ex-
planandum) and the explanation itself (explanans), which is composed

1 https://digital-strategy.ec.europa.eu/en/policies/expert-group-ai (access:
February, 09, 2022).
C. Combi et al.

of general law statements and (empirical) boundary conditions (an-
tecedent statements) as premises. The preliminary work on this was
already served by Karl Popper in his work “Logic of Research” [49].

Colloquially, explanations differ in their completeness or degree
of causality [50]. In his work, Tim Miller (2019) [51] combined in-
sights from the social sciences with explanations in AI and divided
explanatory questions into three classes: (1) what-questions, such as
“What event happened?”; (2) how-questions, such as “How did this
event happen?”; and (3) why-questions, such as “Why did this event
happen?”.

“Moving closer to wisdom” implies also that physicians and other
clinical stakeholders receive some feedback on their own capabilities
and attitudes towards explainability: do we need that AI systems have
sophisticated explainability capabilities when it happens that physi-
cians do not spend any effort to explain their choices? From one point
of view, we could say that requirements about explainability have
to be more strict for AI systems. Indeed, “we may hold physicians
responsible for their lack of explainability and potential mistakes,but
we cannot do the same with AI” (from [52]). On the other side,
XAI systems can support clinicians in providing even more sound and
founded decisions. In this direction, AI systems have to be considered
as tools that require a specific and sound certification process also
with regards to explainability. Similarly to what happens to marketed
drugs, which need to follow strict certification processes before being
approved, also XAI systems should be formally approved before used
in real world clinical and healthcare contexts. Such kind of approach,
followed by a continuous monitoring after the introduction of such
tools in real clinical contexts, would help to clarify responsibilities both
for physicians and for the producers of XAI systems.

4.5. Can an AI system that is not explainable be trustworthy?

Proposition: XAI is an integral component of trustworthy AI
systems. In 2019 the EU has published the Ethics Guidelines for trust-
worthy AI, which contains a general framework where explainability
represents an important component.’ These guidelines have been used
as a basis for some of the sections of the proposal of the Artificial Intel-
ligence Act released by the European Commission in April 2021. The
guidelines correctly states that “Trust in the development, deployment
and use of AI systems concerns not only the technology’s inherent prop-
erties, but also the qualities of the socio-technical systems involving AI
applications ... it is not simply components of the AI system but the
system in its overall context that may or may not engender trust”. To
this end, AI systems should be lawful, i-e., complying with laws and
regulations, ethical, i.e., being to ethical principles and robust, both
from a technical and social perspective. The guidelines also provides
seven requirements for implementation of AI trustworthy solutions,
including:

+ human agency and oversight

technical robustness and safety

privacy and data governance
transparency

diversity non-discrimination and fairness
societal and environmental well-being
accountability.

Explainability is considered as a component of transparency, together
with traceability and communication. In our view explainability has an
horizontal impact which is wider than what is stated in the guidelines.
First of all, within transparency, it has a strong overlap with commu-
nication, which is related to understandability. Second, explainability
is a key component of accountability, since it provides instrument to
keep track of the decisions, going back to the “reasons-why” an AI

2 https://www.aepd.es/sites/default/files/2019- 12/ai-ethics- guidelines. pdf

Artificial Intelligence In Medicine 133 (2022) 102423

tool, or a decision-maker empowered by AI solutions, has suggested
the decision. Finally, it can be considered as a way to ensure technical
robustness, providing explanations about change in decisions related
to changes in the attribute values; this provides ways to control the
performance of the algorithms and identify aberrant situations. Rather
interestingly, trustworthiness allows to jointly consider two related
concepts: explainability and reliability. “Reliability” is a component
of robustness that indicates the degree of trust that we have on the
prediction made by an ML model on a single example [53]. Coupled
with local explainability ensures that local predictions can be used in
a safety critical context as medicine is.

4.6. Is XAI in medicine always required?

Proposition: Explanations are not always required in order for
an AI model to be useful. Functional specifications obtained from
deep analysis of the problem domain and users should determine
when explainability and interpretability are required. While many
recognize the necessity to incorporate explainability features in AI
models, addressing user needs for understanding AI remains an open
question. As the type of interpretability needed varies depending on
the context, it is clear that XAI must take a human-centered approach.
The same explanation may be more or less comprehensible to different
users or even to the same user engaged in different roles and we
should not confuse the different notions of interpretability because each
kind serves a different purpose [54]. For instance, we cannot provide
algorithm designers and end users with the same explanations. An ML
expert might prefer an explanation that helps them debug the model
and understand its inner-working [55]. In contrast, an end user might
require a causal explanation of predictions to ensure that decisions
informed by those predictions are fair [56].

The use of techniques to explain AI models has become central in
human-centered systems. For example, visual analytics systems help
users understand and interact with AI models by providing them with
visualizations and tools that facilitate the exploration, analysis, inter-
action with AI models. To close the gap between XAI methods and user
needs for transparency, the human-computer interaction community
has called for interdisciplinary collaboration [57] and user-centered
approaches to explainability [58]. The need to create effective explain-
ability features in diverse medical applications led to novel ways to
probe user needs. As an explanation can be seen as an answer to a
question, Liao et al. represented user needs for explainability in terms
of questions a user might ask about the AI model, thus creating a
question bank, a list of prototypical user questions that XAI methods
can address [59]. It is essential that model developers understand why
an explanation is needed and what type of explanation is helpful for a
given situation.

AI models do not need to be interpretable to be useful [60]. In
this context, a blanket rejection of black-box methods in decision
support systems may be hasty. For example, suppose an AI model yields
accurate predictions that help clinicians better treat their patients. In
that case, it may be useful even without a detailed explanation of
how or why it works. Therefore, it is essential to identify biomedical
applications in which black-box answers generated by AI models can
have a useful role in decision support systems and thus can be safely
used.

When an AI model produces the best results or yields accurate pre-
dictions that help clinicians better treat patients, it may be useful even
without detailed explanations. For example, in reading medical images,
trained AI systems enhance the performance of human radiologists in
detecting cancers [61,62]. That is not to say that AI interpretability is
not valuable. In particular, when AI models are used in an automated
fashion, laws and regulations should require a causal explanation of AI
decisions to ensure that they are fair [63]. However, in situations when
AI models do not lead to automated decision making, an explanation
C. Combi et al.

may not be needed and auditing [64] together with judicious testing of
AI models via randomized control trials [65] might be sufficient.

Although the process used by AI models to generate predictions can
be limited and biased, it is also different from human thought processes
in ways that can reveal new connections. This creates a case for using
black-box AI models as tools to guide human inquiry [66,67]. For
example, in a groundbreaking medical imaging study, a deep learning
model was trained to diagnose diabetic retinopathy from retinal im-
ages [68]. The model achieved performance comparable to a committee
of ophthalmologists. Further, the model accurately identified several
characteristics that are not generally assessed with retinal images,
including cardiological risk factors, age, and gender [69]. No one had
previously noticed gender-based differences in human retinas, so the
black-box observation inspired researchers to investigate how and why
male and female retinas differ.

Moving to a final example, XAI needs to be declined in differ-
ent ways in different contexts. Indeed, the explanation requirements
regarding clinical medicine, for example, may have to deal with spe-
cialized physicians, who could have a knowledge in the specific domain
that not requires XAI (but an AI system with certified good perfor-
mances), while, considering, for example, the issue of pandemic man-
agement, requirements from epidemiology or national health policies
and management could be extremely demanding, as possible relevant
public decisions have to be suitably justified [70].

5. Conclusions and research directions

The issue of explainability in AI is evolving at a rapid pace. As we
have seen in this paper, there has been considerable research into XAI,
but there is still much to be done. We note here five broad areas where
more research is needed.

- Bridging the gap between symbolic (ante hoc) and sub-
symbolic (black-box) approaches. Sub-symbolic ML approaches
and symbolic ones are currently considered by two research
communities, having often completely different perspectives and
background. XAI requires that such dichotomy has to be over-
came. Indeed, symbolic approaches, as the ones related to logics-
based proposals, ontologies, query systems, Bayesian networks,
and so on, would be grounded in order to use them in estab-
lishing explainability [71]. Research on the seamless proposal
of “hybrid” systems, merging both sub-symbolic and symbolic
approaches still requires a lot of joint efforts.

Engineering explainability into intelligent systems. An impor-
tant, even fundamental question is whether and how explainabil-
ity can actually be engineered into AI. Even given our conceptual
framework for thinking about XAI (Section 2), we still need to
address the idiosyncrasies of individual intelligent systems as well
as those of their users. We contend that more specialized research
into the structural, functional, and behavioral characteristics of
these systems and the environments in which they are situated
should be the targets of rigorous mixed-methods research that
encompasses the entire system ecology, from in silico to in vivo
contexts.

Evaluating and improving the effects of explainable compo-
hents and approaches. The evaluation of intelligent systems, as
a scientific and methodological discipline, is changing, yet there
needs to be more systematic investigation and implementation of
these methods. Too often, there is emphasis on the accuracy of a
decision made by such systems, typical as a proportion — whether
in terms of overall accuracy (percent “correct’’), or more nuanced
indicators such as sensitivity, specificity, predictive values, or
their derivatives such as the F-score or areas under the receiver
operating characteristic curve or under the precision-recall curve.
None of these well-used metrics indicate anything about the
effects of the system on user beliefs, attitudes, or behavior. These

Artificial Intelligence In Medicine 133 (2022) 102423

are effects that require, again, deep mixed-methods research, this
time applied to evaluating the effects of XAI (or its absence) on
such issues as user acceptability, actions taken (or not) based on
the results offered by the system, and overall impact on clinical
or other workflows.

Determining when explainability is needed. Is explainabil-
ity always needed? This is a fair question, indeed. AI systems
(or “subsystems” that work in the background to provide some
inference to assist another systems might not require real-time ex-
plainability. A feature selection algorithm as part of a knowledge
discovery or decision making workflow is one example if such an
AI. However, we would argue that in order for software develop-
ers who need to use such subsystems in their work, explainability
to them is critically important. They have to know how that
subsystem works and why. But to the end-user of that workflow,
it might not be so important in real-time. Rather, an in-depth
description of the entire workflow and its components should
be provided so the end-user understands how the overall system
works. This situation begs the question again: “Is explainability
always needed?”. The answer, we propose here, is yes, but titrated
to the needs of the user at particular times or in response to
specific events.

Investigating the design of user-centered and user-tailored
explainability artifacts. If there were ever a more urgent need
for rigorous research into user-centered design, it is hard to see
one that surpasses the field of XAI. Such design, as noted, must
be sensitive to workflow contexts, certainly, but there are other
equally important considerations. One of the most important of
these is the involvement of users into the design process. Rapid
prototype design paradigms should be used in order to keep users
involved during all phases of the development and implementa-
tion of AI systems. We already do this to some extent in the field
of knowledge engineering, although history is full of examples
where gaps in knowledge acquisition and representation have led
to system failures, some with catastrophic results

We hope that our examination of the issues involved in developing
XAI systems — our manifesto, if you will — will not be construed as the
definitive work in this area. Rather, hope that the issues we considered
here will stimulate further thought and hopefully fruitful research
and development of XAI systems, particularly in medical contexts, but
extending beyond to other contexts as well.

Declaration of competing interest

The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.

Acknowledgments

To avoid any bias in the review process, this paper has been
managed by the guest editors of the AIIM special issue “Explainable
artificial intelligence and real-world applications in healthcare”. At the
end of the review process, we would like to thank Gregor Stiglic, Jose
Juarez, and Peter Haddawy, who provided important comments and
suggestions, allowing us to greatly improve the manuscript.

A.H. is partially funded by the Austrian Science Fund (FWE), Project:
P-32554 “A reference model of explainable Artificial Intelligence for
the Medical Domain”. M.Z. is supported, in part, by National Science
Foundation (NSF), USA under nos. IIS-2030459 and IIS-2033384, US
Air Force Contract No. FA8702-15-D-0001, Harvard Data Science Ini-
tiative, USA, Amazon Research Award, USA, Bayer Early Excellence
in Science Award, USA, AstraZeneca Research, United Kingdom, and
Roche Alliance with Distinguished Scientists Award, USA. R.B. is par-
tially funded by the project Periscope, USA (Pan-European Response
C. Combi et al.

to the ImpactS of COVID-19 and future Pandemics and Epidemics),
ID:101016233, funded by the European Union. J.H. is partially funded
by National Institutes of Health (USA) Clinical Translational Science
Award, UL1-TROO1878. J.M. is partially funded by National Institutes
of Health (USA) grant AG066833. B.A. and C.C. are partially funded
by the Ministry of University and Research, MIUR, Project Italian Out-
standing Departments, 2018-2022. Any opinions, findings, conclusions
or recommendations expressed in this material are those of the authors
and do not necessarily reflect the views of the funders.

References

[1

[2

[3.

[4

[5.

[6

[7

[8

[9

[10]

(11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

Langer M, Oster D, Speith T, Hermanns H, Kastner L, Schmidt E, et al. What
do we want from explainable artificial intelligence (XAI)? - A stakeholder per-
spective on XAI and a conceptual model guiding interdisciplinary XAI research.
Artificial Intelligence 2021;296:103473. http://dx.doi.org/10.1016/j.artint.2021.
103473.

Holzinger A, Langs G, Denk H, Zatloukal K, Miiller H. Causability and ex-
plainability of artificial intelligence in medicine. WIREs Data Min Knowl Discov
2019;9(4). http://dx.doi.org/10.1002/widm.1312.

Tjoa E, Guan C. A survey on explainable artificial intelligence (XAD: toward
medical XAI. IEEE Trans Neural Netw Learn Syst 2021;32(11):4793-813. http:
//dx.doi.org/10.1109/TNNLS.2020.3027314.

Bozzola P, Bortolan G, Combi C, Pinciroli F, Brohet
neuro-fuzzy system for ECG classification of myocardial infarction. In:
IEEE conference on computers in cardiology. 1996, p. 241-4, cited By
23. URL https://www.scopus.com/inward/record. uri?eid=2-s2.0-0030420077&
partmerlD=40&md5=1 e58bbe69e4c64el 29 1e4beeal04ba58.

Adhikari A, Tax DM, Satta R, Faeth M. LEAFAGE: Example-based and fea-
ture importance-based explanations for black-box ML models. In: 2019 IEEE
international conference on fuzzy systems. IEEE; 2019, p. 1-7.

Ahn S, Kim J, Park SY, Cho S. Explaining deep learning-based traffic classification
using a genetic algorithm. IEEE Access 2020;9:4738-51.

Holzinger A, Mueller H. Toward human-Al interfaces to support explainability
and causability in medical Al. IEEE Comput 2021;54(10):78-86. http://dx.doi.
org/10.1109/MC.2021.3092610.

Maweu BM, Dakshit S$, Shamsuddin R, Prabhakaran B. CEFEs: A CNN explainable
framework for ECG signals. Artif Intell Med 2021;115:102059.

Pennisi M, Kavasidis I, Spampinato C, Schinina V, Palazzo S, Salanitri FP, et
al. An explainable AI system for automated COVID-19 assessment and lesion
categorization from CT-scans. Artif Intell Med 2021;118:102114.

Yeboah D, Steinmeister L, Hier DB, Hadi B, Wunsch DC, Olbricht GR, et
al. An explainable and statistically validated ensemble clustering model ap-
plied to the identification of traumatic brain injury subgroups. IEEE Access
2020;8:180690-705.

Gu D, Su K, Zhao H. A case-based ensemble learning system for explainable
breast cancer recurrence prediction. Artif Intell Med 2020;107:101858.
El-Sappagh S, Alonso JM, Ali F, Ali A, Jang J-H, Kwak K-S. An ontology-based
interpretable fuzzy decision support system for diabetes diagnosis. IEEE Access
2018;6:37371-94,

Kavya R, Christopher J, Panda S$, Lazarus YB. Machine learning and XAI
approaches for allergy diagnosis. Biomed Signal Process Control 2021;69:102681.
Schoonderwoerd TA, Jorritsma W, Neerinex MA, Van Den Bosch K. Human-
centered XAI: Developing design patterns for explanations of clinical decision
support systems. Int J Hum Comput Stud 2021;154:102684.

Dragoni M, Donadello I, Eccher C. Explainable AI meets persuasiveness:
Translating reasoning results into behavioral change advice. Artif Intell Med
2020;105:101840.

Reyes M, Meier R, Pereira S, Silva CA, Dahlweid F-M, Tengg-Kobligk Hv, et
al. On the interpretability of artificial intelligence in radiology: Challenges and
opportunities. Radiol Artif Intell 2020;2(3):e190043. http://dx.doi.org/10.1148/
ryai.2020190043, PMID: 32510054.

Landauer TK. The trouble with computers: usefulness, usability, and productivity.
MIT Press; 1995.

Guidotti R, Monreale A, Ruggieri S, Turini F, Giannotti F, Pedreschi D.
A survey of methods for explaining black box models. ACM Comput Surv
2019;51(5):93:1-42. http://dx.doi.org/10.1145/3236009.

Markus AF, Kors JA, Rijnbeek PR. The role of explainability in creating
trustworthy artificial intelligence for health care: a comprehensive survey of
the terminology, design choices, and evaluation strategies. J Biomed Inform
2020;103655.

Barda AJ, Horvat CM, Hochheiser H. A qualitative research framework for the
design of user-centered displays of explanations for machine learning model
predictions in healthcare. BMC Med Inform Decis Mak 2020;20(1):1-16.
Mencar C, Alonso JM. Paving the way to explainable artificial intelligence with
fuzzy modeling. In: International workshop on fuzzy logic and applications.
Springer; 2018, p. 215-27.

C. A hybrid

10

[22]

[23]

[24]

[25]

[26]
[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

Artificial Intelligence In Medicine 133 (2022) 102423

Zhou J, Gandomi AH, Chen F, Holzinger A. Evaluating the quality of ma-
chine learning explanations: A survey on methods and metrics. Electronics
2021;10(5):593. http://dx.doi.org/10.3390/electronics10050593.

Montavon G, Samek W, Miiller K-R. Methods for interpreting and understanding
deep neural networks. Digit Signal Process 2018;73(2):1-15. http://dx.doi.org/
10.1016/j.dsp.2017.10.011.

Holzinger A, Malle B, Saranti A, Pfeifer B. Towards multi-modal causability with
graph neural networks enabling information fusion for explainable AI. Inf Fusion
2021;71(7):28-37. http://dx.doi.org/10.1016/j.inffus.2021.01.008.

Hudec M, Minarikova E, Mesiar R, Saranti A, Holzinger A. Classification by
ordinal sums of conjunctive and disjunctive functions for explainable AI and
interpretable machine learning solutions. Knowl Based Syst 2021;220:106916.
http://dx.doi.org/10.1016/j.knosys.2021.106916.

Brooke J. SUS: A retrospective. J Usability Stud 2003;8(2):29-40.

Holzinger A, Carrington A, Miiller H. Measuring the quality of explanations: the
system causability scale (SCS). In: KI-kiinstliche intelligenz. Springer; 2020, p.
1-6.

Petkovic D, Altman R, Wong M, Vigil A. Improving the explainability of random
forest classifier-user centered approach. In: Pacific symposium on biocomputing
2018: proceedings of the pacific symposium. World Scientific; 2018, p. 204-15.
Mensio M, Bastianelli E, Tiddi I, Rizzo G. Mitigating bias in deep nets with
knowledge bases: The case of natural language understanding for robots. In: AAAI
spring symposium: combining machine learning with knowledge engineering (1).
2020, p. 1-9.

Confalonieri R, Weyde T, Besold TR, Martin FMdP. Trepan reloaded: A
knowledge-driven approach to explaining artificial neural networks. 2019, arXiv:
1906.08362.

Adler-Milstein J, Chen J, Dhaliwal G. Next-generation artificial intelligence
for diagnosis: From predicting diagnostic labels to "wayfinding". JAMA 2021.
http://dx.doi.org/10.1001 /jama.2021.22396.

Bellazzi R, Zupan B. Predictive data mining in clinical medicine: current issues
and guidelines. Int J Med Inform 2008;77(2):81-97. http://dx.doi.org/10.1016/
j.ijmedinf.2006.11.006.

Brachman RJ, Levesque HJ. Knowledge representation and reasoning. Else-
vier; 2004, URL http://www.elsevier.com/wps/find/bookdescription.cws home/
702602/description.

Nemati HR, Steiger DM, Iyer LS, Herschel RT. Knowledge warehouse: an
architectural integration of knowledge management, decision support, artificial
intelligence and data warehousing. Decis Support Syst 2002;33(2):143-61. http:
//dx.doi.org/10.1016/S0167-9236(01)00141-5.

Schreiber G, Akkermans H, Anjewierden A, de Hoog R, Shadbolt N, Van de
Velde W, et al. Knowledge engineering and management: the commonKADS
methodology. Cambridge, MA: MIT Press; 2000.

Vaisman AA, Zimanyi E. Data warehouse systems - design and implementation.
Data-centric systems and applications, 2nd ed.. Springer; 2022, http://dx.doi.
org/10.1007/978-3-662-65167-4.

European Commission. White paper on artificial intelligence: a European
approach to excellence and trust. COM(2020) 65 final, Brussels: Euro-
pean Commission; 2020, https://ec.europa.eu/info/files/white-paper- artificial /-
intelligence- european-approach-excellence-and-trust_en.

Jin W, Li X, Hamameh G. Evaluating explainable AI on a multi-modal med-
ical imaging task: Can existing algorithms fulfill clinical requirements? In:
Thirty-sixth AAAI conference on artificial intelligence, AAAI 2022, thirty-fourth
conference on innovative applications of artificial intelligence, IAAI 2022, the
twelveth symposium on educational advances in artificial intelligence, EAAI 2022
virtual event, February 22 - March 1, 2022. AAAI Press; 2022, p. 11945-53, URL
https: //ojs.aaai.org/index.php/AAAI/article/view/21452.

Payrovnaziri SN, Chen Z, Rengifo-Moreno P, Miller T, Bian J, Chen JH, et al. Ex-
plainable artificial intelligence models using real-world electronic health record
data: a systematic scoping review. J Am Med Inform Assoc 2020;27(7):1173-85.
http://dx.doi.org/10.1093/jamia/ocaa053.

Holzinger A. Explainable AI and multi-modal causability in medicine. I-Com
2021;19(3):171-9. http://dx.doi.org/10.1515/icom- 2020-0024.

Powsner SM, Costa J, Homer RJ. Clinicians are from mars and pathol-
ogists are from venus: Clinician interpretation of pathology reports. Arch
Pathol Lab Med 2000;124(7):1040-6. http://dx.doi.org/10.5858/2000-124-
1040-CAFMAP, arXiv:https://meridian.allenpress.com/aplm/article-pdf/124/7/
1040/2724930/0003-9985(2000)124 1040_cafmap_2_0_co_2.pdf.

Chen J, Druhl E, Polepalli Ramesh B, Houston TK, Brandt CA, Zulman DM, et
al. A natural language processing system that links medical terms in electronic
health record notes to lay definitions: System development using physician
reviews. J Med Internet Res 2018;20(1):e26. http://dx.doi.org/10.2196/jmir.
8669,

Rau NM, Basir MA, Flynn KE. Parental understanding of crucial medical
jargon used in prenatal prematurity counseling. BMC Med Inform Decis Mak
2020;20(1):169. http://dx.doi.org/10.1186/s12911-020-01188-w.

Combi C, Oliboni B, Zardini A, Zerbato F. A methodological framework for
the integrated design of decision-intensive care pathways - an application to
the management of COPD patients. J Heal Inform Res 2017;1(2):157-217. http:
//dx.doi.org/10.1007 /s41666-017-0007-4.
C. Combi et al.

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

Holzinger A, Dehmer M, Emmert-Streib F, Cucchiara R, Augenstein I, Del Ser J,
et al. Information fusion as an integrative cross-cutting enabler to achieve
robust, explainable, and trustworthy medical artificial intelligence. Inf Fusion
2021;79(3):263-78. http://dx.doi.org/10.1016/j.inffus.2021.10.007.

Mueller H, Mayrhofer MT, Veen E-BV, Holzinger A. The ten commandments of
ethical medical Al. IEEE Comput 2021;54(7):119-23. http://dx.doi.org/10.1109/
MC.2021.3074263.

Stoeger K, Schneeberger D, Holzinger A. Medical artificial intelligence: The
European legal perspective. Commun ACM 2021;64(11):34-6. http://dx.doi.org/
10.1145/3458652.

Hempel CG, Oppenheim P. Studies in the logic of explanation. Philos Sci
1948;15(2):135-75.

Popper K. Die logik der forschung. Zur erkenntnistheorie der modernen
naturwissenschaft. Wien: Springer-Verlag; 1935.

Pearl J. The seven tools of causal inference, with reflections on machine learning.
Commun ACM 2019;62(3):54-60.

Miller T. Explanation in artificial intelligence: Insights from the social sciences.
Artificial Intelligence 2019;267:1-38. http://dx.doi.org/10.1016/j.artint.2018.07.
007.

Kempt H, Heilinger J, Nagel SK. Relative explainability and double standards in
medical decision-making. Ethics Inf Technol 2022;24(2):20. http://dx.doi.org/
10.1007/s10676-022-09646-x.

Nicora G, Rios M, Abu-Hanna A, Bellazzi R. Evaluating pointwise reliability of
machine learning prediction. J Biomed Inform 2022. http://dx.doi.org/10.1016/
jjbi.2022.103996.

Weller A. Transparency: Motivations and challenges. In: Explainable Al:
interpreting, explaining and visualizing deep learning. Springer; 2019, p. 23-40.
Ying R, Bourgeois D, You J, Zitnik M, Leskovec J. GNNexplainer: Generating
explanations for graph neural networks. In: Advances in neural information
processing systems. Vol. 32. 2019, p. 9240.

Agarwal C, Lakkaraju H, Zitnik M. Towards a unified framework for fair
and stable graph representation learning. In: Proceedings of conference on
uncertainty in artificial intelligence. 2021.

Abdul A, Vermeulen J, Wang D, Lim BY, Kankanhalli M. Trends and trajectories
for explainable, accountable and intelligible systems: An HCI research agenda.
In: Proceedings of the international conference on human computer interaction.
2018, p. 1-18.

Wang D, Yang Q, Abdul A, Lim BY. Designing theory-driven user-centric ex-
plainable AI. In: Proceedings of the international conference on human computer
interaction. 2019, p. 1-15.

11

[59]

[60]
[61]

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

[71]

Artificial Intelligence In Medicine 133 (2022) 102423

Liao QV, Gruen D, Miller S. Questioning the Al: informing design practices for
explainable AI user experiences. In: Proceedings of the international conference
on human computer interaction. 2020, p. 1-15.

Holm BA. In defense of the black box. Science 2019;364(6435):26-7.

Ardila D, Kiraly AP, Bharadwaj S, Choi B, Reicher JJ, Peng L, et al. End-to-end
lung cancer screening with three-dimensional deep learning on low-dose chest
computed tomography. Nat Med 2019;25(6):954-61.

Kleppe A, Skrede O-J, De Raedt S, Liestol K, Kerr DJ, Danielsen HE. Designing
deep learning studies in cancer diagnostics. Nat Rev Cancer 2021;21(3):199-211.
Babic B, Gerke S, Evgeniou T, Cohen IG. Beware explanations from AI in health
care. Science 2021;373(6552):284-6.

Raji ID, Smart A, White RN, Mitchell M, Gebru T, Hutchinson B, et al.
Closing the AI accountability gap: Defining an end-to-end framework for internal
algorithmic auditing. In: Proceedings of the international conference on fairness,
accountability, and transparency. 2020, p. 33-44.

Rivera SC, Liu X, Chan A-W, Denniston AK, Calvert MJ. Guidelines for clinical
trial protocols for interventions involving artificial intelligence: the SPIRIT-AI
extension. BMJ 2020;370.

Gysi DM, Do Valle i, Zitnik M, Ameli A, Gan X, Varol O, et al. Network medicine
framework for identifying drug-repurposing opportunities for COVID-19. Proc
Natl Acad Sci 2021;118(19).

Zimik M, Feldman MW, Leskovec J, et al. Evolution of resilience in protein
interactomes across the tree of life. Proc Natl Acad Sci 2019;116(10):4426-33.
Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, et al.
Development and validation of a deep learning algorithm for detection of diabetic
retinopathy in retinal fundus photographs. JAMA 2016;316(22):2402-10.
Poplin R, Varadarajan AV, Blumer K, Liu Y, McConnell MV, Corrado GS, et
al. Prediction of cardiovascular risk factors from retinal fundus photographs via
deep learning. Nat Biomed Eng 2018;2(3):158-64.

Cao L. AI in combating the COVID-19 pandemic. IEEE
2022;37(2):3-13. http://dx.doi.org/10.1109/MIS.2022.3164313.
Rudie JD, Rauschecker AM, Xie L, Wang J, Duong MT, Botzolakis EJ, et al.
Subspecialty-level deep gray matter differential diagnoses with deep learning
and Bayesian networks on clinical brain MRI: A pilot study. Radiol Artif
Intell 2020;2(5):e190146. http://dx.doi.org/10.1148/ryai.2020190146, PMID:
33937838.

Intell Syst
Solving the Black Box Problem:
A Normative Framework for Explainable Artificial Intelligence

Carlos Zednik
Otto-von-Guericke-Universitat Magdeburg

carlos.zednik@ovgu.de

http://carloszednik.net
Abstract

Many of the computing systems programmed using Machine Learning are opaque: it is
difficult to know why they do what they do or how they work. The Explainable Artificial
Intelligence research program aims to develop analytic techniques with which to render
opaque computing systems transparent, but lacks a normative Framework with which to
evaluate these techniques’ explanatory success. The aim of the present discussion is to
develop such a Framework, while paying particular attention to different stakeholders’
distinct explanatory requirements. Building on an analysis of ‘opacity’ from philosophy
of science, this framework is modeled after David Marr’s influential account of
explanation in cognitive science. Thus, the Framework distinguishes between the
different questions that might be asked about an opaque computing system, and
specifies the general way in which these questions should be answered. By applying this
normative Framework to current techniques such as input heatmapping, Feature-
detector identification, and diagnostic classification, it will be possible to determine
whether and to what extent the Black Box Problem can be solved.

1. Introduction

Computing systems programmed using Machine Learning (ML) are increasingly capable
of solving complex problems in Artificial Intelligence (Al). Unfortunately, these systems
remain characteristically opaque. it is difficult to “look inside” so as to understand why
they do what they do or how they work.

Opacity is the heart of the Black Box Problem—a problem with significant
practical, legal, and theoretical consequences. Practically, end-users are less likely to
trust and cede control to machines whose workings they do not understand (Burrell,
2016; Ribeiro, Singh, & Guestrin, 2016), and software engineers may be unable to
intervene in order to quickly and systematically improve performance (Hohman, Kahng,
Pienta, & Chau, 2018). Legally, opacity prevents regulatory bodies from determining
whether a particular system processes data Fairly and securely (Rieder & Simon, 2017),
and may hinder end-users From exercising their rights under the European Union's
General Data Protection Regulation (European Commission, 2016). Theoretically, the
Black Box Problem makes it difficult to evaluate the potential similarity between
artificial neural networks and biological brains (Buckner, 2018), and to determine the
extent to which computers being developed using ML may be considered genuinely

Page 1
intelligent (Zednik, 2018).

Investigators within the Explainable Al (XAl) research program intend to ward off
these consequences through the use of analytic techniques with which to render opaque
computing systems transparent.’ Although the XAI research program has already
commanded significant attention (Burrell, 2016; Doran, Schulz, & Besold, 2017; Lipton,
2016; Ras, van Gerven, & Haselager, 2018; Zerilli, Knott, Maclaurin, & Gavaghan, 2018),
important normative questions remain unanswered. Most fundamentally, it remains
unclear how Explainable Al should explain: what is required to render opaque computing
systems transparent? Given different stakeholders’ distinct reasons For interacting with
such computing systems, these stakeholders will require different kinds of explanations.
Moreover, it remains unclear how explanation in this context relates to other epistemic
achievements such as description, prediction, intervention, and understanding—each of
which might be more or less important in different domains, For different stakeholders,
and with respect to different systems. Finally, it remains unclear what the prospects are
of actually explaining the behavior of ML-programmed computing systems as they
become increasingly powerful, sophisticated, and widespread.

The present discussion contributes to the maturation of the Explainable Al
research program by developing a normative Framework For rendering opaque
computing systems transparent. This Framework accommodates different stakeholders’
distinct explanatory requirements, and specifies the questions that should be answered
in order to ward off the Black Box Problems practical and theoretical consequences.
Notably, inspiration will be sought in cognitive science. Indeed, insofar as the problem of
rendering opaque computing systems transparent is not unlike the problem of
explaining the behavior of humans and other biological cognizers, the Explainable Al
research program can benefit from co-opting some of the norms and practices of
cognitive science (see also Rahwan et al., 2019).

The discussion proceeds as Follows. Section 2 builds on previous philosophical
work by Paul Humphreys (2009) to analyze the oft-used but nevertheless ill-understood
notions of ‘opacity’ and ‘transparency’. Section 3 then invokes Tomsett et al.’s (2018)
notion of an ‘ML ecosystem’ to distinguish between the stakeholders who are most likely
to seek explanations of a particular system’s behavior. It also introduces David Marr’s
(1982) levels of analysis account of explanation in cognitive science so as to better
understand these stakeholders’ distinct explanatory requirements. Indeed, different
stakeholders in the ML ecosystem can be aligned with different questions in Marr's
account—questions about what, why, how, and where a computer program is carried out.
Sections 4 and 5 then introduce and evaluate the explanatory contributions of several
current analytic techniques from Explainable Al: fayer-wise relevance propagation
(Montavon, Samek, & Muller, 2018), local interpretable modeL-agnostic explanation

1 There are two distinct streams within the Explainable Al research program. The present discussion
Focuses on attempts to solve the Black Box Problem by analyzing computing systems so as to render
them transparent post hoz, i.e., after they have been developed or deployed. In contrast, the discussion
will not consider efforts to avoidthe Black Box Problem altogether, by modifying the relevant ML
methods so that the computers being programmed do not become opaque in the First place (For
discussion see, e.g., Doran, Schulz, & Besold, 2017).

Page 2
(Ribeiro et al., 2016), feature-detector identification (Bau et al, 2018), and diagnostic
classification (Hupkes, Veldhoen, & Zuidema, 2018).” Indeed, these sections show that
different XAI techniques are capable of answering distinct questions, and thus, are likely
to satisfy different stakeholders’ explanatory requirements.

2. The Black Box Problem in Artificial Intelligence
2.1 From Machine Learning to the Black Box Problem

The Black Box Problem is traditionally said to arise when the computing systems that are
used used to solve problems in Al are opaque. This manner of speaking is grounded in
the metaphorical intuition that a system’s behavior can be explained by “looking inside”
so as to understand why it does what it does or how it works. Although most computing
systems are constructed From well-understood hardware components that afford no
literal obstacle to “looking inside”, they might nevertheless be considered opaque in the
sense that itis difficult to know exactly how they are programmed.

Machine Learning is just one approach among many for programming computers
that solve complex problems in Al. Unlike their colleagues working within other Al
approaches, however, developers in Machine Learning exert limited influence on the way
in which the relevant problems are solved. Of course, ML developers must decide on
basic architectural principles such as whether the system takes the form of a deep neural
network, a support vector machine, a decision tree, or some other kind of system with a
particular set of learnable parameters. Moreover, they must choose an appropriate
learning algorithm, and must identify a suitable learning environment, in which the
learnable parameters can obtain values with which to solve the problem at hand.
Nevertheless, ML developers do not typically decide on the particular values these
parameters eventually obtain (e.g. the weights of individual network connections), and
in this sense, do not wholly determine the way in which the problem is actually solved.

This relative lack of influence is a great advantage insofar as Machine Learning
methods are often capable of identifying highly unintuitive and subtle solutions that are
unlikely to be Found using more traditional methods. Indeed, it is this capability that
explains the recent influx of Machine Learning in many different domains, as well as its
great promise For society as a whole.* Unfortunately, however, this great advantage also

2 This is by no means a comprehensive list of techniques, and inclusion in this list should not be taken to
indicate that these techniques are superior to others. Indeed, XAl is an incredibly dynamic Field in
which original and increasingly powerful techniques are being developed almost on a daily basis. But
although not all relevant XAI techniques can be considered here, the normative framework being
developed is meant to apply generally, including to those techniques that have not been considered or
that may not have yet been developed.

3 Domains in which ML-programmed systems have recently had a great, if not revolutionary, influence
include game-playing, autonomous driving and Flying, question-answering, natural language
processing, machine vision, behavior-prediction, and product-recommendation, among many others.
That said, successes within these domains should not, of course, be taken to imply that Machine
Learning methods are all-conquering. Indeed, many important Al problems remain unsolved, and in
many cases, it is unclear whether, and if so how, ML methods could ever be used to solve them. Indeed,
in many problem domains, traditional Al methods remain Far more effective than the methods
developed in Machine Learning (for discussion see, e.g., Lake, Ullman, Tenenbaum, & Gershman, 2017;

Page 3
comes ata significant cost. Unlike the computing systems being programmed using
more traditional methods, the systems being programmed using Machine Learning are
characteristically opaque.

On the face of it, the reasons For this characteristic opacity are obvious. Because
developers exert relatively limited influence on the setting of parameter values, they
might not know how these parameters contribute to the system’s behavior. In addition,
even if individual parameter values are known, the Fact that they might interact
nonlinearly as well as recurrently means that it is almost impossible to understand,
predict, or systematically intervene on the way in which a particular input is transformed
so as to generate a particular output.

Upon closer inspection, however, it is clear that a system’s opacity cannot be
reduced to an ML developers’ knowledge of parameter values. For example, many deep
neural networks have been Found to learn high-level representations that capture
abstract properties of the learning environment, but that do not map neatly onto
individual parameter values (see, e.g., Bau et al., 2018; Buckner, 2018). In such cases,
knowledge of the relevant representations is arguably far more relevant for the
purposes of explaining the system’s behavior than knowledge of individual connection
weights.‘ In addition, it is not clear that ML developers are the only stakeholders with
respect to which to assess a particular system's opacity. Many other stakeholders, From
end-users to regulatory bodies, are likely to seek a better understand of a particular
system's behavior, despite being unable or unwilling to acquire knowledge of the
system’s learnable parameters. Thus, although it may be tempting to reduce a system's
opacity to a developer’s knowledge of learnable parameters, opacity is in fact a highly
nuanced phenomenon that bears clarification. What is required, in other words, is a
better understanding of the dual notions of ‘opacity’ and ‘transparency’, and thus, a
better understanding of the Black Box Problem itself.

2.2 What is the Black Box Problem?

One of the most influential recent analyses of ‘opacity’ is due to Paul Humphreys (2009).
Although Humphreys’ work is primarily concerned with computer simulations in
scientific disciplines such as high-energy physics and molecular biology, his analysis of
‘opacity’ is also a useful starting point For discussions of the Black Box Problem in
Artificial Intelligence. On Humphreys’ analysis, computing systems are

“opaque relative to a cognitive agent X at time tjust in case X does not know at
all of the epistemically relevant elements of the [system]” (Humphreys, 2009, p.
618).5

Marcus, 2018)

4 Indeed, ML developers typically do have access to the values of learnable parameters. Nevertheless,
these developers are often the First to call For additional explanations so as to, e.g., demonstrate that
a system does in Fact do what it is supposed to do (see Section 4), or to improve its performance when
it does not (See Section 5 and Hohman, Kahng, Pienta, & Chau, 2018).

5 Humphreys subsequently introduces the notion of essential epistemic opacity, which applies to
systems whose epistemically relevant elements are not only unknown to the agent, but that are in Fact
impossible to know by that agent (Humphreys 2009, p. 618). Notably, Humphreys’ notion of possibility
is not logical, but practical—it depends on an agent's limited time, money, and computational power,

Page 4
Two Features of this analysis are worth emphasizing. First, opacity is agent-relative. That
is, a computing system is never opaque in and of itself, but opaque only with respect to
some particular agent. Second, opacity is an epistemic property: it concerns the agent's
(lack of) knowledge. According to Humphreys, this knowledge concerns the system's
epistemically relevant elements (EREs). Although Humphreys does not further analyze the
notion of an epistemically relevant element, it can be Fleshed out in a way that suits
current purposes. Thus, an element can be understood as, For example, a step in the
process of transforming inputs to outputs, or as a momentary state-transition within the
system’s overall evolution over time. An epistemically relevant element is one which is
not only known to the agent, but which can be cited by him or her to explain the
occurrence of some other element, or of the system’s overall output.

The term ‘explain’ is left intentionally ambiguous. A computing system's
epistemically relevant elements may take the form of physical structures, mathematical
states-of-affairs, or even reasons—among many other things. Accordingly, the presence
or absence of any such element might be explained physically (e.g. by appealing to the
magnetization of hardware registers), mathematically (e.g. by appealing to binary
strings), or even rationally (e.g. by appealing to the possibility that the relevant binary
string represents a particular goal state). In general, different kinds of explanations
invoke different kinds of EREs, and different kinds of EREs are likely to be appropriate to
different agents and to different systems.

Two notes on the ambiguity of ‘explain’. First, for any given system, many
different EREs can be considered equally “real”, and thus, many different explanations
can be considered equally legitimate. By way of analogy to the computing system just
above, the behavior of humans and other biological cognizers can also be explained
physically (e.g. by reference to neurobiological mechanisms), mathematically (e.g. by
reference to computational processes), and rationally (e.g. by reference to beliefs and
desires). Staunchly reductionist or eliminativist philosophical tendencies
notwithstanding (e.g., Bickle, 2006; Churchland, 1981), all of these explanations are
equally legitimate, and the EREs invoked therein are equally “real”. The situation in
Explainable Al is analogous: many different kinds of explanations can and should be
considered legitimate For the purposes of rendering opaque computing systems
transparent.

But although many different kinds of explanations are equally legitimate, not all
of them are likely to be equally useful for any individual agent. Whereas some agents
may in fact seek to explain a computing system’s behavior by invoking as EREs the values
of learnable parameters, many other agents—in particular, agents with more limited
cognitive resources, with a more limited understanding of computer programming, or
with altogether different goals and interests—will require rather different explanations.

among other resources (see, e.g., Duran & Formanek, 2018). For this reason, analytic techniques From
XAI that allow an agent to use these resources more efficiently are poised to reduce the scope of the
impossible, all while allowing that some systems may remain opaque even after the most powerful
techniques have been applied. That said, the present discussion need not speculate about the scope of
the impossible. The primary aim of this discussion is to show how systems that can be rendered
transparent should be rendered transparent.

Page 5
Thus, whereas some agents might be satisfied by explanations that cite a system's
parameter values, others are likely to prefer explanations that cite higher-order
representations. Moreover, some agents may require explanations that invoke the
physical properties of hardware components, whereas others may instead invoke
explanations that cite environmental Features such as objects, colors, persons, or
intentions. Thus, the ambiguity in the term ‘explain’ shows why the notions of ‘opacity’
and ‘transparency’ cannot be properly understood solely by considering developers’
(lack of) knowledge of learnable parameters: For many agents, learnable parameters are
simply not the right kind of ERE For the purposes of rendering opaque computing
systems transparent.

The discussion so Far shows that the sense in which computing systems are
opaque—and thus, the sense in which they should be rendered transparent—differs
between agents. At the same time, the outline of a general solution to the Black Box
Problem is already beginning to taking shape: In order to render an opaque computing
system transparent, a particular agent must seek knowledge of an appropriate set of
epistemically relevant elements with which to explain that system’s behavior. Two
questions must be answered before this outline can be Fleshed out into a normative
framework For Explainable Al. Which agents are concerned with explaining the behavior
of computers programmed using Machine Learning? What are, for these agents, the
appropriate epistemically relevant elements? By answering these two questions, it will
eventually be possible to determine how Explainable Al should explain, and to evaluate
the explanatory success of current and Future XAI analytic techniques.

3. From Machine Learning to Marr
3.1. What are the stakeholders?

Tomsett et al. (2018) provide a helpful taxonomy of agents within the ML ecosystem—
that is, a taxonomy of stakeholders who depend on or regularly interact with a
computing system developed using Machine Learning. Six kinds of stakeholders are
distinguished according to their unique roles within the ML ecosystem (Figure 1), and
Tomsett et al. illustrate these stakeholders’ roles by invoking the example of a loan risk
assessment system. Indeed, the risk assessment system is a typical, albeit relatively
simple, ML application: A supervised learning algorithm can be used quite
straightforwardly to correlate previous applicants’ personal data such as income, age,
and home address with their eventual ability to repay loans in a timely manner. On the
basis of such correlations, the automated loan risk assessment system can take a new
individual’s personal data as input, and generate an output to estimate the Financial risk
a bank would incur by accepting that individual’s loan application.

Page 6
Creators
Examiners

ras
we
-

eo
rae
o”
Machine | «uy, —_—_— =—=s
learning

system Operators Executors Decision-
subjects

 

 

 

 

 

<_—

Data-subjects

 

Figure 1: The ML ecosystem. Reproduced from Tomsett et al.
(2018).

 

 

 

In the risk assessor's ecosystem, bank employees are operators and/or executors.
Operators are agents who “provide the system with inputs, and directly receive the
system's outputs” (Tomsett et al., 2018, p. 10). These are likely to include bank tellers
tasked with entering a particular applicant's personal data into the system, and with
receiving the system’s output. In contrast, executors are “agents who make decisions
that are informed by the machine learning system” (Tomsett et al., 2018, p. 10). These
are likely to include back-office employees who rely on the system’s output to make
data-driven decisions about whether or not to accept a particular application.

Another important agent in the ML ecosystem is the decision-subject. In the
present example, the decision-subject is the loan applicant: the individual whose data
are being processed For the purposes of assessing risk, and who is subject to the
executor’s Final decision. Decision-subjects are distinguished from data-subjects,
individuals whose personal data are contained in the learning environment. In the loan-
application example, data-subjects include previous applicants whose personal data and
loan-repayment behavior ground the risk assessor's learned correlations.

Creators are the developers of the computing system. These include software
engineers responsible For designing the learning algorithm and For selecting the learning
environment. They are also likely to include system administrators responsible For
maintaining and possibly fine-tuning the system once it has been deployed. Of course,
creators might also include hardware engineers tasked with building and maintaining the
hardware in which the risk assessor is implemented. That said, many current ML
applications are driven by standard-issue hardware components whose workings require
no ML-specific knowledge.® Perhaps For this reason, Tomsett et al. (2018) do not
explicitly consider creators of this kind.

Finally, examiners are “agents tasked with compliance/safety-testing, auditing, or

6 Applications that depend on ML-specific hardware components are considered in Section 5.

Page 7
forensically investigating a system” (Tomsett et al., 2018, p. 13). Thus, examiners are
likely to include regulatory bodies charged with determining that the risk assessment
system conforms to, For example, data privacy regulations, anti-discrimination laws, and
policies designed to prevent Financial fraud. Given legislative innovations such as the EU
General Data Protection Regulation (GDPR), examiners are likely to play an increasingly
prominent role in Future ecosystems.

3.2. What are the epistemically relevant elements?

In order to perform their designated roles in the ecosystem, every stakeholder must
possess a particular kind of knowledge. More precisely, every stakeholder must possess
knowledge of appropriate epistemically relevant elements. Consider again the loan risk
assessor. Bank tellers tasked with entering inputs and receiving outputs can only
operate the system if they know, for example, something about the INCOME, D.O.B. and
HOME_ADDRESS variables (in particular, their type), and that the most important output
(the output whose value is to be passed on to the back-office executor) is the value of
the RISK variable. In contrast, software engineers charged with developing, maintaining
and improving the risk assessor’s behavior must be able to identify, characterize, and
intervene on the variables—system parameters and/or high-level representations—that
mediate the transformation of inputs to outputs. Perhaps surprisingly, most other
agents are less likely to be concerned with the system’s variables than with the
environmental Features that are represented by those variables. In particular, although
executors, examiners, data- and decision-subjects may not need to know much about
variables such as INCOME, D.O.B., HOME_ADDRESS, and RISK, but will have to know
something about (their own or their customers’) income levels and demographic
information, and eventually, about the level of Financial risk associated therewith.

Given that they must possess a particular kind of knowledge, opacity—a lack of
knowledge—prevents every kind of stakeholder from performing their designated role
in the ecosystem.’ IF a bank teller does not know that an applicant’s date of birth must
be entered YYYYMMDD rather than DDMMYYVYY, he or she will be unable to correctly
operate the system so as to generate meaningful outputs. If a back-office executor does
not know that an output value of 0.794 designates a relatively high level of Financial risk,
he or she will be unable to make an appropriate decision with respect to a particular loan
application. IF current applicants (i.e., decision-subjects) are unable to acquire knowledge
of the Factors that contribute to particular decisions, they might not be able to exercise
their GDPR right to explanation (Goodman & Flaxman, 2016; but cf. Wachter, Mittelstadt,
& Floridi, 2017). Similarly, if previous applicants (i.e., data-subjects) are unable to acquire
knowledge of the personal data that is stored in the learning environment, they will not
be in a position to exercise their GDPR right to information. |F examiners such as lawyers
or regulatory bodies cannot discern whether the loan risk assessment system has

7 Some commentators deny that opacity prevents stakeholders other than creators from fulfilling their
roles (see, e.g., Zerilli, Knott, Maclaurin, & Gavaghan, 2018). However, these commentators Fail to
recognize the diversity of stakeholders within the ML ecosystem, and thus, the different senses in
which computing systems may be opaque, as well as the different reasons For rendering these systems
transparent.

Page 8
learned to correlate a Foreign place of birth with high level of Financial risk, they will be
unable to identify, and if necessary sanction, possible discrimination. Finally, software
engineers who do not know the variables that mediate causally between a system's
inputs and outputs—be they individual system parameters or high-level representations
—will be unable to efficiently modify the system’s performance so as to bring it in line
with the lawyer’s legal recommendations.

Where opacity is the problem, transparency is the solution. Thus, stakeholders
should seek explanations—they should render the relevant computing system
transparent. Of course, given that the knowledge required to perform their roles differs
between stakeholders, so too will the knowledge that should be sought. In particular,
different stakeholders should seek knowledge of different kinds of epistemically
relevant elements. But although the appropriate EREs may be apparent enough in the
context of the loan-application example, in order to extend the discussion to other
(potentially more complex) examples—and thus, to understand in a more general sense
of what is required to render computing systems transparent—it will be necessary to
attain a deeper understanding of the different kinds of knowledge that different
stakeholders require in order to fulfill their designated roles within the ML ecosystem.®

3.3. Toward a Marrian framework for Explainable Al

One way of attaining such a deeper understanding is to adopt a more general
explanatoryframework. Consider David Marr’s (1982) levels of analysis account, which
specifies norms that should be satisfied in order to explain the behavior of cognitive
systems as diverse as the human visual system and the sensorimotor system of the
house fly. Notably, despite having been developed almost four decades ago, Marr's
account remains influential in cognitive science today, and there are reasons to believe
that it can serve double-duty as a normative framework For Explainable Al in the Future.
For one, like many biological cognizers, the computing systems being developed in
Artificial Intelligence can be viewed as information-processing systems in which inputs
are systematically transformed into outputs.’ For another, like the computers being
programmed using Machine Learning, biological cognizers are opaque in the sense that

8 Although the present discussion distinguishes between the explanatory requirements of different
kinds of stakeholders, there may also be differences between the explanatory requirements of any
two individuals, even if they are stakeholders of the same kind. In particular, differences in the
background knowledge, training, and preferences of individuals are likely to influence the receptivity
of these individuals to specific kinds of explanations. But although such individual differences are
undeniably interesting and relevant, the present discussion will not consider them Further, and will
instead only Focus on the the differences that arise between different kinds of stakeholders due to
their distinct roles.

9 Notably, in order to illustrate and motivate his Framework, Marr himself did not only invoke biological
cognizers, but also invoked engineered computing systems such as the supermarket cash register.
Indeed, the agents who regularly interact with the cash register are closely analogous to the agents in
the ML ecosystem: the supermarket employees who operate the register are like operators and/or
executors; customers are like decision subjects; the software or hardware engineers charged with
constructing and maintaining the cash register are like creators; Financial regulatory bodies such as tax
auditors are examiners. Although there exist important disanalogies (e.g. the supermarket cash
register has no data-subjects), the Fact that Marr's Framework is in part motivated by its applicability to
engineered systems can be viewed as an additional reason to believe that may also be useful when
articulating a normative Framework For Explainable Al.

Page 9
we still do not know exactly why they do what they do or how they work (Zerilli et al.,
2018).

That said, the most important reason For thinking that Marr’s account of
explanation in cognitive science can serve double-duty as a normative framework for
Explainable Al is that the account is sufficiently multi-faceted to illuminate the subtle
differences between the knowledge required by different stakeholders. Indeed, Marr’s
account centers on the answering of several different questions at three distinct levels of
analysis (McClamrock, 1991; Shagrir, 2010; Zednik, 2017). Insofar as different
stakeholders can be thought to ask different kinds of questions, the characteristic
answers described by Marr can be used to better understand the different kinds of EREs
these stakeholders should invoke in order to render a computing system transparent.
Indeed, by adopting Marr’s account to evaluate the explanatory contributions of analytic
techniques From Explainable Al, it will be possible to determine exactly which questions
these techniques are capable of answering, and thus, For which stakeholders and to what
extent the analyzed systems can eventually be rendered transparent.

4. Description and Interpretation: The Computational Level
4.1. Questions about what and why

Marr's computational level of analysis centers on questions about whata system is doing,
and on questions about wAyit does what it does. These questions are intimately related,
but importantly different. Questions about whata particular system is doing call For a
description of that system's overall behavior in terms of, for example, a transition
Function fin which ‘input’ states are mapped onto corresponding ‘output’ states (Figure
2). In the context of the loan risk assessment system, what-questions are answered by
specifying the value of the RISK variable that is generated for any particular combination
of input variables such as INCOME, D.O.B., and HOME_ADDRESS.

In contrast, questions about whya system does what it does are questions about
that behavior’s “appropriateness” within some particular environment (Marr, 1982, p.
24f). That is, these questions call For an interpretation of the system’s behavior in terms
of recognizable Features of the environment—be this the learning environment that
(together with the learning algorithm) determines the system’s behavior, or the
behavioral environment in which the system is eventually deployed. Insofar as what-
questions are answered by specifying a transition Function fthat maps ‘input’ states
onto corresponding ‘output’ states, why-questions concern the environmental regularity
or correlation f’that obtains between the inputs and outputs themselves. Specifically,
answering a why-question involves showing that there exists a correspondence between
fand f’ (Shagrir, 2010. See also Figure 2). Thus for example, in the loan-application
scenario, why-questions are not answered by describing the values of variables such as
INCOME, D.O.B., HOME_ADDRESS, and RISK, but rather by interpreting these values in
terms of Features such as a particular applicant’s income, date of birth, home address,
and level of Financial risk.

Page 10
 

input > output

 

Sin > Sout
f

Figure 2. S, and S,,, are the ‘input’ and ‘output’ states of the

computing system, respectively; input and output are the
corresponding features of the environment. The solid arrow at the
bottom designates the overall behavior of the system, presumably
realized in some causal process. The dotted arrow designates a
representation (or other kind of correspondence) relationship
between the system and its environment. What-questions concern
the transition function f from s,, to s,.,. Why-questions, in contrast,

concern the relationship f’ between input and output. Adapted
from Shagrir (2010).

 

 

 

 

It can be helpful to view the correspondence between systems and their
environments semantically, that is, in representational terms. The system’s ‘input’ and
‘output’ states may be thought to represent the input received from the environment
and the output that is generated in response. Likewise, the state-transition fmight be
thought to track the regularity or correlation /’ that obtains between those inputs and
outputs. Although more can and should be said about the correspondence that obtains
between a computing system and its environment (for discussion see, e.g., Shagrir,
2010), For current purposes it suffices to say that whereas what-questions concern local
properties of the computing system itself—often, its representational vehicles—why-
questions concern features of the surrounding environment—the corresponding
representational contents.

4,2. Operators, executors, examiners, data- and decision-subjects

This brief presentation of the computational level can already be used to align different
stakeholders in the ML ecosystem with distinct questions within Marr’s account. For this
reason, it can be used to better understand the kinds of EREs these agents should
identify in order to render computing systems transparent.

Insofar as they are tasked with entering inputs and receiving outputs, operators
are most likely to seek answers to questions about what a computing system is doing.
That is, their explanatory requirements are satisfied—the system is rendered suitably
transparent—by describing the inputs that must be entered and the outputs that are
generated. In contrast, executors and examiners are Far more likely to be concerned with
why-questions in which a system’s behavior must be interpreted. To wit, although a back-

Page 11
office executor in the loan-application scenario must know that the risk assessment
system computes a value of 0.794, his or her most important task is to interpret that
value as an indicator of significant Financial risk. Similarly, it is an examiner’s duty to
determine whether a particular assessment has been generated legitimately, or because
the system discriminates by associating a Foreign place of birth with a high level of
Financial risk.

Data- and decision-subjects are similarly concerned with questions about why
rather than with questions about what. Since coming into force in May 2018, the GDPR
empowers data- and decision-subjects to seek answers to why-questions, but does not
similarly extend to what-questions. In particular, the GDPR right to information allows
data-subjects to know which personal information is represented, but not how that
information is entered, represented, and manipulated. Similarly, the GDPR right to
explanation may allow decision-subjects to know which factors contributed to a
particular outcome (e.g., a low income may lead to a denied application), but not to the
particular way in which those factors are represented, or how the outcomes are actually
calculated."® Notably, the rationale For focusing on questions about whyrather than on
questions about what (or on questions about how, see Section 5) is compelling: whereas
my personal data belong to me, the data-structures and processes that are actually used
to represent and manipulate those data are the property of the Al service provider.

In summary, the distinction between what-questions and why-questions at the
computational level captures an important distinction between two different ways of
rendering a computing system transparent, each of which is appropriate For different
stakeholders within the ML ecosystem. Whereas operators typically seek to render a
system transparent by asking what it does and describing its ‘input’ and ‘output’ states,
many other agents do so by asking whyit does what it does and interpreting those
states in terms of environmental Features and regularities. Perhaps surprisingly, given
the metaphorical underpinnings of “opacity” and “black boxes”, this means that For
several stakeholders in the ML ecosystem, rendering a computing system transparent
does not involve “looking inside” the system at all, but rather “looking out”, at the
environment in which that system’s behavior is learned and performed.

4,3. Input heatmapping

Several XAI techniques can be used to answer questions about what and why. One such
technique is input heatmapping, which highlights the Features of a system’s input that
are particularly relevant to, or predictive of, its output. Some of the most compelling

10 Itis a point of contention to what extent the GDPR right to explanation constitutes a right at all, and if
so, what it guarantees (Wachter, Mittelstadt, & Floridi, 2017). Although the Former is a legal question
that goes beyond the scope of the present discussion, the latter is a normative question that may
benefit From the present discussion. In particular, Goodman & Flaxman (2016) argue that the GDPR
grants data-subjects (and decision-subjects) the right to acquire “meaningful information about the
logic involved.” However, what exactly is meant by "logic" in this context remains unclear. The present
discussion implies that the relevant stakeholders should be primarily concerned with why-questions,
and that in this sense, the "logic" should be specified in terms of learned regularities between
environmental factors. Moreover, the present discussion suggests that Al service providers may, For
legal reasons, be compelled to deploy XAI techniques capable of answering why-questions.

Page 12
examples of this technique come from the domain of machine vision, in which deep
neural networks are used to classify pixelated input images according to the people,
objects, properties, or situations they depict."’ There, input heatmapping typically
involves the generation of visualizations—i.e., heatmaps—that emphasize the particular
pixels or pixel regions that are most responsible For a particular classification (Figure 3).

input > LRP

EMMA) ~ + «

 

Figure 3. Input heatmaps (right) for various input images (left),
generated by applying the method of layer-wise relevance
propagation (LRP) to a deep neural network capable of recognizing
handwritten digits. Reproduced from Montavon et al. (2018).

 

 

 

One concrete approach For developing heatmaps for artificial neural networks is
Layer-wise Relevance Propagation (LRP, Montavon et al., 2018). This method deploys a
subroutine of the popular backpropagation learning algorithm, in which individual unit
activations and connection weights are used to calculate the responsibility that the
individual units of an “upstream” layer (bear for producing particular levels of activity in
the subsequent “downstream” layer (,;. Given a particular classification at the network's
output layer, this subroutine can be deployed in a layer-wise Fashion until responsibility-
values are calculated For every unit in the network's input layer. Insofar as these units
correspond to, For example, pixels of a particular input image, these responsibility-values
can be used to generate a heatmap that highlights the pixels or pixel regions that bear
the greatest responsibility For the Final classification.

Input heatmaps provide particularly Fine-grained answers to questions about
what a computing system is doing. Recall that questions of this sort are answered by
specifying the transition Function fthat obtains between a system’s ‘input’ and ‘output’
states. Input heatmaps can help specify f, not in terms of the ‘input’ state as a whole

11. Although machine vision may be the domain in which input heatmaps are most intuitive, they may also
be used in other domains. For example, input heatmaps may be constructed For audio inputs,
highlighting the moments within an audio recording that are most responsible for classifying that
recording by musical genre. Moreover, although LRP is specifically designed to work with artificial
neural networks, other methods can be used to generate input heatmaps for other kinds of systems.

Thus, input heatmapping can be viewed as a general-purpose XAI technique for answering what- and
why-questions.

Page 13
(e.g., a whole pixelated image), but in terms of a limited number of elements within that
state (e.g., individual pixels). Such fine-grained answers to what-questions are
particularly useful For operators. In particular, they greatly enhance an operator’s ability
to identify the most likely sources of error. IF an error is detected in the ‘output’ state
(e.g., the RISK value is inappropriately high), it is more likely to result From high-
responsibility elements of the ‘input’ state (e.g., a wrongly-formatted D.O.B. value) than
from low-responsibility elements (e.g., a misspelled surname). That said, such fine-
grained answers to what-questions might also be exploited by malicious operators such
as hackers. Indeed, input heatmaps can be used to design adversarial inputs that appear
typical to human observers, but that nevertheless produce radically divergent (and thus,
potentially exploitable) outputs due to minor changes to some high-responsibility
elements (Szegedy et al., 2013).

Notably, input heatmaps can also be used by creators. Creators are regularly
tasked with improving or otherwise changing a system’s overall behavior, and thus, with
changing the system’s transition function f£ An input heatmap’s Fine-grained answers to
what-questions can be used by creators to identify the necessary changes. Thus for
example, in order to maximize a system's processing efficiency and minimize its memory
load, an input heatmap might be used to determine whether certain aspects of the input
(e.g., pixels at the edge of an image) can be ignored, or whether the memory required to
store individual inputs can be reduced by, For example, zeroing the values of low-
responsibility elements. Similarly, creators seeking to minimize a system's susceptibility
to adversarial inputs could deploy input heatmaps to, For example, determine that the
responsibility for Final classifications should be distributed more evenly across all ‘input’
elements. OF course, although creators may invoke input heatmaps to determine the
particular changes that must be made, they will normally have to use other techniques—
in particular, techniques capable of answering how-questions—in order to know how
those changes can actually be achieved.

While input heatmapping can be used by stakeholders seeking to better
understand what a system is doing, this technique can also be used by stakeholders
interested in knowing whythe system does what it does. Recall that why-questions are
answered by specifying a regularity or correlation f’that obtains between features of
the environment, and by showing that this correlation is tracked by the system’s
transition Function f Input heatmaps can be used to answer why-questions if the
environmental Features that participate in f’ can be discerned by inspecting the map—
that is, if the highlighted elements of the ‘input’ state together “look like” some
recognizable feature of the environment. Consider again the input heatmaps in Figure 3.
The highlighted pixels visually resemble handwritten digits. In particular, the highlighted
pixels in the upper-left heatmap visually resemble a handwritten 2 rather than, For
example, a handwritten 7. This Fact answers a question about why the upper-left input
image outputs ‘2’ rather than ‘7’: the output is ‘2’ because the image depicts a 2. Stated
more generally, the input heatmaps in Figure 3 show that the system does what it is in
fact supposed to do, namely, detect and classify handwritten digits.

Page 14
In order to better appreciate the importance of answering why-questions in this
way, it is worth contrasting the above example with one in which the computing system
does not in fact do what it is supposed to do. Consider a well-known historical (albeit
probably apocryphal) example in which a neural network learns to visually distinguish
enemy tanks from friendly ones.” Although the network quickly learns to categorize
images of tanks, it does so by tracking an accidental correlation between tank
allegiances and weather patterns: whereas the images of Friendly tanks were all taken
ona sunny day, the enemy tanks were photographed under cloud cover. For this reason,
although the system correctly classifies images (wha), its reasons for doing so (why)
have nothing at all to do with tanks!

In this example, heatmapping techniques such as LRP would be likely to produce
visualizations in which the highlighted pixels together resemble clouds in the
background, rather than tanks in the foreground. Several different stakeholders would
be likely to benefit from such visualizations: executors at military HQ who must
ultimately decide whether or not to shoot at a particular tank, but also examiners at the
International Criminal Court tasked with determining whether the resultant action ought
to be considered a war crime. Moreover, tank crews who make up the decision-subjects
of the tank-classification system could rest easy in their knowledge that the system can
be trusted, and that they are For this reason less likely to perish in a data-driven barrage
of Friendly Fire.

Before moving on to other kinds of questions, it is important to mention a
significant limitation of the heatmapping technique. Input heatmaps can be used to
answer why-questions when the highlighted elements together “look like” some
recognizable feature of the environment. As was already suggested in Section 2,
however, Machine Learning methods are renowned for their ability to identify and track
subtle as well as complex Features and correlations in the learning environment, many of
which may not be easily recognized or labeled by human observers. In such cases, the
utility of input heatmapping is likely to be limited, and other XAI techniques may have to
be invoked. One such technique might be Local interpretable Model-agnostic Explanations
(LIME, Ribeiro et al., 2016), which can be used to simplify the transition Function f (which
is often nonlinear and therefore difficult to interpret) with a linear approximation that is
more readily interpretable by human observers. Similarly, Local Rule-Based Explanations
(LORE, Guidotti et al., 2018) are designed to approximate limited domains of fby sets of
comprehensible decision-rules. Although these different approximations always only
capture a system’s behavior for a limited range of inputs, and although as
approximations they always bear the risk over-simplification and misrepresentation, they
may nevertheless prove useful for answering what- and why- questions even when input
heatmapping fails.

12 Gwern Branwen maintains a helpful online resource on this particular example, listing different
versions and assessing their veracity: https://www.gwern.net/Tanks (retrieved January 25"", 2019).

Page 15
5. Intervention: The Algorithmic and Implementational Levels
5.1 Questions about how and where

In Marr’s account of explanation in cognitive science, the levels “below” the
computational level of analysis are the algorithmic and implementational levels. The
algorithmic level centers on questions about Ahowa system does what it does. Insofar as
the system's behavior is described using a transition Function ffrom an ‘input’ state to an
‘output’ state, the algorithmic level aims to uncover the mediating states s;, 52,...5,and
state-transitions ss, that appropriately connect ‘input’ and ‘output’ (Figure 4). Put
differently, the algorithmic level is concerned with uncovering the program that
executes the overall transition f£ and to thereby compute or approximate /’ (Shagrir,
2010)."

In contrast, the implementational level of analysis centers on questions about
where the program described at the algorithmic level is realized. Where-questions
concern the physical components pi, p2,..Pm in which states are realized and state-
transitions are performed (Figure 4). Thus, the implementational level is concerned with
the hardware components that are involved in executing the program For f.

f'
input > output

 

 

 

 

Sin >

 

 

 

 

 

 

 

 

 

 

Figure 4. As in Figure 3, S,, and S,,, are the ‘input’ and ‘output’
states of the computing system, respectively; input and output are
the corresponding features of the environment, and the dotted arrow
designates a representation (or other kind of correspondence)
relationship between the system and its environment. The squared
shapes and arrows in the middle designate the individual states and
state-transitions that are performed to compute or approximate f,
the description of which answers how-questions. The circles at the
bottom indicate the physical realizers of individual states and/or
state-transitions, the identification of which answers’ where-
questions.

13 The program that mediates between ‘input’ and 'output'—the program being executed—is not to be
confused with the learning algorithm that is used to develop (i.e., to program) the system in the First
place.

 

 

 

Page 16
Whereas what- and why-questions can both be answered by “looking out” at a
system's surrounding environment, how- and where-questions can only be answered by
“Looking inside”, at the Functional or physical variables, structures, and processes that
intervene between the system’s inputs and outputs. For this reason, these questions are
particularly important for the purposes of intervening on a system's behavior.
Knowledge of a state s;or a transition ss, that mediates the overall transition between
‘input’ and ‘output’ can be used to influence that overall transition by changing either
one of s;or ss; Likewise, knowledge of the fact that either s;or s-s;is physically
realized in some physical structure p, can be used to achieve the same goal by, for
example, replacing p, with some other physical structure p, or by removing px
altogether."

In order to better understand the way in which creators should go about
answering how- and where-questions in the Machine Learning context, it is instructive to
First consider the way these questions are answered in cognitive science. There, answers
to how-questions are typically delivered by developing cognitive models which describe
the processes that govern a particular system’s behavior. Notably, the relevant
processes are only rarely described in terms of “brute causal” interactions between
neuronal structures. More commonly, they are described in terms of the numeric
calculation of values, or the step-wise transition between states (Busemeyer &
Diederich, 2010). The most important advantage of such descriptions is that they are
able to capture a system’s abstract mathematical properties. However, in some cases
they also afford relatively straightforward semantic interpretations (Fodor, 1987). That
is, certain variables or states of a cognitive model might be said to represent specific
features of the environment, so that changes in those variables or transitions between
those states capture changes in the things being represented. That said,
representational interpretations may not always be Forthcoming—for example, because
itis unclear which Features of the environment are actually being represented (Ramsey,
1997)—nor useful—in particular, when the non-representational description is deemed
sufficiently useful For the purposes of interpreting, predicting, and intervening on the
system's behavior (Chemero, 2000).

In turn, where-questions in cognitive science are typically answered by localizing
the elements of a cognitive model—.e., its individual states, state-transitions, variables,
or calculations—in specific physical structures such as neurons, neural populations, or
brain regions (Piccinini & Craver, 2011; Zednik, 2017). Although cognitive scientists had
long denigrated the explanatory relevance of where-questions (Pylyshyn, 1984), it is now
widely acknowledged that a proper understanding of the brain is critically important For
the purposes of explaining the mind (Shallice & Cooper, 2011). That said, it would be a
mistake to think that localization in cognitive science is typically “direct” in the sense of
affording simple one-to-one mappings between the individual steps of a process and the
parts of an underlying physical structure (Bechtel & Richardson, 1993). Indeed, the

14 Although there is a clear sense in which interventions can also be achieved by modifying a system's
inputs—a different s,, will typically lead to a different sou.—interventions on the mediating states,
transitions, or realizers are likely to be Far more wide-ranging and systematic.

Page 17
Functional boundaries between the elements of cognitive models frequently cut across
the physical boundaries between physical structures in the brain (Stinson, 2016).
Although this should not be taken to indicate that the answering of where-questions is
impossible or unimportant, it does show that close attention should be paid to the sense
in which, For example, the states and state-transitions of a particular process—and thus,
possibly, the corresponding representations and representation-manipulations—might
be distributed (Clark, 1993).

5.2. Creators

How might this brief Foray into the explanatory norms and practices of cognitive science
be relevant to Machine Learning? Like scientists working to explain the behavior of
biological cognizers, the creators of an ML-programmed computing system are
frequently preoccupied with questions about Aow and where. On the one hand, software
engineers and system administrators tasked with developing, maintaining, Fixing, and
generally improving a system's behavior need to know not only what that system does,
but also Aowit does it. On the other hand, hardware engineers charged with building
and maintaining the hardware in which the system is implemented must know where
certain processes could be, or are actually, localized. Notably, much about the way in
which how- and where-questions are answered in cognitive science can be used to better
understand the way these questions can and should be answered in Explainable Al.

Consider how-questions first. Just as in cognitive science it is only rarely useful to
look For “brute-causal” interactions between neuronal structures, in the Machine
Learning context it is only rarely useful to cite the values of individual learnable
parameters such as a neural network’s unit activations or connection weights. Indeed, as
has already been discussed in Section 2, their high-dimensional complexity makes many
ML-programmed computing systems unpredictable even with complete knowledge of
the underlying parameter values. Moreover, there is often no way of knowing in advance
whether an intervention on a single parameter will change the relevant system's
behavior entirely, or else affect it in a way that is mostly or entirely imperceptible.

Like in cognitive science, therefore, suitable answers to how-questions in
Explainable Al will in most cases describe abstract mathematical properties of the
system whose behavior is being explained. To better understand the kinds of properties
that might be sought, consider once again the loan risk assessment system From above.
In order to track statistical correlations between previous applicants’ personal data and
their ability to repay loans, the risk assessment system is unlikely to categorize new
applicants on the basis of simple linear combinations of inputs such as age and income.
Rather, the system is more likely to deploy a taxomony of abstract categories in which
the inputs are combined nonlinearly (see also Buckner, 2018). For illustrative purposes, it
can help to assume that these categories approximately correspond to Folk-
psychological character traits such as honest, unscrupulous, high self-control, persevering,
or foolhardy. Although an applicant who once was the victim of a ponzi scheme may
appear inconspicuous to a bank employee, the automated risk assessment system might,
through a nonlinear combination of data points, nevertheless classify that applicant as

Page 18
being foolhardy, and For this reason, risky. In order to properly understand Aow this
system does what it does—and thus, in order to potentially intervene on the system's
behavior—creators would almost certainly profit From identifying the system's learned
categories, and from characterizing the role these categories play in the generation of
particular outputs.

Now, consider where-questions. Recall that questions of this kind were long
denigrated in cognitive science. Because many ML applications are driven by standard-
issue hardware components, it may be tempting to dismiss where-questions as being
similarly irrelevant to Explainable Al. But it is important to resist this temptation. Indeed,
a growing number of ML applications are driven by ML-specific hardware components.
For example, the artificial neural networks used For visual processing in self-driving cars
are increasingly implemented on neuromorphic hardware devices that have considerable
advantages with respect to speed and power consumption (Pfeiffer & Pfeil, 2018). In this
sense, where-questions are hugely important at least for creators tasked with choosing
the hardware in which to implement a particular computing system. But where-
questions can also be important For creators tasked with maintaining, repairing, or
improving a computing system once it has been built and deployed. Knowing the
physical location in which certain kinds of data are stored and processed can allow
creators to selectively replace hardware components so as to improve the speed or
efficiency of the system as a whole. Moreover, in certain scenarios, it may even be
important to know where data are processed after the system has stopped working. For
example, in the aftermath of a Fatal accident involving a self-driving car, it may be
necessary to extract the data that was processed in the moments immediately preceding
the accident, even if the system is no longer operational.'* In such scenarios, a creator's
ability to answer questions about where certain operations were performed may be
instrumental For answering further questions about what the system was doing, as well
as Whyand how.

Surprisingly perhaps, where-questions may sometimes even be important to
stakeholders other than creators. Although many computing systems are colloquially
said to be realized “in the cloud”, what is actually meant is that they are implemented in
a device (or a cluster of devices) that is physically Far-rremoved from the stakeholders in
the relevant ecosystem. Given the differences in the ways different countries regulate
the storage of information, data- and decision-subjects alike may want to know the
particular jurisdiction under which—i.e., where in the world—their data is being stored
and processed. In a related way, examiners may be tasked with developing and enforcing
legislation that limits the extent to which data can be exported. Thus, whereas where-
questions are typically within the purview of creators, there are situations in which these
questions may also become important to other stakeholders within the ML ecosystem.

5.3. Feature-detector identification

One technique for answering how- and possibly even where-questions involves the

15 Curiously, in such scenarios a computing system's hardware components become analogous to the
"black box" voice-recorders used on commercial airliners.

Page 19
identification of feature-detectors. Just as what- and why-questions can be answered by
highlighting Features of the input that bear a high responsibility For the production of
certain outputs, how-questions can sometimes be answered by highlighting those
features of the input that are most responsible For activity in certain mediating
variables. Insofar as the relevant variables are sensitive to a particular Feature (i.e.,
respond reliably when the Feature is present), relatively unique (i.e., no other mediating
variables are similarly sensitive), and causally efficacious (i.e., they significantly influence
the system's overall output), those variables can be viewed as feature-detectors.
Identifying a system’s feature-detectors—assuming there are any—and exploring their
influence on the system’s overall behavior serves well for the purposes of answering
questions about Aowthat system works, and sometimes, even about where the relevant
feature-detecting operations are carried out.

Consider a recent study due to Bau et al (2018). This study considers generative
adversarial networks (GANs) that are capable of producing photorealistic images of
scenes depicting, for example, christian churches (Figure 5a). The aim of the study is to
identify Feature-detectors and explore the systematic effects of surgical interventions
on these detectors."® To this end, Bau et al. “dissect” the relevant networks to identify
the units or unit clusters that are sensitive and unique with respect to recognizable
features such as trees (Figure 5b). Subsequently, they determine the causal efficacy of
these (clusters of) units by performing a series of interventions such as activating or
ablating (i.e., de-activating) the relevant units. Indeed, through these kinds of
interventions the authors are able to systematically control the presence or absence of
features in the generated images, without compromising the overall image quality
(Figures 5c and 5d).

16 Strictly speaking, because the aim of the GANs in this study is not detection but generation, the
relevant units might more appropriately be called Feature-generators.

Page 20
   

(c) Ablating units removes trees

ea li

7
(d) Activating units adds trees

     

 

Figure 5. Images generated by a generative adversarial network:
(a) generated images of christian churches and their surroundings;
(b) features detected by the feature-detector for trees; (c) images
generated after ablating the feature-detector for trees; (d) images
generated after activating the feature-detector for trees.
Reproduced from Bau et al. 2018.

 

 

 

Bau et al.’s study exhibits many of the hallmarks of well-answered how-questions.
For one, it shows that interventions on feature-detectors can be used to repair or
otherwise improve the relevant system's performance—a typical task for creators. For
example, by identifying and subsequently ablating the feature-detectors For unwanted
visual artifacts such as textured patterns where there should be none, they are able to
successfully remove those artifacts from the generated images and therefore improve
the system’s overall performance. For another, most of the Features being detected by
the networks in the study are robust with respect to nuisance variations in color, texture
and spatial orientation. Thus, the GANs appear to learn just the kinds of high-level
representations that one would expect to Find in high-dimensional complex systems
programmed using Machine Learning. Indeed, the authors even advance the hypothesis
that the networks’ representations resemble the conceptual representations that are
used by human brains (see also: Buckner, 2018).

As this example shows, the identification of Feature-detectors is an effective
technique For answering how-questions in the ML ecosystem. That said, it is worth
considering the extent to which this technique might also be used to answer where-
questions. Indeed, insofar as a network’s Feature-detectors are concentrated ina
relatively small number of units and those units are implemented in neuromorphic
hardware components, the identification of Feature detectors will answer questions

Page 21
about howand where simultaneously. Indeed, in such cases, modifications to the
networks’ overall behavior could equally be achieved by, For example, removing or
replacing the hardware components that implement specific detectors.

5.4. Diagnostic classification

Feature-detectors can be invoked to answer questions about how and even where, but
only when the responsibility For generating particular outputs is concentrated in a
relatively small number of system variables (e.g., a small number of network units). In
contrast, they cannot normally be invoked when the responsibility is distributed accross
a large number of variables (e.g., a layer or network as a whole). Indeed, decades-long
discussions of connectionist modeling methods in cognitive science suggest that neural
networks and similarly high-dimensional systems are very likely to exhibit this kind of
distributed responsibility (Smolensky, 1988), and thus, are likely to deploy distributed
representations (Clark, 1993). For this reason, investigators in Explainable Al have good
reason to develop alternative techniques that can be used to answer how-questions
even when no clearly circumscribed Feature-detectors can be Found.

Consider a recent study From computational linguistics, in which Hupkes et al.
(2018) explore the capacity of different networks to evaluate expressions with nested
grammatical structures. It has long been known that simple recurrent networks (SRNs,
Elman, 1990), like other networks with recurrent feedback connections, perform well
when confronted with tasks of this nature. What remains unclear, however, is exactly
how these networks do what they do, and in particular, how they store and deploy
information over extended periods of time. To wit, assuming that a nested arithmetic
expression such as “(5-((2-3)+7))” is processed from left to right, some symbols
encountered early (e.g., the ‘5’ and the leading ‘-’) will have to be evaluated only after
other symbols are encountered later.

Hupkes et al.’s challenge is to determine whether networks capable of evaluating
such nested expressions do so by Following either one of two strategies: a recursive
strategy in which parenthetical clauses are evaluated only once they have been closed
(2-3=-1; -1+7=6; 5-6=-1), or a cumulative strategy, in which parentheses are removed from
left to right while appropriately “Flipping” the operators between ‘+’ and ‘*-’ (5-2=3;
3+3=6; 6-7=-1). In order to answer this question about Aow the relevant networks do
what they do, Hupkes et al. deploy diagnostic classifiers: secondary networks that take as
inputs the primary network’s hidden-unit activations while a particular symbol is being
processed, and that generate as output a value that can be compared to a prior
hypothesis about the information that should be represented at that moment. In the
present example, the diagnostic classifiers’ outputs are compared to the information
that should be represented if the recurrent network were to Follow either one of the
relevant strategies. For example, after processing the ‘3’ in the arithmetic expression
“(5-((2-3)+7))”, a recurrent network that adheres to the recursive strategy should
represent an intermediate sum of -1, whereas one that Follows the cumulative strategy
should represent a 6. Indeed, Hupkes et al. Find that the diagnostic classifiers generate
outputs more in line with the cumulative strategy than with the recursive strategy

Page 22
(Figure 6).

 

 

5
-10
15

 

(0 (-6+10)-(((5-7)-6)--6))- (10-(8+5)))

 

 

 

 

 

 

 

 

—30 - a
(((-940)+(64+(-8-7)))+( (-34+-1)-(6-2))) OCT (erio)-(((5-7)-6)-6))~ (40. (8451)

 

Figure 6. Diagnostic classifier outputs (y-axis) for the cumulative
(top) and recursive (bottom) strategies over the course of two
distinct expressions (x-axis). The classifiers’ outputs (solid line) are
compared to the prior hypotheses (dashed line), revealing an over-
all closer fit to the cumulative than to the recursive strategy.
Reproduced from Hupkes et al. (2018).

 

 

 

Like Feature-detectors, diagnostic classifiers can be used to answer questions
about fowa particular system does what it does. More precisely, they can be used to
determine which information is represented by a system when it receives a particular
input, and thus, how that network processes information as the inputs change over time.
In asense, therefore, diagnostic classifiers can be used to trace the particular computer
program—in this case understood as a series of transitions between information-bearing
states—that is executed by networks that are capable of solving complex problems in Al.

Diagnostic classification has at least one important advantage over techniques
that center on feature-detectors, but also some significant disadvantages. On the one
hand, diagnostic classifiers do not require that the representations in the relevant
system be contained in a small number of variables. For this reason, they appear well-
suited for answering how-questions even when—as is often likely to be the case—
networks solve Al problems by manipulating distributed representations. On the other
hand, diagnostic classifiers may be thought to be of comparatively limited explanatory
value with respect to how-questions insofar as they do not afford the ability to intervene
on these systems’ behavior. Whereas Feature-detectors can be ablated or activated, and
the resultant effects can be recorded, it is not clear how a diagnostic classifier’s outputs
can be used to systematically modify a system’s behavior. Moreover, whereas in certain
cases Feature-detectors may be cited to answer where-questions in addition to
answering how-questions, diagnostic classifiers are unlikely to address the former other
than by indicating that certain representations are realized somewhere within the
system as a whole.

Page 23
6. Conclusion

This discussion has sought to develop a normative Framework for Explainable Artificial
Intelligence—a set of requirements that should be satisfied For the purposes of
rendering opaque computing systems transparent. Beginning with an analysis of
‘opacity’ from philosophy of science, it was shown that opacity is agent-relative, that that
rendering transparent involves acquiring knowledge of epistemically relevant elements.
Notably, given their distinct roles in the Machine Learning ecosystem, different
stakeholders were shown to require knowledge of different (albeit equally “real”)
epistemically relevant elements. In this sense, although the opacity of ML-programmed
computing systems is traditionally said to give rise to the Black Box Problem, it may in
fact be more appropriate to speak of many Black Box Problems. Depending on who you
are and how it is that you interact with an ML-programmed computer, that computer will
be opaque For different reasons, and must be rendered transparent in different ways.

Explainable Artificial Intelligence is in the business of developing analytic
techniques with which to render opaque computing systems transparent, and thus, to
allow the stakeholders in an ML ecosystem to better develop, operate, trust, examine,
and otherwise interact with ML-programmed computing systems. But although many
powerful analytic techniques have already been developed, not enough is known about
when and how these techniques actually explain—that is, when and in which sense these
techniques render computing systems transparent. Here, the present discussion sought
inspiration in cognitive science. As one of the most influential normative Frameworks For
evaluating the explanatory successes of analytic techniques in cognitive science and
neuroscience, Marr’s levels of analysis account was used to determine what it takes to
satisFy the distinct explanatory requirements of different stakeholders in the ML
ecosystem. By aligning these stakeholders with different kinds of questions—
specifically, questions about what, why, how, and where—and by specifying the kinds of
EREs that should be invoked in order to answer these questions, it was possible to
develop a normative Framework with which to evaluate the explanatory contributions of
analytic techniques From Explainable Al.

Finally, by reviewing a series of illustrative examples, it was argued that many
opaque computing systems can already be rendered transparent in the sense required
by specific stakeholders. This review demonstrated that it is increasingly possible to not
only answer operators’ questions about what a computing system is doing, but to also
answer data- and decision-subjects’ questions about why, as well as creators’ and
examiners’ questions about howthe system does what it does, and in certain
circumstances, even questions about where. Thus, Explainable Al appears to be well-
equipped For answering the questions that are most likely to be asked by different
stakeholders, and thus, for Finding solutions to the many Black Box Problems.

That said, it is important to recognize that the analytic techniques that have thus
far been developed have certain characteristic limitations—as well as to consider
whether, and if so how, these limitations might eventually be overcome. As was the case
for input heatmapping, the techniques of diagnostic classification and feature-detector-

Page 24
identification work relatively well when a system’s variables can be interpreted
semantically—that is, when they can be thought to represent recognizable Features of
the environment. Indeed, feature-detectors are only as informative as the Features being
detected can be recognized by human observers, and diagnostic classifiers require
investigators to already possess a detailed understanding of the programs that might be
executed by the system whose behavior is being explained. As has already been
suggested, however, there are reasons to believe that the environmental Features being
detected and the correlations being learned through the use of Machine Learning may
be subtle and difficult to interpret. For this reason, although promising, these XAI
techniques are likely to be limited in scope and utility.

To what extent can this scope and utility be increased? Here it may once again be
worth seeking guidance in cognitive science. There, one long-term trend is the gradual
recognition that semantic interpretability is not always necessary to explain the behavior
of humans and other biological cognizers. Indeed, it is now Far less widely assumed than
before that the neuronal processes described by cognitive models should—or even can
—be described in semantic terms (Chemero, 2000; Ramsey, 1997). For this reason, many
cognitive models today instead deploy sophisticated mathematical concepts and analytic
techniques For idealizing, approximating, or otherwise reducing the dimensionality of
the systems being investigated, even if they do not thereby render these systems
semantically interpretable. Although the use of these concepts and techniques may
render Folk psychological categories inapplicable, they might nevertheless be used to
satisfy important explanatory norms such as description, prediction, and intervention.

In conclusion, it seems Fair to wonder whether Explainable Al might similarly move
away From semantic interpretability, and toward idealization, approximation, and
dimension-reduction. On the one hand, Artificial Intelligence is in part an engineering
discipline tasked with developing technologies that improve the daily lives of regular
individuals. For this reason, it is subject to constraints that pure sciences are not.
Whereas it may not be important For laypeople to understand the processes that
underlie visual perception, it is important that they know why their loan applications are
getting rejected. On the other hand, just as society trusts scientists to possess expert
knowledge that remains inaccessible to laypeople, it may accept that certain ML
applications can only be explained by mastering the most sophisticated XAI techniques.
In this case, it may be necessary to rely on societal mechanisms—for example, the
testimony of an expert witness in a court of law—to ensure that an individual's rights are
being protected even when that individual cannot him- or herself come to know the
relevant Facts. Thus, although semantic interpretability may often be beneficial, it may
not always be essential.

Page 25
References

Bau, D., Zhu, J.-Y., Strobelt, H., Zhou, B., Tenenbaum, J. B., Freeman, W. T., & Torralba, A.
(2018). GAN Dissection: Visualizing and Understanding Generative Adversarial
Networks. ArXiv, 1811.10597.

Bechtel, W., & Richardson, R. C. (1993). Discovering Complexity: Decomposition and
Localization as Strategies in Scientific Research (MIT Press ed.). Cambridge, Mass:
MIT Press.

Bickle, J. (2006). Reducing mind to molecular pathways: explicating the reductionism
implicit in current cellular and molecular neuroscience. Synthese, 151(3), 411-434.
https://doi.org/10.1007/s11229-006-9015-2

Buckner, C. (2018). Empiricism without magic: transformational abstraction in deep
convolutional neural networks. Synthese, 195(12), 5339-5372.
https://doi.org/10.1007/s11229-018-01949-1

Burrell, J. (2016). How the machine ‘thinks’: Understanding opacity in machine learning
algorithms. Big Data & Society, 3(1), 205395171562251.
https://doi.org/10.1177/2053951715622512

Busemeyer, J. R., & Diederich, A. (2010). Cognitive modeling. Sage.

Chemero, A. (2000). Anti-Representationalism and the Dynamical Stance. Philosophy of
Science.

Churchland, P. M. (1981). Eliminative Materialism and the Propositional Attitudes. The
Journal of Philosophy, 78(2), 67-90.

Clark, A. (1993). Associative engines: Connectionism, concepts, and representational
change. MIT Press.

Doran, D., Schulz, S., & Besold, T. R. (2017). What Does Explainable Al Really Mean? A New
Conceptualization of Perspectives. ArXiv, 1710.00794.

Duran, J. M., & Formanek, N. (2018). Grounds for Trust: Essential Epistemic Opacity and
Computational Reliabilism. Minds and Machines, 28(4), 645-666.
https://doi.org/10.1007/s11023-018-9481-6

Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14, 179-211.

Page 26
European Commission. Regulation (EU) 2016/679 of the European Parliament and of the
Council of 27 April 2016 on the protection of natural persons with regard to the
processing of personal data and on the free movement of such data, and repealing
Directive 95/46/EC (General Data Protection Regulation). , (2016).

Fodor, J. A. (1987). Psychosemantics. Cambrdige, MA: MIT Press.

Goodman, B., & Flaxman, S. (2016). European Union regulations on algorithmic decision-
making and a" right to explanation". ArXiv, 1606.08813.

Guidotti, R., Monreale, A., Ruggieri, S., Pedreschi, D., Turini, F., & Giannotti, F. (2018).
Local Rule-Based Explanations of Black Box Decision Systems. ArXiv, 1805.10820.

Hohman, F. M., Kahng, M., Pienta, R., & Chau, D. H. (2018). Visual Analytics in Deep
Learning: An Interrogative Survey for the Next Frontiers. /EFF Transactions on
Visualization and Computer Graphics.

Humphreys, P. (2009). The philosophical novelty of computer simulation methods.
Synthese, 169(3), 615-626.

Hupkes, D., Veldhoen, S., & Zuidema, W. (2018). Visualisation and’diagnostic classifiers’
reveal how recurrent and recursive neural networks process hierarchical
structure. Journal of Artificial intelligence Research, 61, 907-926.

Lake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2017). Building machines
that learn and think like people. Behavioral and Brain Sciences.

Lipton, Z. C. (2016). The mythos of model interpretability. ArXiv, 1606.03490.

Marcus, G. (2018). Deep learning: A critical appraisal. ArXiv, 1801.00631.

Marr, D. (1982). Vision: a computational investigation into the human representation and
processing of visual information. Cambridge, MA: MIT Press.

McClamrock, R. (1991). Marr’s three levels: A re-evaluation. Minds and Machines, 1(2),
185-196.

Montavon, G., Samek, W., & Miller, K.-R. (2018). Methods For interpreting and
understanding deep neural networks. Digital Signal Processing, 73, 1-15.
https://doi.org/10.1016/j.dsp.2017.10.011

Pfeiffer, M., & Pfeil, T. (2018). Deep Learning With Spiking Neurons: Opportunities and

Page 27
Challenges. Frontiers in Neuroscience, 12.
https://doi.org/10.3389/Fnins.2018.007 74

Piccinini, G., & Craver, C. F. (2011). Integrating psychology and neuroscience: Functional
analyses as mechanism sketches. Synthese, 183(3), 283-311.
https://doi.org/10.1007/s11229-011-9898-4

Pylyshyn, Z. W. (1984). Computation and Cognition. Cambridge, MA: MIT Press.

Rahwan, I., Cebrian, M., Obradovich, N., Bongard, J., Bonnefon, J.-F., Breazeal, C., ...
Wellman, M. (2019). Machine behaviour. Nature, 568(7753), 477-486.
https://doi.org/10.1038/s41586-019-1138-y

Ramsey, W. (1997). Do connectionist representations earn their explanatory keep? Mind
& Language, 12(1), 34-66.

Ras, G., van Gerven, M., & Haselager, P. (2018). Explanation methods in deep learning:
Users, values, concerns and challenges. In Explainable and interpretable Models in
Computer Vision and Machine Learning (pp. 19-36). Springer.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should | Trust You?”: Explaining the
Predictions of Any Classifier. ArXiv, 1602.04938v3.

Rieder, G., & Simon, J. (2017). Big Data: A New Empiricism and its Epistemic and Socio-
Political Consequences. In Berechenbarkeit der Welt? Philosophie und Wissenschaft
im Zeitalter von Big Data (pp. 85-105). Wiesbaden: Springer VS.

Shagrir, O. (2010). Marr on computational-level theories. Philosophy of Science, 77(4),
477-500.

Shallice, T., & Cooper, R. P. (2011). The Organisation of Mind. Oxford: Oxford University
Press.

Smolensky, P. (1988). On the proper treatment of connectionism. Behavioral and Brain
Sciences, 11(1), 1-23.

Stinson, C. (2016). Mechanisms in psychology: ripping nature at its seams. Synthese,
193(5), 1585-1614. https://doi.org/10.1007/s11229-015-0871-5

Szegedy, C., Zaremba, W., Sutskever, |., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R.
(2013). Intriguing properties of neural networks. ArXiv, 1312.6199.

Page 28
Tomsett, R., Braines, D., Harborne, D., Preece, A., & Chakraborty, S. (2018). Interpretable
to Whom? A Role-based Model For Analyzing Interpretable Machine Learning
Systems. ArXiv, 1806.07552.

Wachter, S., Mittelstadt, B., & Floridi, L. (2017). Why a Right to Explanation of Automated
Decision-Making Does Not Exist in the General Data Protection Regulation.
International Data Privacy Law, 2017.

Zednik, C. (2017). Mechanisms in Cognitive Science. In S. Glennan & P. Illari (Eds.), The
Routledge Handbook of Mechanisms and Mechanical Philosophy (pp. 389-400).
London: Routledge.

Zednik, C. (2018). Will Machine Learning Yield Machine Intelligence? In Philosophy and
Theory of Artificial Intelligence 2017.

Zerilli, J., Knott, A., Maclaurin, J., & Gavaghan, C. (2018). Transparency in Algorithmic and
Human Decision-Making: Is There a Double Standard? Philosophy & Technology.
https://doi.org/10.1007/s13347-018-0330-6

Page 29
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

4793

A Survey on Explainable Artificial Intelligence
(XAI): Toward Medical XAI

Erico Tjoa® and Cuntai Guan®, Fellow, IEEE

Abstract— Recently, artificial intelligence and machine
learning in general have demonstrated remarkable performances
in many tasks, from image processing to natural language
processing, especially with the advent of deep learning (DL).
Along with research progress, they have encroached upon many
different fields and disciplines. Some of them require high level of
accountability and thus transparency, for example, the medical
sector. Explanations for machine decisions and predictions are
thus needed to justify their reliability. This requires greater
interpretability, which often means we need to understand
the mechanism underlying the algorithms. Unfortunately,
the blackbox nature of the DL is still unresolved, and many
machine decisions are still poorly understood. We provide a
review on interpretabilities suggested by different research works
and categorize them. The different categories show different
dimensions in interpretability research, from approaches that
provide “obviously” interpretable information to the studies
of complex patterns. By applying the same categorization to
interpretability in medical research, it is hoped that: 1) clinicians
and practitioners can subsequently approach these methods with
caution; 2) insight into interpretability will be born with more
considerations for medical practices; and 3) initiatives to push
forward data-based, mathematically grounded, and technically
grounded medical education are encouraged.

Index Terms—Explainable artificial intelligence (XA, inter-
pretability, machine learning (ML), medical information system,
survey.

I. INTRODUCTION

ACHINE learning (ML) has grown large in both

research and industrial applications, especially with the
success of deep learning (DL) and neural networks (NNs),
so large that its impact and possible after-effects can no longer
be taken for granted. In some fields, failure is not an option:
even a momentarily dysfunctional computer vision algorithm
in autonomous vehicle easily leads to fatality. In the medical
field, clearly human lives are on the line. Detection of a disease
at its early phase is often critical to the recovery of patients or
to prevent the disease from advancing to more severe stages.
While ML methods, artificial NNs, brain—machine interfaces,
and related subfields have recently demonstrated promising

Manuscript received October 15, 2019; revised June 7, 2020 and August 10,
2020; accepted September 24, 2020. Date of publication October 20, 2020;
date of current version October 28, 2021. This work was supported by the
Health-Al Division, DAMO Academy, Alibaba Group Holding Ltd., through
the Alibaba-NTU Talent Program. (Corresponding author: Erico Tjoa.)

Erico Tjoa was with the HealthTech Division, Alibaba Group Holding Ltd.,
Hangzhou 311121, China. He is now with the School of Computer Sci-
ence and Engineering, Nanyang Technological University, Singapore 639798
(e-mail: ericotjo001@e.ntu.edu.sg).

Cuntai Guan is with the School of Computer Science and Engineering,
Nanyang Technological University, Singapore 639798.

Digital Object Identifier 10.1109/TNNLS.2020.3027314

performance in performing medical tasks, they are hardly
perfect [1]-[9].

Interpretability and explainability of ML algorithms have
thus become pressing issues: who is accountable if things go
wrong? Can we explain why things go wrong? If things are
working well, do we know why and how to leverage them
further? Many articles have suggested different measures and
frameworks to capture interpretability, and the topic explain-
able artificial intelligence (XAI) has become a hotspot in
ML research community. Popular DL libraries have started
to include their own XAT libraries, such as Pytorch Captum
and tensorflow tf-explain. Furthermore, the proliferation of
interpretability assessment criteria (such as reliability, causal-
ity, and usability) helps ML community keep track of how
algorithms are used and how their usage can be improved,
providing guiding posts for further developments [10]-[12].
In particular, it has been demonstrated that visualization is
capable of helping researchers detect erroneous reasoning
in classification problems that many previous researchers
possibly have missed [13].

The above said, there seems to be a lack of uniform
adoption of interpretability assessment criteria across the
research community. There have been attempts to define
the notions of “interpretability,” “explainability” along with
“reliability,” “trustworthiness,” and other similar notions with-
out clear expositions on how they should be incorporated
into the great diversity of implementations of ML mod-
els; consider [10] and [14]-[18]. In this survey, we will
instead use “explainability” and “interpretability” interchange-
ably, considering a research to be related to interpretability
if it does show any attempts: 1) to explain the decisions
made by algorithms; 2) to uncover the patterns within the
inner mechanism of an algorithm; and 3) to present the
system with coherent models or mathematics, and we will
include even loose attempts to raise the credibility of machine
algorithms.

In this work, we survey through research works related to
the interpretability of ML or computer algorithms in general,
categorize them, and then apply the same categories to inter-
pretability in the medical field. The categorization is especially
aimed to give clinicians and practitioners a perspective on the
use of interpretable algorithms that are available in diverse
forms. The tradeoff between the ease of interpretation and the
need for specialized mathematical knowledge may create a
bias in preference for one method when compared to another
without justification based on medical practices. This may
further provide a ground for specialized education in the
medical sector that is aimed to realize the potentials that

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
4794

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

TABLE I

LIST OF JOURNAL ARTICLES ARRANGED ACCORDING TO THE INTERPRETABILITY METHODS USED, HOW INTERPRETABILITY IS PRESENTED OR THE
SUGGESTED MEANS OF INTERPRETABILITY. THE TABULATION PROVIDES A NONEXHAUSTIVE OVERVIEW OF INTERPRETABILITY METHODS,
PLACING SOME DERIVATIVE METHODS UNDER THE UMBRELLA OF THE MAIN METHODS THEY DERIVE FROM. HSI: HUMAN STUDY ON
INTERPRETABILITY Y MEANS THERE 1S HUMAN STUDY DESIGNED TO VERIFY IF THE SUGGESTED METHODS ARE INTERPRETABLE
BY THE HUMAN SUBJECT. ANN: / MEANS EXPLICITLY INTRODUCES NEW ARTIFICIAL NN ARCHITECTURE, MODIFIES
EXISTING NETWORKS OR PERFORMS TESTS ON NNs

 

Methods

HSI ANN Mechansim

 

CAM with global average pooling [42], [91]

+ Grad-CAM [43] generalizes CAM, utilizing gradient
+ Guided Grad-CAM and Feature Occlusion [68]

+ Respond CAM [44]

+ Multi-layer CAM [92]

LRP (Layer-wise Relevance Propagation) [13], [53]

+ Image classifications. PASCAL VOC 2009 etc [45]
+ Audio classification. AudioMNIST [47]

+ LRP on DeepLight. fMRI data from Human Connectome Project [48]

+ LRP on CNN and on BoW(bag of words)/SVM [49]

+ LRP on compressed domain action recognition algorithm [50]
+ LRP on video deep learning, selective relevance method [52]
+ BiLRP [51]

DeepLIFT [57]

Prediction Difference Analysis [58]

Slot Activation Vectors [41]

PRM (Peak Response Mapping) [59]

LIME (Local Interpretable Model-agnostic Explanations) [14]
+ MUSE with LIME [85]

+ Guidelinebased Additive eXplanation optimizes complexity, similar to LIME [93]

# Also listed elsewhere: [56], [69], [71], [94]

Others. Also listed elsewhere: [95]

+ Direct output labels. Training NN via multiple instance learning [65]
+ Image corruption and testing Region of Interest statistically [66]

+ Attention map with autofocus convolutional layer [67]

x

Decomposition

RNA ARBAN SKK KAAS
Aouaryes

HK WK DW DW De DE De De De De De De De De eS

Aypqeieidreyuy sandeoreg

 

DeconvNet [72]

Inverting representation with natural image prior [73]
Inversion using CNN [74]

Guided backpropagation [75], [91]

Activation maximization/optimization [38]
+ Activation maximization on DBN (Deep Belief Network) [76]

+ Activation maximization, multifaceted feature visualization [77]

Visualization via regularized optimization [78]
Semantic dictionary [39]

Network dissection [36], [37]

Inversion

Teusis

 

 

Decision trees

Propositional logic, rule-based [82]

Sparse decision list [83]

Decision sets, rule sets [84], [85]
Encoder-generator framework [86]

Filter Attribute Probability Density Function [87]

MUSE (Model Understanding through Subspace Explanations) [85]

Verbal

 

 

Ke BV
KS EIS

 

reside within these algorithms. We also find that many journal
articles in the ML and AI community are algorithm-centric.
They often assume that the algorithms used are obviously
interpretable without conducting human subject tests to verify
their interpretability (see column HSI of Tables I and II). It is
noted that assuming that a model is obviously interpretable
is not necessarily wrong, and, in some cases human tests
might be irrelevant (for example, predefined models based on
commonly accepted knowledge specific to the content-subject
may be considered interpretable without human subject tests).
In the tables, we also include a column to indicate whether
the interpretability method applies for artificial NN, since the
issue of interpretability is recently gathering attention due to
its blackbox nature.

We will not attempt to cover all related works many
of which are already presented in the research articles and
survey we cite [1], [2], [15]-[30]. We extend the so-called

integrated interpretability [16] by including considerations for
subject-content-dependent models. Compared to [17], we also
overview the mathematical formulation of common or pop-
ular methods, revealing the great variety of approaches to
interpretability. Our categorization draws a starker borderline
between the different views of interpretability that seem to be
difficult to reconcile. In a sense, our survey is more suitable for
technically oriented readers due to some mathematical details,
although casual readers may find useful references for relevant
popular items, from which they may develop interests in this
young research field. Conversely, algorithm users that need
interpretability in their work might develop an inclination to
understand what is previously hidden in the thick veil of math-
ematical formulation, which might ironically undermine relia-
bility and interpretability. Clinicians and medical practitioners
already having some familiarity with mathematical terms
may get a glimpse on how some proposed interpretability
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

4795

TABLE IL

(CONTINUED FROM TABLE I) LIST OF JOURNAL ARTICLES ARRANGED ACCORDING TO THE
INTERPRETABILITY METHODS USED, HOW INTERPRETABILITY IS PRESENTED OR
THE SUGGESTED MEANS OF INTERPRETABILITY

 

Methods

Mechanism

 

Linear probe [101]
Regression based on CNN [106]
Backwards model for interpretability of linear models [107]

GDM (Generative Discriminative Models): ridge regression + least square [100]

GAM, GA?M (Generative Additive Model) [82], [102], [103]
ProtoAttend [105]

Other content-subject-specific models:

+ Kinetic model for CBF (cerebral blood flow) [131]

+ CNN for PK (Pharmacokinetic) modelling [132]

+ CNN for brain midline shift detection [133]

+ Group-driven RL (reinforcement learning) on personalized healthcare [134]

+ Also see [108]-[112]

Pre-defined models

 

PCA (Principal Components Analysis), SVD (Singular Value Decomposition)

CCA (Canonical Correlation Analysis) [113]

SVCCA (Singular Vector Canonical Correlation Analysis) [97] = CCA+SVD
F-SVD (Frame Singular Value Decomposition) [114] on electromyography data

DWT (Discrete Wavelet Transform) + Neural Network [135]

MODWPT (Maximal Overlap Discrete Wavelet Package Transform) [136]

GAN-based Multi-stage PCA [118]

Estimating probability density with deep feature embedding [119]

t-SNE (t-Distributed Stochastic Neighbour Embedding) [77]
+ t-SNE on CNN [120]

+ t-SNE, activation atlas on GoogleNet [121]

+ t-SNE on latent space in meta-material design [122]

+ t-SNE on genetic data [137]

+ mm-t-SNE on phenotype grouping [138]

Laplacian Eigenmaps visualization for Deep Generative Model [124]

KNN (k-nearest neighbour) on multi-center low-rank rep. learning (MCLRR) [125]
KNN with triplet loss and query-result activation map pair [139]

Group-based Interpretable NN with RW-based Graph Convolutional Layer [123]

Correlation

Rm I 6645 25m eZ

Clustering

uonoenxy amyeey
among TeopemomeW eta Amqererdzoyuy

 

 

TCAV (Testing with Concept Activation Vectors) [96]

+ RCV (Regression Concept Vectors) uses TCAV with Br score [140]

+ Concept Vectors with UBS [141]

+ ACE (Automatic Concept-based Explanations) [56] uses TCAV
Influence function [129] helps understand adversarial training points

Representer theorem [130]

SocRat (Structured-output Causual Rationalizer) [127]
Meta-predictors [126]

Explanation vector [128]

# Also listed elsewhere: [14], [43], [85], [94]

Sensitivity

 

# Also listed elsewhere: [14], [60], [85] etc

Optimization

 

CNN with separable model [142]
Information theoretic: Information Bottleneck [98], [99]

Others

 

Database of methods v.s. interpretability [10]
Case-Based Reasoning [143]

Data Driven

 

Integrated Gradients [69], [94]
Input invariance [71]

Tnvariance

 

Application-based [144], [145]
Human-based [146], [147]
Function-based [2], [5], [42]-[44], [96], [97], [144], [145]

z de] SZ] oe 3q] 2] oe 36 36 3090 S30 96 S90 80 86 96 96 9 EE
Bl aia Bln AE 44K KA SK 8 8K 8 8

‘dsiag EMO

Utilities

 

 

 

methods might be risky and unreliable. The survey [30] views
interpretability in terms of extraction of relational knowledge,
more specifically, by scrutinizing the methods under neural-
symbolic cycle. It presents the framework as a subcategory
within the interpretability literature. We include it under verbal
interpretability, though the framework does demonstrate that
methods in other categories can be perceived under verbal
interpretability as well. The extensive survey [18] provides a
large list of researches categorized under transparent model
and models requiring post hoc analysis with multiple subcat-
egories. Our survey, on the other hand, aims to overview the
state of interpretable ML as applied to the medical field.

This article is arranged as the following. Section II intro-
duces generic types of interpretability and their subtypes.
In each section, where applicable, we provide challenges and
future prospects related to the category. Section III applies the
categorization of interpretabilities in Section II to medical field
and lists a few risks of machine interpretability in the medical
field. Before we proceed, it is also imperative to point out that
the issue of accountability and interpretability has spawned
discussions and recommendations [31]-[33], and even entered
the sphere of ethics and law enforcements [34], engendering
movements to protect the society from possible misuses and
harms in the wake of the increasing use of AI.
4796

Category Mechanism

 

| __— Sensitivity

       
 
    
 
  

Saliency
Signal
Verbal

— Decomposition

 

+ Optimization

Srna ~ Inversion

eh al LY
lee

Pre-defined

Models

Fealure
extraction

\
“ Others

Fig. 1. Overview of categorization with illustration. Orange box: inter-
pretability interface to demarcate the separation between interpretable infor-
mation and the cognitive process required to understand them. Gray box:
algorithm output/product that is proposed to provide interpretability. Black
arrow: computing or comprehension process. The perceptive interpretability
methods generate items that are usually considered immediately interpretable.
On the other hand, methods that provide interpretability via mathematical
structure generate outputs that require one more layer of cognitive processing
interface before reaching the interpretable interface. The eyes and ear icons
represent human senses interacting with items generated for interpretability.

II. TYPES OF INTERPRETABILITY

There has yet to be a widely adopted standard to understand
ML interpretability, though there have been works proposing
frameworks for interpretability [10], [13], [35]. In fact, differ-
ent works use different criteria, and they are justifiable in one
way or another. Network dissection has been suggested [36],
[37] to evaluate the interpretability of visual representations
in deep NN (DNN) inspired by neuroscientists’ procedures to
understand biological neurons. The articles also offer a way
to quantify neuronal network units’ activation in response to
different concepts detected. The interactive websites [38], [39]
have suggested a unified framework to study interpretabilities
that have thus-far been studied separately. The article [40]
defines a unified measure of feature importance in the SHapley
Additive exPlanations (SHAP) framework. Here, we catego-
rize existing interpretabilities and present a nonexhaustive list
of works in each category.

The two major categories presented here, namely perceptive
interpretability and interpretability by mathematical structures,
as illustrated in Fig. 1, appear to present different polari-
ties within the notion of interpretability. An example of the
difficulty with perceptive interpretability is as the following.
When a visual “evidence” is given erroneously, the algorithm
or method used to generate the “evidence” and the underlying
mathematical structure sometimes do not offer any useful clues
on how to fix the mistakes. On the other hand, a mathematical
analysis of patterns may provide information in high dimen-
sions. They can only be easily perceived once the pattern is
brought into lower dimensions, abstracting some fine-grained
information we could not yet prove is not discriminative with
measurable certainty.

A. Perceptive Interpretability

We include in this category interpretabilities that can be
humanly perceived, often one that will be considered “obvi-
ous.” For example, as shown in Fig. 2(a2), an algorithm that

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

classifies an image into the “cat” category can be considered
obviously interpretable if it provides segmented patch showing
the cat as the explanation. We should note that this alone
might on the other hand be considered insufficient, because it:
1) still does not unblackbox an algorithm and 2) ignores the
possibility of using background objects for its decision. The
following are the subcategories to perceptive interpretability.
Refer to Fig. 3 for the overview of the common subcategories.

1) Saliency: Saliency method explains the decision of an
algorithm by assigning values that reflect the importance of
input components in their contribution to that decision. These
values could take the forms of probabilities and super-pixels
such as heatmaps etc. For example, Fig. 2(al) shows how a
model predicts that the patient suffers from flu from a series of
factors, but LIME [14] explains the choice by highlighting the
importance of the particular symptoms that indicate that the
illness should indeed be flu. Similarly, Jacovi er al. [41] com-
putes the scores reflecting the n-grams activating convolution
filters in natural language processing (NLP). Fig. 2(a2) demon-
strates the output that LIME will provide as the explanation for
the choice of classifications “cat” and Fig. 2(a3) demonstrates
a kind of heatmap that shows the contribution of pixels to the
segmentation result (segmentation result not shown, and this
figure is only for demonstration). More formally, given that
model f makes a prediction y = f(x) for input x, for some
metric v, typically large magnitude of v(x;) indicates that the
component x; is a significant reason for the output y.

Saliency methods via decomposition have been devel-
oped. In general, they decompose signals propagated
within their algorithm and selectively rearrange and process
them to provide interpretable information. Class activation
map (CAM) has been a popular method to generate
heat/saliency/relevance-map (from now, we will use the terms
interchangeably) that corresponds to discriminative features
for classifications [42]-[44]. The original implementation of
CAM [42] produces heatmaps using f(x, y), the pixel-wise
activation of unit k across spatial coordinates (x, y) in the
last convolutional layers, weighted by wy, the coefficient
corresponding to unit k for class c. CAM at pixel (x, y) is
thus given by M.(x, y) = Lewe fe (x, y).

Similarly, widely used layer-wise relevance propaga-
tion (LRP) is introduced in [45]. Some articles that use LRP
to construct saliency maps for interpretability include [13]
and [46]-[51]. It is also applicable for video processing [52].
A short summary for LRP is given in [53]. LRP is considered
a decomposition method [54]. Indeed, the importance scores
are decomposed such that the sum of the scores in each
layer will be equal to the output. In short, the relevance
score is the pixel-wise intensity at the input layer R© where
RO =; (a wi)/(Zia wp) ROP is the relevance score
of neuron i at layer / with the input layer being at J = 0.
Each pixel (x, y) at the input layer is assigned the importance
value R(x, y), although some combinations of relevance
scores RO? at inner layer / over different channels {c} have
been demonstrated to be meaningful as well (though possibly
less precise; see the tutorial in its website heatmapping.org).
LRP can be understood in deep Taylor decomposition
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI 4797

headache ~

ee (A2)
(A) nee OO,
weight <—__.- So @

no fatigue “
LIME JL

     

  

ADC with lesion highlighted Conv2d0 layer One of the mixed4 layers

Fig. 2. (al) Using LIME to generate explanation for text classification. Headache and sneeze are assigned positive values. This means both factors have
positive contribution to the model prediction “flu.” On the other hand, weight and no fatigue contribute negatively to the prediction. (a2) LIME is used to
generate the super-pixels for the classification “cat.” (a3) ADC modality of a slice of MRI scan from ISLES 2017 segmentation competition. Reddish intensity
region reflects a possible “explanation” to the choice of segmentation (segmentation not shown). (b) Optimized images that maximize the activation of a
neuron in the indicated layers. In shallower layer, simple patterns activate neurons strongly while in deeper layer, more complex features such as dog faces
and ears do. Figure (b) is obtained from https://distill. pub/2018/building-blocks/ with permission from Chris Olah.

   
   

Output
Bird 99% Duck 1% (c) eee

Output y

  
    

Input x

 

e
ee \ Satine on

fr'fs*0)

   
 
 

 

 

 

1). Files

 

 

 

 

 

Real signal vs. approx. inverse!

 

    
 

 

  

] J ®

 

 

 

 

t——
eh | Model,
Ns MD duck 99% & J ea |—* is scam et, wririrardy
Input x ; [eo .° ~\ eo:
- ; |e 080° t Biservices and HY room. Price s REND
| Model/algorithm Leg el * has typos, fake address = is scam Positive words related to prediction
Logical relation is generated of good review rating are identified

Fig. 3. Overview on perceptive interpretability methods. (a) Saliency method with decomposition mechanism. The input which is an image of a cat is fed
into the model for processing along the blue arrow. The resulting output and intermediate signals (green arrows) are decomposed and selectively picked for
processing, hence providing information for the intermediate mechanism of the model in the form of (often) heatmappings, shown in red/orange/yellow colors.
(b) Saliency method with sensitivity mechanism. The idea is to show how small changes to the input (black figures of birds and ducks) affect the information
extracted for explainability (red silhouette). In this example, red regions indicate high relevance, which we sometimes observe at edges or boundary of objects,
where gradients are high. (c) Signal method by inversion and optimization. Inverses of signals or data propagated in a model could possibly reveal more
sensible information (see arrow labeled “inverse”). Adjusting input to optimize a particular signal (shown as the ith component of the function f|) may provide
us with x, that reveals explainable information (see arrow labeled “optimization”). For illustration, we show that the probability of correctly predicting duck
improves greatly once the head is changed to the head of a duck which the model recognizes. (d) Verbal interpretability is typically achieved by ensuring that
the model is capable of providing humanly understable statements, such as the logical relation or the positive words shown.

framework [55], though, as we speak, many versions of LRP methods that have been developed include, DeepLIFT and gra-
are being developed. The code implementation can also be  dient*input [57], prediction difference analysis [58] and [41].
found in the aforementioned website. Peak response mapping [59] is generated by backpropagating

Automatic concept-based explanations (ACEs) algorithm peak signals. Peak signals are normalized and treated as
[56] uses super-pixels as explanations. Other decomposition probability, and the method can be seen as decomposition
4798

into probability transitions. In [60], removed correlation p
is proposed as a metric to measure the quality of signal
estimators. And then it proposes PatternNet and PatternAt-
tribution that backpropagate parameters optimized against p,
resulting in saliency maps as well. SmoothGrad [61] improves
gradient-based techniques by adding noises. Do visit the
related website that displays numerous visual comparison of
saliency methods; be mindful of how some heatmaps highlight
apparently irrelevant regions.

For NLP or sentiment analysis, saliency map can also take
the form of “heat” scores over words in texts, as demonstrated
by Arras er al. [62] using LRP and by Karpathy et al. [63].
In the medical field (see later section), Irvin et al [6],
Zhao et al. [44], Paschali et al. [64], Couture et al. [65],
Li et al. [66], Qin er al. [67], Tang er al. [68], Papanas-
tasopoulos et al. [69], and Lee et al. [70] have studied
methods employing saliency and visual explanations. It is
noted that we also subcategorize LIME as a method that uses
optimization and sensitivity as its underlying mechanisms,
and many researches on interpretability span more than one
subcategories.

a) Challenges and future prospects: As seen, the formu-
las for CAM and LRP are given on a heuristic: certain ways
of interaction between weights and the strength of activation
of some units within the models will eventually produce the
interpretable information. The intermediate processes are not
amenable to scrutiny. For example, taking one of the weights
and changing its value does not easily reveal any useful
information. How these prescribed ways translate into inter-
pretable information may also benefit from stronger evidences,
especially evidences beyond visual verification of localized
objects. Signal methods to investigate ML models (see later
section) exist, but such methods that probe them with respect
to the above methods have not been attempted systematically,
possibly opening up a different research direction.

2) Signal Method: Methods of interpretability that observe
the stimulation of neurons or a collection of neurons are called
signal methods [71]. On the one hand, the activated values of
neurons can be manipulated or transformed into interpretable
forms. For example, the activation of neurons in a layer can
be used to reconstruct an image similar to the input. This
is possible because neurons store information systematically
[36], [72]: feature maps in the deeper layer activate more
strongly to complex features, such as human face, keyboard,
etc., while feature maps in the shallower layers show simple
patterns such as lines and curves. An example of feature
map is the output of a convolutional filter in a convolutional
NN (CNN). Network dissection procedure evaluates neuronal
unit’s activation by computing its IoU score that is relevant to
a concept in question [36], [37]. On the other hand, parameters
or even the input data might be optimized with respect to the
activation values of particular neurons using methods known
as activation optimization (see a later section). The following
are the relevant subcategories.

a) Feature maps and inversions for input reconstructions:
A feature map often looks like a highly blurred image with
most region showing zero (or low intensity), except for the

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

patch that a human could roughly discern as a detected
feature. Sometimes, these discernible features are considered
interpretable, as in [72]. However, they might be too distorted.

Then, how else can a feature map be related to a humanly
perceptible feature? An inverse convolution map can be
defined: for example, if feature map in layer 2 is computed
in the network via yo = fo(fi(x)) where x is the input,
fi) consists of 7 x 7 convolutions of stride 2 followed
by max-pooling and likewise f2(.). Then [72] reconstructs
an image using a deconvolution network by approximately
inversing the trained convolutional network + = deconv(y) =
fy’ fro) which is an approximation, because layers such
as max-pooling have no unique inverse. It is shown that x
does appear like slightly blurred version of the original image,
which is distinct to human eye. Inversion of image representa-
tions within the layers has also been used to demonstrate that
CNN layers do store important information of an input image
accurately [73], [74]. Guided backpropagation [75] modifies
the way backpropagation is performed to achieve inversion by
zeroing negative signals from both the output or input signals
backwards through a layer. Indeed, inversion-based methods
do use saliency maps for visualization of the “activated”
signals.

b) Activation optimization: Besides transforming the acti-
vation of neurons, signal method also includes finding input
images that optimize the activation of a neuron or a collection
of neurons. This is called the activation maximization. Starting
with a noise as an input x, the noise is slowly adjusted to
increase the activation of a select (collection of) neuron(s)
{az}. In simple mathematical terms, the task is to find x9 =
argmax ||{ax}|| where optimization is performed over input x
and ||.|| is a suitable metric to measure the combined strength
of activations. Finally, the optimized input that maximizes the
activation of the neuron(s) can emerge as something visually
recognizable. For example, the image could be a surreal
fuzzy combination of swirling patterns and parts of dog faces,
as shown in Fig. 2(b).

Research works on activation maximization include [76]
on MNIST data set, [77] and [78] that uses a regularization
function. In particular, Olah er al. [38] provides an excel-
lent interactive interface (feature visualization) demonstrating
activation-maximized images for GoogLeNet [79]. GoogLeNet
has a deep architecture, from which we can see how neurons
in deeper layer stores complex features while shallower layer
stores simple patterns [see Fig. 2(b)]. To bring this one step
further, the “semantic dictionary” is used [39] to provide a
visualization of activations within a higher level organization
and semantically more meaningful arrangements.

c) Other observations of signal activations: Ablation
studies [80], [81] also study the roles of neurons in shallower
and deeper layers. In essence, some neurons are corrupted and
the output of the corrupted NN is compared to the original
network.

d) Challenges and future prospects: Signal methods
might have revealed some parts of the black-box mechanisms.
Many questions still remain which are as follows.

1) What do we do with the (partially) reconstructed images

and images that optimize activation?
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

2) We might have learned how to approximately inverse
signals to recover images, can this help improve inter-
pretability further?

3) The components and parts in the intermediate process
that reconstruct the approximate images might contain
important information; will we be able to utilize them
in the future?

4) How is explaining the components in this “inverse
space” more useful than explaining signals that are
forward propagated?

5) Similarly, how does looking at intermediate signals that
lead to activation optimization help us pinpoint the role
of a collection of neurons?

6) Optimization of highly parameterized functions notori-
ously gives nonunique solutions. Can we be sure that
optimization that yields combination of surreal dog
faces will not yield other strange images with minor
alteration?

In the process of answering these questions, we may find

hidden clues required to get closer to interpretable AI.

3) Verbal Interpretability: This form of interpretability
takes the form of verbal chunks that human can grasp naturally.
Examples include sentences that indicate causality, as shown
in the examples below.

Logical statements can be formed from proper concatena-
tion of predicates, connectives, etc. An example of logical
statement is the conditional statement. Conditional statements
are statements of the form A — B, in another words “if A
then B.” An ML model from which logical statements can be
extracted directly has been considered obviously interpretable.
The survey [30] shows how interpretability methods in general
can be viewed under such symbolic and relational system.
In the medical field, see [82], [83].

Similarly, decision sets or rule sets have been studied for
interpretability [84]. The following is a single line in a rule set
“rainy and grumpy or calm — dairy or vegetables,” directly
quoted from the article. Each line in a rule set contains
a clause with an input in disjunctive normal form (DNF)
mapped to an output in DNF as well. The example above
is formally written (rainy A grumpy) V calm — dairy v
vegetables. Comparing three different variables, it is suggested
that interpretability of explanations in the form of rule sets
is most affected by cognitive chunks, explanation size and
little effected by variable repetition. Here, a cognitive chunk
is defined as a clause of inputs in DNF and the number
of (repeated) cognitive chunks in a rule set is varied. The
explanation size is self-explanatory (a longer/shorter line in a
tule set, or more/less lines in a rule set). MUSE [85] also
produces explanation in the form of decision sets, where
interpretable model is chosen to approximate the black-box
function and optimized against a number of metrics, including
direct optimization of interpretability metrics.

It is not surprising that verbal segments are provided
as the explanation in NLP problems. An encoder-generator
framework [86] extracts segment like “a very pleasant ruby
red-amber color” to justify 5 out of 5-star rating for a product
review. Given a sequence of words x = (x1,...,%;) with xz, €
R¢, explanation is given as the subset of the sentence that gives

4799

a summary of why the rating is justified. The subset can be
expressed as the binary sequence (z1,..., z/) where zz = 1(0)
indicates x, is (not) in the subset. Then z follows a probability
distribution with p(z|x) decomposed by assuming z, indepen-
dence to He pele) where p(zg|x) = o,(W*[he, he] + B*),
with #,, h; being the usual hidden units in the recurrent cell
(forward and backward, respectively). Similar segments are
generated using filter-attribute probability density function to
improve the relation between the activation of certain filters
and specific attributes [87]. Earlier works on visual question
answering (VQA) [88]-[90] are concerned with the generation
of texts discussing objects appearing in images.

a) Challenges and future prospects: While texts appear
to provide explanations, the underlying mechanisms used to
generate the texts are not necessarily explained. For example,
NNs and the common variants/components used in text-related
tasks such as recurrent NN (RNN), long short-term mem-
ory (LSTM) are still black boxes that are hard to troubleshoot
in the case of wrong predictions. There have been less works
that probe into the inner signals of LSTM and RNN NNs. This
is a possible research direction, although similar problem as
mentioned in Section II-A2d may arise (what to do with the
intermediate signals?). Furthermore, while word embedding
is often optimized with the usual loss minimization, there
does not seem to be a coherent explanation to the process
and shape of the optimized embedding. There may be some
clues regarding optimization residing within the embedding,
and thus successfully interpreting the shape of embedding may
help shed light into the mechanism of the algorithm.

B. Interpretability via Mathematical Structure

Mathematical structures have been used to reveal the mech-
anisms of ML and NN algorithms. In the previous section,
deeper layer of NN is shown to store complex information
while shallower layer stores simpler information [72]. Testing
with concept activation vector (TCAV) [96] has been used to
show similar trend, as suggested in Fig. 4(a2). Other methods
include clustering, such as t-distributed stochastic neighbor
embedding (t-SNE) shown in Fig. 4(b) and subspace-related
methods, for example correlation-based singular vector canon-
ical correlation analysis (SVCCA) [97] is used to find the
significant directions in the subspace of input for accurate
prediction, as shown in Fig. 4(c). Information theory has
been used to study interpretability by considering Information
Bottleneck principle [98], [99]. The rich ways in which
mathematical structures add to the interpretability pave ways
to a comprehensive view of the interpretability of algorithms,
hopefully providing a ground for unifying the different views
under a coherent framework in the future. Fig. 5 provides an
overview of ideas under this category.

1) Predefined Model: To study a system of interest, espe-
cially complex systems with not well-understood behavior,
mathematical formula such as parametric models can help
simplify the tasks. With a proper hypothesis, relevant terms
and parameters can be designed into the model. Interpreta-
tion of the terms come naturally if the hypothesis is either
consistent with available knowledge or at least developed
 

 

   
   

4800
w
o
(ie) f Si RE) p 3
; fi Si @) a fy ) :
I re =
fle) . fi 5 5
imi. :
( ) &
f Al ) U¢ , Si BD) g
7 . 3
a) / i)  @ fit) *
CIFAR10: Accuracy with SVCCA directions
and random neurons
RAEN Se eer re a een —
zt
a
a
3 2
% = p2 (4096 neurons) SVCCA
—— p2 max acts neurons
—— p2 random neurons
=== fc (512 neurons) SVCCA
a fe1 random neurons
fc2 (256 neurons) SVCCA
(B) ——- fc2 max acts neurons
Number of directions ,
Fig. 4.

used to learn CAVS.

  

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

 

arms, lampshade, bolo tie
caucasian, eastasian, latino, african

ee
—— red, blue, green, yellow
— windsor tie, bolo tie, bow tie
—— striped, dotted, zigzagged
(A2) —— honeycombed, bumpy, lacelike

mixed3a mixed3b mixed4a mixed4b mixed4c

siberian_husky, zebra, corgis
* whiteman, female, baby

mixed4d mixedde mixedSa mixedSb — logit

 

(al) TCAV [96] method finds the hyperplane CAV that separates concepts of interest. (a2) Accuracies of CAV applied to different layers supports

the idea that deeper NN layers contain more complex concepts, and shallower layers contain simpler concepts. (b) SVCCA [97] finds the most significant
subspace (direction) that contains the most information. The graph shows that as few as 25 directions out of 500 are enough to produce the accuracies of the
full network. (c) t-SNE clusters images in meaningful arrangement, for example, dog images are close together. Figures (al) and (a2) are used with permission
from the authors Been Kim; figure (b) and (c) from Maithra Raghu and Jascha Sohl-dickstein.

Heart attack risk =

Aeating patternjX1 + Aexercise freq}*2 A(o(Wyo(W20(... Wy [x1 Xn)”
+4fadaress}*3 t+ Aoccupation}*4 Ee + by.) + bz) + by)

"SRE iJ The roles of x; values are obscured in

Clearly defined model; easy to interpret neural networks; harder to interpret

Heart attack risk =

(@)

   

 

 
 
  

®: ee ®® Study the pattern

Understand the model through
existing mathematical knowledge

 

Fig. 5. Overview of methods whose interpretability depend on interpreting
underlying mathematical structure. (a) Predefined models. Modeling with
clear, easily understandable model, such as linear model can help improve
readability, and hence interpretability. On the other hand, using NN could
obscure the meaning of input variables. (b) Feature extraction. Data, predicted
values, signals, and parameters from a model are processed, transformed, and
selectively picked to provide useful information. Mathematical knowledge is
usually required to understand the resulting pattern. (c) Sensitivity. Models
that rely on sensitivity, gradients, perturbations, and related concepts will try
to account for how different data are differently represented. In the figure,
the small changes transforming the bird to the duck can be traced along a
map obtained. using clustering.

with good reasons. When the systems are better understood,
these formula can be improved by the inclusion of more
complex components. In the medical field (see later section),
an example is kinetic modeling. ML can be used to compute
the parameters defined in the models. Other methods exist,
such as integrating commonly available methodologies with
subject specific contents, etc. For example, generative dis-
criminative models [100], combine ridge regression and least
square method to handle variables for analyzing Alzheimer’s
disease and schizophrenia.

a) Linearity: The simplest interpretable predefined model
is the linear combination of variables y = L;a;x;, where a;
is the degree of how much x; contributes to the prediction y.
A linear combination model with x; € {0, 1} has been referred
to as the additive feature attribution method [40]. If the model
performs well, this can be considered highly interpretable.
However, many models are highly nonlinear. In such cases,
studying interpretability via linear properties (for example,
using linear probe; see below) are useful in several ways,
including the ease of implementation. When linear property
appears to be insufficient, nonlinearity can be introduced; it is
typically not difficult to replace the linear component wa
within the system with a nonlinear version f (6, @).

A linear probe is used in [101] to extract information
from each layer in a NN. More technically, assume we have
DL classifier F(x) € [0, 1]? where F(x) € [0,1] is the
probability that input x is classified into class i out of D
classes. Given a set of features Hy, at layer k of a NN, then
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

the linear probe f; at layer k& is defined as a linear classifier
fi 2 Hy — [0,1]? that is, f(ap) = softmax(Wh,; + b).
In another words, the probe tells us how well the information
from only layer k can predict the output, and each of this
predictive probe is a linear classifier by design. The article
then shows plots of the error rate of the prediction made by
each f; against & and demonstrates that these linear classifiers
generally perform better at deeper layer, that is, at larger k.

b) General additive models: Linear model is generalized
by the generalized additive model (GAM) [102], [103] with
standard form g(E[y]) = fo + =f;(x;) where g is the
link function. The equation is general, and specific imple-
mentations of f; and link function depend on the task. The
familiar general linear model (GLM) is GAM with the specific
implementation of linear f; and g is the identity. Modifications
can be duly implemented. As a natural extension to the model,
interaction terms between variables fj;(x;,.x;) are used [104];
we can certainly extend this indefinitely. ProtoAttend [105]
uses probabilities as weights in the linear component of the
NN. Such model is considered inherently interpretable by the
authors. In the medical field, see [82], [100], [106], [107].

c) Content-subject-specific model: Some algorithms are
considered obviously interpretable within its field. Models are
designed based on existing knowledge or empirical evidence,
and thus interpretation of the models is innately embedded into
the system. ML algorithms can then be incorporated in rich
and diverse ways, for example, through parameter fitting. The
following lists just a few works to illustrate the usage diversity
of ML algorithms. Deep Tensor NN is used for quantum
many-body systems [108]. Atomistic NN architecture for
quantum chemistry is used in [109], where each atom is like
a node in a graph with a set of feature vectors. The specifics
depend on the NN used, but this model is considered inherently
interpretable. NN has been used for programmable wireless
environments (PWEs) [110]. TS approximation [111] is a
fuzzy network approximation of other NNs. The approximate
fuzzy system is constructed with choices of components that
can be adapted to the context of interpretation. The article itself
uses sigmoid-based membership function, which it considers
interpretable. A so-called model-based reinforcement learn-
ing (RL) is suggested to be interpretable after the addition
of high-level knowledge about the system that is realized as
Bayesian structure [112].

d) Challenges and future prospects: The challenge
of formulating the “correct” model exists regardless of
ML trend. It might be interesting if a system is found
that is fundamentally operating on a specific ML model.
Backpropagation-based DNN itself is inspired by the brain,
but they are not operating at fundamental level of similarity
(nor is there any guarantee that such model exists). When
interpretability is concerned, having fundamental similarity to
real, existing systems may push forward our understanding of
ML model in unprecedented ways. Otherwise, in the standard
uses of ML algorithm, different optimization paradigms are
still being discovered. Having optimization paradigm that is
specialized for specific models may be contribute to a new
aspect of interpretable ML.

4801

2) Feature Extraction: We give an intuitive explanation
via a hypothetical example of a classifier for heart-attack
prediction. Given, say, 100-D features including eating pattern,
job, and residential area of a subject. A kernel function
can be used to find out that the strong predictor for heart
attack is a 100-D vector which is significant in the following
axes: eating pattern, exercise frequency, and sleeping pattern.
Then, this model is considered interpretable because we can
link heart-attack risk with healthy habits rather than, say
socio-geographical factors. More information can be drawn
from the next most significant predictor and so on.

a) Correlation: The methods discussed in this section
include the use of correlation in a general sense. This will
naturally include covariance matrix and correlation coefficients
after transformation by kernel functions. A kernel function
transforms high-dimensional vectors such that the transformed
vectors better distinguish different features in the data. For
example, the principal component (PC) analysis transforms
vectors into the PCs that can be ordered by the eigenvalues of
singular-value-decomposed (SVD) covariance matrix. The PC
with the highest eigenvalue is roughly the most informative
feature. Many kernel functions have been introduced, includ-
ing the canonical correlation analysis (CCA) [113]. CCA pro-
vides the set of features that transforms the original variables
to the pairs of canonical variables, where each pair is a pair
of variables that are “best correlated” but not correlated with
other pairs. Quoted from [114], “such features can inherently
characterize the object and thus it can better explore the
insights and finer details of the problems at hand.” In the
previous sections, interpretability research using correlation
includes [60].

SVCCA combines CCA and SVD to analyze interpretability
[97]. Given an input data set X = {x1,...,%m} where each
input x; is possibly multidimensional. Denote the activation
of neuron i at layer J as Z = (zie)... ., 2! Gin). It is
noted that one such output is defined for the entire input
data set. SVCCA finds out the relation between two layers
of a network , = {ki = l,...,m,} for k = 1,2 by
taking /; and Jy as the input (generally, J, does not have to
be the entire layer). SVCCA uses SVD to extract the most
informative components l, and uses CCA to transform i and
J, such that /) = Wyl) and 1; = Wxl, have the maximum
correlation p = {Pj,..-, PminGnjyn2)}- One of the SVCCA
experiments on CIFAR-10 demonstrates that only 25 most-
significant axes in [, are needed to obtain nearly the full
accuracy of a full-network with 512 dimensions. Besides,
the similarity between two compared layers is defined to be
p = (1/(min(ny, m2))) Zi pi.

The successful development of generative adversarial net-
works (GANs) [115]-[117] for generative tasks have spawned
many derivative works. GAN-based models have been able
to generate new images not distinguishable from synthetic
images and perform many other tasks, including transferring
style from one set of images to another or even producing new
designs for products and arts. Studies related to interpretabili-
ties exist. For example, [118] uses encoder—decoder system
to perform multistage PCA. Generative model is used to
show that natural image distribution modeled using probability
4802

density is fundamentally difficult to interpret [119]. This is
demonstrated through the use of GAN for the estimation
of image distribution density. The resulting density shows
preferential accumulation of density of images with certain
features (for examples, images featuring small object with
few foreground distractions) in the pixel space. The article
then suggests that interpretability is improved once it is
embedded in the deep feature space, for example, from GAN.
In this sense, the interpretability is offered by better correlation
between the densities of images with the correct identification
of the objects. Consider also the GAN-based works they cite.

b) Clustering: Algorithm such as t-SNE has been used
to cluster input images based on their activation of neurons
in a network [77], [120]. The core idea relies on the distance
between objects being considered. If the distance between two
objects are short in some measurement space, then they are
similar. This possibly appeals to the notion of human learning
by the Law of Association. It differs from correlation-based
method which provides some metrics that relate the change of
one variable with another, where the two related objects can
originate from completely different domains; clustering simply
presents their similarity, more sensibly in similar domain or in
the subsets thereof. In [120], the activations { fie7(x)} of 4096-
D layer fc7 in the CNN are collected over all input {x}. Then
{ ftc7(x)} is fed into t-SNE to be arranged and embedded into
two dimensions for visualization (each point then is visually
represented by the input image x). Activation atlases are intro-
duced in [121], which similarly uses t-SNE to arrange some
activations { fact(x)}, except that each point is represented
by the average activations of feature visualization. In meta-
material design [122], design pattern and optical responses are
encoded into latent variables to be characterized by variational
auto encoder (VAE). Then, t-SNE is used to visualize the latent
space.

In the medical field (also see later section), we
have [123], [124] (uses Laplacian eigenmap (LE) for inter-
pretability), and [125] (introduces a low-rank representation
method for autistic spectrum diagnosis).

c) Challenges and future prospects: This section exem-
plifies the difficulty in integrating mathematics and human
intuition. Having extracted “relevant” or “significant” fea-
tures, sometimes we are left with still a combination of
high-dimensional vectors. Further analysis comes in the form
of correlations or other metrics that attempt to show simi-
larities or proximity. The interpretation may stay as math-
ematical artifact, but there is a potential that separation of
concepts attained by these methods can be used to reorganize
a black-box model from within. It might be an interesting
research direction that lacks justification in terms of real-life
application: however, progress in unraveling black-boxes may
be a high-risk high-return investment.

3) Sensitivity: We group together methods that rely on
localization, gradients, and perturbations under the category
of “sensitivity.” These methods rely on the notion of small
changes dx in calculus and the neighborhood of a point in
metric spaces.

a) Sensitivity to input noises or neighborhood of data
points: Some methods rely on the locality of some input x. Let

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

a model f(.) predicts f(x) accurately for some x. Denote x+0
as a slightly noisy version of x. The model is locally faithful
if f(x + 6) produces correct prediction, otherwise, the model
is unfaithful and clearly such instability reduces its reliability.
Fong and Vedaldi [126] introduces meta-predictors as inter-
pretability methods and emphasizes the importance of the
variation of input x to NN in explaining a network. They define
explanation and local explanation in terms of the response
of blackbox f to some input. Amongst many of the studies
conducted, they provide experimental results on the effect of
varying input such as via deletion of some regions in the input.
Likewise, when random pixels of an image are deleted (hence
the data point is shifted to its neighborhood in the feature
space) and the resulting change in the output is tested [57],
pixels that are important to the prediction can be determined.
In text classification, Alvarez-Melis and Jaakkola [127] pro-
vides “explanations” in the form of partitioned graphs. The
explanation is produced in three main steps, where the first step
involves sampling perturbed versions of the data using VAE.

TCAVs has also been introduced as a technique to inter-
pret the low-level representation of NN layer [96]. First,
the concept activation vector (CAV) is defined. Given input
x € R" and a feedforward layer / having m neurons,
the activation at that layer is given by f; : R" — R".
If we are interested in the concept C, for example “striped”
pattern, then, using TCAV, we supply a set Pc of examples
corresponding to “striped” pattern (zebra, clothing pattern,
etc.) and the negative examples N. This collection is used to
train a binary classifier ol € R” for layer / that partitions
{fitx) : x © Pc} and {f)(x) : x € N}. In another words,
a kernel function extracts features by mapping out a set of acti-
vations that has relevant information about the “stripe”-ness.
CAV is thus defined as the normal vector to the hyperplane
that separates the positive examples from the negative ones,
as shown in Fig. 4(al). It then computes directional derivative
Sok) = Vir (filx)) - ol to obtain the sensitivity of the
model with respect to the concept C, where fi; is the logit
function for class k of C for layer J.

LIME [14] optimizes over models g € G where G is a
set of interpretable models G by minimizing locality-aware
loss and complexity. In another words, it seeks to obtain the
optimal model ¢(x) = argmin,<g L(f,g, 2x) + Q(g) where
Q is the complexity and f is the true function we want to
model. An example of the loss function is L(f,g,7x) =
Ze vezt@r(L f(x) — g(<)? with m2(z) being, for example,
Euclidean distance and Z is the vicinity of x. From the
equation, it can be seen that the desired g will be close to
f in the vicinity Z of x, because f(z) © g(z’) for z,z’ € Z.
In another words, noisy inputs z, z’ do not add too much losses.

Gradient-based explanation vector €(x9) = (€/0x)P(Y #
g(xo)|X = x) is introduced in [128] for Bayesian classifier
g(x) = argmin,gy cy) POY # clX = x), where x,¢ are
d-dimensional. For any i = 1,...,d, high absolute value of
[€(xo)]; means that component i contributes significantly to
the decision of the classifier. If it is positive, the higher the
value is, the less likely xg contributes to decision g(xo).

ACE algorithm [56] uses TCAV to compute saliency score
and generate super-pixels as explanations. Grad-CAM [43] is
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

a saliency method that uses gradient for its sensitivity mea-
sure. In [129], influence function is used. While theoretical,
the article also practically demonstrates how understanding the
underlying mathematics will help develop perturbative training
point for adversarial attack.

b) Sensitivity to data set: A model is possibly sensitive to
the training data set {x;} as well. Influence function is also used
to understand the effect of removing x; for some i and shows
the consequent possibility of adversarial attack [129]. Studies
on adversarial training examples can be found in the article
and its citations, where seemingly random, insignificant noises
can degrade machine decision considerably. The representer
theorem is introduced for studying the extent of effect x; has
on a decision made by a DNN [130].

c) Challenges and future prospects: There seems to
be a concern with locality and globality of the concepts.
As mentioned in [96], to achieve global quantification for
interpretability, explanation must be given for a set of exam-
ples or the entire class rather than “just explain individual data
inputs.” As a specific example, there may be a concern with
the globality of TCAV. From our understanding, TCAV is a
perturbation method by the virtue of stable continuity in the
usual derivative and it is global because the whole subset of
data set with label k of concept C has been shown to be well
distinguished by TCAV. However, we may want to point out
that despite their claim to globality, it is possible to view the
success of TCAV as local, since it is only “global” within each
label & rather than within all data set considered at once.

From the point of view of image processing, the neighbor-
hood of a data point (an image) in the feature space poses
a rather subtle question; also refer to Fig. 5(c) for related
illustration. For example, after rotating and stretching the
image or deleting some pixels, how does the position of the
image in the feature space change? Is there any way to control
the effect of random noises and improve robustness of machine
prediction in a way that is sensible to human’s perception? The
transition in the feature space from one point to another point
that belongs to different classes is also unexplored.

On a related note, gradients have played important roles in
formulating interpretability methods, be it in image processing
or other fields. Current trend recognizes that regions in the
input space with significant gradients provide interpretability.
Deforming these regions quickly degrades the prediction;
conversely, the particular values at these regions are important
to the reach a certain prediction. This is helpful, since calculus
exists to help analyse gradients. However, this has shown to
be disruptive as well. For example, imperceptible noises can
degrade prediction drastically (see manipulation of explana-
tions under Section III-D). Since gradient is also in the core
of loss optimization, it is a natural target for further studies.

4) Optimization: We have described several researches that
seek to attain interpretability via optimization methods. Some
have optimization at the core of their algorithm, but the
interpretability is left to visual observation, while others opti-
mize interpretability mathematically.

a) Quantitatively maximizing interpretability: To approx-
imate a function f, as previously mentioned, LIME [14]
performs optimization by finding optimal model ¢ € G so

4803

that f(z) ~ €(z’) for z,z’ € Z where Z is the vicinity of
x, so that local fidelity is said to be achieved. Concurrently,
the complexity Q(¢€) is minimized. Minimized © means the
model’s interpretability is maximized. MUSE [85] takes in
blackbox model, prediction and user-input features to output
decision sets based on optimization with respect to fidelity,
interpretability, and unambiguity. The available measures of
interpretability that can be optimized include size, featureover-
lap, etc. (refer to Table II of its Appendix).

b) Activation optimization: Activation optimizations are
used in research works such as [38] and [76]-[78] as explained
in the previous section. The interpretability relies on direct
observation of the neuron-activation-optimized images. While
the quality of the optimized images are not evaluated, the fact
that parts of coherent images emerge with respect to a (col-
lection of) neuron(s) does demonstrate some organization of
information in the NNs.

C. Other Perspectives to Interpretability

There are many other concepts that can be related to
interpretability. Selvaraju et al. [43] conducted experiments
to test the improvements of human performance on a task
after being given explanations (in the form of visualization)
produced by ML algorithms. We believe this might be an
exemplary form of interpretability evaluation. For example,
we want to compare ML algorithms MLa with MLp. Say,
human subjects are given difficult classification tasks and
attain a baseline 40% accuracy. Repeat the task with different
set of human subjects, but they are given explanations churned
out by ML and MLp. If the accuracies attained are now 50%
and 80%, respectively, then ML x is more interpretable.

Even then, if human subjects cannot really explain why
they can perform better with the given explanations, then
the interpretability may be questionable. This brings us to
the question of what kind of interpretability is necessary in
different tasks and certainly points to the possibility that there
is no need for a unified version of interpretability.

1) Data-Driven Interpretability:

a) Data in catalog: A large amount of data has been
crucial to the functioning of many ML algorithms, mainly
as the input data. In this section, we mention works that
put a different emphasize on the treatment of these data
arranged in catalog. In essence, Doshi-Velez and Kim [10]
suggests that we create a matrix whose rows are different
real-world tasks (e.g., pneumonia detection), columns are
different methods (e.g., decision tree with different depths)
and the entries are the performance of the methods on some
end-task. How can we gather a large collection of entries into
such a large matrix? Apart from competitions and challenges,
crowd-sourcing efforts will aid the formation of such database
[148], [149]. A clear problem is how multidimensional and
gigantic such tabulation will become, not to mention that the
collection of entries is very likely uncountably many. Formal-
izing interpretability here means we pick latent dimensions
(common criteria) that human can evaluate for example, time
constraint or time-spent, cognitive chunks (defined as the basic
unit of explanation, also see the definition in [84]), etc. These
4804

dimensions are to be refined along iterative processes as more
user inputs enter the repository.

b) Incompleteness: In [10], the problem of incomplete-
ness of problem formulation is first posed as the issue in
interpretability. Incompleteness is present in many forms, from
the impracticality to produce all test cases to the difficulty in
justifying why a choice of proxy is the best for some scenarios.
At the end, it suggests that interpretability criteria are to be
born out of collective agreements of the majority, through a
cyclical process of discoveries, justifications, and rebuttals.
In our opinion, a disadvantage is that there is a possibility
that no unique convergence will be born, and the situation
may aggravate if, say, two different conflicting factions are
born, each with enough advocate. The advantage lies in the
existence of strong roots for the advocacy of certain choice of
interpretability. This prevents malicious intent from tweaking
interpretability criteria to suit ad hoc purposes.

2) Invariances:

a) Implementation invariance: Sundararajan et al. [94]
suggests implementation invariance as an axiomatic require-
ment to interpretability. In the article, it is stated as the
following. Define two functionally equivalent functions as
Si, fo so that fi(x) = f(x) for any x regardless of their
implementation details. Given any two such networks using
attribution method, then the attribution functional A will map
the importance of each component of an input to f; the
same way it does to fo. In another words, (A[fi](*)); =
(Al fol(x)); for any j = 1,...,d where d is the dimension
of the input. The statement can be easily extended to methods
that do not use attribution as well.

b) Input invariance: To illustrate using image
classification problem, translating an image will also
translate super-pixels demarcating the area that provides an
explanation to the choice of classification correspondingly.
Clearly, this property is desirable and has been proposed as
an axiomatic invariance of a reliable saliency method. There
has also been a study on the input invariance of some saliency
methods with respect to translation of input x > x +c for
some c [71]. Of the methods studied, gradients/sensitivity-
based methods [128] and signal methods [72], [75] are input
invariant while some attribution methods, such as integrated
gradient [94], are not.

3) Interpretabilities by Utilities: The following
utilities-based categorization of interpretability is proposed
in [10].

a) Application-based: First, an evaluation is
application-grounded if human A_ gives explanation Xa
on a specific application, so-called the end-task (e.g., a doctor
performs diagnosis) to human B, and B performs the same
task. Then A has given B a useful explanation if B performs
better in the task. Suppose A is now an ML model, then
the model is highly interpretable if human B performs the
same task with improved performance after given X4. Some
medical segmentation works will fall into this category
as well, since the segmentation will constitute a visual
explanation for further diagnosis/prognosis [144], [145] (also
see other categories of the grand challenge). Such evaluation
is performed, for example, in [43]. They proposed Grad-CAM

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

applied on guided backpropagation (proposed in [75]) of
AlexNet CNN and VGG. The produced visualizations are
used to help human subjects in Amazon mechanical turks
identify objects with higher accuracy in predicting VOC
2007 images. The human subjects achieved 61.23% accuracy,
which is 16.79% higher than visualization provided by guided
backpropagation.

b) Human-based: This evaluation involves real humans
and simplified tasks. It can be used when, for some reasons
or another, having human A give a good explanation Xa is
challenging, possibly because the performance on the task
cannot be evaluated easily or the explanation itself requires
specialized knowledge. In this case, a simplified or partial
problem may be posed and Xq is still demanded. Unlike
the application-based approach, it is now necessary to look
at Xq specifically for interpretability evaluation. Bigger pool
of human subjects can then be hired to give a generic valuation
to Xa or create a model answer Xx A to compare X4 with, and
then a generic valuation is computed.

Now, suppose A is an ML model, A is more interpretable
compared to another ML model if it scores better in this
generic valuation. In [146], an ML model is given a document
containing the conversation of humans making a plan. The
ML model produces a “report” containing relevant predi-
cates (words) for the task of inferring what the final plan is.
The metric used for interpretability evaluation is, for example,
the percentage of the predicates that appear, compared to
human-made report. We believe the format of human-based
evaluation needs not be strictly like the above. For example,
hybrid human and interactive ML classifiers require human
users to nominate features for training [147]. Two different
standard MLs can be compared to the hybrid, and one can be
said to be more interpretable than another if it picks up features
similar to the hybrid, assuming they perform at similarly
acceptable level.

c) Functions-based: Third, an evaluation is function-
ally grounded if there exist proxies (which can be defined
a priori) for evaluation, for example, sparsity [10]. Some
articles [2], [5], [42]-[44], [96], [97], [144], and [145] use
metrics that rely on this evaluation include many supervised
learning models with clearly defined metrics such as: 1) dice
coefficients (related to visual interpretability) and 2) attribution
values, components of canonically transformed variables (see
for example CCA) or values obtained from dimensionality
reduction methods (such as components of principal compo-
nents from PCA and their corresponding eigenvalues), where
interpretability is related to the degree an object relates to a
feature, for example, classification of a dog has high values
in the feature space related to four limbs, shape of snout and
paws, etc. Which suitable metrics to use are highly dependent
on the tasks at hand.

III. XAT IN MEDICAL FIELD

ML has also gained traction recently in the medical field,
with large volume of works on automated diagnosis, prog-
nosis [150]. From the grand-challenge.org, we can see many
different challenges in the medical field have emerged and
galvanized researches that use ML and AI methods. Amongst
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

TABLE Il

CATEGORIZATION BY THE ORGANS AFFECTED BY THE
DISEASES. NEURO* REFERS TO ANY NEUROLOGICAL,
NEURODEVELOPMENTAL, NEURODEGENERATIVE, ETC. DISEASES.
THE ROWS ARE ARRANGED ACCORDING TO THE FOCUS OF
THE INTERPRETABILITY AS THE FOLLOWING: APPL. =
APPLICATION, METHOD. = METHODOLOGY,

COMP. = COMPARISON

brain, neuro* breast [69], lung [6], [82],
[48], [68], [131], [153] sleep [154], skin [155]
[132], [133], [136], [156] others [106]

breast [65], [70], [140], [141]
[66], [67], [83], [91], [100]

skin [139], heart [124]
[114], [123], [135], [157] others [44], [67], [138], [142]

brain, neuro* [107], [158] lung [93], sleep [159]
skin [160], other [137]

 

 

 

 

successful DL models are [2], [5], using U-Net for medical
segmentation. However, being a DL NN, U-Net is still a
blackbox; it is not very interpretable. Other domain specific
methods and special transformations (denoising etc.) have
been published as well; consider for example [131] and many
other works in MICCAI publications.

In the medical field, the question of interpretability is far
from just intellectual curiosity. More specifically, it is pointed
out that interpretabilities in the medical fields include factors
other fields do not consider, including risk and responsibilities
[21], [151], [152]. When medical responses are made, lives
may be at stake. To leave such important decisions to machines
that could not provide accountabilities would be akin to
shirking the responsibilities altogether. Apart from ethical
issues, this is a serious loophole that could turn catastrophic
when exploited with malicious intent.

Many more works have thus been dedicated to explor-
ing explainability in the medical fields [11], [20], [44].
They provide summaries of previous works [21] including
subfield-specific reviews such as [25] for chest radiograph
and sentiment analysis in medicine [161], or at least set
aside a section to promote awareness for the importance
of interpretability in the medical field [162]. In [163], it is
stated directly that being a black box is a “strong limitation”
for AI in dermatology, as it is not capable of performing
customized assessment by certified dermatologist that can be
used to explain clinical evidence. On the other hand, the
exposition [164] argues that a certain degree of opaqueness is
acceptable, that is, it might be more important that we produce
empirically verified accurate results than focusing too much on
how to the unravel the black-box. We recommend readers to
consider them first, at least for an overview of interpretability
in the medical field.

We apply categorization from the previous section to the
ML and AI in the medical field. Table III shows catego-
rization obtained by tagging: 1) how interpretability method
is incorporated: either through direct application of existing
methods, methodology improvements, or comparison between
interpretability methods and 2) the organs targeted by the
diseases for example, brain, skin, etc. As there is not yet
a substantial number of significant medical researches that
address interpretability, we will refrain from presenting any
conclusive trend. However, from a quick overview, we see
that the XAI research community might benefit from more

4805

studies comparing different existing methods, especially those
with more informative conclusion on how they contribute to
interpretability.

A. Perceptive Interpretability

Medical data could come in the form of traditional 2-D
images or more complex formats such as NIFTI or DCOM
which contain 3-D images with multiple modalities and even
4-D images which are time-evolving 3-D volumes. The dif-
ficulties in using ML for these data include the following.
Medical images are sometimes far less available in quan-
tity than common images. Obtaining these data requires
consent from the patients and other administrative barriers.
High-dimensional data also add complexity to data process-
ing and the large memory space requirement might prevent
data to be input without modification, random sampling or
down-sizing, which may compromise analysis. Other possible
difficulties with data collection and management include as
left/right-censoring, patients’ death due to unrelated causes or
other complications etc.

When medical data is available, ground-truth images may
not be “correct.” Not only do these data require some spe-
cialized knowledge to understand, the lack of comprehen-
sive understanding of biological components complicates the
analysis. For example, ADC modality of MR images and the
isotropic version of DWI are in some sense derivative, since
both are computed from raw images collected by the scanner.
Furthermore, many CT or MRI scans are presented with
skull-stripping or other preprocessing. However, without a
more complete knowledge of what fine details might have been
accidentally removed, we cannot guarantee that an algorithm
can capture the correct features.

1) Saliency: The following articles consist of direct
applications of existing saliency methods. Chexpert [6] uses
GradCAM for visualization of pleural effusion in a radiograph.
CAM is also used for interpretability in brain tumor grad-
ing [153]. Tang er al. [68] uses guided Grad-CAM and feature
occlusion, providing complementary heatmaps for the classifi-
cation of Alzheimer’s disease pathologies. Integrated gradient
method and SmoothGrad are applied for the visualization of
CNN ensemble that classifies estrogen receptor status using
breast MRI [69]. LRP on DeepLight [48] was applied on {MRI
data from Human Connectome Project to generate heatmap
visualization. Saliency map has also been computed using
primitive gradient of loss, providing interpretability to the
NN used for electroencephalogram (EEG) sleep stage scor-
ing [154]. There has even been a direct comparison between
the feature maps within CNN and skin lesion images [155],
overlaying the scaled feature maps on top of the images as a
means to interpretability. Some images correspond to relevant
features in the lesion, while others appear to explicitly capture
artifacts that might lead to prediction bias.

The following articles are focused more on comparison
between popular saliency methods, including their derivative/
improved versions. Jansen et al. [159] trains an artificial
NN for the classification of insomnia using physiological
network (PN). The feature relevance scores are computed
4806

from several methods, including DeepLIFT [57]. Comparison
between four different visualizations is performed in [158].
It shows different attributions between different methods, and
concluded that LRP and guided backpropagation provide the
most coherent attribution maps in their Alzheimer’s disease
study. Basic tests on GradCAM and SHAP on dermoscopy
images for melanoma classification are conducted, concluding
with the need for significant improvements to heatmaps before
practical deployment [160].

The following includes slightly different focus on
methodological improvements on top of the visualization.
Respond-CAM [44] is derived from [42] and [43], and pro-
vides a saliency map in the form of heat-map on 3-D images
obtained from cellular electron cryo-tomography. High inten-
sity in the heatmap marks the region where macromolecular
complexes are present. Multilayer CAM (MLCAM) is intro-
duced in [91] for glioma (a type of brain tumor) localization.
Multiinstance (MI) aggregation method is used with CNN to
classify breast tumor tissue microarray (TMA) image’s for
five different tasks [65], for example the classification of the
histologic subtype. Super-pixel maps indicate the region in
each TMA image where the tumor cells are; each label cor-
responds to a class of tumor. These maps are proposed as the
means for visual interpretability. Also, see the activation maps
in [66] where interpretability is studied by corrupting image
and inspecting region of interest (ROD. The autofocus module
from [67] promises improvements in visual interpretability for
segmentation on pelvic CT scans and segmentation of tumor in
brain MRI using CNN. It uses attention mechanism (proposed
in [92]) and improves it with adaptive selection of scale with
which the network “sees” an object within an image. With the
correct scale adopted by the network while performing a single
task, human observer analyzing the network can understand
that a NN is properly identifying the object, rather than
mistaking the combination of the object plus the surrounding
as the object itself.

There is also a different formulation for the generation
of saliency maps [70]. It defines a different softmax-like
formula to extract signals from DNN for visual justification
in classification of breast mass (malignant/benign). Textual
justification is generated as well.

2) Verbal: In [82], a rule-based system could provide the
statement “has asthma — lower risk,’ where risk here refers
to death risk due to pneumonia. Likewise, Letham ez al. [83]
creates a model called Bayesian rule lists that provides
such statements for stroke prediction. Textual justification
is also provided in the LSTM-based breast mass classifier
system [70]. The argumentation theory is implemented in the
ML training process [156], extracting arguments or decision
rules as the explanations for the prediction of stroke based on
the asymptomatic carotid stenosis and risk of stroke (ACSRS)
data set.

One should indeed look closer at the interpretability in [82].
Just as many MLs are able to extract some humanly nonintu-
itive pattern, the rule-based system seems to have captured the
strange link between asthma and pneumonia. The link becomes
clear once the actual explanation based on real situation is
provided: a pneumonia patient which also suffers from asthma

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

is often sent directly to the intensive care unit (ICU) rather
than a standard ward. Obviously, if there is a variable ICU =
0 or 1 that indicates admission to ICU, then a better model
can provide more coherent explanation “asthma > ICU >
lower risk.” In the article, the model appears not to identify
such variable. We can see that interpretability issues are not
always clear-cut.

Several researches on VQA in the medical field have also
been developed. The initiative by ImageCLEF [165], [166]
appears to be at its center, though VQA itself has yet to gain
more traction and successful practical demonstration in the
medical sector before widespread adoption.

a) Challenges and future prospects: For perceptive inter-
pretability in medical sector. In many cases, where saliency
maps are provided, they are provided with insufficient evalua-
tion with respect to their utilities within the medical practices.
For example, when providing importance attribution to a CT
scan used for lesion detection, are radiologists interested in
heatmaps highlighting just the lesion? Are they more interested
in looking for reasons why a hemorrhage is epidural or
subdural when the lesion is not very clear to the naked eyes?
There may be many such medically related subtleties that
interpretable AI researchers may need to know about.

B. Interpretability via Mathematical Structure

1) Predefined Model: Models help with interpretability by
providing a generic sense of what a variable does to the
output variable in question, whether in medical fields or
not. A parametric model is usually designed with at least
an estimate of the working mechanism of the system, with
simplification and based on empirically observed patterns. For
example, Ulas et ai. [131] uses kinetic model for the cerebral
blood flow in mi/100g/min with

60006 AM exp (32)

which depends on perfusion-weighted image AM obtained
from the signal difference between labeled image of arterial
blood water treated with RF pulses and the control image.
This function is incorporated in the loss function in the
training pipeline of a fully CNN. At least, an interpretation
can be made partially: the NN model is designed to denoise
a perfusion-weighted image (and thus improve its quality) by
considering CBF. How the network “understands” the CBF is
again an interpretability problem of a NN which has yet to be
resolved.

There is an inherent simplicity in the interpretability of
models based on linearity, and thus they have been considered
obviously interpretable as well; some examples include linear
combination of clinical variables [100], metabolites signals for
magnetic resonance spectroscopy (MRS), [106] etc. Linearity
in different models used in the estimation of brain states
is discussed in [107], including how it is misinterpreted.
It compares what it refers to as forward and backward
models and then suggested improvement on linear models.
In [82], a logistic regression model picked up a relation

CBF = f(AM) (1)
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

between asthma and lower risk of pneumonia death, that
is, asthma has a negative weight as a risk predictor in the
regression model. Generative discriminative machine (GDM)
combines ordinary least square regression and ridge regression
to handle confounding variables in Alzheimer’s disease and
schizophrenia data set [100]. GDM parameters are said to be
interpretable, since they are linear combinations of the clinical
variables. DL has been used for PET pharmacokinetic (PK)
modeling to quantify tracer target density [132]. CNN has
helped PK modeling as a part of a sequence of processes
to reduce PET acquisition time, and the output is interpreted
with respect to the golden standard PK model, which is
the linearized version of simplified reference tissue model
(SRTM). DL method is also used to perform parameters fitting
for MRS [106]. The parametric part of the MRS signal model
specified, x(t) = Lan xm(ryehem't27 4imt | consists of linear
combination of metabolite signals x,,(t). The article shows
that the error measured in symmetric mean absolute percentage
error (SMAPE) is smallest for most metabolites when their
CNN model is used. In cases like this, clinicians may find
the model interpretable as long as the parameters are well-fit,
although the NN itself may still not be interpretable.

The models above use linearity for studies related to brain or
neuro-related diseases. Beyond linear models, other brain and
neuro-systems can be modeled with relevant subject-content
knowledge for better interpretability as well. Segmentation
task for the detection of brain midline shift is performed
using using CNN with standard structural knowledge incor-
porated [133]. A template called model-derived age norm is
derived from mean values of sleep EEG features of healthy
subjects [157]. Interpretability is given as the deviation of the
features of unhealthy subject from the age norm.

On a different note, RL has been applied to personal-
ized healthcare. In particular, Zhu er al. [134] introduces
group-driven RL in personalized healthcare, taking into con-
siderations different groups, each having similar agents.
As usual, Q-value is optimized with respect to policy 79,
which can be qualitatively interpreted as the maximization of
rewards over time over the choices of action selected by many
participating agents in the system.

a) Challenges and future prospects: Models may be
simplifying intractable system. As such, the full potential of
ML, especially DNN with huge number of parameters, may
be under-used. A possible research direction that taps onto
the hype of predictive science is as the following: given
a model, is it possible to augment the model with new,
sophisticated components, such that parts of these components
can be identified with (and thus interpreted as) new insights?
Naturally, the augmented model needs to be comparable to
previous models and shown with clear interpretation why
the new components correspond to insighs previously missed.
Do note that there are critiques against the hype around the
potential of AI which we will leave to the readers.

2) Feature Extraction: Vanilla CNN is used in [142]
but it is suggested that interpretability can be attained
using a separable model. The separability is achieved by
polynomial-transforming scalar variables and further process-
ing, giving rise to weights useful for interpretation. In [123],

4807

fMRI is analyzed using correlation-based functional graphs.
They are then clustered into super-graph, consisting of sub-
networks that are defined to be interpretable. A convolutional
layer is then used on the super-graph. For more references
about NNs designed for graph-based problems, see the article’s
citations. The following are further subcategorization for meth-
ods that revolve around feature extraction and the evaluations
or measurements (such as correlations) used to obtain the
features, similar to the previous section.

a) Correlation: DWT-based method (discrete wavelet
transform) is used to perform feature extraction before even-
tually feeding the EEG data (after a series of processings)
into a NN for epilepsy classification [135]. A fuzzy relation
analogous to correlation coefficient is then defined. Further-
more, as with other transform methods, the components (the
wavelets) can be interpreted componentwise. As a simple
illustration, the components for Fourier transform could be
taken as how much certain frequency is contained in a time
series. Zhang er al. [136] mentioned a host of wavelet-based
feature extraction methods and introduced maximal overlap
discrete wavelet package transform (MODWPT) also applied
on EEG data for epilepsy classification.

Frame singular value decomposition (F-SVD) is introduced
for classifications of electromyography (EMG) data [114]. It is
a pipeline involving a number of processing that includes
DWT, CCA, and SVD, achieving around 98% accuracies on
classifications between amyotrophic lateral sclerosis, myopa-
thy, and healthy subjects. Consider also CCA-based articles
that are cited in the article, in particular citations 18-21 for
EMG and EEG signals.

b) Clustering: VAE is used to obtain vectors in 64-D
latent dimension to predict whether the subjects suffer from
hypertrophic cardiomyopathy (HCM) [124]. A nonlinear trans-
formation is used to create LE with two dimensions, which is
suggested as the means for interpretability. Skin images are
clustered [139] for melanoma classification using k-nearest-
neighbor that is customized to include CNN and triplet loss.
A queried image is then compared with training images ranked
according to similarity measure visually displayed as query-
result activation map pair.

t-SNE has been applied on human genetic data and shown
to provide more robust dimensionality reduction compared
to PCA and other methods [137]. Multiple maps t-SNE
(mm-t-SNE) is introduced in [138], performing clustering on
phenotype similarity data.

c) Sensitivity: Regression concept vectors (RCVs) is
proposed along with a metric Br score as improvements to
TCAV’s concept separation [140]. The method is applied
on breast cancer histopathology classification problem. Fur-
thermore, unit ball surface sampling (UBS) metric is intro-
duced [141] to address the shortcoming of Br score.
It uses NNs for classification of nodules for mammographic
images. Guidelinebased Additive eXplanation (GAX) is intro-
duced in [93] for diagnosis using CT lung images. Its
pipeline includes LIME-like perturbation analysis and SHAP.
Comparisons are then made with LIME, Grad-CAM, and
feature importance generated by SHAP.
4808

Perceptive

_

Fig. 6.

d) Challenges and future prospects: We observe popular
uses of certain methods ingrained in specific sectors on the
one hand and, on the other hand, emerging applications of
sophisticated ML algorithms. As medical ML (in particular
the application of recently successful DNN) is still a young
field, we see fragmented and experimental uses of existing or
customized interpretable methods. As medical ML research
progresses, the tradeoff between many practical factors of
ML methods (such as ease of use, ease of interpretation of
mathematical structure possibly regarded as complex) and
its contribution to the subject matter will become clearer.
Future research and application may benefit from a prac-
tice of consciously and consistently extracting interpretable
information for further processing, and the process should be
systematically documented for good dissemination. Currently,
with feature selections and extractions focused on improving
accuracy and performance, we may still have vast unexplored
opportunities in interpretability research.

C. Other Perspectives

1) Data-Driven: Case-based reasoning (CBR) performs
medical evaluation (classifications etc.) by comparing a query
case (new data) with similar existing data from a database.
Lamy et al. [143] combines CBR with an algorithm that
presents the similarity between these cases by visually provid-
ing proxies and measures for users to interpret. By observing
these proxies, the user can decide to take the decision sug-
gested by the algorithm or not. The article also asserts that
medical experts appreciate such visual information with clear
decision-support system.

D. Risk of Machine Interpretation in Medical Field

1) Jumping Conclusion: According to [82], logical state-
ments such as has asthma — lower risk are considered

 

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

Medical XAI

Overview of challenges and future prospects arranged in a Venn diagram.

interpretable. However, in the example, the statement indicates
that a patient with asthma has lower risk of death from
pneumonia, which might be strange without any clarification
from the intermediate thought process. While human can
infer that the lowered risk is due to the fact that pneumonia
patients with asthma history tend to be given more aggressive
treatment, we cannot always assume there is a similar humanly
inferable reason behind each decision. Furthermore, inter-
pretability method such as LRP, deconvolution, and guided
backpropagation introduced earlier are shown to not work for
simple model, such as linear model, bringing into question
their reliability [60].

IV. CONCLUSION

We present a survey on interpretability and explainability of
ML algorithms in general, and place different interpretations
suggested by different research works into distinct categories.
From general interpretabilities, we apply the categorization
into the medical field. Some attempts are made to formalize
interpretabilities mathematically, some provide visual expla-
nations, while others might focus on the improvement in task
performance after being given explanations produced by algo-
rithms. At each section, we also discuss related challenges and
future prospects. Fig. 6 provides a diagram that summarizes
all the challenges and prospects.

A. Manipulation of Explanations

Given an image, a similar image can be generated that is
perceptibly indistinguishable from the original, yet produces
radically different output [95]. Naturally, its significance
attribution and interpretable information become unreliable.
Furthermore, explanation can even be manipulated arbitrar-
ily [167]. For example, an explanation for the classification of
a cat image (i.e., particular significant values that contribute
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

to the prediction of cat) can be implanted into the image of
a dog, and the algorithm could be fooled into classifying the
dog image as a cat image. The risk in medical field is clear:
even without malicious, intentional manipulation, noises can
render “explanations” wrong. Manipulation of algorithm that
is designed to provide explanation is also explored in [168].

B. Incomplete Constraints

In [131], the loss function for the training of a fully convo-
lutional network includes CBF as a constraint. However, many
other constraints may play important roles in the mechanism of
a living organ or tissue, not to mention applying kinetic model
is itself a simplification. Giving an interpretation within limited
constraints may place undue emphasis on the constraint itself.
Other works that use predefined models might suffer similar
problems [100], [106], [132].

C. Noisy Training Data

The so-called ground truths for medical tasks, provided
by professionals, are not always absolutely correct. In fact,
news regarding how AI beats human performance in medical
imaging diagnosis [169] indicates that human judgment could
be brittle. This is true even of trained medical personnel. This
might give rise to the classic garbage-in-garbage-out situation.

The above risks are presented in large part as a reminder of
the nature of automation. It is true that algorithms have been
used to extract invisible patterns with some successes. How-
ever, one ought to view scientific problems with the correct
order of priority. The society should not risk over-allocating
resources into building machine and DL models, especially
since due improvements to understanding the underlying sci-
ence might be the key to solving the root problem. For
example, higher quality MRI scans might reveal key informa-
tion not “visible” with current technology, and many models
built nowadays might not be very successful because there is
simply not enough detailed information contained in currently
available MRI scans.

D. Future Directions for Clinicians and Practitioners

Visual and textual explanation supplied by an algorithm
might seem like the obvious choice; unfortunately, the details
of decision-making by algorithms such as DNNs are still
not clearly exposed. When an otherwise reliable DL model
provides a strangely wrong visual or textual explanation, sys-
tematic methods to probe into the wrong explanations do not
seem to exist, let alone methods to correct them. A specialized
education combining medical expertise, applied mathematics,
data science, etc. might be necessary to overcome this. For
now, if “interpretable” algorithms are deployed in medical
practices, human supervision is still necessary. Interpretability
information should be considered nothing more than comple-
mentary support for the medical practices before there is a
robust way to handle interpretability.

E. Future Directions for Algorithm Developers and
Researchers

Before the blackbox is unblackboxed, machine decision
always carries some exploitable risks. It is also clear that

4809

a unified notion of interpretability is elusive. For medical
ML interpretability, more comparative studies between the
performance of methods will be useful. The interpretability
output such as heatmaps should be displayed and compared
clearly, including poor results. In the best case scenario,
clinicians and practitioners recognize the shortcomings of
interpretable methods but have a general idea on how to
handle them in ways that are suitable to medical practices.
In the worst case scenario, the inconsistencies between these
methods can be exposed. The very troubling trend of journal
publications emphasizing good results is precarious, and we
should thus continue interpretability research with a mindset
open to evaluation from all related parties. Clinicians and
practitioners need to be given the opportunity for fair judgment
of utilities of the proposed interpretability methods, not just
flooded with performance metrics possibly irrelevant to the
adoption of medical technology.

Also, there may be a need to shift interpretability study
away from algorithm-centric studies. An authoritative body
setting up the standard of requirements for the deployment
of model building might stifle the progress of the research
itself, though it might be the most efficient way to reach
an agreement. This might be necessary to prevent damages,
seeing that even corporate companies and other bodies nonaca-
demic in the traditional sense have joined the fray (consider
health-tech start-ups and the implications). Acknowledging
that machine and DL might not be fully mature for large-scale
deployment, it might be wise to deploy the algorithms as a
secondary support system for now and leave most decisions
to the traditional methods. It might take a long time before
humanity graduates from this stage, but it might be timely:
we can collect more data to compare machine predictions with
traditional predictions and sort out data ownership issues along
the way.

ACKNOWLEDGMENT

The Alibaba-NTU Program is a collaboration between
Alibaba and Nanyang Technological University, Singapore.

REFERENCES

[1] E.-J. Lee, Y.-H. Kim, N. Kim, and D.-W. Kang, “Deep into the brain:
Artificial intelligence in stroke imaging,” J. Stroke, vol. 19, no. 3,
pp. 277-285, Sep. 2017.

[2] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convo-
lutional networks for biomedical image segmentation,” CoRR,
vol. abs/1505.04597, no. 3, pp. 234-241, Nov. 2015. [Online]. Avail-
able: http://arxiv.org/abs/1505.04597

[3] M. T. Dzindolet, S. A. Peterson, R. A. Pomranky, L. G. Pierce, and
H. P. Beck, “The role of trust in automation reliance,” Int. J. Hum.-
Comput. Stud., vol. 58, no. 6, pp. 697-718, Jun. 2003.

[4] L. Chen, P. Bentley, and D. Rueckert, “Fully automatic acute ischemic

lesion segmentation in DWI using convolutional neural networks,”

Neuroimage, Clin., vol. 15, pp. 633-643, Jun. 2017.

O. Cicek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,

“3D U-Net: Learning dense volumetric segmentation from sparse

annotation,” CoRR, vol. abs/1606.06650, pp. 6-7, Aug. 2016. [Online].

Available: http://arxiv.org/abs/1606.06650

[6] J. Irvin et al., “Chexpert: A large chest radiograph dataset with

uncertainty labels and expert comparison,” CoRR, vol. abs/1901.07031,

pp. 4-7, Jul. 2019. [Online]. Available: http://arxiv.org/abs/1901.07031

F. Milletari, N. Navab, and S. Ahmadi, “V-Net: Fully convolu-

tional neural networks for volumetric medical image segmentation,”

CoRR, vol. abs/1606.04797, pp. 7-9, Jun. 2016. [Online]. Available:

http://arxiv.org/abs/1606.04797

[5

[7
4810

[8]

[9]

[10]

Qi]

[12]

[13]

[14]

[15]

[16]

17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

L.-C. Chen, G. Papandreou, I Kokkinos, K. Murphy, and A. L. Yuille,
“Deeplab: Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected CRFs,” CoRR,
vol. abs/1606.00915, pp. 7-11, Jun. 2016. [Online]. Available:
http://dblp.uni-trier.de/db/journals/corr/corr1606.html

C. J. Kelly, A. Karthikesalingam, M. Suleyman, G. Corrado, and
D. King, “Key challenges for delivering clinical impact with artificial
intelligence,” BMC Med., vol. 17, no. 1, p. 195, Dec. 2019.

F. Doshi-Velez and B. Kim, “Towards a rigorous science of inter-
pretable machine learning,” 2017, arXiv:1702.08608. [Online]. Avail-
able: http://arxiv.org/abs/1702.08608

S. Tonekaboni, S. Joshi, M. D. McCradden, and A. Goldenberg,
“What clinicians want: Contextualizing explainable machine learning
for clinical end use,” CoRR, vol. abs/1905.05134, pp. 1-12, May 2019.
[Online]. Available: http://arxiv.org/abs/1905.05 134

J. L. Herlocker, J. A. Konstan, and J. Riedl, “Explaining collaborative
filtering recommendations,” in Proc. ACM Conf. Comput. Supported
Cooperat. Work (CSCW). New York, NY, USA: Association Computing
Machinery, 2000, pp. 241-250.

S. Lapuschkin, S. Waldchen, A. Binder, G. Montavon, W. Samek,
and K.-R. Miiller, “Unmasking clever hans predictors and assessing
what machines really learn,” Nature Commun., vol. 10, no. 1, p. 1096,
Dec. 2019.

M. T. Ribeiro, S. Singh, and C. Guestrin, ““Why should i trust you?’:
Explaining the predictions of any classifier,’ in Proc. 22nd ACM
SIGKDD Int. Conf: Knowl. Discovery Data Mining. New York, NY,
USA: Association Computing Machinery, Aug. 2016, pp. 1135-1144.
Z. C. Lipton, “The Mythos of model interpretability,” CoRR,
vol. abs/1606.03490, pp. 1-8, Mar. 2016. [Online]. Available:
http://arxiv.org/abs/1606.03490

F. K. Dosilovic, M. Brcic, and N. Hlupic, “Explainable artificial
intelligence: A survey,” in Proc. 41st Int. Conv. Inf. Commun. Technol,
Electron. Microelectron. (MIPRO), May 2018, pp. 210-215.

L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal,
“Explaining explanations: An overview of interpretability of machine
learning,” in Proc. IEEE Sth Int. Conf. Data Sci. Adv. Anal. (DSAA),
Oct. 2018, pp. 80-89.

A.B. Arrieta ef al., “Explainable artificial intelligence (XAI): Concepts,
taxonomies, opportunities and challenges toward responsible AL,” Inf:
Fusion, vol. 58, pp. 82-115, Jun. 2020.

S. R. Soekadar, N. Birbaumer, M. W. Slutzky, and L. G. Cohen,
“Brain—machine interfaces in neurorehabilitation of stroke,” Neurobiol.
Disease, vol. 83, pp. 172-179, Nov. 2015.

A. Holzinger, G. Langs, H. Denk, K. Zatloukal, and H. Miller,
“Causability and explainability of artificial intelligence in medicine,”
WIREs Data Mining Knowl. Discovery, vol. 9, no. 4, p. e1312,
Jul. 2019.

Y. Xie, G. Gao, and X. A. Chen, “Outlining the design space
of explainable intelligent systems for medical diagnosis,” CoRR,
vol. abs/1902.06019, pp. 1-5, Mar. 2019. [Online]. Available:
http://arxiv.org/abs/1902.06019

A. Vellido, “The importance of interpretability and visualization in
machine learning for applications in medicine and health care,” Neural
Comput. Appl., early access, Feb. 4, 2019, doi: 10.1007/s00521-019-
0405 1-w.

E. J. Topol, “High-performance medicine: The convergence of human
and artificial intelligence,” Nature Med., vol. 25, no. 1, pp. 44-56,
Jan. 2019.

A. Fernandez, F. Herrera, O. Cordon, M. J. D. Jesus, and F. Marcelloni,
“Evolutionary fuzzy systems for explainable artificial intelligence:
Why, when, what for, and where to?” [EEE Comput. Intell. Mag.,
vol. 14, no. 1, pp. 69-81, Feb. 2019.

K. Kallianos et ai, “How far have we come? Artificial intelligence
for chest radiograph interpretation,” Clin. Radiol., vol. 74, no. 5,
pp. 338-345, May 2019.

G. Montavon, W. Samek, and K.-R. Miiller, “Methods for interpreting
and understanding deep neural networks,” Digit. Signal Process.,
vol. 73, pp. 1-15, Feb. 2018.

W. Samek, T. Wiegand, and K. Miiller, “Explainable artificial intel-
ligence: Understanding, visualizing and interpreting deep learning
models,” CoRR, vol. abs/1708.08296, pp. 1-6, Aug. 2017. [Online].
Available: http://arxiv.org/abs/1708.08296

L. Rieger, P. Chormai, G. Montavon, L. K. Hansen, and K.-R. Miiller,
“Structuring neural networks for more explainable predictions,” in
Explainable and Interpretable Models in Computer Vision and Machine
Learning. Cham, Switzerland: Springer, 2018, pp. 115-131.

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

S. Meacham, G. Isaac, D. Nauck, and B. Virginas, “Towards explain-
able Al: Design and development for explanation of machine learning
predictions for a patient readmittance medical application,” in Jnte/-
ligent Computing, K. Arai, R. Bhatia, and S. Kapoor, Eds. Cham,
Switzerland: Springer, 2019, pp. 939-955.

J. Townsend, T. Chaton, and J. M. Monteiro, “Extracting relational
explanations from deep neural networks: A survey from a neural-
symbolic perspective,” JEEE Trans. Neural Netw. Learn. Syst., vol. 31,
no. 9, pp. 3456-3470, Sep. 2020.

(Oct. 2016). Can We Open the Black Box of Al? [Online]. Avail-
able: https://www.nature.com/news/can-we-open-the-black-box-of-ai-
1.20731

B. Heinrichs and S. B. Eickhoff, “Your evidence? Machine learning
algorithms for medical diagnosis and prediction,” Hum. Brain Mapping,
vol. 41, no. 6, pp. 1435-1444, Apr. 2020.

M. Brundage et a/., “Toward trustworthy AI development: Mechanisms
for supporting verifiable claims,” Eur. Commission, Brussels, Belgium,
Tech. Rep., 2020. [Online]. Available: https://ec.europa.eu/digital-
single-market/en/news/draft-ethics-guidelines-trustworthy-ai

(Nov. 2019). Ethics Guidelines for Trustworthy Al. [Online]. Available:
https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-
trustworthy-ai

D. Wang, Q. Yang, A. Abdul, and B. Y. Lim, “Designing theory-driven
user-centric explainable AI,” in Proc. CHI Conf. Hum. Factors Comput.
Syst. (CHI). New York, NY, USA: Association Computing Machinery,
2019, pp. 1-15.

D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, “Network
dissection: Quantifying interpretability of deep visual representations,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,
pp. 3319-3327.

B. Zhou, D. Bau, A. Oliva, and A. Torralba, “Interpreting deep visual
representations via network dissection,” JEEE Trans. Pattern Anal.
Mach. Intell., vol. 41, no. 9, pp. 2131-2145, Sep. 2019.

C. Olah, A. Mordvintsev, and L. Schubert, “Feature visualization,”
Distill, vol. 2, no. 11, p. e7, Nov. 2017.

C. Olah et ai., “The building blocks of interpretability,” Tech. Rep.,
Jan. 2020. [Online]. Available: https://distill. pub/2018/building-blocks/
S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting
model predictions,” in Advances in Neural Information Processing
Systems 30, I. Guyon et ai, Eds. Red Hook, NY, USA: Curran
Associates, 2017, pp. 4765-4774.

A. Jacovi, O. S. Shalom, and Y. Goldberg, “Understanding
convolutional neural networks for text classification,’ CoRR,
vol. abs/1809.08037, pp. 3-7, Apr. 2018. [Online]. Available: http://
arxiv.org/abs/1809.08037

B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
deep features for discriminative localization,” in Proc. IEEE Conf:
Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2921-2929.
R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh,
and D. Batra, “Grad-CAM: Why did you say that? Visual expla-
nations from deep networks via gradient-based localization,” CoRR,
vol. abs/1610.02391, pp. 1-21, Oct. 2016. [Online]. Available:
http://arxiv.org/abs/1610.02391

G. Zhao, B. Zhou, K. Wang, R. Jiang, and M. Xu, “Respond-
CAM: Analyzing deep models for 3D imaging data by
visualizations,” in Medical Image Computing and Computer
Assisted Intervention—MICCAI 2018, A. F. Frangi, J. A. Schnabel,
C. Davatzikos, C. Alberola-Ldépez, and G. Fichtinger, Eds. Cham,
Switzerland: Springer, 2018, pp. 485-492.

S. Bach, A. Binder, G. Montavon, F. Klauschen, K. Miiller, and
W. Samek, “On pixel-wise explanations for non-linear classifier deci-
sions by layer-wise relevance propagation,” PLOS ONE, vol. 10, no. 7,
pp. 146, 2015.

W. Samek, A. Binder, G. Montavon, 8. Lapuschkin, and K.-R. Miiller,
“Evaluating the visualization of what a deep neural network has
learned,” [EEE Trans. Neural Netw. Learn. Syst., vol. 28, no. 11,
pp. 2660-2673, Nov. 2017.

S. Becker, M. Ackermann, S. Lapuschkin, K. Miiller, and W. Samek,
“Interpreting and explaining deep neural networks for classification of
audio signals,” CoRR, vol. abs/1807.03418, p. 3, Jul. 2018. [Online].
Available: http://arxiv.org/abs/1807.03418

A. W. Thomas, H. R. Heekeren, K.-R. Miiller, and W. Samek, “Ana-
lyzing neuroimaging data through recurrent deep learning models,”
Frontiers Neurosci., vol. 13, p. 1321, Dec. 2019.

L. Arras, F. Horn, G. Montavon, K. Miiller, and W. Samek, “‘What
is relevant in a text document?’: An interpretable machine learning
approach,” CoRR, vol. abs/1612.07843, pp. 1-17, Dec. 2016. [Online].
Available: http://arxiv.org/abs/1612.07843
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

[50]

[51]

[52]

[53]

[54]

[55]
[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

V. Srinivasan, S. Lapuschkin, C. Hellge, K.-R. Miiller, and W. Samek,
“Interpretable human action recognition in compressed domain,” in
Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP),
Mar. 2017, pp. 1692-1696.

O. Eberle, J. Buttner, F. Krautli, K.-R. Mueller, M. Valleriani, and
G. Montavon, “Building and interpreting deep similarity models,” IEEE
Trans. Pattern Anal. Mach. Intell., p. 1, 2020.

L. Hiley, A. Preece, Y. Hicks, S$. Chakraborty, P. Gurram, and
R. Tomsett, “Explaining motion relevance for activity recognition in
video deep learning models,” pp. 1-8, Mar. 2020, arX1v:2003.14285.
[Online]. Available: https://arxiv.org/abs/2003.14285

W. Samek, G. Montavon, A. Binder, S. Lapuschkin, and K. Miiller,
“Interpreting the predictions of complex ML models by layer-wise rel-
evance propagation,” pp. 1-4, Nov. 2016, arXiv:1611.08191. [Online].
Available: https://arxiv.org/abs/1611.08191

(2018). Machine Learning and Al for the Sciences—Towards Inter-
pretability. [Online]. Available: http:/Avww.heatmapping.org/slides/
2018_WCCLpdf

(2016). Deep Taylor Decomposition of Neural Networks. [Online].
Available: http:/Aphome.hhi.de/samek/pdf/MonICML 16. pdf

A. Ghorbani, J. Wexler, J. Y. Zou, and B. Kim, “Towards auto-
matic concept-based explanations,” in Advances in Neural Information
Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer,
F. D’Alché-Buc, E. Fox, and R. Garnett, Eds. Red Hook, NY, USA:
Curran Associates, 2019, pp. 9277-9286.

A. Shrikumar, P. Greenside, and A. Kundaje, “Learning impor-
tant features through propagating activation differences,” CoRR,
vol. abs/1704.02685, pp. 1-8, Oct. 2017. [Online]. Available:
http://arxiv.org/abs/1704.02685

L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling, ‘“Visualiz-
ing deep neural network decisions: Prediction difference analysis,”
CoRR, vol. abs/1702.04595, pp. 1-10, Feb. 2017. [Online]. Available:
http://arxiv.org/abs/1702.04595

Y. Zhou, Y. Zhu, Q. Ye, Q. Qiu, and J. Jiao, “Weakly super-
vised instance segmentation using class peak response,” CoRR,
vol. abs/1804.00880, no. 2, pp. 1-8, Jun. 2018. [Online]. Available:
http://arxiv.org/abs/1804.00880

P. Kindermans ef ai, “Learning how to explain neural net-
works: Patternnet and patternattribution,” in Proc. ICLR, May 2018,

pp. I-11.
D. Smilkov, N. Thorat, B. Kim, F. B. Viégas, and M. Wattenberg,
“Smoothgrad: Removing noise by adding noise,” CoRR,

vol. abs/1706.03825, pp. 1-8, Jun. 2017.

L. Arras, G. Montavon, K.-R. Miiller, and W. Samek, “Explaining
recurrent neural network predictions in sentiment analysis,” in Proc. 8th
Workshop Comput. Approaches Subjectivity, Sentiment Social Media
Anal. Copenhagen, Denmark: Association Computational Linguistics,
Sep. 2017, pp. 159-168.

A. Karpathy, J. Johnson, and L. Fei-Fei, “Visualizing and understanding
recurrent networks,” pp. 1-9, Jun. 2015, arXiv:1506.02078. [Online].
Available: https://arxiv.org/abs/1506.02078

M. Paschali, S$. Conjeti, F. Navarro, and N. Navab, “‘Generalizabil-
ity vs. Robustness: Investigating medical imaging networks using
adversarial examples,” in Medical Image Computing and Computer
Assisted Intervention—MICCAI 2018, A. F. Frangi, J. A. Schnabel,
C. Davatzikos, C. Alberola-Lépez, and G. Fichtinger, Eds. Cham,
Switzerland: Springer, 2018, pp. 493-501.

H. D. Couture, J. S. Marron, C. M. Perou, M. A. Troester, and
M. Niethammer, “Multiple instance learning for heterogeneous images:
Training a CNN for histopathology,” in Medical Image Computing
and Computer Assisted Intervention—MICCAI 2018, A. F. Frangi,
J. A. Schnabel, C. Davatzikos, C. Alberola-Lépez, and G. Fichtinger,
Eds. Cham, Switzerland: Springer, 2018, pp. 254-262.

X. Li, N. C. Dvomek, J. Zhuang, P. Ventola, and J. S. Duncan,
“Brain biomarker interpretation in ASD using deep learning and fMRI”
in Medical Image Computing and Computer Assisted Intervention—
MICCAI 2018, A. F. Frangi, J. A. Schnabel, C. Davatzikos,
C. Alberola-Lépez, and G. Fichtinger, Eds. Cham, Switzerland:
Springer, 2018, pp. 206-214.

Y. Qin ef al., “Autofocus layer for semantic segmentation,” CoRR,
vol. abs/1805.08403, pp. 1-8, Jun. 2018. [Online]. Available:
http://arxiv.org/abs/1805.08403

Z. Tang et al, “Interpretable classification of Alzheimer’s disease
pathologies with a convolutional neural network pipeline,” Nature
Commun., vol. 10, no. 1, p. 2173, Dec. 2019.

Z. Papanastasopoulos et al., “Explainable AI for medical imaging:
Deep-learning CNN ensemble for classification of estrogen recep-
tor status from breast MRL” Proc. SPIE, vol. 11314, pp. 228-235,
Mar. 2020.

[70]

(71)

[72]

[73]

[74]

[75]

[76]

(77)

[78]

[79]
[80]

[81]

[82]

[83]

[84]

[85]

[86]

[87]

[88]

[89]

[90]
[91]

4811

H. Lee, S. T. Kim, and Y. M. Ro, “Generation of multimodal justifica-
tion using visual word constraint model for explainable computer-aided
diagnosis,” in interpretability of Machine Intelligence in Medical Image
Computing and Multimodal Learning for Clinical Decision Support,
K. Suzuki et al., Eds. Cham, Switzerland: Springer, 2019, pp. 21-29.
P.-J. Kindermans e¢ al., The (Un)reliability of Saliency Methods. Cham,
Switzerland: Springer, 2019, pp. 267-280.

M. D. Zeiler and R. Fergus, “Visualizing and understanding convo-
lutional networks,” CoRR, vol. abs/1311.2901, pp. 2-3, Nov. 2013.
[Online]. Available: http://arxiv.org/abs/13 11.2901

A. Mahendran and A. Vedaldi, “Understanding deep image repre-
sentations by inverting them,’ CoRR, vol. abs/1412.0035, pp. 2-5,
Nov. 2014. [Online]. Available: http://arxiv.org/abs/1412.0035

A. Dosovitskiy and T. Brox, “Inverting convolutional networks
with convolutional networks,” CoRR, vol. abs/1506.02753, pp. 2-3,
Apr. 2015. [Online]. Available: http://arxiv.org/abs/1506.02753

J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
miller, “Striving for simplicity: The all convolutional net,” CoRR,
vol. abs/1412.6806, pp. 1-10, Apr. 2015.

D. Erhan, Y. Bengio, A. Courville, and P. Vincent, “Visualiz-
ing higher-layer features of a deep network,” Dept. d’Informatique
Recherche Operationnelle, Univ. Montreal, Montreal, QC, Canada,
Tech. Rep. 1341, Jun. 2009.

A. M. Nguyen, J. Yosinski, and J. Clune, “Multifaceted feature visu-
alization: Uncovering the different types of features learned by each
neuron in deep neural networks,” CoRR, vol. abs/1602.03616, pp. 3-4,
May 2016. [Online]. Available: http://arxiv.org/abs/1602.03616

J. Yosinski, J. Clune, A. M. Nguyen, T. J. Fuchs, and H. Lipson,
“Understanding neural networks through deep visualization,” CoRR,
vol. abs/1506.06579, pp. 4-8, Jun. 2015. [Online]. Available:
http://arxiv.org/abs/1506.06579

C. Szegedy et al., “Going deeper with convolutions,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 1-9.

R. Meyes, M. Lu, C. W. de Puiseau, and T. Meisen, “Ablation studies
in artificial neural networks,” CoRR, vol. abs/1901.08644, pp. 3-14,
Feb. 2019. [Online]. Available: http://arxiv.org/abs/1901.08644

R. Meyes, C. W. de Puiseau, A. Posada-Moreno, and T. Meisen,
“Under the hood of neural networks: Characterizing learned rep-
resentations by functional neuron populations and network abla-
tions,” pp. 1-8, May 2020, arXiv:2004.01254. [Online]. Available:
https://arxiv.org/abs/2004.01254

R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad,
“Intelligible models for HealthCare: Predicting pneumonia risk and
hospital 30-day readmission,” in Proc. 21th ACM SIGKDD Int. Conf.
Knowl. Discovery Data Mining (KDD). New York, NY, USA: Associ-
ation Computing Machinery, 2015, pp. 1721-1730.

B. Letham, C. Rudin, T. H. McCormick, and D. Madigan, “Inter-
pretable classifiers using rules and Bayesian analysis: Building a
better stroke prediction model,” Ann. Appl. Statist., vol. 9, no. 3,
pp. 1350-1371, Sep. 2015.

I. Lage et a/., “An evaluation of the human-interpretability of expla-
nation,” CoRR, vol. abs/1902.00006, pp. 4-11, Jan. 2019. [Online].
Available: http://arxiv.org/abs/1902.00006

H. Lakkaraju, E. Kamar, R. Caruana, and J. Leskovec, “Faithful and
customizable explanations of black box models,” in Proc. AAAI/ACM
Conf. Al, Ethics, Soc. New York, NY, USA: Association Computing
Machinery, Jan. 2019, pp. 131-138.

T. Lei, R. Barzilay, and T. Jaakkola, “Rationalizing neural predictions,”
in Proc. Conf. Empirical Methods Natural Lang. Process. Austin, TX,
USA: Association Computational Linguistics, 2016, pp. 107-117.

P. Guo, C. Anderson, K. Pearson, and R. Farrell, “Neural net-
work interpretation via fine grained textual summarization,” CoRR,
vol. abs/1805.08969, pp. 2-5, Sep. 2018. [Online]. Available:
http://arxiv.org/abs/1805.08969

A. Agrawal et al., “WQA: Visual question answering,” Int. J. Comput.
Vis., vol. 123, no. 1, pp. 4-31, May 2017.

J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image
co-attention for visual question answering,” in Proc. 30th Int. Conf.
Neural Inf. Process. Syst. Red Hook, NY, USA: Curran Associates,
2016, pp. 289-297.

A. Das et al., “Visual dialog,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. (CVPR), Jul. 2017, pp. 326-335.

M. Izadyyazdanabadi et ai., “Weakly-supervised learning-based fea-
ture localization for confocal laser endomicroscopy glioma images,”
in Medical Image Computing and Computer Assisted Intervention—
MICCAI 2018, A. F. Frangi, J. A. Schnabel, C. Davatzikos,
C. Alberola-Lépez, and G. Fichtinger, Eds. Cham, Switzerland:
Springer, 2018, pp. 300-308.
4812

[92]

[93]

[94]

[95]
[96]

[97]

[98]

[99]

[100]

[101]

[102]
[103]

[104]

[105]

[106]

[107]

[108]

[109]

[110]

{111

[112]

[113]

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 11, NOVEMBER 2021

D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation
by jointly learning to align and translate,” 2014, arXiv: 1409.0473.
[Online]. Available: http://arxiv.org/abs/1409.0473

P. Zhu and M. Ogino, “Guideline-based additive explanation for
computer-aided diagnosis of lung nodules,” in Interpretability of
Machine Intelligence in Medical Image Computing and Multimodal
Learning for Clinical Decision Support, K. Suzuki et al., Eds. Cham,
Switzerland: Springer, 2019, pp. 39-47.

M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for
deep networks,” in Proc. 34th Int. Conf. Mach. Learn., vol. 70, 2017,
pp. 3319-3328.

A. Ghorbani, A. Abid, and J. Y. Zou, “Interpretation of neural networks
is fragile,” in Proc. AAAI, Jul. 2019, pp. 1-7.

B. Kim et al., “Interpretability beyond feature attribution: Quantitative
testing with concept activation vectors (TCAV),” in Proc. ICML,
vol. 80, J. G. Dy and A. Krause, Eds. PMLR, 2018, pp. 2673-2682.

M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein, “SVCCA: Sin-
gular vector canonical correlation analysis for deep learning dynamics
and interpretability,” in Proc. 31st Int. Conf: Neural Inf. Process. Syst.
Red Hook, NY, USA: Curran Associates, 2017, pp. 6078-6087.

N. Tishby and N. Zaslavsky, “Deep learning and the information
bottleneck principle,” in Proc. IEEE Inf. Theory Workshop (ITW),
Apr. 2015, pp. 1-5.

R. Shwartz-Ziv and N. Tishby, “Opening the black box of deep neural
networks via information,” CoRR, vol. abs/1703.00810, pp. 4-17,
Apr. 2017. [Online]. Available: http://arxiv.org/abs/1703.00810

E. Varol, A. Sotiras, K. Zeng, and C. Davatzikos, “Generative dis-
criminative models for multivariate inference and statistical mapping
in medical imaging,” in Medical Image Computing and Computer
Assisted Intervention—MICCAI 2018, A. F. Frangi, J. A. Schnabel,
C. Davatzikos, C. Alberola-Lépez, and G. Fichtinger, Eds. Cham,
Switzerland: Springer, 2018, pp. 540-548.

G. Alain and Y. Bengio, “Understanding intermediate layers using lin-
ear classifier probes,” pp. 1-9, Nov. 2018, arXiv: 1610.01644. [Online].
Available: https://arxiv.org/abs/1610.01644

T. Hastie and R. Tibshirani, “Generalized additive models,” Stat. Set.,
vol. 1, no. 3, pp. 297-310, Aug. 1986.

Y. Lou, R. Caruana, and J. Gehrke, “Intelligible models for classifica-
tion and regression,” in Proc. 18th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining (KDD). New York, NY, USA: Association for
Computing Machinery, 2012, pp. 150-153.

Y. Lou, R. Caruana, J. Gehrke, and G. Hooker, “Accurate intelligible
models with pairwise interactions,” in Proc. 19t ACM SIGKDD Int.
Conf. Knowl. Discovery Data Mining (KDD). New York, NY, USA:
Association for Computing Machinery, 2013, p. 623.

S. O. Arik and T. Pfister, “Attention-based prototypical learning
towards interpretable, confident and robust deep neural networks,”
CoRR, vol. abs/1902.06292, pp. 3-6, Sep. 2019. [Online]. Available:
http://arxiv.org/abs/1902.06292

N. Hatami, M. Sdika, and H. Ratiney, “Magnetic resonance
spectroscopy quantification using deep  leaming,” CoRR,
vol. abs/1806.07237, p. 3, Jun. 2018. [Online]. Available: http://arxiv.
org/abs/1806.07237

S. Haufe et af, “On the interpretation of weight vectors of lin-
ear models in multivariate neuroimaging,” Neurolmage, vol. 87,
pp. 96-110, Feb. 2014.

K. T. Schiitt, Fo Arbabzadah, S. Chmiela, K. R. Miiller, and
A. Tkatchenko, “Quantum-chemical insights from deep tensor neural
networks,” Nature Commun., vol. 8, no. 1, p. 13890, Apr. 2017.

K. T. Schiitt, M. Gastegger, A. Tkatchenko, and K.-R. Miiller,
Quantum-Chemical Insights from Interpretable Atomistic Neural Net-
works. Cham, Switzerland: Springer, 2019, pp. 311-330.

C. Liaskos, A. Tsioliaridou, S. Nie, A. Pitsillides, S. Ioannidis,
and I. F. Akyildiz, “An interpretable neural network for configuring
programmable wireless environments,” CoRR, vol. abs/1905.02495,
pp. 2-3, May 2019. [Online]. Available: http://arxiv.org/abs/1905.
02495

B. Bede, “Fuzzy systems with sigmoid-based membership functions as
interpretable neural networks,” in Fuzzy Techniques: Theory and Appli-
cations, R. B. Kearfott, I. Batyrshin, M. Reformat, M. Ceberio, and
V. Kreinovich, Eds. Cham, Switzerland: Springer, 2019, pp. 157-166.
M. Kaiser, C. Otte, T. A. Runkler, and C. H. Ek, “Inter-
pretable dynamics models for data-efficient reinforcement learning,”
CoRR, vol. abs/1907.04902, pp. 2-5, Jul. 2019. [Online]. Available:
http://arxiv.org/abs/1907.04902

D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor, “Canoni-
cal correlation analysis: An overview with application to learn-
ing methods,” Neural Comput., vol. 16, no. 12, pp. 2639-2664,
Dec. 2004.

[114]

[115]

[116]

(117)

[118]

[119]

[120]
[121]

[122]

[123]

[124]

[125]

[126]

[127]

[128]

[129]

[130]

[131]

[132]

[133]

A. Hazarika, M. Barthakur, L. Dutta, and M. Bhuyan, “F-SVD based
algorithm for variability and stability measurement of bio-signals,
feature extraction and fusion for pattern recognition,” Biomed. Signal
Process. Control, vol. 47, pp. 26-40, Jan. 2019.

I. Goodfellow et al., “Generative adversarial nets,” in Advances
in Neural Information Processing Systems 27, Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds.
Red Hook, NY, USA: Curran Associates, 2014, pp. 2672-2680.

M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative
adversarial networks,” in Proc. 34th Int. Conf: Mach. Learn., vol. 70,
2017, pp. 214-223.

J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in Proc. IEEE
Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2223-2232.

Y. Zhu, S. Suri, P. Kulkarni, Y. Chen, J. Duan, and C.-C.-J. Kuo,
“An interpretable generative model for handwritten digits synthe-
sis,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2019,
pp. 1910-1914.

R. Krusinga, $8. Shah, M. Zwicker, T. Goldstein, and D. W. Jacobs,
“Understanding the (un)interpretability of natural image distribu-
tions using generative models,” CoRR, vol. abs/1901.01499, pp. 5-9,
Feb. 2019. [Online]. Available: http://arxiv.org/abs/1901.01499

A. Karpathy. (2014). SNE Visualization of CNN Codes. [Online].
Available: https://cs.stanford.edu/people/karpathy/cnnembed

S. Carter, Z. Armstrong, L. Schubert, I. Johnson, and C. Olah. (2019).
Exploring Neural Networks With Activation Atlases. [Online]. Avail-
able: https://distill. pub/2019/activation-atlas

W. Ma, F. Cheng, Y. Xu, Q. Wen, and Y. Liu, “Probabilistic represen-
tation and inverse design of metamaterials based on a deep generative
model with semi-supervised learning strategy,” Adv. Mater, vol. 31,
no. 35, Aug. 2019, Art. no. 1901111.

Y. Yan, J. Zhu, M. Duda, E. Solarz, C. Sripada, and D. Koutra,
“Groupinn: Grouping-based interpretable neural network for classi-
fication of limited, noisy brain data,” in Proc. 25th ACM SIGKDD
Int. Conf. Knowl. Discovery Data Mining. New York, NY, USA:
Association for Computing Machinery, 2019, pp. 772-782.

C. Biffi et ai., “Learning interpretable anatomical features through
deep generative models: Application to cardiac remodeling,” CoRR,
vol. abs/1807.06843, pp. 3-6, Jul. 2018. [Online]. Available:
http://arxiv.org/abs/1807.06843

M. Wang, D. Zhang, J. Huang, D. Shen, and M. Liu, “Low-rank
representation for multi-center autism spectrum disorder identification,”
in Medical Image Computing and Computer Assisted Intervention—
MICCAI 2018, A. F. Frangi, J. A. Schnabel, C. Davatzikos, C. Alberola-
Lopez, and G. Fichtinger, Eds. Cham, Switzerland: Springer, 2018,
pp. 647-654.

R. Fong and A. Vedaldi, “Interpretable explanations of black boxes
by meaningful perturbation,’ CoRR, vol. abs/1704.03296, pp. 1-8,
Jan. 2017. [Online]. Available: http://arxiv.org/abs/1704.03296

D. Alvarez-Melis and T. Jaakkola, “A causal framework for
explaining the predictions of black-box sequence-to-sequence mod-
els,” in Proc. Conf. Empirical Methods Natural Lang. Process.
Copenhagen, Denmark: Association Computational Linguistics, 2017,
pp. 412-421.

D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen,
and K.-R. Miiller, “How to explain individual classification decisions,”
J. Mach, Learn. Res., vol. 11, pp. 1803-1831, Aug. 2010.

P. W. Koh and P. Liang, “Understanding black-box predictions via
influence functions,” in Proc. 34th Int. Conf. Mach. Learn., vol. 70,
2017, pp. 1885-1894.

C.-K. Yeh, J. S. Kim, LE. Yen, and P. Ravikumar, ““Representer point
selection for explaining deep neural networks,” in Proc. 32nd Int. Conf:
Neural Inf. Process. Syst. Red Hook, NY, USA: Curran Associates,
2018, pp. 9311-9321.

C. Ulas, G. Tetteh, S. Kaczmarz, C. Preibisch, and B. H. Menze,
“Deepasl: Kinetic model incorporated loss for denoising arterial spin
labeled MRI via deep residual learning,” in Medical Image Computing
and Computer Assisted Intervention—MICCAI 2018, A. F. Frangi,
J. A. Schnabel, C. Davatzikos, C. Alberola-Lépez, and G. Fichtinger,
Eds. Cham, Switzerland: Springer, 2018, pp. 30-38.

C. J. Scott et al, “Reduced acquisition time PET pharmacoki-
netic modelling using simultaneous ASL—MRI: Proof of concept,” /.
Cerebral Blood Flow Metabolism, vol. 39, no. 12, pp. 2419-2432,
Dec. 2019.

M. Pisov et al, “Incorporating task-specific structural knowledge into
CNNs for brain midline shift detection,” in Interpretability of Machine
Intelligence in Medical Image Computing and Multimodal Learning for
Clinical Decision Support, K. Suzuki et al., Eds. Cham, Switzerland:
Springer, 2019, pp. 30-38.
TJOA AND GUAN: SURVEY ON XAI: TOWARD MEDICAL XAI

[134]

[135]

[136]

[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

[146]

[147]

[148]

[149]

[150]
[151]

F. Zhu, J. Guo, Z. Xu, P. Liao, L. Yang, and J. Huang, “Group-
driven reinforcement learning for personalized mhealth intervention,”
in Medical Image Computing and Computer Assisted Intervention—
MICCAI 2018, A. F. Frangi, J. A. Schnabel, C. Davatzikos,
C. Alberola-Lépez, and G. Fichtinger, Eds. Cham, Switzerland:
Springer, 2018, pp. 590-598.

©. Kocadagli and R. Langari, “Classification of EEG signals for
epileptic seizures using hybrid artificial neural networks based
wavelet transforms and fuzzy relations,” Expert Syst. Appl., vol. 88,
pp. 419-434, Dec. 2017.

T. Zhang, W. Chen, and M. Li, “Classification of inter-ictal and ictal
EEGs using multi-basis MODWPT, dimensionality reduction algo-
rithms and LS-SVM: A comparative study,” Biomed. Signal Process.
Control, vol. 47, pp. 240-251, Jan. 2019.

W. Li, J. E. Cerise, Y. Yang, and H. Han, “Application of t-SNE to
human genetic data,” /. Bioinf: Comput. Biol., vol. 15, no. 4, Aug. 2017,
Art. no. 1750017.

W. Xu, X. Jiang, X. Hu, and G. Li, “Visualization of genetic disease-
phenotype similarities by multiple maps tSNE with Laplacian regu-
larization,” BMC Med. Genomics, vol. 7, no. 2, p. S1, 2014.

N. C. F. Codella, C.-C. Lin, A. Halpern, M. Hind, R. Feris,
and J. R. Smith, “Collaborative human-AI (CHAI): Evidence-based
interpretable melanoma classification in dermoscopic images,” in
Understanding and Interpreting Machine Learning in Medical Image
Computing Applications, D. Stoyanov et al., Eds. Cham, Switzerland:
Springer, 2018, pp. 97-105.

M. Graziani, V. Andrearczyk, and H. Miiller, “Regression concept vec-
tors for bidirectional explanations in histopathology,” in Understanding
and Interpreting Machine Learning in Medical Image Computing
Applications, D. Stoyanov et ai., Eds. Cham, Switzerland: Springer,
2018, pp. 124-132.

H. Yeche, J. Harrison, and T. Berthier, “UBS: A dimension-agnostic
metric for concept vector interpretability applied to radiomics,” in
Interpretability of Machine Intelligence in Medical Image Computing
and Multimodal Learning for Clinical Decision Support, K. Suzuki
et al., Eds. Cham, Switzerland: Springer, 2019, pp. 12-20.

A. E. U. Cerna et al., “Interpretable neural networks for pre-
dicting mortality risk using multi-modal electronic health records,”
CoRR, vol. abs/1901.08125, pp. 3-6, Jan. 2019. [Online]. Available:
http://arxiv.org/abs/1901.08125

J.-B. Lamy, B. Sekar, G. Guezennec, J. Bouaud, and B. Séroussi,
“Explainable artificial intelligence for breast cancer: A visual case-
based reasoning approach,” Artif. Intell. Med., vol. 94, pp. 42-53,
Mar. 2019.

Y. Choi, Y. Kwon, H. Lee, B. J. Kim, M. C. Paik, and J.-H. Won,
“Ensemble of deep convolutional neural networks for prognosis of
ischemic stroke,” in Brainlesion: Glioma, Multiple Sclerosis, Stroke and
Traumatic Brain Injuries, A. Crimi, B. Menze, O. Maier, M. Reyes,
S. Winzeck, and H. Handels, Eds. Cham, Switzerland: Springer, 2016,
pp. 231-243.

O. Maier and H. Handels, “Predicting stroke lesion and clinical out-
come with random forests,” in Brainlesion: Glioma, Multiple Sclerosis,
Stroke Traumatic Brain Injuries, A. Crimi, B. Menze, O. Maier,
M. Reyes, S. Winzeck, and H. Handels, Eds. Cham, Switzerland:
Springer, 2016, pp. 219-230.

B. Kim, C. M. Chacha, and J. Shah, “Inferring robot task plans from
human team meetings: A generative modeling approach with logic-
based prior,” in Proc. 27th AAAI Conf. Artif. Intell. Palo Alto, CA,
USA: AAAI Press, 2013, pp. 1394-1400.

J. Cheng and M. S. Bernstein, “Flock: Hybrid crowd-machine learning
classifiers,” in Proc. 18th ACM Conf. Comput. Supported Cooperat.
Work Social Comput. New York, NY, USA: Association Computing
Machinery, 2015, pp. 600-611.

L. Kuhlmann ef al., “Epilepsyecosystem.Org: Crowd-sourcing repro-
ducible seizure prediction with long-term human intracranial EEG,”
Brain, vol. 141, pp. 2619-2630, Aug. 2018.

M. Wiener, F. T. Sommer, Z. G. Ives, R. A. Poldrack, and B. Litt,
“Enabling an open data ecosystem for the neurosciences,” Neuron,
vol. 92, no. 4, p. 929, Nov. 2016.

F. Jiang et al., “Artificial intelligence in healthcare: Past, present and
future,” Stroke Vascular Neurol., vol. 2, no. 4, pp. 230-243, Dec. 2017.
C. K. Cassel and A. L. Jameton, “Dementia in the elderly: An
analysis of medical responsibility,’ Ann. Intern. Med., vol. 94, no. 6,
pp. 802-807, Jun. 1981.

[152]

[153]

[154]

[155]

[156]

[157]

[158]

[159]

[160]

[161]

[162]

[163]

[164]

[165]

[166]

[167]

[168]

[169]

4813

P. Croskerry, K. Cosby, M. L. Graber, and H. Singh, Diagnosis:
Interpreting the Shadows. London, U.K.: CRC Press, 2017.
S. Pereira, R. Meier, V. Alves, M. Reyes, and C. A. Silva,
“Automatic brain tumor grading from mri data using convolutional
neural networks and quality assessment,” in Understanding and Inter-
preting Machine Learning in Medical Image Computing Applica-
tions, D. Stoyanov et al., Eds. Cham, Switzerland: Springer, 2018,
pp. 106-114.
A. Vilamala, K. H. Madsen, and L. K. Hansen, “Deep convolutional
neural networks for interpretable analysis of EEG sleep stage scoring,”
in Proc. IEEE 27th Int. Workshop Mach. Learn. for Signal Process.
(MLSP), Sep. 2017, pp. 1-6.
P. Van Molle, M. De Strooper, T. Verbelen, B. Vankeirsbilck,
P. Simoens, and B. Dhoedt, “Visualizing convolutional neural net-
works to improve decision support for skin lesion classification,” in
Understanding and Interpreting Machine Learning in Medical Image
Computing Applications, D. Stoyanov et al., Eds. Cham, Switzerland:
Springer, 2018, pp. 115-123.
N. Prentzas, A. Nicolaides, E. Kyriacou, A. Kakas, and C. Pattichis,
“Integrating machine learning with symbolic reasoning to build an
explainable AI model for stroke prediction,” in Proc. IEEE 19th Int.
Conf. Bioinf: Bioeng. (BIBE), Oct. 2019, pp. 817-821.
H. Sun et a/., “Brain age from the electroencephalogram of sleep,”
Neurobiol. Aging, vol. 74, pp. 112-120, Feb. 2019.
F. Eitel and K. Ritter, “Testing the robustness of attribution methods
for convolutional neural networks in MRI-based Alzheimer’s disease
classification,” in Interpretability of Machine Intelligence in Medical
Image Computing and Multimodal Learning for Clinical Decision
Support, K. Suzuki et a/., Eds. Cham, Switzerland: Springer, 2019,
. 3-11.
ral Jansen, T. Penzel, S. Hodel, S. Breuer, M. Spott, and D. Krefting,
“Network physiology in insomnia patients: Assessment of relevant
changes in network topology with interpretable machine learning mod-
els,” Chaos, Interdiscipl. J. Nonlinear Sci., vol. 29, no. 12, Dec. 2019,
Art. no, 123129.
K. Young, G. Booth, B. Simpson, R. Dutton, and S. Shrapnel, “Deep
neural network or dermatologist?” in interpretability of Machine Intel-
ligence in Medical Image Computing and Multimodal Learning for
Clinical Decision Support, K. Suzuki et al., Eds. Cham, Switzerland:
Springer, 2019, pp. 48-55.
C. Zucco, H. Liang, G. D. Fatta, and M. Cannataro, “Explainable
sentiment analysis with applications in medicine,” in Proc. IEEE Int.
Conf. Bioinf: Biomed. (BIBM), Dec. 2018, pp. 1740-1747.
C. P. Langlotz et ai., “A roadmap for foundational research on artificial
intelligence in medical imaging: From the 2018 NIH/RSNA/ACR/the
academy workshop,” Radiology, vol. 291, no. 3, pp. 781-791,
Jun. 2019.
A. Gomolin, E. Netchiporouk, R. Gniadecki, and I. V. Litvinov, “Arti-
ficial intelligence applications in dermatology: Where do we stand?”
Frontiers Med., vol. 7, p. 100, Mar. 2020.
A. J. London, “Artificial intelligence and black-box medical decisions:
Accuracy versus explainability,” Hastings Center Rep., vol. 49, no. 1,
pp. 15-21, Jan. 2019.
S. A. Hasan, Y. Ling, O. Farri, J. Liu, M. Lungren, and H. Miller,
“Overview of the ImageCLEF 2018 medical domain visual question
answering task,” in Proc. CLEF Working Notes, CEUR Workshop,
Avignon, France, Sep. 2018. [Online]. Available: http://ceur-ws.org
A. Ben Abacha, S. A. Hasan, V. V. Datla, J. Liu, D. Demner-Fushman,
and H. Miiller, “VQA-Med: Overview of the medical visual question
answering task at ImageCLEF 2019,” in Proc. CLEF Working Notes,
CEUR Workshop, Lugano, Switzerland, Sep. 2019. [Online]. Available:
http://ceurws.org
A.-K. Dombrowski, M. Alber, C. Anders, M. Ackermann, K.-R. Miiller,
and P. Kessel, “Explanations can be manipulated and geometry is to
blame,” in Advances in Neural Information Processing Systems 32,
H. Wallach, H. Larochelle, A. Beygelzimer, F. D’Alché-Buc, E. Fox,
and R. Garnett, Eds. Red Hook, NY, USA: Curran Associates, 2019,
pp. 13589-13600.
H. Lakkaraju and O. Bastani, “‘How do i fool you?’ Manipulating user
trust via misleading black box explanations,” in Proc. AAAI/ACM Conf.
Al, Ethics, Soc., Feb. 2020, pp. 79-85.
Y. Liu et ai, “Detecting cancer metastases on gigapixel pathology
images,” CoRR, vol. abs/1703.02442, pp. 1-2, Mar. 2017. [Online].
Available: http://arxiv.org/abs/1703.02442
An Explainable Artificial Intelligence System

for Small-unit Tactical Behavior

Michael van Lent
The Institute for Creative Technologies, The University of Southern California
13274 Fiji Way, Marina del Rey, CA 90292 USA
vanlent @ict.usc.edu

William Fisher, Michael Mancuso
Quicksilver Software, Inc.
Irvine, CA USA
bfisher @ quicksilver.com, mmancuso@quicksilver.com

Abstract

As the artificial intelligence (AI) systems in military
simulations and computer games become more complex,
their actions become increasingly difficult for users to
understand. Expert systems for medical diagnosis have
addressed this challenge though the addition of explanation
generation systems that explain a system’s internal
processes. This paper describes the AI architecture and
associated explanation capability used by Full Spectrum
Command, a training system developed for the U.S. Army
by commercial game developers and academic researchers.

Introduction

Full Spectrum Command (FSC) is a “commercial platform
training aid” developed by the USC Institute for Creative
Technologies and Quicksilver Software, Inc. for the U.S.
Army. A commercial platform training aid is a computer
game designed not for entertainment, but as a training tool
to achieve a targeted training objective. Like most video
games and training simulations, Full Spectrum Command
includes a significant artificial intelligence (AI) system that
controls the behavior of the user’s subordinate units and the
opposing forces. This complex AI system, which controls
up to 200 entities and uses 60% of the CPU’s processing
cycles, is an excellent example of game industry AI
technology, a branch of AI little studied by the academic
research community. In addition, Full Spectrum Command
includes an Explainable AI (XAI) feature that is the result
of an academic research effort motivated by previous work
in systems such as Mycin (Shortliffe 1976), Debrief
(Johnson 1994), and the Explainable Expert System
(Swartout, Paris and Moore 1994). The goal of this paper
is to describe the XAI system and its application in Full
Spectrum Command. However, this requires an
understanding of the AI system in FSC that controls the
Non-Player Characters (NPCs) (i. the computer

 

Copyright © 2004, American Association for Artificial Intelligence
(www.aaai.org). All rights reserved.

900 IAAI EMERGING APPLICATIONS

controlled soldiers). This NPC ATI is interesting in its own
right as an example of a game industry AI system,
including the methodologies and technologies employed by
game developers.

Video games are probably the largest commercial
applications of artificial intelligence today. Many video
game reviews, in magazines or on the web, use the quality
of a game’s AI as a major criterion for evaluation. Most
game developers list “AI Programmer” as a job title and
game technology conferences include numerous talks on
advances in AI technology. While a number of researchers
are working to apply research techniques to video games
(Laird and van Lent 2001, Young et. al. 2004), few
researchers have studied and reported on the AI techniques
being developed by the video game industry. Describing
FSC’s NPC AIT system will providing one example of an AI
system developed by a commercial game developer.

In the military modeling and simulation community the
academic AI research community is well-represented and
state-of-the-art AI technologies are being used in many
systems (Laird, Jones and Neilsen 1994). Interestingly, in
both games and military simulations as the AI systems get
more complicated and generate more complex behaviors a
new problem arises. The correlation between a player or
human controller’s orders and the Al-controlled entity’s
behavior sometimes becomes less obvious as the AI
becomes more sophisticated. One solution is to extend the
AI systems so they can explain their behavior either during
execution or after the fact. Ideally, this Explainable AI can
present the user with an easily understood chain of
reasoning from the user’s order, through the AI’s
knowledge and inference, to the resulting behavior.

Full Spectrum Command includes an Explainable AI
system that allows the user to click on any subordinate
soldier during the after-action review phase of the game
and ask that soldier questions about the current behavior of
its platoon. In addition, the Explainable AI identifies key
events to be highlighted in the after-action review.

The next section of this paper describes the Full
Spectrum Command commercial platform training aid.
 

 

 

 

cs

(is) UNDO

CLEAR FRAGO

OPORD RPCA Li) a9

y

ad

   

Figure 1: The execution matrix development screen from the planning phase. The

LCM

Ca alas

sta aa

eco Teele tenet)

COMMIT ACTIONS.

Ls

column on the right is one phase of an execution matrix populated with a task for each
of four platoons. A fifth platoon has no task assigned.

The following section gives a detailed overview of the
game’s AI system. The next section describes the
Explainable AI system in more detail and fits it into the
context of previous work on explanation systems. The fifth
section details the deployment of Full Spectrum Command
and provides some feedback and evaluation results.
Finally, the last section describes a number of directions for
future research that could improve the effectiveness of the
Explainable AI system.

Full Spectrum Command

Unlike traditional video games, which are designed for
entertainment and profitability, Full Spectrum Command is
designed to train and exercise the cognitive skills involved
in commanding a light infantry company. However, this
doesn’t mean that FSC isn’t entertaining. Entertainment is
used as a powerful tool to increase the application’s
effectiveness as a training system. Because FSC is
engaging, soldiers spend more time using the training aid
and even choose to train in their free time.

Full Spectrum Command is a Windows XP application
that requires a 2GHz Pentium 4 processor, 512 MB of
system RAM, and a GeForce4 graphics accelerator with
128 MB of video RAM. In the training aid the user is
placed in the role of a U.S. Light Infantry Company
Commander with the rank of Captain. The user controls
about 120 soldiers who are divided into 3-5 platoons and
associated elements. The FSC missions focus on urban

combat against an asymmetric opponent (i.e. terrorists and
insurgents who don’t wear uniforms or fight as an
organized military unit). The game’s virtual environment
recreates the physical urban combat training site located at
Ft. Benning, GA. Called the McKenna MOUT (Military
Operations, Urban Terrain) site, this is where light infantry
company commanders conduct live training exercise in
urban combat. Our virtual McKenna can be populated with
up to 80 civilians and opposing soldiers and configured for
daytime or nighttime missions.

Full Spectrum Command’s game-play is most similar to
the Real Time Strategy (RTS) game genre which includes
games such as Warcraft and Age of Empires. In RTS
games the player acts as a commander controlling the
gathering of resources, production of units, and issuing
orders to these units in battles against one or more
opponent commanders. In most RTS games the player
views the world from a top-down or “God’s eye” view.

Full Spectrum Command differs from traditional RTS
games in a number of key ways. First, FSC doesn’t include
the resource gathering and unit production aspects of most
RTS games. In the game, as in real life, company
commanders begin each mission with an assigned company
of soldiers that may or may not be at full strength. A
“mission” in FSC proceeds in three phases: planning,
execution, and after-action review. In the planning phase
the user reads the operation order (the goal and details of
the assigned mission), organizes the composition of his
forces, and creates an initial plan for the entire mission.

IAAI EMERGING APPLICATIONS 901
FULL SPECTRUM COMMAND

DECISION POINTS:

@ @ 12:08:50 - Enemy Contact
@ @ 12:08:50 - Friendly WIA
@ @ 12:08:52 - Enemy WIA
@ @ 12:08:52 - Enemy WIA
@ @ 12:08:54 - Enemy KIA
@ @ 12:08:54 - Enemy WIA
@ @ 12:08:54 - Enemy WIA
@ @ 12:08:54 - Enemy WIA
@ @ 12:08:54 - Enemy KIA

ea eA a

come ae Cle ee Ly

@ @ 12:08:54 - Enemy WIA
@ @ 12:08:54 - Enemy WIA
@ @ 12:08:55 - Enemy WIA
@ @ 12:08:55 - Enemy KIA
By 12:08:55 — Enemy KIA
EN a Cone aa
ca ea

Breach Obstacle, This mission consists of a
Sees

eee
Osea eee aa

SUV OPORD Pat tw nese REVIEW

Taree eee ced
Se gee u nee secure Ey
How many soldiers are in 2nd Platoon?

 

TENET Ce ae tee

Figure 2: The mission playback screen from a Full Spectrum Command after-action

review session.

This plan is represented as an “execution matrix” which
includes a row for each sub-unit of the company (each
platoon) and a column for each time-based phase of the
mission plan (see Figure 1). “Go codes” (key words called
over the radio) or pre-defined time stamps are set up to
coordinate the actions of the platoons and _ transition
between phases of execution. Once the plan is complete
the user moves on to the execution phase by clicking the
“Execute” button.

In the execution phase the user monitors the behavior of
his soldiers as they carry out the execution matrix, gathers
information about the location, composition and actions of
the opposing soldiers, and modifies the initial plan by
issuing “on the fly” orders called fragmentary orders or
Fragos. Unlike the God’s eye view of traditional RTS
games, the user in FSC must monitor the battle and adjust
his plan from the first-person perspective of the company
commander’s avatar in the game world, which is sometimes
fairly far away from the action. This includes the
possibility that the user’s avatar will be killed which results
in immediate mission failure. During the execution phase
the user moves around the environment, sends and receives
radio transmissions from both subordinate and superior
units, monitors the actions of his soldiers and the
opposition, and issues Fragos to modify his initial plan. An
important part of the company commander’s job is
maintaining “‘situational awareness” — a clear mental model
of what is happening in the simulated world. This includes
recognizing when the mission objective has been achieved
or when the company is incapable of achieving the mission.

902 IAAI EMERGING APPLICATIONS

The final phase of Full Spectrum Command is the after-
action review (AAR). Both the military (Morrison and
Meliza 1999) and educational researchers (Katz and
Lesgold 1994) have identified the importance of post-
problem reflection. During the AAR the user and an
instructor are provided with a new set of user interfaces and
tools designed to help uncover what happened, why it
happened, and how the user could have done better. The
AAR interfaces include a mission statistics screen, an
operation order review screen, a screen that presents the
user’s initial plan for review, a similar screen that presents
the opponent’s plan for review, and a mission playback
screen. From an AI perspective the most interesting
components of the after-action review is the mission
playback screen.

The mission playback screen gives the user and
instructor the ability to replay the mission with controls to
pause, rewind and fast forward the playback (see Figure 2).
Unlike the execution phase, the user views the playback
through a God’s eye camera that can move freely to see
what was happening at any point in the game at any time.

The Explainable AI system enhances this playback
capability in two ways. First, a set of rules extracts key
events from the mission log and represents these as hash
marks on a mission time line. Places on the time line where
these event and decision hash marks are tightly grouped are
often particularly important points in the replay. The user
can scroll along the time line and view some details of each
hash mark including buttons to jump to the time and
location of that event or decision. The second
enhancement the XAI system provides is the capability to
click on any Al-controlled soldier in the playback window
and access a pop-up menu of questions that can be asked of
that soldier. The content of these questions and the
corresponding answers, as well as the key events and
decision points, will be described in more detail during the
discussion of the XAI system later in this paper.

The AI System

There are two artificial intelligence systems in Full
Spectrum Command. The Non-Player Character (NPC) AI
controls the behavior of the player’s subordinate units, the
opposing force, and any civilians during the execution
phase. The Explainable AI (XAI) works during the after-
action review phase to extract key events and decision
points from the playback log and allow the NPC AI
controlled soldiers to explain their behavior in response to
the questions selected from the XAI menu. In order to
describe the application of the explainable AI research in
Full Spectrum Command it is necessary to first provide an
overview of the NPC AI which the XAI seeks to explain.
A description of the NPC AI is also interesting as an
example of an artificial intelligence system developed not
by academic researchers but by commercial game
developers. While the underlying ideas have similarities to
a number of AI research areas their realization is driven by
the processing, memory, and development effort constraints
of industry developers.

The NPC AI in Full Spectrum Command is divided into
two AI subsystems; a Control AI and a Command AI (see
Figure 3). The Command AI and Control AI software
modules act as cooperative, event-driven message handlers.
The simulation sends a stream of event messages to the
Control AI which determines if each event requires an
immediate reaction (i.e. return fire or take cover). If so, the
Control AI responds with an action notification.
Depending on the event type the Control AI may pass the
event to the Command AI which generates higher-level
tactical behavior. Because the Command AI operates in a
separate processing thread, it can take additional time to
generate more complex behavior based on a database of
task knowledge.

The Control AI is responsible for reactive behavior and
low level actions of individual soldiers and, in some cases,
the smallest unit or grouping of soldiers (a four soldier Fire
Team). It manages all low level decision-making and
reactive behaviors (e.g. path planning, find cover, return
fire) for NPC Objects. The Command AT is responsible for
the behavior of the higher echelon units such as platoons
(about 30 to 50 soldiers in 3-5 squads), squads (typically 9
soldiers in two fire teams plus a squad leader), and the fire
team behavior not covered by the Control AI. The
Command AI manages all strategic level decision making
functions (e.g. task selection, task assignment, and unit
organization). The Control AI is integrated into the
application’s basic simulation loop along with standard
game components such as rendering, user input and physics
simulation. To maintain an acceptable frame rate the

Manager Services

avenis

 

natificatians

Gama
Engine

 

 

D aGessayy
I'¥ [eujueD

 

 

 

Figure 3: The structure of the AI systems in Full
Spectrum Command.

simulation needs to cycle through this loop at least 30 times
a second placing strict time constraints on each component
of the loop. Thus the Control AI must generate NPC
behaviors, not only quickly, but in a consistently
predictable number of milliseconds. The Command AI
runs in a separate processing thread from the simulation
loop and therefore can take more time to generate higher
level unit behavior without impacting the simulation loop
and frame rate. Both the Control AI and the Command AI
interact with a series of manager services to sense and act
on the state of the simulation. These services include an
NPC manager, a task manager, a map manager a node
manager, and a path manager.

In each cycle through the simulation loop the Control AI
is called to execute the atomic actions of the Control AI’s
current set of tasks. These atomic actions include moving a
soldier or fire team to a new location, changing a soldier’s
stance (standing, kneeling, prone) or firing a soldier’s
weapon. The majority of the Control AI’s time is spent on
path planning for moving soldiers to new locations. Full
Spectrum Command’s map includes an embedded
navigation graph stored in the map manager. NPCs moving
through the environment are actually moving from node to
node along the edges of this graph. The graph is carefully
constructed so that an NPC that moves along the edges of
the graph will never collide with a barrier in the game
world. Thus, the path planning code doesn’t need to know
anything about the locations of buildings, trees and other
barriers in the game world. All this spatial information is
implicitly represented in the navigation graph. The
navigation graph was developed by an automated graph
generation algorithm with a great deal of fine tuning by the
developers. This is a common approach to path planning in
the game industry that has the advantage of very efficient
path planning through an A* search over the navigation
graph. The navigation graph does require some additional

IAAI EMERGING APPLICATIONS 903
storage, beyond the polygonal map representation used by
the graphics renderer, and fine tuning the graph is a time
consuming process. First person shooter games, such as
Quake and Unreal Tournament, usually include fully
automated navigation graph generation which allows users
to easily develop and modify maps without having to
embed their own navigation graphs. The transfer of such
computationally intense tasks from run time to compile
time is key to the success of this implementation strategy.

The Control AI changes the stance of an NPC soldier by
simply updating the state of that NPC in the NPC manager
which in turn changes how the character is rendered by the
graphics engine. Weapon firing is decided when a line of
sight calculation determines that an NPC has spotted an
opposing soldier. The Command AI sends a message to
the Control AI calling for a fire action and specifying a
shooter NPC and a target NPC. The fire-at-enemy action
notification is then sent by the Control AI to the simulation.
The outcome of the weapons fire is determined by a
statistical “combat calculator” that takes into account
factors such as range, shooter and target stance, weapon
type, and the target’s body armor.

The Command AI, running in a separate thread, takes as
input the platoon-level tasks assigned by the user, the task
knowledge stored in the task manager, and a constantly
updated list of events passed from the simulation thread
through the Control AI to the Command AI. Full Spectrum
Command tasks are organized into a task hierarchy
containing N total tasks in M levels. At the top of the task
hierarchy are N platoon-level tasks, such as clear-building,
move-to-checkpoint, and execute-supporting-fire, and M
high-level tasks used only by the opposing force (or
OPFOR), such as shoot-and-scoot and ambush. As
described previously, during the planning phase of FSC the
user creates an execution matrix that assigns one of these
platoon-level tasks to each platoon for each phase of the
mission. Similarly, for each mission the mission designer
creates an execution matrix assigning tasks for each
OPFOR unit. The Command AI takes each top-level task
and decomposes it into sub-tasks, sub-sub-tasks and so on.
Each time a task is decomposed the Control AI breaks the
associated unit into sub-units and assigns a sub-task to each
unit.

For example, when a platoon begins a clear-building task
the Command AI will decompose that task into two sub-
tasks; secure-building-perimeter and assault-building. The
platoon will be broken into two parts, in this case a single
security squad and a multiple squad assault unit. The
security squad will be assigned the secure-building-
perimeter tasks while the assault unit will be assigned the
assault-building tasks. The assault-building task will then
be further decomposed into one or more clear-floor tasks,
one for each floor of the building. The assault unit will be
divided into floor clearing units (typically squads), who are
then assigned to clear the floors in a specific order, one at a
time. The clear-floor task for the initial floor will be
divided again into multiple clear-room tasks, assigned to
fire teams, with each clear-room task resulting in a set of

904 IAAI EMERGING APPLICATIONS

movement orders for the individual soldiers to achieve
“positions of dominance” within that room. Once the first
room is cleared that fire team becomes available and can be
assigned to another clear-room task on the same floor. In
decomposing the tasks and forces the Command AT
generally follows U.S. Army doctrine although some
deviations do occur due to difficulties in getting subject
matter experts to agree on an interpretation of the doctrine
and constraints on the development timeline. While these
differences aren’t apparent when observing the resulting
behavior they will have ramifications for the explainable
AL

Events in Full Spectrum Command consist of a set of
pre-conditions, a set of goal conditions, and a set of task
abandonment conditions. In addition, each task includes a
list of child tasks in the task hierarchy and is associated
with a C++ function that is called to execute the task. A
task is executed when it is invoked by a parent task (or
directly from the user’s initial plan) and its pre-conditions
are satisfied by the events passed to the Command AI.
When a task is executed its C++ function is called which
performs the unit and task decomposition and/or sends an
action notification to the Control AI. If a task’s
abandonment conditions are met the Command AT stops
executing the task and a report is sent to the user. For
example, if too many soldiers are wounded or killed a unit
may become “combat ineffective” in which case the
remaining soldiers will seek cover and a radio report to the
user will report that the unit has taken heavy casualties.

Explainable AI in Games and Simulations

As the previous description makes clear, the behavior of
the individual NPCs in Full Spectrum Command starts with
the user’s platoon-level orders but is the result of a number
of unit and task decomposition mixed with the reactive
behavior of the Control AI. Other video games and
military simulation systems include similarly complex
processes to generate entity level behavior from orders to
larger groups or units. As these AI systems have become
more complex their inner workings have become less
obvious to users. In training simulations, this can result in
users who feel their orders don’t really affect the NPC’s
behavior or users who may try to place blame for failure on
the AI systems rather than their own orders.

Related Work

Interestingly, the problem of explaining the internal
processing of an AI system has been previously considered
by the research community only in a different context.
Medical diagnosis expert systems, such as Mycin
(Shortliffe, 1976) used a complex set of rules to diagnose
illness and suggest treatments based on patient statistics
and test results. Early on the developers of these systems
realized that doctors weren’t willing to accept the expert
system’s diagnosis on faith. As a result these systems were
designed not only to diagnose an illness but also provide
explanations of how the diagnosis was generated.
Explanation systems have also been applied to educational
applications such as teaching LISP programmers to
improve their code (Swartout, Paris and Moore 1994) and
to assist knowledge engineer with debugging description
logics (McGuinness and Borgida 1995).

One previous effort in the military simulation field is
the Debrief system (Johnson, 1994). Debrief allows Soar
agents to justify their actions in the TacAirSoar tactical air
combat domain (Laird, Jones and Nielsen 1994). Debrief
uses Soar’s built in learning mechanism to continually store
the agent’s state during a mission. During the after action
review (or debriefing) the system can recall the state of the
agent at various points during the mission and experiment
with different modifications to that state to determine
which aspects of the state were critical to the agent’s next
decision. By performing this analysis Debrief can report
the critical attribute values that resulted in the agent’s
decision. Debrief can also justify an agent’s internal
beliefs through a similar process of recalling an agent’s
state and examining how the agent’s beliefs change in
response to changes to state attribute values. Debrief
explanations are presented through structured natural
language phrases and graphical displays of aircraft
positions. Debrief makes use of a number of Soar-specific
mechanisms, such as chunking, and thus isn’t directly
applicable to systems not based on Soar. However, the
underlying idea of logging an agent’s behavior and
examining that log during the after-action review to extract
critical state features and explanations was influential in the
development of the XAI system for Full Spectrum
Command.

Explainable AI in Full Spectrum Command

The Explainable AI system in Full Spectrum Command
logs the activities of the NPC AI system during the
execution phase and uses that log during the after-action
review phase. The log consists of a long sequence of AI
events records each with an associated time stamp. The
events that are recorded in the log include:

Weapon Fire: generated when an NPC shoots at another
NPC. Records the weapon type, range, shooter and target
details, chance to hit, hit result (did the shooter hit) and the
effect of the shot on the target.

Unit Generation: generated when the player or NPC AI
creates a new unit (ie. the player creates a fourth platoon
within the company or the NPC AI decomposes a platoon
into squads). Records the unit id, parent unit’s id,
formation, unit purpose (security, assault, guard...), NPC
count, task assignment, status (idle, waiting, active), leader
id, and whether the unit is in position or on the move.

Unit Update: generated when the details of a unit are
modified by the player or NPC AI. Records the aspects of a
unit that could be changed which are formation, unit
purpose, task assignment, status and in-position. NPC
count doesn’t change since wounded or killed NPC are still
considered part of the unit.

Task Generation: generated when a unit is assigned a new
task (i.e. clear-building is decomposed into  secure-
building-perimeter and assault-building). Records task id,
id of the unit to which the task is assigned, task status
(complete, incomplete, inactive) and parent task id.

Task Update: generated when the details of a task are
modified (i.e. records that secure-building-perimeter is
completed). Records the task id and task status which is
the only detail of a task that can change.

A typical mission in FSC will take between 30 minutes and
two hours and will generate a log consisting of thousands
or tens of thousands of AI event records. These records are
continually streamed to the hard drive as part of the after-
action review file generated for each mission.

At the beginning of the after-action review phase the AI
log is read from the hard drive and the XAI system reads
through all the records. During this initial pass the XAT
system gathers mission statistics and a list of important
events to support the after-action review process. The
mission statistics gathered includes mission duration,
casualty statistics and ammo usage statistics. While
gathering these statistics doesn’t involve any AI technique,
the information logged for the XAI is the easiest place to
gather this data.

During this first pass through the logged AI events the
system also gathers a list of important events that should be
highlighted for the user. These events are indicated by
hash marks on a mission time line displayed at the bottom
of the mission playback screen (see Figure 2). The key
event hash marks are color coded according to which side
the event relates (blue for the user’s units, red for enemy
units and white for civilians). The XAI system detects nine
types of key event:

e Friendly Soldier Killed in Action (KIA)
Friendly Soldier Wounded in Action (WIA)
Enemy Soldier Killed in Action
Enemy Soldier Wounded in Action
Civilian Killed in Action
Civilian Wounded in Action
Friendly Unit First Contact with the Enemy
Platoon Task Started
Platoon Task Completed
The KIA and WIA events are a subset of the logged
Weapon Fire records in which the effect of the fire is a
killed or wounded NPC. The platoon task started and
completed events are a subset of the logged Task
Generation and Task Completion records in which the unit
associated with the task is a platoon. The XAI system has
the capability to also include squad and fire-team level task
events. However, as a Company Commander the user
should be focusing on the activities of the platoons which
are his immediate subordinate units. Finally, a “First
Contact with the Enemy” event is generated from a
Weapon Fire record that represents the first time an NPC in
the associated unit has fired on the enemy.

The third job of the XAI system is to explain the
behavior of the platoons during the mission playback. As
the user moves through the mission playback the XAI

IAAI EMERGING APPLICATIONS 905
system moves through the logged AI records to maintain
the current state of each unit and task. Similar to Debrief,
the XAI system uses the logged information to recall the
state of the NPC AI at any point during the mission.
Unlike Debrief, the XAI system doesn’t perform “what if”
simulations but does allow the user to inquire about the
status of tasks and units. On the playback screen the user
can click on any friendly NPC to pop up a menu of
questions the user can select to inquire about the current
behavior of that NPC’s platoon. Early versions of the XAI
system included questions and explanations of squad and
fire-team level behaviors. However, these were later
removed in part to focus the user on the behavior of the
platoons and in part because the explanations uncovered
aspects of the NPC behavior that doesn’t match U.S. Army
doctrine. Limiting the user to a menu of pre-defined
questions ensures that the system can answer the posed
question.

The questions and answers are presented in English.
Each question and answer has an associated sentence (or
multi-sentence) template that is filled in with the details of
the specific situation. The questions available to the user
describe the platoon’s current task, the status of that task,
and the parameters that might affect task execution. These
questions and the associated explanations are presented in
English. These questions are:

What is the platoon’s mission? The answer describes the
platoon’s top-level task and how that task is decomposed
into the next level of sub-tasks. An example answer might
be “2"' platoon’s mission is clear building. This mission
consists of two steps: secure building perimeter and assault
building.”

What is the platoon’s mission status? The answer
describes the status of the platoon’s top-level task (active,
complete, waiting) and the status of each sub-task. An
example answer might be “2™ platoon’s clear building task
is active. lst squad’s secure building perimeter task is
active. 2" squad’s assault building tasks is waiting.”

How is the platoon task organized? The answer
describes how the platoon is decomposed into the next
level of sub-units and the number of soldiers in each sub-
unit. An example answer might be “2” platoon is task
organized into 3 squads. 1% squad has 9 soldiers. 2"
squad has 9 soldiers. 3™ squad has 7 soldiers.”

How many soldiers are in the platoon? The answer
describes the total number of soldiers in the platoon and
how many of these soldiers have been wounded or killed.
An example answer might be “2"° platoon has 25 soldiers.
Of these 2 soldiers are KIA and 1 soldier is WIA.”

What is the platoon’s ammo status? The answer
describes the ammo status of the platoon as green (less than
75% of ammo has been used), red (more than 75% of
ammo has been used), or black (no ammo remaining) for
each weapon type. An example answer might be cegnd
platoon is ammo status black for heavy weapons and ammo
status red for light weapons.”

906 IAAI EMERGING APPLICATIONS

Preliminary Deployment and Evaluation

Full Spectrum Command has been deployed for
evaluation purposes in the Infantry Captain’s Career
Course (ICCC) at the Infantry School at Ft. Benning.
Starting in the summer of 2002 beta versions were used in
classroom exercises to teach concepts such as execution
matrix development and battlefield synchronization. The
final software was delivered in February 2003 and has been
in use since. In addition, Full Spectrum Command was
used in Afghanistan by U.S. soldiers tasked with training
Captains in the newly created Afghan National Army.

In 2003 the Army Research Institute conducted an
evaluation of the pedagogical effectiveness of Full
Spectrum Command. Although the evaluation wasn’t
designed to explicitly test the NPC AI or XAI, some of the
results provide indirect feedback on these systems.
Soldiers who used FSC in the context of the ICCC were
asked to rank the “relative importance of FSC fidelity” in a
number of different areas. The most important area of
fidelity was “tactical blue force” or the NPC AI of the
soldier’s subordinate units. The second most important
area of fidelity was “tactical red force” or the NPC AI of
the opponent units. Overall 88% of the soldiers indicated
that the fidelity of FSC was “excellent” or “adequate.”
Taken together these results suggest that the soldiers were
watching the NPC AI closely and most found it to be
acceptable. Overall 60% of the soldiers felt FSC has high
training value while 18% of the soldiers felt it has little or
no training value. Unfortunately, the XAI was not
explicitly mentioned in the results of the evaluation. As
discussed in the following section, a separate evaluation is
planned that will focus on the effectiveness of the XAT in
answering user’s questions and determining how the system
should be extended to better support FSC’s training
objectives.

Currently Full Spectrum Command is used by
approximately 20% of the instructors who teach the ICCC.
This number is likely to increase as more computers with
the appropriate graphics accelerator cards are obtained and
more instructors are trained to use the training aid. An
expanded and improved version of Full Spectrum
Command, called FSC 1.5, is currently under development.
This second round of development is being funded by the
Singapore Armed Forces in collaboration with the U.S.
Army.

Future Work

The current NPC AI and XAI systems have a number of
limitations that provide fertile ground for future work.
Probably the greatest limitation with the NPC AI is the
difficulty in modifying the map and internal behaviors.
This has become particularly apparent during the
development of FSC 1.5 as we attempt to modify the NPC
AI to follow the doctrine of the Singapore Armed Forces.
A more modular NPC AI system that includes an
automated spatial reasoning system that can automatically
generate a navigation graph would make adding new maps,
tactics and doctrine much easier. To this end we are in the
process of developing a standard software interface
between the simulation system and key components. This
standard interface will modularize these components and
allow different AI systems to be swapped in and out. This
effort will start with the Command AI, the map/navigation
graph manager, and adding a new strategic AI module
described below. Ideally, this will result in a version of
Full Spectrum Command that serves as a research testbed
in which different ideas and approaches can be compared
and evaluated with respect to fidelity, flexibility,
processing demands, and ease of development.

For the XAI, an immediate next step is to evaluate the
current explanation capability to determine if it meets the
needs of the users. This evaluation should include both
testing of the FSC XAI component and an exploration of
common types of explanations that occur in after-action
reviews of live and simulated training exercises. The
fundamental limitation of the XAI is the depth of the
explanations it can provide. The current XAI system is
limited to explaining how the pre-existing task knowledge
was applied. Unfortunately, this task knowledge defines
how each task should be carried out, but doesn’t include
any deeper knowledge about why each task should be
approached in the specified way. For example, the clear-
building task indicates that the secure-building-perimeter
sub-task should be completed before starting the assault-
building sub-task but no information on why this constraint
is necessary. The NPC AI performs these steps by rote
without any knowledge of why it is important to secure the
perimeter. As a result the XAI system can’t answer
questions such as “Why do I need to secure the perimeter
before I assault the building?”

Previous research into explanation systems have
identified this problem and suggested a number of
solutions. The obvious solution, encoding additional
knowledge into each task providing the underlying
motivation, is unwieldy. It requires an additional, and not
insignificant, step each time a task is developed or
modified. A more promising solution is to encode the
domain’s motivating first principles and use an automated
planning approach to develop behaviors in response to
specific task or mission goals. Because the system is
compiling behaviors from first principles, the deeper “why”
questions can be answered by retracing the behavior
generation process. Although this approach is more
complex it has a number of advantages. Behavior
knowledge and explanations are generated by the same
process from the same source and are guaranteed to match.
Once a domain’s first principles are encoded they are likely
to apply to a wide range of goals and behaviors. It is easier
to validate the behaviors once the first principles are
defined. Finally, the automated planner might find a range
of behaviors that can achieve the same task goals
increasing the variability of NPC behavior.

Acknowledgments

This paper was developed with funds of the Department of
the Army under contract number DAAD 19-99-D-0046.
Any opinions, findings and conclusions — or
recommendations expressed in this paper are those of the
authors and do not necessarily reflect the views of the
Department of the Army.

References

Johnson, W. L., 1994. Agents that learn to explain
themselves. In Proceedings of the Twelfth National
Conference on Artificial Intelligence, pgs. 1257—1263.

Katz, S. and Lesgold, A. 1994. Implementing post-problem
reflection within coached practice environments. In
Proceedings of the East-West International Conference on
Computer Technologies in Education. Crimea, Ukraine.
Pgs. 125-130.

Laird, J. E., Jones, R. and Nielsen, P. E. 1994. Coordinated
behavior of computer generated forces in TacAir-Soar. In
Proceedings of the Fourth Conference on Computer
Generated Forces and Behavioral Representation.
Orlando, FL.

Laird, J. E. 2000 It Knows What You’re Going to Do:
Adding Anticipation to a Quakebot. AAAI 2000 Spring
Symposium Series: Artificial Intelligence and Interactive
Entertainment, AAAT Technical Report SS00 -02.

Laird, J. E. and van Lent, M. 2001. Human-Level AI’s
Killer Application: Interactive Computer Games. Al
Magazine, Volume 22 (2), pgs. 15-25.

McGuinness, D. L. and Borgida, A. 1995. Explaining
Subsumption in Description Logics. In Proceedings of the
14” International Joint Conference on Artificial
Intelligence, Montreal, Canada, August 1995.

Morrison, J. E. and Meliza, L. L. 1999. Foundations of the
After Action Review Process. U.S. Army Research Institute
for the Behavioral and Social Sciences Special Report #42.

Shortliffe, E. H. 1976. Computer-based Medical
Consultations: MYCIN. Elsevier, New York.

Swartout, W. R., Paris, C. L., and Moore, J. D. 1994.
Design For Explainable Expert Systems. TEEE Expert,
Volume 6, Number 3, pages 58-64.

Young, R. M., Riedl, M., Branly, M., Jhala, A., Martin, R.
J. and Saretto, C. J. 2004. An architecture for integrating
plan-based behavior generation with interactive game
environments, The Journal of Game Development, Volume

1 (1).

IAAI EMERGING APPLICATIONS 907
NISTIR 8367

Psychological Foundations of

Explainability and Interpretability in
Artificial Intelligence

David A. Broniatowski

This publication is available free of charge from:
https://doi.org/10.6028/NIST.IR.8367

NIST

National Institute of
Standards and Technology

U.S. Department of Commerce
NISTIR 8367

Psychological Foundations of
Explainability and Interpretability in
Artificial Intelligence

David A. Broniatowski
Information Technology Laboratory

This publication is available free of charge from:
https://doi.org/10.6028/NIST.IR.8367

April 2021

«“ gent OF Com,
<,

 

U.S. Department of Commerce
Gina M. Raimondo, Secretary

National Institute of Standards and Technology
James K. Olthoff, Performing the Non-Exclusive Functions and Duties of the Under Secretary of Commerce
for Standards and Technology & Director, National Institute of Standards and Technology
Certain commercial entities, equipment, or materials may be identified in this document in order to describe
an experimental procedure or concept adequately. Such identification is not intended to imply
recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to
imply that the entities, materials, or equipment are necessarily the best available for the purpose.

National Institute of Standards and Technology Interagency or Internal Report 8367
Natl. Inst. Stand. Technol. Interag. Intern. Rep. 8367, 56 pages (April 2021)

This publication is available free of charge from:
https://doi.org/10.6028/NIST.IR.8367
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Abstract

In this paper, we make the case that interpretability and explainability are distinct require-
ments for machine learning systems. To make this case, we provide an overview of the
literature in experimental psychology pertaining to interpretation (especially of numerical
stimuli) and comprehension. We find that interpretation refers to the ability to contextualize
a model’s output in a manner that relates it to the system’s designed functional purpose, and
the goals, values, and preferences of end users. In contrast, explanation refers to the ability
to accurately describe the mechanism, or implementation, that led to an algorithm’s output,
often so that the algorithm can be improved in some way. Beyond these definitions, our
review shows that humans differ from one another in systematic ways, that affect the extent
to which they prefer to make decisions based on detailed explanations versus less precise
interpretations. These individual differences, such as personality traits and skills, are asso-
ciated with their abilities to derive meaningful interpretations from precise explanations of
model output. This implies that system output should be tailored to different types of users.

Key words

abstraction; implementation; fuzzy-trace theory
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Table of Contents

1 Introduction 1
1.1 Why Define Interpretability and Explainability? 1

1.2 Proposed Definitions of Interpretability and Explainability 2
1.2.1 Illustrative Example: Rental Applications 5

1.2.2 Illustrative Example: Medical Diagnosis 7

1.3. Historical context 8

9

2 The Psychology of Interpretability and Explainability

2.1 Interpretations Provide Meaning in Context 9
2.1.1 Categorical Gists 11

2.1.2 Ordinal Gists 12

2.1.3. Precise Verbatim Representations 12

2.1.4 Moving Beyond Rote Optimization: Gists are Insightful 12

2.2 Explanations Emphasize Implementation 13
2.2.1 Causal Mental Models 13

2.3 Individual differences 16
2.3.1 Experts prefer to rely on meaningful interpretations 18

2.4 Relationship of Fuzzy-Trace Theory to Prior Theories 18
2.4.1 Schema Theories and Association Theories 18

2.4.2 Heuristics and Biases 19

2.4.3 Naturalistic Decision Making 19

3 Computer Science Definitions of Interpretability and Explainability 20

3.1 Comparison of Mental Representations to Current Machine Learning Paradigms 20
3.2 Algorithmic Paradigms Designed to Promote Interpretability and Explain-

ability 21
3.2.1 Local Feature Importance 21
3.2.2 “Simpler Models Are Inherently More Interpretable” 25
3.2.3 Limitations of Current Explainable AI Models 29
3.2.4 Purpose-Built Graphical User Interfaces 30
3.2.5 Coherent Topic Models 30
4 Incorporating Insights from Psychology Into Design 31
4.1 Psychological Correlates of AI Expert System Paradigms 32
4.1.1 Coherence and “White-Box Models” 32
4.1.2 Correspondence and “Black-Box Models” 33
4.1.3. A Third Way: Enhancing Interpretability by Querying Human Ex-
perts and “Grey-Box Models” 34

4.2 Interpretable and Explainable Outputs Are Different Abstractions of a System 34
4.2.1 Psychological Evidence that Abstraction Improves Interpretability

and Decision Quality 35

4.2.2 Abstraction Hierarchies in Engineering 36
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

4.2.3 Design for Interpretability and Explainability as Requirements En-
gineering

5 Conclusion
5.1 Implications for Designing Explainable and Interpretable Artificial Intelli-
gence

References

List of Tables

Table | Performance of three hypothetical models to detect online malicious behaviors.

Table 2 Example of SBRL output, which seeks to explain whether a customer will
leave the service provider. PP = Probability that the label is positive. Source:
[147]

37
38

39
40

11

27
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

List of Figures

Fig. 1 Diagram of the interaction between a machine learning model and human

cognition. For example, the initial training dataset may contain records of

the hourly rainfall at a nearby beach. This trained model is then used to

generate predictions from new evaluation data, such as a probability distri-

bution over the amount of rain that a beachgoer might expect in a given hour.

These predictions and other model output are then provided to the human as

a stimulus. The human encodes the stimulus into multiple mental represen-

tations. The verbatim representation, is a detailed symbolic representation of

the stimulus such as a graphical representation of the probability distribution

over rainfall amounts per hour. In parallel, humans employ their background

knowledge to encode a meaningful gist interpretation from the stimulus. For

example, a simple categorical gist might be the distinction between “essen-

tially no chance of rain” vs. “some chance of rain”. Additionally, humans

with appropriate expertise might be able to examine the form of the model

to determine how it arrived at its conclusion. For example, a meteorologist

with domain expertise might be able to examine the coefficients of a model’s

time series equations and recognize it as an indication of an incoming cold

front. A human would then make a decision (e.g., whether or not to go to

the beach) based on a combination of these representations. For example, a

human without technical expertise might look at the stimulus and determine

that the probability of rain is essentially nil, leading them to go to the beach

(since the beach with no rain is fun, and the beach with some rain is not fun,

and having fun is good). On the other hand, a human with meteorology ex-

pertise and data science expertise might recognize the signs of an oncoming

cold front and realize that rain is a non-negligible possibility, leading them

to choose another activity. 3
Fig. 2 Multiple Levels of Mental Representation Encoded in Parallel. 10
Fig. 3 Representational Hierarchy for a Text. By Original uploader was Aschoeke

at en.wikibooks. Later version(s) were uploaded by Asarwary at en.wikibooks.

- Transferred from en.wikibooks, CC BY 2.5, https://commons. wikimedia.org/w/index.php?curid=50€
Fig. 4 Representational Hierarchy for a Support Vector Machine. 15
Fig. 5 An example of output from LIME which emphasizes text features that led a

specific paragraph to be classified as about atheism, rather than Christianity.

The original image may be found at this URL: https://github.com/marcotcr/lime/blob/master/doc/imag
Fig.6 An example of output from LIME which emphasizes input features that are

diagnostic of a specific image classification. In this case, a picture of a husky

was incorrectly classified as a wolf because of the presence of snow in the

background. Such information may help tool developers debug overfit clas-

sifiers. This image was originally presented in [125] 23
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Fig. 7 Anexample of output from SHAP which indicates which indicates the model’s
baseline value, the marginal contributions of each of its features, and the
final prediction. Such an approach is analogous to a graphical interpreta-
tion of linear regression coefficients. The original image may be found at
https://github.com/slundberg/shap

Fig. 8 An example of output from Grad-CAM, indicating which pixels in an image
are diagnostic of the predicted class (dog or cat). The original image may be
found at [129]

Fig.9 An example of output from a GA*M which indicates how several features
(horizontal axes) vary with relative risk of readmission for pneumonia at 30
days (vertical axis). Pairwise interactions are shown in the heatmaps at the
bottom of the figure. The original image is in [34].

Fig. 10 A visualization of Latent Dirichlet Allocation output from [13]. Probabilis-
tic topic models such as LDA map each word in a text corpus to a topic. The
most frequent words in that topic are then presented to humans for interpre-
tation.

Fig. 11 An example of an Abstraction Hierarchy for a ML system designed to make
loan recommendations. Each higher level implemented by the level immedi-
ately below it and each lower level implements a technical solution to carry
out a function specified by the higher level.

24

26

28

31

37
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

1. Introduction

This paper draws upon literature in computer science, systems engineering, and experi-
mental psychology to better define the concepts of interpretability and explainability for
complex engineered systems. Our specific focus is on systems enabled by artificial intelli-
gence and machine learning (AI/ML).

1.1. Why Define Interpretability and Explainability?

We focus on these terms because of their recent importance to the uptake of machine learn-
ing algorithms, as indicated by several recent laws that require algorithmic output to pro-
vide explanations or interpretations to users, who may differ significantly from one another
in terms of their goals, education, or personality traits. For example, the Equal Credit
Opportunity Act (ECOA)

._.Teflect[s] Congress’s determination that consumers and businesses applying
for credit should receive notice of the reasons a creditor took adverse action on
the application or on an existing credit account...to help consumers and busi-
nesses by providing transparency to the credit underwriting process and pro-
tecting against potential credit discrimination by requiring creditors to explain
the reasons adverse action was taken. [Ammermann] (emphasis added)

The implementation of ECOA in Regulation B further specifies that “A creditor must
disclose the principal reasons for denying an application or taking other adverse action...and
accurately describe the factors actually considered or scored by a creditor.” [Com]

Additionally, the European Union’s General Data Protection Regulation (GDPR) re-
quires that AI systems provide human subjects about whom data are being gathered the
right “...to obtain an explanation of the decision reached after such assessment and to
challenge the decision.” [111]!, with similar language reflected in France’s Loi pour une
République numérique [43] and ongoing debate about adopting similar regulations in the
United States in the wake of California’s adoption of the California Consumer Privacy
Act of 2018 and its recently adopted amendment — the California Privacy Rights Act of
November, 2020.

In parallel with attempts to address the societal concerns that drove the adoption of
this legislation, major government investments (e.g., DARPA’s eXplainable Artificial In-
telligence [XAI] program) and highly-cited literature (e.g., [6, 42, 74, 87, 94, 125]) have
attempted to define design requirements that engineers and computer scientists might adopt
in order to determine whether their systems are interpretable or explainable. For example,
Doshi-Velez and Kim [42] define model interpretability as a ML system possessing “the
ability to explain or to present [output] in understandable terms to a human.” Similarly,
Singh defines an explanation by a ML model as a “collection of visual and/or interactive
artifacts that provide a user with sufficient description of a model’s behavior to accurately

 

‘Recital 71, EU GDPR, https://www.privacy-regulation.eu/en/r71.htm
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

perform tasks like evaluation, trusting, predicting, or improving a model.” (Singh, as cited
in [61]). Gilpin et al. [54] posit that a good explanation occurs when modelers or con-
sumers “can no longer keep asking why” in regards to some ML model behavior. Finally,
Rudin [128] defines an interpretable machine learning model as one that is “constrained
in model form so that it is either useful to someone, or obeys structural knowledge of the
domain, such as monotonicity, causality, structural (generative) constraints, additivity, or
physical constraints that come from domain knowledge.” In contrast, she defines an ex-
plainable machine learning model as “a second (posthoc) model [that] is created to explain
the first black box model”.

Although these definitions identify interpretability and explainability as features of ma-
chine learning models, they point to important factors that are beyond the scope of tradi-
tional design: notions of simplicity, utility to the consumer, human comprehension, causal
inference, interaction with domain knowledge, content and context, and social evaluation
(such as trustworthiness).

These definitions, although localized to ML models, may be productively informed by
decades of literature in experimental psychology, which treats interpretability and explain-
ability as psychological constructs. The Key insight of this literature is that interpretation
and explanation are distinct psychological processes, characterized by distinct mental rep-
resentations.” The question of whether or not a result is interpretable or explainable de-
pends on the user. The designer must ask: “explainable or interpretable for whom?” (e.g.
[136]).

1.2 Proposed Definitions of Interpretability and Explainability

Although the terms interpretability and explainability are frequently used interchangeably,
especially in the computer science literature [6], this paper’s fundamental argument is that
interpretability and explainability are distinct concepts.

Interpretation refers to a human’s ability to make sense, or derive meaning, from a
given stimulus [71] (e.g., a machine learning model’s output) so that the human can make
a decision. Interpretations are simple, yet meaningful, “gist” mental representations that
contextualize a stimulus and leverage a human’s background knowledge (see Figure 1).
A gist is a simple, yet productive, representation of a stimulus that nevertheless captures
essential, or meaningful, distinctions that human users need to make informed, insightful
decisions. Thus, an interpretable model should provide users with a description of what
a stimulus, such as a datapoint or model output, means in context. In so doing, it enables
that human to achieve insight by cuing values, goals, and principles which, in turn, enable
high-level decision making.

Whereas humans rely on simple, imprecise gists to make decisions, machine learning
models rely on programmatic verbatim processes to generate predictions. Explanations are
relatively detailed mental representations that seek to describe the mechanisms underly-

 

2 mental representation of a stimulus (e.g., data or model output) is a symbolic image of that stimulus in a
human’s mind.
L9€8' Ul LSIN/8Z09'0 1/610 lopy/:sdyy :wo.y a6reyo Jo aay aiqelfene S| uoWeo!qnd siyL

 

Machine Learning Human Cognition Individual

  
  
 
 
   
  
 
      

 

 

Background Meaningful Differences
Knowledge Gist Interpretation
Evaluation
Data s
=
| 8
Training Associative Verbatim Representation a
Data a
a
Model a
L Prediction a
& Output 2
(6.1342%) a
TC
@
Family of Models Fit Model Explanatory Representation
Va=f(ayxy+...4a,X,) Yn=f(5x1+...+9Xx,) Mi.
frontal iting

 

 

 

 

 

‘movement,

. Decision
Technical

Expertise

Fig. 1. Diagram of the interaction between a machine learning model and human cognition. For
example, the initial training dataset may contain records of the hourly rainfall at a nearby beach.
This trained model is then used to generate predictions from new evaluation data, such as a
probability distribution over the amount of rain that a beachgoer might expect in a given hour.
These predictions and other model output are then provided to the human as a stimulus. The
human encodes the stimulus into multiple mental representations. The verbatim representation, is a
detailed symbolic representation of the stimulus such as a graphical representation of the
probability distribution over rainfall amounts per hour. In parallel, hamans employ their
background knowledge to encode a meaningful gist interpretation from the stimulus. For example,
a simple categorical gist might be the distinction between “essentially no chance of rain” vs. “some
chance of rain”. Additionally, humans with appropriate expertise might be able to examine the
form of the model to determine how it arrived at its conclusion. For example, a meteorologist with
domain expertise might be able to examine the coefficients of a model’s time series equations and
recognize it as an indication of an incoming cold front. A human would then make a decision (e.g.,
whether or not to go to the beach) based on a combination of these representations. For example, a
human without technical expertise might look at the stimulus and determine that the probability of
rain is essentially nil, leading them to go to the beach (since the beach with no rain is fun, and the
beach with some rain is not fun, and having fun is good). On the other hand, a human with
meteorology expertise and data science expertise might recognize the signs of an oncoming cold
front and realize that rain is a non-negligible possybility, leading them to choose another activity.
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

ing these verbatim processes: an explanation of a model result is a description of how a
model’s outcome came to be. Explanations thus seek to describe the process, or rules, that
were implemented to achieve an outcome independent of context. Typically, explanations
are detailed, technical, and may be causative. For example, an explanation may be a pro-
cedure outlining how a model achieved its result. Thus, explanations are typically more
appropriate for technical practitioners, who can rely on extensive background knowledge
to enable debugging tasks.

Although they are not necessarily verbatim processes themselves, explanations are, in
this way, closer to verbatim mental representations than interpretations are. Whereas an
interpretation seeks to make sense of a stimulus presented to a human subject, an expla-
nation seeks to describe the process that generated an output.’ Thus, an explanation of an
algorithm’s output is justified relative to an implementation, or technical process, that was
used to generate a specific output. In contrast, an interpretation is justified relative to the
functional purpose of the algorithm.

Explanations of ML algorithms can provide implementation details for how the algo-
rithm carries out a known set of requirements. In contrast, interpretations justify these
implementations in terms of the system’s functional purpose. For example, the purpose of
a Support Vector Machine classifier is to map datapoints into discrete classes, a task that
must be justified in terms of the utility of the classification to a human decision-maker such
as if this classifier is used to assign job applicants’ resumes into merit-based categories dur-
ing an interview process. The quality of the classification would then be evaluated relative
to the requirements of this interview process — a classifier that is biased (e.g., that makes
classifications based upon categories that are not merit-based, such as age, race, ethnicity,
etc.) or that has a high error rate would be considered a poor classifier because it does not
meet its functional purpose. In contrast, the explanation for why a particular classification
decision was made is typically justified relative to its implementation. For example, when
asking how a particular job candidate was classified as “not eligible”, one must seek an
explanation in terms of the algorithm’s details, such as that the algorithm selected a set of
candidates’ profiles as “minimally acceptable” — i.e., they were support vectors — based on
training data, and that this particular candidate’s qualifications were, on the whole, inferior
to those reference candidates. An even more detailed explanation would entail examining
specific mathematical parameter values, such as the algorithm’s regularization weights, to
understand how specific attributes were combined and how support vectors were chosen.

In this paper, we make the case that explanations and interpretations are distinct mental
representations that are encoded simultaneously, and in parallel, in the minds of the sys-
tem’s users. Furthermore, users differ from one another in the degree to which they are
willing and able to utilize their own background knowledge to interpret detailed technical
information. In effect, interpretable systems should provide no more detail than necessary
to make a consequential decision, with the information provided justified relative to sys-

 

3Consider a car with a “check engine” light that is illuminated. An explanation might indicate that the check
engine light turned on because the car’s internal programming detected fuel flow irregularities. However,
the interpretation for the driver is that the car needs to be taken to a mechanic for further evaluation.

4
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

tem’s functional purpose. In contrast, explainable systems provide detailed mechanisms
underlying how a certain implementation generated a certain output, regardless of what
that output means to the decision-maker. An explanation seeks to replicate the decision in
a more detailed manner, whereas an interpretation seeks to communicate the bottom line
meaning.

The above definitions suggest that the efficacy of interpretations and explanations may
differ between individuals, and indeed, we will review literature showing that they do so
in systematic ways. That is, the audiences for these different types of outputs are likely to
differ, such that developers who lack domain knowledge would be able to use a detailed
mechanistic explanation to ensure that their design meets a specific functional requirement
(e.g., acertain accuracy target), but may not understand the implications of this requirement
for human users. In contrast, users who lack machine learning expertise but possess domain
knowledge would likely find these detailed mechanistic explanations confusing, instead
preferring a simple description of model output in terms of constructs with which they
are familiar. Finally, a developer with domain knowledge can often bring this combined
expertise to bear to make sense of a detailed mechanistic explanation in terms of its ultimate
use case, thus ensuring that the algorithm moves beyond the rote requirements to best
address the user’s needs.

Disentangling explainability — whether one can describe the mechanistic description of
how a system made a specific prediction — from interpretability - whether a human can
derive meaning from a system’s output for a specific use case — may form the basis for ro-
bust and reliable standards for explainable and interpretable ML system design, and should
allow the development of standards that isolate technical design features from specific sys-
tem functional requirements. This, in turn, should allow developers to segment the design
process, such that system requirements may be defined at the appropriate level of abstrac-
tion. Additionally, we expect that better definitions of these terms will allow the ultimate
development of metrics to assure compliance with these standards, thus enabling the cre-
ation of coherent regulatory policies for artificial intelligence that promote innovation while
building public trust.

1.2.1 Illustrative Example: Rental Applications

Applications of machine learning for property rental have recently received negative at-
tention given concerns about potentially discriminatory incidents* and potential violations
of data privacy*. Under these circumstances, algorithmic interpretability may be an en-
abler of transparency, helping users better understand why a given decision was made.

 

“For example, the Landlord Tech Watch website reports on the technologies and
data sources that landlords may use to deny applicants access to affordable housing:
https://antievictionmappingproject.github.io/landlordtech

*For example, the “Locked Out” series reports on incidents of loan and rental applications that were
denied because of improper data cleaning, recording, transportation, etc., and the difficulties that
some renters applicants face in correcting these errors and, consequently, their personal records:
https://themarkup.org/series/locked-out
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

For example, consider an algorithm that recommends rejection of a rental applicant. The
algorithm would make this determination based on a family of mathematical models fit
to training data, followed by evaluation of a model output generated from an additional
held-out datapoint that represents the applicant’s case. An interpretation of the algorithm’s
recommendation would contextualize the datapoint representing the applicant. A human
would use their background knowledge to generate this context. For example, a human as-
sessor might conclude that the applicant was a risk, based on the absence of the applicant’s
rental history. In contrast, a machine learning model would use a combination of training
data and the model selected by the machine learning algorithm (including any associated
sources of bias). Here, the algorithm might associate length of rental history with success,
and therefore also categorize the applicant with a short history represents a financial risk.
As will be discussed below, human interpretations differ from algorithmic ones in that the
former are flexible whereas the latter tend to be brittle. Importantly, both interpretations
are justified relative to a higher-level construct — a “rental history” — that contextualizes
the decision relative to domain knowledge. Furthermore, this output provides the user with
actionable insight. The solution to the problem is not to change the algorithm’s implemen-
tation, but rather for the applicant to establish a rental history. In order to understand the
meaning of this output, the applicant does not need to have any experience in AI or ML;
rather, they must possess sufficient domain expertise to understand why rental histories
are important indicators of approvals (we will discuss how interpretability may vary with
domain expertise in section 2.3).

In contrast, an explanation of the same algorithm’s output would start with the obser-
vation that the applicant was rejected and then seek to answer the question of how that
decision came to be. For example, the explanation might specify that the algorithm was
trained using a logistic regression classifier with specific coefficient values. Given the ap-
plicant’s datapoint, one could then plug the values into the logistic regression equation,
generate the model’s probability of success for the applicant, and then observe that it is
below the decision threshold. This explanation would not necessarily highlight the specific
role of rental history, but a human analyst with access to this equation, and the expertise to
interpret it, might observe that the largest marginal contribution to the algorithm’s decision
is the rental history.° Similarly, a human, asked to explain a rejection, might provide a
causal explanation (“Your application was rejected because you don’t have a rental history.
People without rental histories are higher risk because they don’t have any experience with
paying rent on time, and because we don’t have any evidence that they are responsible. As
a rule, we prefer to rent to people with a reliable record of payments”). However, as we
will discuss below, humans, and especially subject-matter experts, routinely violate such
causal rules when making judgments. Arguably, this is because they are able to recognize
necessary exceptions through the application of educated intuition (however, these same
processes may also be a source of systematic bias if the underlying intuition is uneducated
or otherwise inapplicable).

 

“Importantly, many modern machine learning algorithms, and especially deep neural networks, are not as
easily analyzed by humans.
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

1.2.2 Illustrative Example: Medical Diagnosis

Like rental applications, medical diagnosis is a field about which concerns regarding al-
gorithmic discrimination have been levied, necessitating transparency and, consequently
interpretable AI.’ Consider an AI system designed to make recommendations on antibiotic
prescription for upper respiratory tract infections. For simplicity, we will again assume
that this model is implemented using a logistic regression classifier with two classes cor-
responding to recommendations for, and against, antibiotic prescription. Finally, given the
data reported to the system, suppose that the model has determined that the probability
that a patient has a bacterial illness is 5%-10% [Cen]. The system would then provide the
prescribing physician with a stimulus — the recommendation not to prescribe.

An explanation of this recommendation would reference the model’s implementation.
For example, the system might list the coefficients of the logistic regression model and
the values of all of the model’s variables (whether the patient has a sore throat, pain when
swallowing, fever, red and swollen tonsils with white patches or streaks of pus, tiny red
spots on the roof of the mouth, swollen lymph nodes in the front of the neck, cough, runny
nose, hoarseness, or conjunctivitis [Wor]). Given these coefficients, the system might fur-
ther explain that, when one multiplies the coefficients by the variable values, and then sums
the result, the combined probability that the illness is bacterial is 5%-10%, indicating “No
further testing nor antibiotics” [Wor]. This is where the explanation would stop.®

In contrast, an interpretation of the system’s recommendation would reference simple,
categorical representations of the relative risk and then link these to values. For example,
the following values could apply: when the patient is sick getting better is good, whereas
the staying sick is bad. Additionally uncomfortable side effects are bad (they will make the
patient feel worse) and no side effects are good. Finally, unnecessary prescription promotes
antibiotic resistance, potentially harming others (bad), whereas not prescribing has no ef-
fect on others. Given these values, the system would state: 1) the likelihood antibiotics
would help is virtually nil; 2) antibiotics, if prescribed, could lead to uncomfortable side
effects; 3) using antibiotics when they are not necessary could harm others [23, 24, 72],
indicating “No further testing nor antibiotics”.

Despite these recommendations, there are several reasons why an expert physician
might prescribe antibiotics under these circumstances. For example, the expert might rec-
ognize that a patient is especially susceptible to bacterial infection, or might simply make
the gist-based strategic choice that “it’s better to be safe than sorry” [23].

 

7For example, a recent report [132] indicates how an attempt to adjust a statistical model to account for racial
disparities in training data may have led to under-treatment for African-American patients needing kidney
transplants.

8In practice an expert physician would probably not rely on rote output from this system, instead perhaps
acknowledging that patient followup is needed if the symptoms don’t clear up, since there is a possibility
that the illness could be bacterial. Depending on the specific circumstances and context, this possibility
could even lead the physician to prescribe “just in case” [24].

7
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

1.3 Historical context

Although interest in explainable artificial intelligence (AT) dates back to the development of
expert systems in the 1980s [33, 56], explainability has recently reemerged as a desideratum
for modern complex AI/ML systems. This is, in large part, due to the proliferation of such
systems throughout society and because of the increasingly complex, and computationally
intensive algorithms — sometimes trained on terabytes of data — that are being deployed to
solve real-world problems.

This development is not unique to AJ; rather it is the consequence of an increasingly
complex infusion of technology into all aspects of society. Although our focus here is
restricted to computational, and especially machine learning, technologies, these develop-
ments are part of a larger trend that extends throughout all areas of technology. ? Pervasive
embedded computation has accelerated this trend. It is rare to find a piece of technology
that doesn’t have some kind of computational component — from learning thermostats to
credit score adjudications to visa applications. These technologies also require several dif-
ferent types of expertise to regulate appropriately. First, technical expertise is required to
understand how these technologies operate and, because the technologies are so complex,
this expertise is restricted to a relatively small number of people, whereas the number of
people whose lives are directly impacted has grown significantly. However, many types of
expertise are relevant. For example, evaluating the legal consequences of AI/ML technolo-
gies requires in-depth familiarity with relevant areas of law. Similar concerns may apply to
financial credit assessment, job application review, issues of political and social equity, and
other ethical concerns. Thus, it is not sufficient to query the experts in a specific area. Ef-
fectively assessing, and thus regulating, interpretable and explainable AI systems requires
pooling expertise from across fields that have not traditionally interacted.

We can expect the pace of this trend towards increasingly complex systems to acceler-
ate. This evolution in major technological trends has been documented by the Engineer-
ing Systems movement. [39] Started in the early 2000s, this movement recognized that
technology and modern society are highly intertwined and that the pace of technological
and social change requires that the design of complex systems must adapt to account for
what these scholars have called the “ilities”.!° Explainability and interpretability are both
“ilities” and exhibit similar difficulties associated with their measurement. “Tlities” have
historically been subject to problems of both polysemy, meaning that the same terms are
frequently used to describe distinct concepts, and synonymy, meaning that different terms

 

°Consider automotive technologies: A mechanically-inclined person in the 1950s could often diagnose and
fix problems with their own automobiles. However, as these automotive systems increased in complexity,
professional expertise became increasingly necessary. By the early 2000s, with the rise of embedded com-
puting, specialized tools became necessary to even diagnose, let alone fix, problems. For example, a car’s
internal computer may trigger a check-engine light, but provide no additional information to the driver. Fur-
ther diagnosing this problem instead requires a specialized tool, which provides a diagnostic code that must
be interpreted by trained mechanics who specialize in that particular brand of automobile.
!0So called because several of them end with the suffix “ility”. Key “ilities” include flexibility — the need for
a system to adapt to changes in its environment — survivability — the need for a system to continue operation
despite disruption, etc.
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

sometimes refer to the same underlying construct [28]. Furthermore, these terms entail a
significant social component that cannot be disentangled from core values of users, design-
ers, and decision-makers. Finally, “ilities” have a strong policy component because they
cannot be studied in isolation from their effects on the public, and especially on vulnerable
populations. Thus, attempts to define explainability and interpretability in artificial intelli-
gence are comparable to challenges faced by scholars studying other complex engineered
systems [39], for which definitions of abstract, yet important, concepts such as flexibility,
resilience, etc., strongly depend on social evaluations. Two decades of research in this area
have discovered that these highly-abstract requirements may be difficult to measure in a
standard way because of their highly context-sensitive, socially-contingent nature. Never-
theless, their importance justifies the challenge of establishing standards that are flexible
enough to be responsive in different contexts.

2. The Psychology of Interpretability and Explainability

The definitions of interpretability and explainability proposed in this paper build upon
decades of empirical research in experimental psychology (see [115, 118] for reviews).
We draw upon this extensive literature to posit a distinction between interpretation — the
process of deriving meaning from a stimulus (such as a model’s output) — and explanation
— the process of generating a detailed description of how an outcome was reached (see [55]
for preliminary data supporting this claim). We argue that the relationship between human
decision making and algorithmic decision making is analogous to different levels of mental
representation. Human individual differences also consistently predict which human sub-
jects prefer to rely on these different representations when making consequential decisions,
especially about how to use numerical information [27].'*

2.1 Interpretations Provide Meaning in Context

Interpretable machine learning is concerned with helping humans generate interpretations
of data and model output. Thus, we review literature in human psychology pertaining to
how humans derive interpretations from stimuli, and especially quantitative stimuli. A
leading theory in this area, Fuzzy-Trace Theory, posits that humans encode stimuli into
multiple mental representations simultaneously and in parallel [115] (see Figure 2). These
mental representations vary from one another in their level of precision, with humans pre-
ferring to rely on the least precise representation that still makes a meaningful distinction
when making a decision.

 

lor example, a recent preprint provides some preliminary data suggesting that one leading algorithm for
explainable AI [126] did not improve decision-makers’ objective accuracy — i.e., their ability to retrieve
the right (verbatim) number — whereas a more accurate AI system did. Similarly, a recent human subjects
experiment [107] shows that model transparency — i.e., a model with a small number of features — did not
improve humans’ prediction accuracy and may have actually impaired their ability to correct inaccurate
predictions due to information overload (see also [55]).

9
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

   
 
   
 
   
   

 

 

 

Option A: Not predictive; Option B: Predictive

 

 

Option A: Less predictive; Option B: More Predictive

 

 

 

 

Option A: F,=0.52; Option B: F,=0.78

 

 

 

|

Fig. 2. Multiple Levels of Mental Representation Encoded in Parallel.

10
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Table 1. Performance of three hypothetical models to detect online malicious behaviors.

 

Model Accuracy | Precision | Recall | F, Score
Naive Bayes 0.267 0.563 0.897 | 0.692
Support Vector Machine | 0.732 1.000 0.716 | 0.834
k-Nearest-Neighbor 0.524 0.834 0.637 | 0.722
Logistic Regression 0.907 0.859 0.617 | 0.718

 

 

 

 

 

Humans tend to make decisions based on the simplest of these representations — the
gist interpretations of stimuli. Humans may encode multiple gists at varying levels of pre-
cision, forming a hierarchy of gists [27]. In contrast, algorithms follow only rote verbatim
processes when making predictions. Humans also encode verbatim mental representations,
which are simply detailed representations of the stimulus itself (e.g., raw system output).
These different levels of mental representation of a stimulus are encoded simultaneously
and in parallel [115]. Furthermore, these representations may compete with, or build upon
one another [22] when providing input into human decision-making.

2.1.1 Categorical Gists

Mental representations are hierarchical in nature, with humans preferring to make deci-
sions on the least precise, often categorical, representation of a stimulus. These categories
nevertheless make meaningful distinctions. For example, for numbers, these categorical
representations often take the form of simple contrasts such as between “some” and “none”
of a quantity. (The categorical distinction between “some” and “none” is one of the most
basic gists [22].) Under these circumstances, humans draw upon their prior knowledge
when making these determinations. For example, consider a set of machine learning mod-
els that are designed to detect malicious online behavior. A social media platform might use
this classifier to automatically remove accounts that seem to violate the platform’s Terms
of Service (see Table 1).

A computer scientist evaluating these classifiers may notice that the k-Nearest-Neighbor
(KNN) classifier has an accuracy of 52.4% on this binary classification task and determine
that it has “essentially no” predictive accuracy (where 50% is the performance of a random
coin flip). Notably, this assessment requires some background knowledge: 1) that there
are only two classes; 2) that the classes are balanced in the training set. In contrast, the
accuracy of the other two models would both be an improvement over that of the SVM.
Both would have “some” accuracy. In this case, it would rule out the KNN classifier. How-
ever, these gists are not just simple — they are also insightful. For example, the Naive Bayes
Classifier has an accuracy of 26.7% which, although the smallest value, should also be clas-
sified as having a gist of “some accuracy” since a computer scientist would recognize that,
for binary classifiers, an accuracy of 26.7% is equivalent to an accuracy of 73.3% if one
simply flips the class labels. In contrast, a novice applying verbatim rules would not share
this insight and might erroneously consider the Naive Bayes Classifier to be less predictive

11
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

than the KNN classifier.

Humans, when deciding, must draw upon values [135] to determine which category, in
a binary pair, is better. Here, some accuracy is good and no accuracy is bad. These binary
valences are held in long-term memory [114] and constitute part of what the human brings
to the evaluation process.

2.1.2 Ordinal Gists

Several options can have the same gist. For example, all but the kNN classifier models
have “some” accuracy. Here, categorical gists do not make meaningful distinctions and
therefore cannot assist a decision. To distinguish between these classifiers, a more precise
level of mental representation can be used. The Logistic Regression classifier has “more
accuracy” than the other classifiers. “More” vs. “less” is an ordinal gist. However, this gist
is only helpful in selecting a model when there is a single evaluation metric. In practice,
ML models may be evaluated using several metrics. For example, the Naive Bayes model
has higher recall, but lower precision, whereas the Logistic Regression model has higher
precision but lower recall. Thus, these models cannot be ranked along these dimensions
using only an ordinal gist.

2.1.3 Precise Verbatim Representations

Typically, practitioners try to reduce these multiple metrics to a single metric for compar-
ison purposes. The process of deriving these composite, precise metrics requires rote ap-
plication of mathematical rules. For example, one may rely on a composite metric, such as
the F,-score (i.e., the harmonic mean of precision and recall). In this example, the Logistic
Regression and Naive Bayes classifiers have equal values of F,, which means humans (or
algorithms) relying on this verbatim rule would be indifferent between the two. In contrast,
the Support Vector Machine has the highest F, score of the three models.

2.1.4 Moving Beyond Rote Optimization: Gists are Insightful

Does this mean that the Support Vector Machine is the best model? Although humans
encode multiple levels of representation in parallel, we prefer to make decisions based
on gists whenever possible. These gists are not arbitrary, but correspond to meaningful
distinctions. In the case of machine learning models, we use these models to achieve a
goal. In the example above, the classifier was used to identify malicious online behavior.
In the context of this task, it makes sense to favor precision over recall and accuracy since
the consequences of a false positive are significantly worse than those of a false negative. A
novice might blindly apply this rule and thus select the Support Vector Machine. However,
a human expert would not necessarily do so. Consider that the Support Vector Machine
has a precision of 1.000 — a “perfect score”. Although someone relying on the verbatim
representation — such as an inexperienced student — might judge this to be the best possible
precision score, an experienced modeler would recognize that such a high value could be an

12
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

indicator of a problem in the algorithm’s implementation. For example, one might achieve
perfect precision if only a very small number of cases are being correctly classified. The
corresponding gist would be “too good to be true”. As described above, the KNN classifier
would be ruled out because it has a gist of no accuracy which, regardless of precision, is
problematic. Thus, the human expert might rely on an ordinal gist to choose the model
with “better” precision — the Logistic Regression model — because contextual cues indicate
that the other two models are inferior for the functional purpose — 1.e., the goal — of the
machine learning task. Research based on Fuzzy-Trace Theory has shown that models
that emphasize the gist — such as by displaying output in ways that more readily allow
users to draw meaningful conclusions — inspire greater trust, confidence, and understanding
[37, 146? ]. This implies a clear design goal for ML system designers who are concerned
with interpretability — system output must communicate the gist.

2.2 Explanations Emphasize Implementation

One of the key tenets of Fuzzy-Trace Theory is that humans encode multiple mental rep-
resentations. Whereas interpretations are mental representations that communicate new
categorical insights, explanations are detailed mental representations that communicate
the implementation mechanisms that led to a particular output.

2.2.1 Causal Mental Models

Several theories of explanation emphasize the importance of inferring the “causal chain”
leading to a specific model’s output (for a review, see [94, 95])2. Lombrozo’s [86] review
of explanations in human cognition also indicates that causal structures are one enabler
of explainability '* Following these traditions, the XAI literature has focused on causality
as the theoretical foundation of explanation. For example Hoffman & Klein [65] relate
explanation to causal inference, especially arguing that humans explain things by creating
prospective (i.e., predictive) causal explanations and highlighting a lacuna in the litera-
ture around this specific type of explanation, instead arguing that most prior work has fo-
cused on physical causality and retrospective causality. Hoffman et al., [66] describe prior
work extracting various different structures of causal chains pertaining to events around
the world, and Klein [74], further develops this theme by claiming that causal networks
can and should be the basis for communicating explanations. Finally, Hoffman et al. [64]

 

'2Beyond the role of causality, these theories assert that explanations are “contrastive” (e.g., Why P rather
than Q?). As will be discussed in 2.3 some human subjects are more likely to carry out these contrastive
comparisons between successive stimuli. Beyond causal relationships, Miller also emphasizes that expla-
nations also entail social attributions [88].

One of Lombrozo’s primary insights is that explanations are nor exclusively causal, but rather that similarity
and diversity of stimuli can help subjects derive general principles that go beyond the surface forms of the
causal structures that one might infer. These general principles draw upon extensive prior knowledge that
make certain explanations more probable than others. This implies that attempts to define “explainability”
(here, understood as encompassing both explanation and interpretation) entirely based on causality (or
conversely, on counterfactual reasoning) may be missing an essential element — generalizability/gist.

13
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

 

Fig. 3. Representational Hierarchy for a Text. By Original uploader was Aschoeke at
en.wikibooks. Later version(s) were uploaded by Asarwary at en.wikibooks. - Transferred from
en.wikibooks, CC BY 2.5, https://commons. wikimedia.org/w/index.php?curid=5063569

emphasize the role of exploration in forming causal explanations, differentiating between
global and local explanations that accord with users’ mental models, and emphasizing lo-
cal explanations’ needs for contrastive or counterfactual accounts; implicitly putting causal
structures at the center of an explanation. According to these scholars, an explanation is,
at its core, a causal mental model.“

These claims are supported by extensive prior work in psycholinguistics and narrative
reasoning, especially in the legal domain. Several decades of prior work in psycholinguis-
tics have emphasized the role of causal structure building in generating a “situation model”
—i.e., a structured mental representation — of a given text (see Figure 3). By analogy, a
similar hierarchy may be constructed for ML model output (see Figure 4).

Causal relations are among the most important (although certainly not the only) types of
inferences that are extracted from narrative texts by readers seeking to comprehend a text.
Furthermore, studies in psycholinguistics have identified a narrative’s causal coherence as
a key factor driving a story’s comprehensibility [137, 143]. Although several dimensions
of narrative coherence have been identified [50, 110, 151], there is a consensus within
the literature that coherent narratives allow readers to construct causal situation models of
the events described [41, 51, 89, 138, 143]. By analogy, one might expect that system

 

'4Notably, these authors identify several structures beyond causal chains, including abstractions, “swarm”,
etc., and relate these to different types of explanations, concluding that “The property of ’being an ex-
planation’ is not a property of statements: It is an interaction of statements with knowledge, context, and
intention.”

14
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

3-Class classification using Support Vector Machine with custom kernel
5.0
45

4.0

Model Output as

3.0

25

 

 

 

 

 

se 1
maximize f(c1 ...¢,) = Ya 2  wei(e(@) 9(%j))yje;
Mathematical eet
; = = icik(Bi, B;)yj¢;
Basis ye 2202 yicsk(%;, Bj ye;

 

en
Mane ber

Surface Form
(Code Base)

 

arent

£ Plt tne decision bomdry. For that, wt wit asionoeatr tech
I
sin x
in,

 

Fig. 4. Representational Hierarchy for a Support Vector Machine.

15
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

users seeking to understand or explain model output would be assisted by coherent causal
explanations that would explain how a system came to a certain conclusion.!5

This interpretation of the literature is supported by an extensive body of work on mental
models, which studies how technical experts represent, and make decisions about, com-
plex systems. Although a full review of this literature is beyond the scope of this paper,
scholars agree that mental models “represent (perceived) cause-and-effect dynamics of a
phenomenon” [68], thus helping people to make predictions. Thus, literature that seeks to
derive causal descriptions of complex system operation is concordant with classical mental
model theories. Finally, these causal representations are also widely used in legal reason-
ing. For example, the “story model” of juror decisionmaking [103] generally assumes that
jurors jointly construct a causal story regarding the facts of a court case, and that these
causal stories are then matched to verdicts. Similar techniques for building “cognitive
maps” have been applied to fields as diverse as political science [10], and power plant de-
sign [99]. Thus, several domains have independently converged on the same conclusion —
that comprehension of mechanisms is facilitated by a structured causal model. However,
these domains also agree that applying these mechanisms to real-world problems must go
beyond causal reasoning. In the story model, jurors must match story structure to knowl-
edge about verdicts. In the mental model literature, Rasmussen’s [108] abstraction hierar-
chy has been widely applied to demonstrate the contingency of causal representations on
functional purpose. Finally, in the narrative comprehension literature, causal structures ex-
ist at multiple levels, with several substructures. Furthermore, the most meaningful levels
in these substructures interface with other narrative elements associated with preferences
(e.g., goals and characters) [58]. Importantly, in each of these cases, later research has
documented that causal explanations are interpreted before a decision is made. Beyond
Rasmussen’s abstraction hierarchy, which applies to causal mental models [99], scholars
of legal reasoning have found that amounts awarded as damages for legal verdicts depend
on categorical and ordinal contextual cues that allow jurors to compare amounts to a mean-
ingful reference point [63].

Thus, although one might think that a meaningful interpretation is a consequence of an
unambiguous or otherwise precise explanation, Fuzzy-trace theorists have shown that dif-
ferent mental representations are encoded in parallel. This means that a mental representa-
tion that provides an interpretation can be distinct from a mental representation providing
an explanation, and vice versa. As will be discussed in the next sections, the choice of
mental representation on which to rely is also a function of individual differences in skills
and personality traits.

2.3 Individual differences

Although one might think that a meaningful interpretation is a consequence of an unam-
biguous or otherwise precise explanation, Fuzzy-trace theorists have shown that different

 

These structures, in turn, can facilitate extraction of gist of the story (see [25], for a review); however, the
causal structures are not, themselves, the gist.

16
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

mental representations are encoded in parallel. Meaningful interpretations and mechanistic
explanations are often not derived from one another or from precise verbatim data. The
choice of mental representation on which a human relies is also a function of individual
differences in skills and personality traits.

Humans differ from one another in systematic ways. Some of these differences are
matters of skill. For example, a professional computer scientist with years of training is
endowed with a skillset that is quite different from that of a professional legal scholar.
Individuals with relevant skills may prefer to rely on more precise levels of mental repre-
sentation if they have the ability to process them. For example “numeracy” — mathematical
ability [44, 84, 104] — allows individuals to make sense of complex numeric data such as
percentages and fractions, such that they are less susceptible to statistical bias when making
decisions. Similarly, in a machine learning context, [67] found that users with computer
science backgrounds (and especially doctoral-level training) were more likely to agree that
the system was useful and trustworthy if they understood how the system worked (and vice
versa), and Linderholm et al. [85] found that more skilled readers, and those with more
relevant background knowledge, were better able to extract the gist from narratives with
poorly-defined causal structures. These interpretive processes are associated with domain
expertise [115] — a hallmark of gist processing [121].

Other differences are matters of personality traits. For example, some individuals pre-
fer to rely on their “gut feelings” — i.e., their intuitive judgments — when making a decision,
whereas others prefer to engage in extensive deliberation. The Cognitive Reflection Test
(CRT; [46]) is a measure of this trait (although it is also correlated with numeracy and in-
telligence [84, 104]), and researchers have found that individuals with high CRT are less
susceptible to decision biases that oppose intuitive to deliberative modes of thought (such
as the well-known “framing effect”; [141]). Similarly, the Need for Cognition (NFC) Scale
[31, 32] measures subjects’ preferences to exert mental effort. For example, [57] describes
evidence for a model of narrative comprehension in which multiple levels of mental rep-
resentation are encoded, with some readers preferring to use coherence-building strategies
relying on effortful “close-to-the-text” reading and those who utilize a more interpretive
strategy that is “farther” from the text. In the domain of decision-making under risky, re-
searchers have found that individuals possessing high NFC scores are more likely to answer
several risky choice framing problems consistently [38, 82], presumably because they ex-
ert effort to notice similarities or contradictions between different problems with similar
structure. This explanation of these findings is supported by evidence that within-subjects
comparisons between stimuli can lead subjects to censor gist-based responses when con-
tradictions are detected, thus encouraging subjects to focus on more detailed features [27].
Similarly, research has shown that some human subjects have difficulty making determina-
tions about whether models are fair” or just” — both categorical gists — in the abstract (1.e.,
absent important context), and instead compare those explanations to prior experience or to
a second system, enabling ordinal comparisons ("more fair/just” vs. “less fair/just’) [12].
For this reason, Mittelstadt and colleagues [97] argue that models should be contrastive to
facilitate interpretability. However, these authors also take pains to emphasize that such

17
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

contrastive explanations frequently miss important context — 1.e., they spur reliance on de-
contextualized verbatim representations.

The above discussion implies that there is no single measure for interpretability or ex-
plainability that applies to all humans; however, there may be a measure that can be de-
fined relative to the expected distribution of skills and personality traits for each intended
audience. Future work should therefore focus on characterizing these factors within user
communities.

2.3.1 Experts prefer to rely on meaningful interpretations

Above, we stated that most individuals reason, recall, and prefer to rely on less precise
representations when making decisions [118]. This reliance on gist representation is a de-
velopmental feature of human cognition: compared to non-experts, experts are more likely
to rely on gist representations in their domains of expertise [7, 123, 124]. Fuzzy Trace
Theory [115] therefore distinguishes between rote knowledge — recall of verbatim facts or
associations — and insightful expertise. Compared to novices, experts are better able ex-
tract the essence, or most relevant information, and ignore less meaningful details [123].
Experts have therefore developed intuitive categorical representations of stimuli that are
simple, yet powerful, and enable them to make decisions. For example, NASA engineers
rely on categorical determinations of “costly” or “costless” when making determinations
about launching cargo missions [90], whereas expert physicians rely on categorical deter-
minations of risk — reflecting an intelligent strategic choice that takes the very low proba-
bility, but non-negligible possibility, that the patient may require antibiotic therapy — when
treating very sick patients who might require antibiotics [23, 24, 72].

2.4 Relationship of Fuzzy-Trace Theory to Prior Theories

Fuzzy-Trace Theory moves beyond alternative accounts that are found in the AI and psy-
chology literatures.

2.4.1 Schema Theories and Association Theories

Several prior theories may be categorized into two broad groups: schema theories and as-
sociation theories (e.g., [49, 133]). Schema theories posit that humans use higher-level data
structures — called “schemata” or “frames” — that impose “top-down” structure on memo-
ries and experiences, making sense of world stimuli, and thus imposing biases. In contrast,
association theories assume that meaning emerges “bottom-up” from frequently-observed
patterns that co-occur in the world. Rather than making sense of these co-occurrence pat-
terns, associationist theories posit that meaning is simply a function of statistical regularity.
As early as 1983, Alba and Hasher [8] found that human memory displayed characteris-
tics of both schematic and associationist theories. However, elements of both models were
also repeatedly falsified, meaning that neither schematic nor associationist theories could
account for all of the experimental findings (see [26, 115] for a more detailed exposition).

18
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Fuzzy-Trace Theory explains these contradictory findings with the core theoretical dis-
tinction between gist and verbatim mental representations, that are encoded distinctly, yet
in parallel (gist representations are not derived from verbatim representations). Although
humans prefer to rely on gists, they also encode, and can therefore recognize, verbatim
representations. In contrast, algorithms are verbatim by their very nature. Thus, humans
working together with ML algorithms may get the best of both worlds — applying gist-based
structured background knowledge to interpret association-based algorithmic output.

2.4.2 Heuristics and Biases

The key construct of Fuzzy-Trace Theory, gist, also moves beyond other theories that posit
reliance on intuitive judgment. The “heuristics and biases” research paradigm (e.g., [53])
also acknowledges the role of intuition in human behavior, but considers intuitive judg-
ments to be primitive and therefore associated with poor decision-making. This tradition
points to routine human violations of statistical and decision-axioms as evidence for this
claim; however, developmentally-advanced educated intuition often leads to better out-
comes [112] even when experts may achieve these outcomes for what external observer
may consider to be the “wrong reasons” (see 4.1). For example, an experienced physician
may make the right decision regarding how to treat a patient given test results, even though
their mathematical calculations regarding the numerical probabilities that they assign to
different treatment outcomes may be incorrect [7]. Indeed, gist representations of complex
problems allow experts to make decisions that are driven by context that is grounded in ex-
tensive background knowledge [17, 113, 117, 118, 122, 123]. This context allows experts
to focus on the essence of information when making decisions, neglecting less important
features that do not deliver insight [112]. Thus, gist representations, when informed by
expertise, are not just based on a rote simplification, but are rather driven by insightful
simplifications that are meaningful to decision-makers.

Evidence in favor of Fuzzy-Trace Theory’s account of decision-making shows that the
theory is both scientifically parsimonious and has more predictive accuracy than does Cu-
mulative Prospect Theory [142] — the leading theoretical account in the heuristics and bi-
ases tradition — which nevertheless cannot account for key experimental effects that Fuzzy-
Trace Theory does account for (see [22, 115] for details).

2.4.3 Naturalistic Decision Making

Naturalistic Decision Making [73, 150, KLEIN et al.], another leading framework that is
especially popular in the human factors engineering and XAI literatures, posits that humans
draw upon their prior experience to recognize patterns, which, in turn, drive decisions [76].
Both Naturalistic Decision Making and Fuzzy-Trace Theory acknowledge the role of intu-
ition in improving decision-making; however, decisions that are based on gist intuitions are
not simply “recognition primed decisions” as posited by the Naturalistic Decision Making
tradition [76]. Rather, context cues (such as when subjects are encouraged to think about
a problem from a medical or statistical perspective) can influence reliance on the level of

19
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

mental representation [15] meaning that recognition does not guarantee that a decision will
rely on expert intuition. Whereas recognition is a rote verbatim strategy (theorized by as-
sociationism), gist representations bring background knowledge to bear, contextualizing
scenarios such that they make sense and therefore providing insight to the human decision-
maker. In fact, an extensive body of literature shows that humans can recognize both gist
and verbatim representations in parallel, and yet prefer to rely on the gist when making
decisions [18, 19, 115, 119].

Thus, an extensive body of literature supports the contention that Fuzzy-Trace Theory
is both more parsimonious and more predictive than competing theoretical accounts of the
role of interpretation in judgments and decisions. These findings apply to both texts, such
as are found in the legal reasoning domain, and numerical stimuli such as are found in the
engineering domain [90] or generated by machine learning models.

3. Computer Science Definitions of Interpretability and Explainability

The above discussion emphasizes that interpretability and explainability are functions of
the user, the use case, and other contextual factors, as much as they are functions of the
system being used. However, the psychometric properties of users are generally not under
designers’ control. Here, we discuss the state-of-the-art for explainable AI algorithms, and
how systems might be designed to promote interpretability and explainability.

3.1 Comparison of Mental Representations to Current Machine Learning Paradigms

Whereas humans generate multiple mental representations in parallel, “shallow learning”
algorithms generate a single model, or distribution of models from the same mathematical
family, when representing a dataset — a verbatim process. Beyond shallow learning, several
ML techniques do generate multiple representations. For example, ensemble learning is a
process by which multiple models are generated and then ultimately aggregated to form a
single hypothesis. However, these models do not differ from one another in terms of their
level of precision — they simply apply different families of mathematical operators to the
same set of features. In contrast, multitask learning algorithms seek to replicate the flexibil-
ity of human gist representations by training a model to generate a common representation
of several stimuli from different domains, thus enabling “far transfer”. When successful,
these models may learn more abstract representations that are superficially similar to gist
representations; however, they still generate only a single model. Finally, deep neural nets
generate multiple representations of a dataset; however, they do so by deriving abstract
representations from more concrete representations, whereas humans encode these repre-
sentations simultaneously and in parallel, meaning that humans do not derive more simple
interpretations from more detailed representations [118].

20
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

3.2 Algorithmic Paradigms Designed to Promote Interpretability and Explainability

A recent comprehensive literature review of computational approaches to explainable AI
notes that, for computer scientists, the concepts of interpretability and explainability are
“closely related” [6]. These authors assert that “intepretable systems are explainable if
their operations can be understood by humans” (pp. 52140-52141; emphasis added). Al-
though explainability and interpretability are sometimes used interchangeably in the com-
puter science literature, this review provides data supporting the contention that “in [the]
ML community the term ‘interpretable’ is more used than ‘explainable”’ (p. 52141; see
also [42]), especially when compared to the usage of these terms by the general public.
Consistent with the psychological definitions outlined above, this finding may indicate that
producers of AI products are more able to interpret the output of these systems because
they posses specialized background knowledge. Indeed, Bhatt et al. [11] posit that this
distinction may belie a difference in the design goals of these user groups: algorithm de-
velopers generally seek explanations so that they may debug or otherwise improve their
algorithms, and they might therefore develop explainable AI tools for that purpose. Thus,
an explanation is generally understood by computer scientists to indicate how a computa-
tional system arrived at, or generated, a certain output. A good explanation is often causal
and justified relative to a system’s implementation — e.g., “the algorithm is biased towards
visa application refusal BECAUSE the training data are unbalanced”. This sort of expla-
nation is quite useful for debugging these complex systems, but only if the user has the
appropriate background knowledge and technical expertise to do so.!© For example, the
explanation given above would lead a developer to gather more balanced data and retrain
the algorithm, but would not suggest an immediate action to an end user, except perhaps to
abandon use of the algorithm.

3.2.1 Local Feature Importance

Much of the work in explainable artificial intelligence attempts to help developers to deter-
mine simple rote verbatim associations between inputs and outputs, with the aim of assist-
ing them to infer potential causal mechanisms. For example, the local feature importance
paradigm (e.g., [33, 87, 125]) may be the most popular way for practitioners to interact
with technical explanations. This approach seeks to communicate how small changes in
specific features may lead to changes in specific model outputs.

Local Interpretable Model-agnostic Explanations (LIME). LIME [125], one of the
leading algorithms using the local feature importance paradigm, aims to “explain the pre-

 

'6Consider our prior analogy of an automotive system. Just as a diagnostic code on a modern automobile can
be used to determine whether a car’s check engine light is on because of an electrical problem in the sensor,
or because of a legitimate engine malfunction, a tradition within computer science identifies an explanation
with an outcome that enables other trained technical experts to debug, assess, or otherwise improve a faulty
system. Nevertheless, interpreting a car’s diagnostic code, and knowing what to do next, usually requires
some specialized expertise (either mechanical, computational, or both — or sometimes, just deep familiarity
with the system).

21
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Prediction probabilities christian

atheism

christian 0.42

Text with highlighted words

From: johnchad@triton.unm 8 (jchadwic)

Subject: Another request for Darwin Fish
Organization: University of New Mexico, Albuquerque
Lines: 11

RUNGE Bost HGS: triton. unm Fai

Hello Gang,

 

GMB been some notes recently asking where to obtain the
DARWIN fish.
This is the same question I B&We and I MAWe not seen an answer on
the
net. If anyone has a contact please post on the net or email me.

Fig. 5. An example of output from LIME which emphasizes text features that led a specific
paragraph to be classified as about atheism, rather than Christianity. The original image may be
found at this URL: https://github.com/marcotcr/lime/blob/master/doc/images/twoclass.png

dictions of any classifier or regressor in a faithful way, by approximating it locally with an
interpretable model...by presenting textual or visual artifacts that provide qualitative under-
standing of the relationship between the instance’s components (e.g. words in text, patches
in an image) and the model’s prediction.” LIME can help developers to understand how
changes in individual features might change the model’s output around a specific predic-
tion. To the extent that these insights generalize, and are based on meaningful features, they
may help developers to infer the model’s causal mechanisms; however, these approaches
may also mislead if they become subject to spurious correlations. For example, Figure 5
shows the output of LIME when applied to a paragraph of text that was classified as about
atheism rather than about Christianity. This classifier appears to focus on properties of the
author (e.g., the fact that they originate from an academic institution, as indicated by the
.edu in their email address) and specific stylometric features (e.g., use of the words “have”
and “there’’) rather than words that might be indicative of content.

In so doing, LIME draws users’ attention to specific features that the model uses to
make a specific prediction, thus connecting a specific output to a simplified representation
of the model that generated that output. For example, Figure 6 demonstrates how a classi-
fier designed to tell the difference between wolves and huskies classified a particular image
based on the presence of snow in the background (and not based on the anatomical features
that would actually differentiate these two species). A data scientist with appropriate do-
main knowledge would be able to use this information to modify or otherwise debug this
faulty classification.

Thus, this process bears some resemblance to the definition of explanation presented
above; however, there are also important differences. First, LIME does not provide the
user with an explanation of the model per se, but rather provides the users with a simplified
model that approximates the more complex model that the algorithm is trying to explain.
In effect, LIME replaces a complex, causal, description of a model’s inner workings with a
simpler description of a different model whose results are only correlated with the original
model. For example, LIME provides no information regarding whether the wolf vs. husky

22
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

   

(a) Husky classified as wolf (b) Explanation

Fig. 6. An example of output from LIME which emphasizes input features that are diagnostic of a
specific image classification. In this case, a picture of a husky was incorrectly classified as a wolf
because of the presence of snow in the background. Such information may help tool developers
debug overfit classifiers. This image was originally presented in [125]

classifier in Figure 6 would make accurate predictions on images that did not have snow in
their backgrounds.

The authors of LIME argue that these simplified models (e.g., regression models with
a small number of coefficients) are inherently more interpretable because they “provide
qualitative understanding between the input variables and the response.” While this goal
is broadly consistent with Fuzzy-Trace Theory’s definition of gist, gists, when educated,
capture a human expert’s insight regarding which features are most likely to generalize.
Techniques such as LIME may aid humans in generating these representations, and indeed,
preliminary experiments seem to suggest that human subjects could use these techniques
to remove features that interfered with predictive accuracy — i.e., they could make a better
classifier — and that a small sample of human subjects with with data science expertise
(and, in particular, familiarity with the concept of spurious correlation) might be able to
use LIME to derive better explanations.

SHapley Additive exPlanations (SHAP). Like LIME, the SHAP [87] family of models
start from the premise that “The best explanation of a simple model is the model itself” and
attempts to therefore represent complex models with simpler models. SHAP therefore
returns importance scores for each feature, which are analogous to regression coefficients.
For a given prediction, SHAP scores indicate how much any of these features contributed
to that prediction, as in Figure 7.

As such, SHAP models possess many of the same strengths and weaknesses of LIME,

23
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

higher = lower
base value model output

14.34 16.34 18.34 20.34 22.34 24:41 26.34 28.34 30.34
SSS
PTRATIO = 15.3 LSTAT = 4.98 RM = 6.575 | NOX = 0.538 | AGE = 65.2 | RAD = 1

Fig. 7. An example of output from SHAP which indicates which indicates the model’s baseline
value, the marginal contributions of each of its features, and the final prediction. Such an approach
is analogous to a graphical interpretation of linear regression coefficients. The original image may
be found at https://github.com/slundberg/shap

albeit with the ability to generalize to a larger class of machine-learning models. These
models are verbatim in the most concrete sense — they output a set of rules (feature impor-
tance scores), which may be applied in a rote manner to generate a post-hoc description of
the desired prediction. However, they do not communicate causal mechanisms, and they
are prone to unknown error as the model is applied outside of the local neighborhood of a
specific prediction. Individual human subjects — such as informed practitioners — that have
the willingness and the ability to examine these findings in depth may be able to leverage
their own background knowledge to generate an explanation, but SHAP does not provide
enough information to help these practitioners figure out when the model no longer applies.
In effect, these techniques provide human users with a stimulus that they must then explain
or interpret whereas true “black box” models do not even provide this stimulus.

Explainable Neural Networks. Whereas SHAP and LIME seek to explain complex
models using a regression-like paradigm (i.e., a linear additive function), Explainable Neu-
ral Networks (XNNs) [144] use a more general formulation based on an “additive index
model” [127]. Here, the algorithm seeks to return a function that describes how model pre-
dictions vary with changes to individual parameters (or, more recently, pairs of parameters
[148]). As in LIME and SHAP, these models can help data scientists with the appropriate
training to understand how changing a specific feature might change the model’s predic-
tion, albeit at the risk of inferring spurious correlations. These approaches have especially
been applied to deep neural network models, in which one neural network is used to pro-
vide a simplified representation of another, and then rendered into a table that is analogous
to an Analysis of Variance, showing main effects and, in some cases, two-way interactions
[35].

However, LIME is not without limitations: the explanations that analysts might draw
from applying these tools may, themselves, be based on spurious correlations or may engen-
der false confidence in model predictions outside the scope of the immediate neighborhood
of the datapoint that LIME is attempting to explain!’. Worse, these misleading explana-
tions may be engineered by adversaries seeking to take advantage of humans’ tendency to

 

'/Precisely this sort of inappropriate model generalization led NASA engineers to conclude that an impact
from foam insulation upon the space shuttle Columbia’s wing from its external tank did not pose a threat,
precipitating the loss of the space shuttle upon re-entry into Earth’s atmosphere [16]

24
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

impute causality where none exists [134].

Gradient-weighted Class Activation Mapping (Grad-CAM). Grad-CAM is a method
designed to explain computer vision models that use deep learning architectures (specifi-
cally, convolutional neural nets — currently the state-of-the-art architecture for computer
vision). Specifically, Grad-CAM “uses the gradients of any target concept (say ‘dog’ in
a classification network or a sequence of words in captioning network) flowing into the
final convolutional layer to produce a coarse localization map highlighting the important
regions in the image for predicting the concept” [129]. Grad-CAM takes advantage of the
layered architecture of CNNs to identify those regions of an image that are most diagnostic
of a particular prediction. For example, Figure 8 shows how Grad-CAM output can draw
a user’s attention to the part of an image that are diagnostic of a specific prediction that a
user wishes to explain. This is a visual version of the feature importance paradigm — where
the features are ensembles of specific pixels — with several of the corresponding strengths
and limitations.

3.2.2 ‘Simpler Models Are Inherently More Interpretable”’

Rudin [128] has sharply critiqued techniques which seek to generate simple explanations of
complex models, arguing that they can obfuscate the actual inner-workings of these models
in a manner that misleads decision-makers and analysts. Models that are locally-accurate
do not provide information on the extent of that accuracy or whether its degradation is
graceful or sudden. Instead of trying to approximate more complex models with simpler
models, Rudin argues that simpler models should be used directly, because they are more
“interpretable” (i.e., by data scientists), especially when the stakes are high. The rationale
for this approach is that data scientists, at least, may understand the model’s inner workings.

Scalable Bayesian Rule Lists. Scalabale Bayesian Rule Lists [147] are one example
of a technique that aims to avoid model complexity. In contrast to the techniques out-
lined above, which seek to provide continuous representations of complex models, Scal-
able Bayesian Rule Lists explicitly do not attempt to compete with “black box classifiers
such as neural networks, support vector machines, gradient boosting or random forests. It
is useful when machine learning tools are used as a decision aid to humans, who need to
understand the model in order to trust it and make data-driven decisions.” As such, SBRLs
do not aim to achieve both high predictive accuracy and explainability; rather, they seek
to provide a set of simplified (verbatim) probabilistic rules that can be used to partition a
datset (see Table 2).

Generalized Additive Models with pairwise interactions. One approach that may
address Rudin’s critique relies on using Generalized Additive Models with pairwise inter-
actions (GA7Ms) — a class of models which restrict the “contribution of a single feature
to the final prediction” to depend only on that feature [34]. The intent of these models
is to disentangle each feature from all other features such that they may be evaluated in-
dependently of one another. Figure 9 shows the output of an GA?M applied to a dataset
that predicts risk of hospital 30-day admission for pneumonia. As above, these models are

25
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

 

(1) Grad-CAM ‘Dog’

Fig. 8. An example of output from Grad-CAM, indicating which pixels in an image are diagnostic
of the predicted class (dog or cat). The original image may be found at [129]

26
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Table 2. Example of SBRL output, which seeks to explain whether a customer will leave the
service provider. PP = Probability that the label is positive. Source: [147]

 

Rule-list PP Test Accuracy
if ( Contract=One year&StreamingMovies=Yes ), 0.20 0.81
else if ( Contract=Two year ), 0.032 0.98
else if ( Contract=One year ), 0.054 0.97
else if ( tenure< 1 year&InternetService=Fiber optic ), 0.70 0.72
else if ( PaymentMethod=Electronic check & InternetService=Fiber optic ), 0.48 0.45
else ( TechSupport=No&OnlineSecurity=No ), 0.42 0.64
else ( default ), 0.22 0.78

primarily correlational in nature and may help domain experts to select features — for exam-
ple, the authors note that pneumonia readmission risk decreases, rather than increases, with
asthma — a counterintuitive finding. This model surfaces that finding. However, domain
experts must then explain that finding post-hoc, as follows:

[P]atients with a history of asthma who presented with pneumonia usually were
admitted not only to the hospital but directly to the ICU (Intensive Care Unit).
The good news is that the aggressive care received by asthmatic pneumonia pa-
tients was so effective that it lowered their risk of dying from pneumonia com-
pared to the general population. The bad news is that because the prognosis
for these patients is better than average, models trained on the data incorrectly
learn that asthma lowers risk, when in fact asthmatics have much higher risk
Gf not hospitalized)[34].

The discussion above indicates that these concerns apply to explainability — where the
goal is to assist a data scientist to understand how a model works — but may apply less to
interpretability, where the aim is fundamentally to assist a decision maker to link a model
output to a meaningful distinction that allows them to leverage their values, goals, and
preferences to make a choice. Specifically, the explanation given above might help a user
to debug the model, or even to decide whether or not to trust the model; however, it may
not explicitly provide the user with meaningful information that can inform their ultimate
treatment decision.

Monotonically Constrained Gradient-Boosting Machines Gradient-Boosting Ma-
chines seek to use an ensemble of “weak learners” — i.e., models with low predictive
accuracy — to jointly make accurate predictions. This approach leads to significant im-
provements in predictive capability, at the cost of model complexity. In order to cope with
this complexity, Monotonically Constrained Gradient-Boosting Machines impose a con-
straint that any given feature in the model must have a monotonic relationship with the
output. This is theorized to increase explainability because these monotonic relationships
restrict the relationship between features and predictions to have clear qualitative directions
— an increase in the feature must consistently lead to either an increase or decrease in the

27
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

 

 

 

 

 

      
 

 

0 50 100 150 200 250 A 05 0 05 1

BUN level cancer

 

A 0.5 0 os 1 A 05 0 05 1 41°01 2 3 4 5 6 7 8 oO 50 100 150 200

chronic lung disease congestive heart failure # of diseases heart rate
12 12

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    

     

-20 0 (20 40 60 80 30 32 34 36 38 40 42 -20 oO 20 40 60 80 1 05 oO 0.5 1
respiration rate temperature diastolic blood pressure history of chest pain
12 12 12 12
1 1 1
08 08 08
06 06 06
04 04 04
02 + 0.2 02
0 0 o——
0.2 0.2 02
0.4 -0.4 04
1-05 0 05 1°15 2 25 i) 0 5 10 15 20 25 0 500 1000 1500
albumin level creatinine level glucose level
1.2 1.2
1 1
08 08
06 06
04 04
0.2 02
0 0
0.2 0.2
04 + 04
10 -5 oO 5 10 15 20 25 oO 2 4 6 8 -100 50 oO 50 100 150
hematocrit potassium level sodium level
03
0.25
0.2
bt
0.05
0
-0.05
-0.1
Be
-20 0 20 40 60 40 -20 0 20 40 60 80 100 0 2 40 60 80 100 120 -20 0 20 40 60 80
pO2 pCo2 WEC count age vs. respiration rate
400 0.15 250 0.5 250 0.6
990 01 200 b4 200 oa
70 0.05 150 0.2 180 $3
60 100 0.1 400 04
50 0 0 5
40 oo 01 90 0.1
30 . 0 -0.2 0 -0.2
20 -0.1 0.3 -0.3
1 05 0 O58 1 0 50 100 150 200 250 0 500 1000 1500 -100-50 0 50 100150
age vs. cancer respiration rate vs. BUN BUN vs. glucose BUN vs. sodium

Fig. 9. An example of output from a GA7M which indicates how several features (horizontal axes)
vary with relative risk of readmission for pneumonia at 30 days (vertical axis). Pairwise
interactions are shown in the heatmaps at the bottom of the figure. The original image is in [34].

28
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

prediction. As above, these models assume that simpler functional forms are inherently
more explainable. However, these models, in their current form, may simply apply a form
of regularization that is not necessarily grounded in domain knowledge. Monotonicity may
be appropriate in some cases — such as a dose-response curve — but not in others — such as
in modeling waves or other sinusoidal behavior. Domain knowledge is required to deter-
mine whether monotonicity constraints, or any other constraints, are appropriate. Absent
that domain knowledge, application of such constraints may indeed simplify the model, but
may do so in a misleading way that can foster inference of incorrect explanations.

3.2.3. Limitations of Current Explainable AI Models

Generally speaking, the assumption that simplified models are inherently interpretable as-
sumes some degree of domain knowledge on the part of model users — i.e., that they have
sufficient data science expertise to make sense of linear models, decision trees, rule lists,
etc. Furthermore, these “interpretable” models may not provide users with sufficient con-
text to apply their values, goals, and principles to enable a decision. These techniques are
truly verbatim in the sense that they provide a rule but with no insight as to the algorithm’s
actual mechanism. They provide correlation but not causation. However, they may assist
subject-matter experts or data scientists to infer causation. These techniques can encour-
age experts with appropriate background expertise to more deeply explore the mechanisms
by which a particular classification was made, although without making those mechanisms
themselves explicit. Thus, a technical expert can perhaps leverage their background know!l-
edge of the type of algorithm used to infer causality from these tools. This may enable them
to construct an explanation in the same way that a juror or a reader can infer coherent struc-
ture from cohesive text. However, it is ultimately the human that imputes the explanation to
the model output. The techniques outlined above do not provide explicit representations of
causal mechanisms or interface with users’ values, goals, or preferences. Rather, they must
rely on humans’ background knowledge for their utility. Thus, these models assume much
of the viewer, including potentially-significant domain knowledge regarding the meaning
of technical terms (such as “hematocrit” in the GA*M pneumonia diagnosis example), the
ability to distinguish between continuous and discrete variables, etc. Similarly, subjects
must have extensive subject-matter expertise to be able to recognize, for example, that prior
history of asthma should not be associated with lower pneumonia risk. Thus, the model,
on its own, is not interpretable or explainable in the way that psychologists conceive these
terms, but may help users with the appropriate background knowledge, and willingness to
investigate, to draw more meaningful and accurate conclusions.

Because these models are correlational in nature, they may be subject to spurious as-
sociation. Indeed, it has long been acknowledged in the social sciences [130] that the
identification of meaningful structure in data (e.g., due to a correlation or regression), is
only the first step in the imputation of a causal mechanism and, absent a counterfactual
(such as an experimental control group), cannot be relied upon to make causal claims.
Thus, approaches that simplify complex models by reducing them to a set of monotonic

29
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

relationships may mislead users into imputing a causal mechanism within the model where
none exists. This problem is not limited to computational systems, but is a general fea-
ture of complex engineered system with multiple interacting parts [29]. Thus, future work
in explainable artificial intelligence may productively focus on how to help data scientists
and domain experts to accurately impute causal claims while avoiding drawing inferences
based on spurious correlation.

3.2.4 Purpose-Built Graphical User Interfaces

In general, the approaches outlined above seek to enhance explainability by helping users
to understand how changes to a specific feature might change model output. Although these
are theorized to enhance explainability when a data scientist can use them to infer causal
mechanisms about how the algorithm works, these techniques may be less effective for es-
tablishing interpretability — i.e., meaning in context for the end user. Whereas developers
need to know how the system works so that they can identify flaws in their implementation
and fix them, members of the public or subject-matter experts from other fields typically
lack the in-depth technical training and expertise of computer scientists; nor should they be
expected to develop it. For example, an immigration lawyer may want to know the legal
implications of a visa review algorithm or a financial analyst may want to know the finan-
cial implications of a credit rating algorithm. Often, these users will simply assume that
the algorithm was implemented correctly, and that the training data were adequately rep-
resentative. Finally, job/visa/credit applicants would naturally want to know the standards
by which they are being assessed and whether they are competitive for a specific position.
These users need to know why a system generated its result. That is, they seek to make
sense of model output such that they can contextualize it in terms that are meaningful to
them.

In some cases, Graphical User Interfaces (GUIs) such as Google’s What-If tool, may
be paired with model output to assist users with limited numeracy or statistical knowledge
to “get the gist”. For example, there is a body of work in medical decision-making exam-
ining individual differences in graph literacy (e.g., [48]) and techniques that may be used
to overcome these differences to communicate the gist of complex medical information
[37, 37, 145]. However, designers must take care not to assume that a graphical format is
necessarily more interpretable. Rather, the graphical output must be contextualized with
appropriate representations of base rates, thresholds, and other indicators of meaningful
categorical distinctions that, in many cases, may need to be elicited from users. Ultimately,
machine-generated interpretations must be contextualized in terms of background knowl-
edge and goals, and tailored to individual differences, if they are to be effective.

3.2.5 Coherent Topic Models

Topic models are a family of Bayesian inference algorithms that have been widely applied
to text data for information retrieval and document summarization [13]. The most widely-
applied of these algorithms, Latent Dirichlet Allocation (LDA) [14], infers latent “topics”

30
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Topic proportions and

Topics Documents assignments

 

Seeking Life’s Bare ne Necessities

 

life 0.02
evolve 0.01
organism 0.01

SS

ina simple
nated that for this

 

brain 0.04
neuron 0.02
nerve 0.01

data 0.02
number 0.02
computer 0.01 I

| __eae

Stripping down. Comp 8 yields an osti-
mate of the minimum modern and ancient genomes

 

 

 

 

 

 

 

Fig. 10. A visualization of Latent Dirichlet Allocation output from [13]. Probabilistic topic models
such as LDA map each word in a text corpus to a topic. The most frequent words in that topic are
then presented to humans for interpretation.

that are supposed to contain semantic content held in common across multiple documents.
In practice, these topics are actually a probability distribution over words in the text corpus
upon which the LDA model is trained. Humans use topic models by inspecting the top
words, or top documents, for any given topic and then assigning meaning to those top-
ics [59], with some even going so far as to claim that topic models explicitly measure the
gist of text [60]. However, more recent work has shown that humans have difficulty inter-
preting some topic model output [36], especially when they are not familiar with how the
algorithm works [83]. Although computer scientists have developed measures to improve
the coherence (hypothesized to increase interpretability) of topic model output [81, 96],
the resulting output does not explicitly provide an interpretation to human users, but re-
mains a list of words with associated topic probabilities, which humans must interpret (see
Figure 10). Nevertheless, topic models are perhaps unique among ML algorithms in that
their users have attempted to explicitly engineer interpretability into their structure and
output using tasks that are evaluated by non-expert humans with no knowledge of how the
algorithm works. Future work should focus on evaluating this approach and potentially
applying it to other algorithmic paradigms.

4. Incorporating Insights from Psychology Into Design

How might one evaluate the explainability and interpretability of AI systems in a manner
that is psychologically plausible? How might we design systems that satisfy these psycho-

31
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

logical definitions? This section addresses these questions.

4.1 Psychological Correlates of AI Expert System Paradigms

The ML techniques described in section 3.2 reflect a tension between two different ap-
proaches to evaluating the quality of mathematical models, and “rational” behavior more
broadly.

4.1.1 Coherence and “White-Box Models”

The first approach, which Hammond [62] called “coherence”, emphasizes the process by
which a result is obtained. According to this approach, a result is judged according to
whether it is obtained by following logical rules that start from universally-accepted ax-
ioms. Early AI systems — especially rule-based systems — exhibited high degrees of coher-
ence and, by extension, explainability per the psychology-based definitions in this paper.
Strengths of the coherence approach include its guarantees of logical completeness — if
the axioms are correct, and the rules are followed unerringly, then the conclusions must
necessarily be correct. However, these systems have been criticized for their fragility in
real-world decision-making (e.g., the work that inspired the GA7M approach [33]). In prac-
tice, they may fail if the axioms are not correct (but at least one could determine how that
conclusion was attained!) For example, a classical expert system is typically constructed
by eliciting rules from experts; however, these rules could be applied “mindlessly” (e.g.,
without relevant background knowledge, such as about time, human anatomy, or important
exceptions, as in the case of a rule-based medical expert system [93]). To the extent that
those rules are correct, the system’s recommendations should be correct; however, the pro-
cess of eliciting these rules may introduce sources of error that would invalidate the results,
such as when patients do not disclose all relevant information to an algorithm since they
do not know the algorithm requires it, or because they do not trust the algorithm to use that
information appropriately. Indeed, traditional rule-based AI systems are marked by a strict
adherence to verbatim rules that occasionally lead to the wrong conclusions. Attempts
to oversimplify machine learning models based on purely algorithmic considerations may
actually impose harmful biases in some circumstances [77].

White-box models. Like human decision processes emphasizing coherence, “white-
box” ML models are transparent with humans able to readily understand how they operate
because they follow a set of transparent rules. Examples of white-box models include
linear models, which can be readily transformed from input to prediction by multiplying
by well-defined coefficient values. These models also seem to accord with Rudin’s [128]
definition of interpretability. Furthermore the explainable AI techniques outlined in section
3.2 appear to be designed to make black-box models more like white-box models (at the
risk of introducing potential spurious correlations).

32
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

4.1.2 Correspondence and “Black-Box Models”

It is generally assumed that explainability and predictive accuracy must be traded against
one another. Consistent with this perceived dichotomy, Hammond [62] defined “correspon-
dence” approaches as those which emphasizes empirical accuracy all else. Here, a decision
is considered to be good if it leads to a good result, regardless of how this result is ob-
tained. This is analogous to the machine learning paradigm, which emphasizes prediction
over explanation [131, 149]. Standard machine learning techniques aim to optimize spe-
cific predictive metrics, such as accuracy, precision, recall, F-score, etc. Furthermore, any
number of algorithms may be employed regardless of whether the underlying theory of the
algorithm is a good description of the process generating the data. This approach is consis-
tent with Hammond’s definition of correspondence since it privileges predictive accuracy
over a specific causal theory. Deep neural nets, in particular, have been criticized — but also
lionized — because they often achieve significant predictive predictive performance at the
cost of explainability. Thus, like ML, the weaknesses of the correspondence approach are
fundamentally connected to low explainability — a method may achieve the right answers
for the wrong reasons — i.e., dut to spurious correlation — thus, there is no confidence that
future model results will be correct. As Hammond [62] states,

Scientific research seeks both coherence and correspondence, but gets both
only in advanced, successful work. Most scientific disciplines are forced to
tolerate contradictory facts and competitive theories...But policymakers find
it much harder than researchers to live with this tension because they are ex-
pected to act on the basis of information. (p. 54).

Black-box models. Like decision approaches favoring correspondence, “black-box
models” are those whose inner workings are inaccessible, and hence incomprehensible, to
human users becuase they emphasize predictve accuracy over explainability. These models
can only be evaluated for their predictive qualities, and one must simply “trust” that they
will continue to perform in the real world as they do on training data. Prototypical examples
of black-box models include deep neural nets.

Hammond’s discussion highlights that the current tension between explanation and pre-
diction in machine learning and statistics (see also [131]) is, in fact, a longstanding feature
of the scientific method that can nevertheless be at odds with policy and legal requirements
for data-driven decision-making. Indeed, there seems to be a common perception that
models possessing high correspondence are likely to have low coherence and vice versa
(although see [97]). However, the discussion above highlights that explanations are fun-
damentally about providing coherent outputs that describe the process by which a model
achieved a given result. In contrast, interpretations emphasize how a stimulus (either a
model’s output, a datapoint or dataset, or a description of the model itself) is contextual-
ized in the broader world setting, and thus could be evaluated relative to correspondence
criteria.

33
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

4.1.3. A Third Way: Enhancing Interpretability by Querying Human Experts and
“Grey-Box Models”

By distinguishing between interpretation and explanation, we propose that human experts’
gists may be thought of as analogous to a “grey box model” — one for which a full mecha-
nistic explanation (i.e., a white box model) is not available, but for which blind trust (.e.,
a black box model) is also not required. This middle road is achieved by experts’ com-
municating the gists of their decision-making processes, rather than trying to explain all of
the details of their structured mental models. Specifically, experts can communicate how
what they are doing is consistent with the values of users in easy-to-understand categori-
cal terms while not necessarily possessing the ability to describe the exact mechanisms in
every detail. We propose grey box model design as a goal for interpretable AI.

Human decision-making exhibits varying degrees of coherence and correspondence.
Specifically, experts’ gist representations tend to correspond to better real-world outcomes,
exhibiting correspondence; however, experts may violate coherence [7] — i.e., they can
provide an explanation for an action in a particular context, but that explanation may not
necessarily generalize to all contexts, roughly analogous to a linear estimate of a nonlinear
model. Unlike these linear estimates which provide explanatory power over a narrow pa-
rameter range, experts can be queried for their rationales. The above discussion emphasizes
how, far from being a unique feature of machine learning models, high correspondence with
low explainability may also be a feature of some types of human expertise. In fact, there is
a significant body of literature in engineering management that discusses the “tacit” nature
of human expertise [100—-102, 105, 106]. In other words, like the most complex models,
human experts may not be consciously aware of how they have obtained a certain outcome.
Nevertheless, they are often able to describe why they did what they did — for example
expert tennis players were more likely justify their actions relative to the goals of the game
whereas novices focused more conscious attention on the mechanics of executing specific
maneuvers [92]. Thus, experts’ decisions show a high degree of empirical correspondence
despite being subject to “biases” under predictable circumstances [112]. Furthermore, sub-
ject matter experts tend to rely on, consume, and prefer to use model output that interprets,
rather than explains, relevant results.

This discussion suggests that designers may be able to enhance interpretability by cre-
ating “grey-box models”, that can provide the rationale for a given decision relative to a
set of functional requirements. In section 4.2, we argue that this goal is aligned with has
long dealt with similar problems when attempting to integrate large-scale systems from
across several different complex domains of expertise into a common artifact to be used by
consumers, including policymakers, with varying levels of technical sophistication [21].

4.2 Interpretable and Explainable Outputs Are Different Abstractions of a System

Arguably, designing an explainable or interpretable AI system boils down to selecting the
appropriate level of abstraction at which to communicate system output given a user’s
needs. Here, we review research in both psychology and systems engineering research

34
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

supporting this claim. This literature primarily focuses on drawing an analogy between the
concept of mental representation and levels of abstraction.

Computer science depends on successive levels of abstraction for its successful oper-
ation simply because the operations of computational systems are far too complex, at the
physical level, to explain to even the most expert computer scientists. Consider that a com-
puter is itself an abstraction — a “Turing machine” — that is implemented using another
abstraction — bits — that are themselves implemented in silicon semiconductors. Abbott
[4, 5] points out that bits themselves are abstractions and, to the extent that the abstraction
isn’t violated (e.g., because of physical limits such as too much heat), system develop-
ers do not need to understand the mechanism (e.g., the physical regularities) underlying
the implementation of the computational system that they are using. Similar logic applies
to software development. Although it is certainly helpful, in some cases, to understand
computer architecture when designing software, most software developers do not need a
detailed understanding of the code implementation underlying a particular computer’s op-
erating system when designing an application. Kroll [79] even argues that explanations
that focus on the mechanisms of an AI system’s operations actually obscure users’ abili-
ties to understand how the system operates within its social context (e.g., power structure).
Indeed, users routinely utilize applications — e.g., on the Internet — without detailed expla-
nations of how those applications work. Rather, they are familiar with a set of functions
that the application is intended to perform and, as well as those functions are carried out in
a way that does not impose undue externalities, the user generally does not need, or even
care to, know about implementation details. It is precisely these externalities that lie at the
heart of the need for system interpretability. Thus, Abbott provides a section pertaining to
“platform governance” that provides an overview of some of the research on governance
of common resources that could productively be adapted to algorithmic explainability and
interpretability, and especially to the development of standards in this area. Specifically,
these standards might be framed in terms of high-level requirements [40] with measures of
effectiveness for interpretable and explainable systems.

4.2.1 Psychological Evidence that Abstraction Improves Interpretability and Deci-
sion Quality

The distinction between “how” and “why” levels of mental representation is theoretically-
motivated and empirically-supported in the psychology literature. Construal Level The-
ory (CLT) [52, 139], a leading theory of abstraction in human psychology, distinguishes
between mental representations in terms of their “psychological distance” (although see
[120] for an alternative account). Here, a less distant representation entails memory for
things that are either spatially or temporally proximate, whereas a more distant representa-
tion entails memory for things that are more distant. According to CLT, questions that ask
“how” — what we have called explanations — are more psychologically proximate than are
questions that ask “why” — interpretations. Furthermore, CLT characterizes the distinction
between more and less distant representations in terms of their level of abstraction [52].

35
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Finally, recent findings [47] show that increasing psychological distance, including by pro-
viding more abstract representations — lead to better decisions because these more distant
representations make use of gist interpretations. This has direct implications for design:
interpretable AI systems — i.e., those that help end users to make meaningful decisions —
may, in fact, be less explainable, and vice versa, at least for end users without data science
expertise.

4.2.2 Abstraction Hierarchies in Engineering

The idea that explanations — justified relative to system implementation — and interpre-
tations — justified relative to system goals — are distinct is also supported by extensive
literature in human factors engineering. This paper has described how Fuzzy-Trace Theory
posits that these representations are encoded distinctly and in parallel, with experts pre-
ferring to rely on more abstract descriptions. This section discusses how these notions of
abstraction are used in engineering design by practicing engineers. Specifically, CLT’s core
construct — the abstraction hierarchy distinguishing between more detailed “how” questions
— that are defined relative to specific implementations and thus less meaningful with respect
to system goals — and less detailed, but more meaningful “why” questions — was presaged
in the human factors engineering/systems engineering literature by the work of Jens Ras-
mussen [108] (see Figure 11). Rasmussen’s work, which was conducted exclusively on
technical experts (largely electrical engineers making consequential decisions pertaining to
complex systems, such as nuclear power plants), defined an abstraction hierarchy as one in
which “...the functional properties of a technical system are represented in several levels of
functional abstraction along the means-end dimension” [108](p. 235) — a continuum of rep-
resentations bounded below by the system’s “physical form” (here, analogous to the actual
physical implementation of a machine-learning algorithm on a computer, in terms of which
bits are flipped), through to a system’s “generalized” and “abstract function” (analogous to
the software functions implemented by a specific ML system, which give outputs justified
relative to those functions), which define the system’s causal structure and/or information
flow topology, and bounded above by the system’s “functional purpose” — i.e., its objec-
tives relative to its end users. Rasmussen and Lind [109] further note that, when coping
with complexity, engineers often rely on multiple levels of representation, even switching
between them for the purpose of diagnoses, with the higher levels indicating the functional
requirements for why the lower levels are implemented, and the lower levels indicating the
specific concrete realization of how the higher level requirements are carried out. Thus,
users and designers can only understand system output to the extent that they possess the
technical or domain expertise required to utilize corresponding levels of the abstraction hi-
erarchy. In general, moving between levels of abstraction requires understanding that level
of abstraction on its own terms and bringing to bear relevant background knowledge.Thus,
the form of abstraction that we discuss here bears some similarity to Kolmogorov’s [78]
definition of complexity — it is a simplified representation that can be expressed using a
short description, where the description length is defined relative to a pre-defined knowl-

36
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Functiorfial Rurpose

How? (Loan
Owr écommendation

 

 

Why?
Abstract Function (Credit
How? score determination)
: Why?
General Function (Deep neural net
How? implementation)

wi?

| Why?

Physical Function (computation) NO]
How?

Why?
Physical Form (workstation and data storage units)

Fig. 11. An example of an Abstraction Hierarchy for a ML system designed to make loan
recommendations. Each higher level implemented by the level immediately below it and each
lower level implements a technical solution to carry out a function specified by the higher level.

edge base (usually, a programming language, but see also [20, 45, 91]). However, the term
abstraction is itself polysemous (for a review, see [30]) and we emphasize that the form of
abstraction that we refer to is not just simplification, but rather is augmented by the user’s
background knowledge [120].

4.2.3 Design for Interpretability and Explainability as Requirements Engineering

CLT and the Abstraction Hierarchy both suggest that systems should be evaluated in terms
of their “requirements”, rather than their specific technical specifications (see also [79].
Good requirements are “solution neutral” — they do not specify details of implementation,
but only the function that a system should carry out. The aim is to specify this function as
clearly as possible while not overly constricting the technical design of the system itself.
For example, a solution neutral requirement might specify that a system should achieve an
accuracy of at least 0.85 without specifying the particular algorithm used. Thus, require-
ments are specified in terms of measures of effectiveness, or measures of performance,

37
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

that have a meaning in terms of the system’s ultimate function; not its implementation.
This form of design is familiar to machine learning researchers who frequently use several
different algorithms to obtain the same function. For example, the function may be classi-
fication, which has well-defined metrics — precision, recall, accuracy, etc. — with the best
metric selected based on the task. Given the function required, and the metric benchmark
that the system is supposed to achieve (i.e., the requirement), system designers may use
any number of algorithms to achieve this function. For classification, candidate algorithms
might include logistic regression classifiers, Naive Bayes classifiers, support vector clas-
sifiers, k-nearest neighbor classifiers, convolutional neural nets, etc., all perform the same
function, yet using very different implementations.

The concept of solution neutrality is equally familiar to legal scholars and regulators
and legal scholars, who have significant experience with evaluating complex systems, such
as human-drug interactions, in other highly technical domains [80, 140]. Thus, the key to
a good design is matching the system’s evaluation metric to the end-user’s goals. Stan-
dard ML metrics, such as those defined above, may be insufficient to address the system’s
functional requirements. Systems engineering, and especially requirements engineering, is
the discipline that focuses on evaluating end-user needs and translating these into a suite
of metrics. However, for sufficiently complex systems, groups of engineers must pool
their knowledge to achieve better design outcomes. Attempts to generate unified models
seamlessly integrating input from different engineering fields (“Model-Based Systems En-
gineering’) have yielded mixed results [21]. In contrast, techniques that seek to translate
meaningful information between engineering specialties are both more widely accepted by
practicing engineers. Although there is no guarantee that these techniques generate op-
timal outcomes, they are, at a minimum, acceptable [22, 69, 70]. Thus, other fields of
engineering have developed methods to ensure that expert input from multiple fields can
be integrated into a larger, complex project. Specifically, systems engineering is the field
of engineering that is concerned with coordinating these multiple experts from multiple
domains. Since, nowadays, no one person can be an expert in all fields of human inquiry,
systems engineers have developed a set of tools and techniques to increase the confidence
with which expertise is deployed when developing complex systems.

5. Conclusion

Our review indicates that explainability and interpretability should be distinct requirements
for ML systems, with the former primarily being of value to developers who seek to de-
bug systems or otherwise improve their design, while the latter more useful to regulators,
policymakers, and general users. However, this dichotomy is not absolute — individual dif-
ferences may be associated with reliance on one representation of a system more so than
another. For developers, the explanation may be very similar to the interpretation — similar
to how numerate individuals derive decisions that are informed by verbatim calculations
— however, if developers lack domain expertise, they may be unable to contextualize their
interpretations in terms that are meaningful to end users. Similarly, end users lacking de-

38
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

velopers’ expertise may be unable to understand how the system arrived at its conclusion
(but they also may not desire to).

5.1 Implications for Designing Explainable and Interpretable Artificial Intelligence

The history of modern engineering demonstrates that systems can be designed to facilitate
both explainability and interpretability and, indeed, there are examples of such throughout
recent history, ranging from drug approvals to the automotive sector. Although significant
effort has been devoted to developing automated approaches to create explanations of AI
algorithms, comparatively little attention has focused on interpretability. Since interpreta-
tions differ between individuals, more research is needed in order to determine how best
to link model output to specific gists so that users can appropriately contextualize this out-
put. The extent to which this process can be fully automated, or would require curation by
domain experts, remains an open question.

The above discussion indicates that interpretable algorithms are those that contextual-
ize data by placing it in the context of structured background knowledge, representing it
in a simple manner that captures essential, insightful distinctions, and then justifying the
corresponding output relative to values elicited from human users. Such representations
contextualize the model’s output and provide meaning to the human user in terms of val-
ues stored in long-term memory. Typically, these values (or similar preference-generating
constructs, such as goals) cannot be elicited directly from data based on rote, brittle, ver-
batim association. Thus, techniques to simplify complex models are likely to share this
brittleness [98]. In contrast, gist representations are simple, yet flexible and insightful; they
bring to bear contextual elements — such as goals and values — that are not explicitly rep-
resented in the data. Future work may therefore productively focus on eliciting these gist
representations from experts in the form of mappings from structured background know]-
edge to meaningful, yet simple, categories — associated with goals, values, principles, or
other preferences. Previous “expert systems” lacked the ability to scale precisely because
of the difficulty eliciting these essential distinctions [33]. To move beyond this impasse,
the discussion in this paper highlights the need to account for multiple levels of mental
representation when generating interpretable AI output. In short, rather than assimilating
human cognition to machine learning, we might benefit from designing machine learning
models to better reflect empirical insights about human cognition [116]. In between a ver-
batim, data-driven approach, and an inflexible top-down schematic approach lies one in
which human users engage in a process of contextualizing model output that is then used
to select between, and refine, existing background knowledge structures. There is some
preliminary evidence that these “communicative” approaches — in which human users in-
teract with and curate the output of AI systems — may show some promise. Furthermore,
users’ needs vary with individual differences, e.g., in metacognition and training. Future
work should therefore focus on characterizing these factors within user communities. This
review provides the theoretical basis for such an approach, and provides explicit directions
for future work: such approaches must communicate the gist of the data to the user.

39
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Acknowledgments

The authors thank Andrew Burt, Carina Hahn, Patrick Hall, Sharon Laskowski, P. Jonathon
Phillips, Mark Przybocki, Valerie F. Reyna, Reva Schwartz, Brian , and Paul Witherell for
their insightful comments and discussions.

References

[Com]

[Cen] Centor score (modified/mcisaac) for strep pharyngitis - mdcalc. https://
www.mdcalc.com/centor-score-modified-mcisaac-strep-pharyngitis. (Accessed on
10/07/2020).

[Wor] Worried your sore throat may be strep? — cdc. https://www.cdc.gov/groupastrep/
diseases-public/strep-throat.html. (Accessed on 10/07/2020).

[4] Abbott, R. (2006). Emergence explained: Abstractions: Getting epiphenomena to do
real work. Complexity, 12(1):13-26.

[5] Abbott, R. (2007). Putting complex systems to work. Complexity, 13(2):30-49.

[6] Adadi, A. and Berrada, M. (2018). Peeking Inside the Black-Box: A Survey on Ex-
plainable Artificial Intelligence (XAI). IEEE Access, 6:52138-52160.

[7] Adam, M. B. and Reyna, V. F. (2005). Coherence and correspondence criteria for
rationality: Experts’ estimation of risks of sexually transmitted infections. Journal of
Behavioral Decision Making, 18(3):169-186.

[8] Alba, J. W. and Hasher, L. (1983). Is memory schematic? Psychological Bulletin,
93(2):203.

[Ammermann] Ammermann, S. Adverse action notice requirements under the ecoa and
the fera - consumer compliance outlook: Second quarter 2013 - philadelphia fed.

[10] Axelrod, R. (2015). Structure of decision: The cognitive maps of political elites.
Princeton university press.

[11] Bhatt, U., Xiang, A., Sharma, S., Weller, A., Taly, A., Jia, Y., Ghosh, J., Puri, R.,
Moura, J. M. FE, and Eckersley, P. (2019). Explainable machine learning in deployment.
arXiv: 1909.06342 [cs, stat]. arXiv: 1909.06342.

[12] Binns, R., Van Kleek, M., Veale, M., Lyngs, U., Zhao, J., and Shadbolt, N. (2018).
“it’s reducing a human being to a percentage”; perceptions of justice in algorithmic
decisions. Proceedings of the 2018 CHI Conference on Human Factors in Computing
Systems - CHI ’18, page 1-14. arXiv: 1801.10408.

[13] Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM,
55(4):77-84.

[14] Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal
of machine Learning research, 3(Jan):993—1022.

[15] Bless, H., Betsch, T., and Franzen, A. (1998). Framing the framing effect: The impact
of context cues on solutions to the ‘asian disease’ problem. European Journal of Social
Psychology, 28(2):287-291.

40
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

[16] Board, U. S. C. A. I. (2003). Columbia Accident Investigation Board Report.
Columbia Accident Investigation Board. Google-Books-ID: J7F7c4kRy_wC.

[17] Brainerd, C. J. and Reyna, V. F. (2001). Fuzzy-trace theory: Dual processes in mem-
ory, reasoning, and cognitive neuroscience.

[18] Brainerd, C. J. and Reyna, V. F. (2002). Fuzzy-trace theory and false memory. Current
Directions in Psychological Science, 11(5):164—-169.

[19] Brainerd, C. J. and Reyna, V. F (2005). The science of false memory, volume 38.
Oxford University Press.

[20] Briscoe, E. and Feldman, J. (2011). Conceptual complexity and the bias/variance
tradeoff. Cognition, 118(1):2-16.

[21] Broniatowski, D. A. (2018a). Building the tower without climbing it: Progress in
engineering systems. Systems Engineering, 21(3):259-281.

[22] Broniatowski, D. A. (2018b). Do design decisions depend on “dictators”? Research
in Engineering Design, 29(1):67-85.

[23] Broniatowski, D. A., Klein, E. Y., May, L., Martinez, E. M., Ware, C., and Reyna,
V. F. (2018). Patients’ and clinicians’ perceptions of antibiotic prescribing for upper
respiratory infections in the acute care setting:. Medical Decision Making.

[24] Broniatowski, D. A., Klein, E. Y., and Reyna, V. F. (2014). Germs are germs, and
why not take a risk? patients’ expectations for prescribing antibiotics in an inner-city
emergency department. Medical Decision Making.

[25] Broniatowski, D. A. and Reyna, V. F. (2013a). Gist and verbatim in narrative memory.
In 2013 Workshop on Computational Models of Narrative, volume 32, page 43-51.

[26] Broniatowski, D. A. and Reyna, V. F. (2013b). Gist and verbatim in narrative memory.
In 2013 Workshop on Computational Models of Narrative. Schloss Dagstuhl-Leibniz-
Zentrum fuer Informatik.

[27] Broniatowski, D. A. and Reyna, V. F (2018). A formal model of fuzzy-trace theory:
Variations on framing effects and the allais paradox. Decision, 5(4):205.

[28] Broniatowski, D. A. and Tucker, C. (2017a). Assessing causal claims about complex
engineered systems with quantitative data: internal, external, and construct validity. Sys-
tems Engineering, 20(6):483-496.

[29] Broniatowski, D. A. and Tucker, C. (2017b). Assessing causal claims about complex
engineered systems with quantitative data: internal, external, and construct validity. Sys-
tems Engineering, 20(6):483-496.

[30] Burgoon, E. M., Henderson, M. D., and Markman, A. B. (2013). There are many ways
to see the forest for the trees: A tour guide for abstraction. Perspectives on Psychological
Science, 8(5):501-520.

[31] Cacioppo, J. T., Petty, R. E., Feinstein, J. A., and Jarvis, W. B. G. (1996). Disposi-
tional differences in cognitive motivation: The life and times of individuals varying in
need for cognition. Psychological Bulletin, 119(2):197-253.

[32] Cacioppo, J. T., Petty, R. E., and Feng Kao, C. (1984). The efficient assessment of
need for cognition. Journal of personality assessment, 48(3):306—307.

[33] Caruana, R. (2017). Intelligible Machine Learning for Critical Applications Such As

41
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Health Care. aaas.

(34] Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., and Elhadad, N. (2015). Intel-
ligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmis-
sion. In Proceedings of the 21th ACM SIGKDD international conference on knowledge
discovery and data mining, page 1721-1730.

[35] Chen, J., Vaughan, J., Nair, V. N., and Sudjianto, A. (2020). Adaptive explainable
neural networks (axnns). arXiv:2004.02353 [cs, stat]. arXiv: 2004.02353.

[36] Cheng, H.-F, Wang, R., Zhang, Z., O’Connell, F., Gray, T., Harper, KF M., and Zhu, H.
(2019). Explaining Decision-Making Algorithms through UI: Strategies to Help Non-
Expert Stakeholders. In Proceedings of the 2019 CHI Conference on Human Factors
in Computing Systems, CHI 19, pages 1-12, Glasgow, Scotland Uk. Association for
Computing Machinery.

[37] Cozmuta, R., Wilhelms, E., Cornell, D., Nolte, J., Reyna, V., and Fraenkel, L. (2018).
Influence of explanatory images on risk perceptions and treatment preference. Arthritis
care & research, 70(11):1707-1711.

[38] Curseu, P. L. (2006). Need for cognition and rationality in decision-making. Studia
Psychologica, 48(2):141.

[39] De Weck, O. L., Roos, D., and Magee, C. L. (2011). Engineering systems: Meeting
human needs in a complex technological world. Mit Press.

[40] Dick, J., Hull, E., and Jackson, K. (2017). Requirements engineering. Springer.

[41] Diehl, J. J., Bennetto, L., and Young, E. C. (2006). Story recall and narrative coher-
ence of high-functioning children with autism spectrum disorders. Journal of abnormal
child psychology, 34(1):83-98.

[42] Doshi-Velez, F. and Kim, B. (2017). Towards a rigorous science of interpretable
machine learning. arXiv preprint arXiv: 1702.08608.

[43] Edwards, L. and Veale, M. (2018). Enslaving the algorithm: From a “right to an
explanation” to a “right to better decisions”? [EEE Security Privacy, 16(3):46—54.

[44] Fagerlin, A., Zikmund-Fisher, B. J., Ubel, P. A., Jankovic, A., Derry, H. A., and
Smith, D. M. (2007). Measuring numeracy without a math test: development of the
subjective numeracy scale. Medical Decision Making, 27(5):672-680.

[45] Feldman, J. (2009). Bayes and the simplicity principle in perception. Psychological
Review, 116(4):875.

[46] Frederick, S$. (2005). Cognitive reflection and decision making. The Journal of Eco-
nomic Perspectives, 19(4):25-42.

[47] Fukukura, J., Ferguson, M. J., and Fujita, K. (2013). Psychological distance can
improve decision making under information overload via gist memory. Journal of Ex-
perimental Psychology: General, 142(3):658.

[48] Galesic, M. and Garcia-Retamero, R. (2011). Graph literacy: A cross-cultural com-
parison. Medical Decision Making, 31(3):444-457.

[49] Gallo, D. (2013). Associative illusions of memory: False memory research in DRM
and related tasks. Psychology Press.

[50] Gernsbacher, M. A. (1996). The structure-building framework: What it is, what it

42
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

might also be, and why, page 289-311. Psychology Press.

[51] Gernsbacher, M. A., Varner, K. R., and Faust, M. E. (1990). Investigating differ-
ences in general comprehension skill. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 16(3):430.

[52] Gilead, M., Trope, Y., and Liberman, N. (2020). Above and beyond the concrete:
The diverse representational substrates of the predictive brain. Behavioral and Brain
Sciences, 43:e121.

[53] Gilovich, T., Griffin, D., and Kahneman, D. (2002). Heuristics and biases: The
psychology of intuitive judgment. Cambridge university press.

[54] Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., and Kagal, L. (2018).
Explaining explanations: An overview of interpretability of machine learning. In 2018
IEEE 5th International Conference on data science and advanced analytics (DSAA),
pages 80-89. IEEE.

[55] Gleaves, L. P., Schwartz, R., and Broniatowski, D. A. (2020). The role of individ-
ual user differences in interpretable and explainable machine learning systems. arXiv
preprint arXiv:2009.06675.

[56] Goebel, R., Chander, A., Holzinger, K., Lecue, F.,, Akata, Z., Stumpf, $., Kieseberg,
P., and Holzinger, A. (2018). Explainable ai: The new 42? In Holzinger, A., Kieseberg,
P., Tjoa, A. M., and Weippl, E., editors, Machine Learning and Knowledge Extraction,
Lecture Notes in Computer Science, page 295-303. Springer International Publishing.

[57] Goldman, S. R., McCarthy, K. S., and Burkett, C. (2015). 17 interpretive inferences
in literature. Inferences during reading, page 386.

[58] Graesser, A. C., Singer, M., and Trabasso, T. (1994). Constructing inferences during
narrative text comprehension. Psychological review, 101(3):371.

[59] Griffiths, T. L. and Steyvers, M. (2004). Finding scientific topics. Proceedings of the
National academy of Sciences, 101(suppl 1):5228—-5235.

[60] Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B. (2007). Topics in semantic repre-
sentation. Psychological review, 114(2):211.

[61] Hall, P., Gill, N., and Schmidt, N. (2019). Proposed guidelines for the responsible use
of explainable machine learning. arXiv: 1906.03533 [cs, stat]. arXiv: 1906.03533.

[62] Hammond, K. R. (2000). Coherence and correspondence theories in judgment and
decision making. Judgment and decision making: An interdisciplinary reader, page
53-65.

[63] Hans, V. P, Helm, R. K., and Reyna, V. F. (2018). From meaning to money: Trans-
lating injury into dollars. Law and human behavior, 42(2):95.

[64] Hoffman, R., Miller, T., Mueller, S. T., Klein, G., and Clancey, W. J. (2018). Explain-
ing explanation, part 4: a deep dive on deep nets. JEEE Intelligent Systems, 33(3):87-95.
Publisher: IEEE.

[65] Hoffman, R. R. and Klein, G. (2017). Explaining explanation, part 1: theoretical
foundations. [EEE Intelligent Systems, 32(3):68—73. Publisher: IEEE.

[66] Hoffman, R. R., Mueller, S. T., and Klein, G. (2017). Explaining explanation, part 2:
Empirical foundations. [EEE Intelligent Systems, 32(4):78—86. Publisher: TEEE.

43
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

[67] Hoffman, R. R., Mueller, S. T., Klein, G., and Litman, J. (2019). Metrics for explain-
able ai: Challenges and prospects. arXiv: 1812.04608 [cs]. arXiv: 1812.04608.

[68] Jones, N. A., Ross, H., Lynam, T., Perez, P., and Leitch, A. (2011). Mental models:
an interdisciplinary synthesis of theory and methods. Ecology and Society, 16(1).

[69] Katsikopoulos, K. V. (2009). Coherence and correspondence in engineering de-
sign: informing the conversation and connecting with judgment and decision-making
research. Judgment and Decision Making, 4(2):147-153.

[70] Katsikopoulos, K. V. (2012). Decision methods for design: insights from psychology.
Journal of Mechanical Design, 134(8):084504—-1-084504-4.

[71] Kintsch, W. (1974). The representation of meaning in memory.

[72] Klein, E. Y., Martinez, E. M., May, L., Saheed, M., Reyna, V., and Broniatowski,
D. A. (2017). Categorical risk perception drives variability in antibiotic prescribing in
the emergency department: A mixed methods observational study. Journal of General
Internal Medicine.

[73] Klein, G. (2008). Naturalistic decision making. Human factors, 50(3):456-460.

[74] Klein, G. (2018). Explaining explanation, part 3: The causal landscape. [EEE Intel-
ligent Systems, 33(2):83-88. Publisher: IEEE.

[KLEIN et al.] KLEIN, G., HOFFMAN, R., and MUELLER, S. Naturalistic Psycholog-
ical Model of Explanatory Reasoning: How People Explain Things to Others and to
Themselves.

[76] Klein, G. A., Orasanu, J., Calderwood, R., Zsambok, C. E., et al. (1993). Decision
making in action: Models and methods. Ablex Norwood, NJ.

[77] Kleinberg, J. and Mullainathan, S. (2019). Simplicity creates inequity: implications
for fairness, stereotypes, and interpretability. In Proceedings of the 2019 ACM Confer-
ence on Economics and Computation, pages 807-808.

[78] Kolmogorov, A. N. (1965). Three approaches to the definition of the concept “quan-
tity of information”. Problemy peredachi informatsii, 1(1):3-11.

[79] Kroll, J. A. (2018). The fallacy of inscrutability. Philosophical Transac-
tions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
376(2133):20180084.

[80] Kroll, J. A., Barocas, S., Felten, E. W., Reidenberg, J. R., Robinson, D. G., and Yu, H.
(2016). Accountable Algorithms. University of Pennsylvania Law Review, 163(3):633-
706.

[81] Lau, J. H., Newman, D., and Baldwin, T. (2014). Machine reading tea leaves: Auto-
matically evaluating topic coherence and topic model quality. In Proceedings of the 14th
Conference of the European Chapter of the Association for Computational Linguistics,
pages 530-539.

[82] LeBoeuf, R. A. and Shafir, E. (2003). Deep thoughts and shallow frames: On the
susceptibility to framing effects. Journal of Behavioral Decision Making, 16(2):77-92.

[83] Lee, T. Y., Smith, A., Seppi, K., Elmqvist, N., Boyd-Graber, J., and Findlater, L.
(2017). The human touch: How non-expert users perceive, interpret, and fix topic mod-
els. International Journal of Human-Computer Studies, 105:28-42.

4A
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

[84] Liberali, J. M., Reyna, V. F., Furlan, S., Stein, L. M., and Pardo, S. T. (2012). Individ-
ual differences in numeracy and cognitive reflection, with implications for biases and fal-
lacies in probability judgment. Journal of behavioral decision making, 25(4):361-381.

[85] Linderholm, T., Everson, M. G., van den Broek, P., Mischinski, M., Crittenden, A.,
and Samuels, J. (2000). Effects of causal text revisions on more- and less-skilled readers’
comprehension of easy and difficult texts. Cognition and Instruction, 18(4):525—-556.

[86] Lombrozo, T. (2006). The structure and function of explanations. Trends in Cognitive
Sciences, 10(10):464470.

[87] Lundberg, S. M. and Lee, S.-I. (2017). A unified approach to interpreting model
predictions. In Advances in neural information processing systems, pages 4765-4774.
[88] Malle, B. F (2010). Intentional action in folk psychology. A companion to the phi-

losophy of action, pages 357-365.

[89] Mandler, J. M. (1983). What a story is. Behavioral and Brain sciences,
6(04):603-604.

[90] Marti, D. and Broniatowski, D. A. (2020). Does gist drive nasa experts’ design deci-
sions? Systems Engineering.

[91] Mathy, F. and Feldman, J. (2012). What’s magic about magic numbers? chunking
and data compression in short-term memory. Cognition, 122(3):346-362.

[92] McPherson, S. L. and Thomas, J. R. (1989). Relation of knowledge and perfor-
mance in boys’ tennis: Age and expertise. Journal of experimental child psychology,
48(2):190-211.

[93] Miller, R. A., Pople Jr, H. E., and Myers, J. D. (1982). Internist-i, an experimen-
tal computer-based diagnostic consultant for general internal medicine. New England
Journal of Medicine, 307(8):468-476.

[94] Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sci-
ences. Artificial Intelligence, 267:1—38.

[95] Miller, T., Howe, P., and Sonenberg, L. (2017). Explainable AI: Beware of Inmates
Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Be-
havioural Sciences. arXiv:1712.00547 [cs]. arXiv: 1712.00547.

[96] Mimno, D., Wallach, H., Talley, E., Leenders, M., and McCallum, A. (2011). Opti-
mizing semantic coherence in topic models. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing, pages 262-272.

[97] Mittelstadt, B., Russell, C., and Wachter, S. (2019a). Explaining explanations in ai.
In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT*
*19, page 279-288. Association for Computing Machinery.

[98] Mittelstadt, B., Russell, C., and Wachter, S. (2019b). Explaining Explanations in AI.
In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT*
°19, pages 279-288, Atlanta, GA, USA. Association for Computing Machinery.

[99] Moray, N. (1990). A lattice theory approach to the structure of mental models.
Philosophical Transactions of the Royal Society of London. B, Biological Sciences,
327(1241):577-583.

[100] Nonaka, I (1994). A dynamic theory of organizational knowledge creation. Orga-

45
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

nization science, 5(1):14—37.

[101] Nonaka, I. and Takeuchi, H. (1995). The knowledge-creating company: How
Japanese companies create the dynamics of innovation. Oxford university press.

[102] Nonaka, I., Toyama, R., and Konno, N. (2000). Seci, ba and leadership: a unified
model of dynamic knowledge creation. Long range planning, 33(1):5—34. 21.

[103] Pennington, N. and Hastie, R. (1993). The story model for juror decision making.
Cambridge University Press Cambridge.

[104] Peters, E., Vastfjall, D., Slovic, P., Mertz, C. K., Mazzocco, K., and Dickert, S.
(2006). Numeracy and decision making. Psychological Science, 17(5):407-413.

[105] Polanyi, M. (1962). Tacit knowing: Its bearing on some problems of philosophy.
Reviews of modern physics, 34(4):601. 20.

[106] Polanyi, M. (1967). The tacit dimension.

[107] Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Vaughan, J. W., and Wal-
lach, H. (2019). Manipulating and measuring model interpretability. arXiv: 1802.07810
[cs]. arXiv: 1802.07810.

[108] Rasmussen, J. (1985). The role of hierarchical knowledge representation in decision-
making and system management. JEEE Transactions on systems, man, and cybernetics,
(2):234-243,.

[109] Rasmussen, J. and Lind, M. (1981). Coping with complexity. Technical Report
Risg-M-2293, Ris¢ National Laboratory, Roskilde, Denmark.

[110] Reese, E., Haden, C. A., Baker-Ward, L., Bauer, P., Fivush, R., and Ornstein, P. A.
(2011). Coherence of personal narratives across the lifespan: A multidimensional model
and coding method. Journal of Cognition and Development, 12(4):424-462.

[111] Regulation, G. D. P. (2018). General data protection regulation (gdpr). Intersoft
Consulting, Accessed in October 24, 1.

[112] Reyna, V. (2018). When irrational biases are smart: A fuzzy-trace theory of complex
decision making. Journal of Intelligence, 6(2):29.

[113] Reyna, V. F. (2004). How people make decisions that involve risk: A dual-processes
approach. Current directions in psychological science, 13(2):60-66.

[114] Reyna, V. F. (2008). A theory of medical decision making and health: fuzzy trace
theory. Medical decision making, 28(6):850—865.

[115] Reyna, V. F. (2012). A new intuitionism: Meaning, memory, and development in
fuzzy-trace theory. Judgment and Decision making.

[116] Reyna, V. F. (2020). Of viruses, vaccines, and variability: Qualitative meaning
matters. Trends in Cognitive Sciences.

[117] Reyna, V. F. and Adam, M. B. (2003). Fuzzy-trace theory, risk communication, and
product labeling in sexually transmitted diseases. Risk Analysis, 23(2):325—342. 29.
[118] Reyna, V. F. and Brainerd, C. J. (1995). Fuzzy-trace theory: An interim synthesis.

Learning and individual Differences, 7(1):1-75.

[119] Reyna, V. F. and Brainerd, C. J. (1998). Fuzzy-trace theory and false memory: New
frontiers. Journal of experimental child psychology, 71(2):194—209.

[120] Reyna, V. F and Broniatowski, D. A. (2020). Abstraction: An alternative neurocog-

46
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

nitive account of recognition, prediction, and decision making. Behavioral and Brain
Sciences, 43.

[121] Reyna, V. F., Chick, C. FE, Corbin, J. C., and Hsia, A. N. (2014). Developmental
reversals in risky decision making intelligence agents show larger decision biases than
college students. Psychological Science, 25(1):76—-84.

[122] Reyna, V. F. and Farley, F. (2006). Risk and rationality in adolescent decision making
implications for theory, practice, and public policy. Psychological science in the public
interest, 7(1):144.

[123] Reyna, V. F. and Lloyd, F. J. (2006). Physician decision making and cardiac risk:
effects of knowledge, risk perception, risk tolerance, and fuzzy processing. Journal of
Experimental Psychology: Applied, 12(3):179.

[124] Reyna, V. F, Lloyd, F. J., and Brainerd, C. J. (2003). Memory, development, and
rationality: An integrative theory of judgment and decision making. Emerging perspec-
tives on judgment and decision research, pages 201-245.

[125] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). ” why should i trust you?”
explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD
international conference on knowledge discovery and data mining, pages 1135-1144.

[126] Ribeiro, M. T., Singh, S., and Guestrin, C. (2018). Anchors: High-precision model-
agnostic explanations. In AAAT, volume 18, pages 1527-1535.

[127] Ruan, L. and Yuan, M. (2010). Dimension reduction and parameter estimation for
additive index models. Statistics and its Interface, 3(4):493-499.

[128] Rudin, C. (2019). Stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead. arXiv:1811.10154 [cs, stat].
arXiv: 1811.10154.

[129] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D.
(2019). Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion. arXiv:1610.02391 [cs]. arXiv: 1610.02391.

[130] Shadish, W. R., Cook, T. D., and Campbell, D. T. (2002). Experimental and quasi-
experimental designs for generalized causal inference. Wadsworth Cengage learning.
[131] Shmueli, G. (2010). To explain or to predict? Statistical Science, 25(3):289-3 10.

Zbl: 1329.62045.

[132] Simonite, T. (2020). How an algorithm blocked kidney transplants to black patients.
Wired.

[133] Singer, M. and Remillard, G. (2008). Veridical and false memory for text: A multi-
process analysis. Journal of Memory and Language, 59(1):18-35.

[134] Slack, D., Hilgard, S., Jia, E., Singh, S., and Lakkaraju, H. (2020). Fooling lime
and shap: Adversarial attacks on post hoc explanation methods. arXiv:1911.02508 [cs,
stat]. arXiv: 1911.02508.

[135] Slovic, P., Finucane, M. L., Peters, E., and MacGregor, D. G. (2004). Risk as anal-
ysis and risk as feelings: Some thoughts about affect, reason, risk, and rationality. Risk
Analysis: An International Journal, 24(2):311-322.

[136] Tomsett, R., Braines, D., Harborne, D., Preece, A., and Chakraborty, S. (2018).

47
L9€8'Ul LSIN/8Z09'01/Bi0 lopy/:sdyy :woy s6seyo jo vay ajqeiiene si UoHeoliqnd siyL

 

Interpretable to whom? a role-based model for analyzing interpretable machine learning
systems. arXiv: 1806.07552 [cs]. arXiv: 1806.07552.

[137] Trabasso, T., Secco, T., and Van Den Broek, P. (1984). Causal cohesion and story
coherence., page 83-110. Lawrence Erlbaum Associates.

[138] Trabasso, T. and Van Den Broek, P. (1985). Causal thinking and the representation
of narrative events. Journal of memory and language, 24(5):612-630.

[139] Trope, Y. and Liberman, N. (2010). Construal-level theory of psychological distance.
Psychological review, 117(2):440.

[140] Tutt, A. (2017). An fda for algorithms. Admin. L. Rev., 69:83.

[141] Tversky, A. and Kahneman, D. (1981). The framing of decisions and the psychology
of choice. science, 211(4481):453-458.

[142] Tversky, A. and Kahneman, D. (1992). Advances in prospect theory: Cumulative
representation of uncertainty. Journal of Risk and uncertainty, 5(4):297-323.

[143] Van den Broek, P. (2010). Using texts in science education: Cognitive processes and
knowledge representation. Science, 328(5977):453-456.

[144] Vaughan, J., Sudjianto, A., Brahimi, E., Chen, J., and Nair, V. N. (2018). Explainable
neural networks based on additive index models. arXiv preprint arXiv: 1806.01933.

[145] Wilhelms, E. A., Reyna, V. F, Brust-Renck, P., Weldon, R. B., and Corbin, J. C.
(2015). Gist representations and communication of risks about hiv-aids: A fuzzy-trace
theory approach. Current HIV Research, 13(5):399-407.

[146] Wolfe, C. R., Widmer, C. L., Reyna, V. F., Hu, X., Cedillos, E. M., Fisher, C. R.,
Brust-Renck, P. G., Williams, T. C., Vannucchi, I. D., and Weil, A. M. (2013). The devel-
opment and analysis of tutorial dialogues in autotutor lite. Behavior research methods,
45(3):623-636.

[147] Yang, H., Rudin, C., and Seltzer, M. (2017). Scalable bayesian rule lists. In Inter-
national Conference on Machine Learning, pages 3921-3930. PMLR.

[148] Yang, Z., Zhang, A., and Sudjianto, A. (2020). Enhancing explainability of neural
networks through architecture constraints. IEEE Transactions on Neural Networks and
Learning Systems.

[149] Yarkoni, T. and Westfall, J. (2017). Choosing prediction over explanation in psy-
chology: Lessons from machine learning. Perspectives on Psychological Science,
12(6):1100-1122.

[150] Zsambok, C. E. and Klein, G. (2014). Naturalistic decision making. Psychology
Press.

[151] Zwaan, R. A. and Radvansky, G. A. (1998). Situation models in language compre-
hension and memory. Psychological bulletin, 123(2):162.

48
Proceedings of the 54th Hawaii International Conference on System Sciences | 2021

Reviewing the Need for Explainable Artificial Intelligence (xAI)

Julie Gerlings Arisa Shollo Ioanna Constantiou
Copenhagen Business School Copenhagen Business School Copenhagen Business School
jge.digi@cbs.dk ash.digi@cbs.dk ic. digi@cbs.dk
Abstract understandable by humans. xAI refers to methods and

The diffusion of artificial intelligence (AD
applications in organizations and society has fueled
research on explaining AI decisions. The explainable
Al (xAD) field is rapidly expanding with numerous ways
of extracting information and visualizing the output of
Al technologies (e.g. deep neural networks). Yet, we
have a limited understanding of how xAlI research
addresses the need for explainable Al. We conduct a
systematic review of xAl literature on the topic and
identify four thematic debates central to how xAl
addresses the black-box problem. Based on this critical
analysis of the xAI scholarship we synthesize the
findings into a future research agenda to further the
XAI body of knowledge.

1. Introduction

Machine learning (ML) / Artificial Intelligence
(AI) technologies including neural networks (NN)
variations, are evolving at a rapid pace and thereby
expanding their capabilities, resulting in advanced
models being used more frequently in decision making
processes [1]. As these models are integrated in
organizations and daily work, academics and
practitioners need to pay more attention to the
development process of the models and_ the
interpretation of the outcome [2]-[4]. This is important
as vital decisions are increasingly being supported or
fully automated by different forms of algorithms that
are not fully understood by people, - known as the AI
black box explanation problem [5]. If we cannot
explain the algorithms, we cannot argue against them,
verify them, improve them, or learn from them [6], [7].
Both practitioners and academics call for better
understanding of the complex and opaque models by
implementing transparent and understandable ML-
models. Generating explanations of how the ML-
models work and how results are produced, leads to
increased trust in machine learning models [5], [8]-—
[10].

Explainable AI has emerged as a response to the
increasing “black box” problem of AI, according to
which models and their performance are not

URI: https://hal.handle.net/10125/70768
978-0-9981331-4-0
(CC BY-NC-ND 4.0}

H#CSS

techniques seeking to provide insights into the outcome
of a ML-model and present it in qualitative
understandable terms or visualizations to the
stakeholders of the model [9], [25], [33].

xAlI initially started from the computer science
community who were building xAI methods providing
a technical solution to the problem. With the diffusion
of AI applications in business and society xAI has
evolved to a broader research domain. From being
viewed as the extraction of advanced statistical values
for developers to identify information gain and entropy
of variables, more advanced frameworks like LIME
offer user friendly visualization of local instances for
the user to interpret. The emerging adoption of AI adds
another layer of complexity to human-computer-
interaction (HCI), where xAI has the potential to play a
central role in the behavioral and cognitive aspects of
Al- assisted decision making. The Al-based systems
are not new, they are descendants of expert systems
[92] utilizing more advanced technologies (e.g.
machine learning algorithms). Yet, they suffer from
opacity in terms of their ability to explain how
conclusions are reached. Hence, the xAI technology
could potentially either clutter or better explain the
decisions made or assisted by an algorithm. The critical
and systematic analysis of published research
constitutes an important vehicle in understanding the
current state of xAI scholarship [15]. This analysis
identifies the major thematic debates within xAI
research and proposes future research directions.

We reviewed xAI literature focusing on purposes,
definitions and actions related to xAI. We respond to
the call for focusing on metahuman systems - a unique
kind of sociotechnical systems - which are blurring the
boundaries between the social and the technical in
unanticipated ways [11]. We consider xAI as such a
system that calls for novel inquiry [11], [16].

Our contribution is twofold. First, we identify four
thematic debates central to how xAI research addresses
the need for explainable AI. Second, taking a
sociotechnical perspective in assessing the debates we
identify two future research avenues: a) the need for a
stakeholder approach and recognizing that different

Page 1284
stakeholders have different explainability needs b) the
need for a holistic view on explainability - jointly
accounting for the social and the technical aspects, the
process and the outcome aspects, as well as the factual
and the storytelling aspects of xAI. We argue that to
advance theories and practice on xAI, the Information
Systems (IS) field is in need of empirical studies that
show how different xAI frameworks address the
different stakeholder needs.

In the following sections, we first describe the
methods used to conduct a systematic literature review.
This is followed by our findings, which we structured
around four thematic debates in the xAI literature.
Finally, we discuss the findings and propose a future
research agenda for xAI research.

2. Method

We performed a systematic literature review using
strategies from Webster and Watson [17], and Jones
and Gatrell [18] focused on creating a theme- centered
retrospective view of current literature in xAl,
generating insights about the current debates and
identifying gaps for guiding further research [19]. The
review consists of search, selection, analysis, and
synthesis processes. Our aim was to provide a critical
analysis of the field rather than providing a descriptive
overview [18], [20].

2.1. Search and Selection Criteria

In order to identify relevant literature for this study
and accommodate the interdisciplinary field of xAI, we
used a broad collection of scientific databases: ArXiv,
Association for Information Systems (AIS), JSTOR,
ACM Digital Library, IEEE Xplore Digital Library,
SAGE, and Science Direct. These databases were
chosen to ensure a diversity in both computer science
and social sciences for abetter representation of papers.
Due to the emerging nature of the field of xAI, ArXiv
was included in the selection of databases to allow
previews and submission of papers for journal articles
and conferences.

We used the terms “xAI’, ‘explainable artificial
intelligence’ or ‘explainable AI’ to search for articles in
the above databases. The reason for not including
‘interpretable’ is partly due to the shift in terminology
in recent years [21], [22], and the notion that
‘interpretation’ has mostly been focused on technical
methods for information extraction and not generating
explanations [13]. We searched for articles where the
terms appeared in titles, keywords, abstracts, and full
text. Due to the novelty of the xAI field and its rapid
development we conduct the search for the last 5 years
from 2016-2020. This search resulted in a total of 221
articles comprised of journal articles and conference
papers.

Next, articles were screened to determine their

relevance based on their abstracts and full text.
Inclusion criteria were theme-related to the purpose of
the literature review. Papers discussing the purpose,
need, reason and capabilities of xAI were initially
included. Conceptual and technical studies focusing
only on technical performance of xAI methods and
techniques that did not engage in a discussion of the
xAI problem or need were discarded. Thus, when a
method was explained, without paying attention to
issues such as validity, interpretation of output, or trust,
but merely generating transparency in opaque models,
the study would be excluded. Along the same lines,
discussion and opinion papers advocating for xAI as a
way to increase transparency of ML-models without
further elaboration were also discarded.

The search strategy included going both
backwards and forwards searching in citations to
identify prior articles of interest for possible inclusion,
and identify key articles with high impact in the field of
xAI [21]. After the inclusion of further literature and
selection based on the above inclusion and exclusion
criteria, the final review included 64 papers.

2.2. Literature Analysis and Synthesis

Considering xAI as a metahuman system - a
unique kind of sociotechnical systems [11] we
examined the articles with a sociotechnical lens in mind
[16]. A sociotechnical approach takes a holistic view
where relations among people, technology, tasks and
organization are sustainable. From previous IS research
we know that “poorly designed sociotechnical systems
with inadequate concern with mutual relationships
were shown to fail and produce unintended or
unwanted outcomes” [11, p. 8]. Taking this as a
departure point we examine the article pool if concerns
like a) a holistic view of social and technical aspect are
considered in the xAI literature, b) consideration or
participation of relevant stakeholders in xAI design,
development and use processes.

During the analysis the primary researcher read all
the articles thoroughly and identified topics that were
discussed across articles. For each paper, key findings
were included in a summary and comments on their
different approaches to the topic was noted [17] while
open coding [90] was applied by highlighting insights
that seem relevant to the review’s scope i.e. how the
papers addressed the need for xAI. In this way, the
primary researcher built a set of concepts and insights
based on the excerpts supported by the papers. As a
result, a concept matrix was made that was enriched and
updated as more articles were read. In the next stage, all
three researchers engaged in axial coding [90] in order
to identify higher order categories by understanding how
some concepts relate to other concepts in the matrix.
The analysis of the article pool was ongoing and
evolving in an inductive manner, as thematic debates

Page 1285
emerged under a) motivating the need for xAI, b) the
completeness vs interpretability dilemma c) explanation
for humans, d) technical frameworks for xAI. As
themes took form, some themes cover broader areas
than others as their nature is more diffused.

3. Findings

In this section, we present the four thematic
debates that emerged from analyzing the article pool.

3. 1. Motivating the Need for xAI

Exploring the recent literature on xAI and the
purpose of the technology, we observe conceptual
differences on how xAI is defined.

There are various interpretations of the
foundational concepts such as explanation vs.
interpretation and their related concepts. Some
researchers use both terms interchangeably, while
others depict the differences in the two conceptual
chunks.

Miller [8] articulates how an explanation in social
science is regarded as a two-step process consisting of
a) the cognitive process, explanandum describing the
cause for an event whereof a subset of causes is
selected as the explanation (exp/anas) and b) the social
process of transferring knowledge between explainer
and explainee in an interactive manner. Whereas
Brand§o et al. [23] takes their stance describing a ‘good
explanation’ as an explanation where the explainer
understands what the explanation means to the person
who asks for it, as they stress the need for investigating
the social meaning of what it means to others than
developers and researchers.

As Brian and Cotton argue, the terms of
interpretability and explainability (and the variations
hereof) are intertwined and still cluttered in their
definitions “Explanation is closely related to the
concept of interpretability: systems are interpretable if
their operations can be understood by a human, either
through introspection or through a_ produced
explanation. ” [24].

Other scholars [9], [22] take a more pragmatic
view and argue that ‘interpretation’ is closer to the
development of models and constitutes the opposite of a
‘black-box’ model, where the search for a direct
understanding of the mechanism by which a model
works is the aim of interpretable ML.

Others define interpretation as the means to
explain or to present in understandable terms to a
human [25] and directs the research in terms of how
humans interpret information. Liao et al. argue for a
more diverse approach to xAI, taking into account the
different user needs. They describe xAI as “...an
example, one of the most popular approaches to
explain a prediction made by a ML classifier, as
dozens of XAI algorithms strive to do, is by listing the

features with the highest weights contributing to a
model’s prediction” [26] which for developers might
be of high value, however not for the average layman.
These different definitions reveal the need for further
conceptual alignment in the xAI field. The following
sections present the main drivers for xAI systems.

3.1.1. Generate Trust, Transparency and
Understanding. Generating trust, is a major driver for
xAI and is strongly related to transparency. DARPAs
XAI program promotes the need for xAI, as we need to
further understand, trust and manage the emerging
generation of artificially intelligent machines [27].
Along these lines, lies a great effort and research
focused on extracting information from models or build
simpler models in the quest of delivering both
transparency, understanding and thereby generate trust
in the models [8], [22], [28]— [34]. Gilpin et al. [34]
argue that:” ... models that are able to summarize the
reasons for neural network behavior, gain the trust of
users, or produce insights about the causes of their
decisions...” [34]. Along with DARPA, the general
increase in ML performance and use has created the
search for better understanding of the models to
increase trust and thereby an increased use of ML in the
industry [35], [36]. Moreover Miller [8] argues that two
complementary approaches will generate more
transparent, interpretable, and explainable systems to
thereby make us more equipped to understand and trust
the models [8]: 1) Interpretability and Explainability
understood as how well a human understand an
explanation in a given context and 2) An explanation of
a prediction (decisions) to people (target audience).
Most technical xAI approaches aim at extracting
information from a model (it may be e.g. neural
networks or random forest) such as feature importance,
relative importance scores, sensitivity analysis, rule
extractions or other methods to generate greater
transparency. These xAI approaches and frameworks
mainly work from the perception of transparency may
improve understanding and thereby increase trust — or
the opposite that ‘black-box’ models are not to be
trusted [9], [31], [33], [37].

Few papers include socio-technical aspects in the
technical models presented. However few address the
obstacles in stakeholders understanding the output
which includes a consideration of the output as a socio-
technical aspect of an explanation, a HCI- dilemma and
addressing the risks in producing explanations for
developers, created by developers (inmates running the
asylum) [38], [39]. For example Zang and Zhu [40]
presents a graphic logic (or symbolic logic) to ease the
understanding of Convolutional Neural Networks
(CNN), instead of only information extraction; While
Mueller et al. [29] visualizes pixels used to determine a
husky from a wolf to test participants through LIME. In
this way, they address the need for human

Page 1286
understanding by testing their explanation on
participants evaluating whether they trust the algorithm
[33]. Furthermore, the literature emphasized that the
xAI frameworks generating explanations are built by
developers or technical people focusing on the
computational problems in extracting data, which will
not necessarily solve the issue of trust [8], [23], [38],
[41], [42].

However, many conceptual papers call for
interdisciplinary work and discuss the need for more
focus on human understanding or interpretability, and
not only transparency [9], [21], [25], [26], [34], [43].

3.1.2. Ensure Compliance, follow Regulations and
GDPR Laws. One of the many reactions to new
regulations and GDPR laws is the call for xAI to
provide explanations not only to the users, but society as
a whole [22]. This, along with other regulations, makes
it urgent for practitioners and industries to ramp up the
investment in explaining opaque models [29], [34]. The
GDPR regulation and ’the right to an explanation’ has
caused great stir within both research and the industry,
directing them towards xAI - as a possible solution for
being compliant [44]-[46]. Moreover, some researchers
argue for regulation of xAI itself, or the possibility of
setting a standard or quality measure to ensure a
responsible use of xAI and avoid building persuasive
models instead of explainable ones [47], [48]. The
fallacies of building persuasive explanations are very
well described in Gosiewska and Biecek’s [49]
examples of how additive models can cause misleading
guidance on instance-level explanations which is
backed by Rudin [50] who argues against the recent
trend of building (additive) explainable post-hoc
“misleading” explanations.

3.1.3. For the Sake of Social Responsibility,
Fairness and Risk Avoidance. Especially, within
healthcare, clinical and justice work, risks and
responsibility are a major concern, as they are
potentially dealing with human lives and not merely
cost-benefit analyses [9], [51], [52]. Risk avoidance
occurs as responsibility is assigned to the individual
professional. Hence, developing mental models for
expert (e.g. clinical) reasoning to develop better
understanding of the reasoning behind deep neural
networks and opaque models [53]-[55]. Moreover,
recent events of discrimination and recidivism in
opaque models have fueled the debate on ensuring
fairness in model performance and _ deeper
understanding of how they are built. Cases of
minorities in hiring processes [56], recidivism in the
COMPAS system and general fairness [48] have added
to the surge in the xAI literature [6], [22], [34], [47],
[57].

3.1.4. Generate Accountable, Reliable and Sound
Models for Justification. A theme that has caused

great attraction towards xAI is the possibility to ensure
fairness and unbiased models by auditing them or create
proof of their rightfulness. Adadi and Berrada [21]
follow this approach and argue that xAI provide the
required results for auditing the algorithms and
generates a provable way for defending algorithmic
decisions as being fair and ethical. Hence, generating
algorithms that are not only fair and socially
responsible, but also accountable and able to justify
their output is another aspect motivating the need for
xAI. Amongst others, Abdul et al. [51] describes the
complications herein and present one of the more
popular approaches to generate xAI, by building
counterfactual explanations which are based on the
notion of causality and not just correlation. When
looking at the specific xAI methods, Doshi-Velez and
Kim [25] argue that global explanations of entire
models or groups are more appropriate for scientific
understanding or bias detection in models, whereas
local explanations are better suited for a justification
of a specific decision. Moreover, Liao and Anderson
[58] present methods for generating argumentation-
based justifications and explanations, based on formal
argumentation which provide natural arguments for
better reasoning of the models. Lastly, Ananny and
Crawford [59] bring forward a discussion of
transparency not being sufficient to govern and hold
algorithms accountable. They present different pitfalls
in the ideal transparent model and its limitations. They
claim that transparency does not necessarily build trust
as different stakeholders trust systems differently,
depending on their confidence upon when and what
information is disclosed, and how accurate and relevant
it is perceived to be [59].

3.1.5. Minimize Biases and Misinterpretation in
Model Performance and Interpretation. Biases in
models and their performance have shown to be an
important driver for xAI, as media coverage of models
performing sub-par to humans in e.g. filtering out
appropriate candidates in hiring processes [60] or
failing at recognizing people of color [61]. Especially
when dealing with neural network learning patterns
from training data, biased training data becomes an
issue that impacts the validity of the model output [62].
For these reasons, this sub-theme is also tightly linked
to the two previous sub-themes, as biased or
discriminatory models and their results are to no
purpose if not minacious in their implementation.
Besides biased training data [22], [63], variable
selection [64], [62] and representation [32], [33] , our
own cognitive bias can furthermore hinder our
interpretation of the visualized output from a model as
we tend to oversimplify the information [52]. Our
cognitive biases are argued to be mitigated by xAI
frameworks of various kinds, e.g. by Wang et al. [65]
who argue for pursuing reasoning theories and Arrieta et

Page 1287
al. [22], who argue that human cognitive capabilities
favors visual presentation of data.

3.1.6. Being Able to Validate Models and Validate
Explanations Generated by xAI. In response to biased
models and sub-par performance to humans,
researchers have found four types of evaluation
methods for deep neural networks (DNN) measured in
1) completeness compared to the original model, 2)
completeness as measured on substitute tasks, 3) the
ability to detect biases within a model, 4) human
evaluations [34]. Evaluating xAI is not only a question of
precision and feature extraction, as the user of the
output might not be able to understand the model
output. Others present a thorough taxonomy for
evaluation of interpretability, where the costliest is the
application-grounded approach which entails testing of
an implemented explanation and letting end-users test it
[25]. In this case the explanation is evaluated based on
identification of errors, new facts or less discrimination,
to the baseline of human-performance [25].
Furthermore, they present a human-grounded
evaluation for testing more general notions of quality
such as which type of explanation is best under time
constraints. The last evaluation approach is
functionally-grounded and fits the evaluation of
interpretability in already evaluated models or
immature model testing. This approach requires no
human interaction but rather measures optimization or

quality [25].

3.2. Completeness vs. Interpretability
Dilemma

From the debate of evaluating xAI, a debate of
whether we are able to make good explanations
emerges. The tradeoff between a correct (complete)
and a good (interpretable) explanation, is discussed by
Kim [66] where a pragmatic approach is needed if the
user (audience) should be able to understand the
explanation. Furthermore, Kim [66] continues to
present the notion of a ‘grasp-ability’ test to ensure the
audience can use an explanation that is not necessarily
perfectly transparent or rigorous, but graspable to the
audience. However, researchers have argued that the
need for explainability stems from incompleteness
producing different biases and argue that the nature of
the user’s expertise will influence the level of
sophistication the explanation can contain [25].

Many other researchers argue for the same tradeoff
between completeness (the goal of describing the
operation of a system in an accurate way) and
interpretability (here, the goal of interpretability is to
describe the internals of a system in a way that is
understandable to humans), as interpretability alone is
insufficient [34]. Hence, we should be cautious with
this tradeoff as humans have a strong specific bias
towards simple descriptions which can lead researchers

to create persuasive systems rather than transparent
systems. Humans lose trust in the explanation when
soundness is low [21], [34].

3.3. Human Explanations

In general, the literature agrees on the need for
building explanations that the user can understand.
However, different approaches on how to get there
appear. Different algorithmic approaches are capable of
producing different xAI outputs, depending on the level
of dependency on the model [21], [33]. The literature is
rather divided on the approaches of how explanations
are built as scholars either draw on_ theoretical
underpinnings from decision making _ theories,
philosophy and psychology [8], [65], [67], [68] or on
very sparse information on human understanding
focusing on computational problems, extraction of
performance measures and produce transparency in the
model output [23], [30], [31], [33], [69]. “Much of the
research about how to interpret and explain AI
behavior, they say, is driven by the needs of those who
build AI, and not necessarily of those who use if? (23,
p. 3].

On the other hand, studies that take a social
perspective focus on how humans_ experience
explanations:

1. Explanations are contrastive: Sought for in
response to counterfactual cases (foils). People ask
why P happened instead of Q.

2. Explanations are selected - in a biased
manner: People do not expect explanations that consist
of actual and complete cause of an event.

3. Probabilities in explanations do not matter,
causalities do: Referring to probabilities or statistical
relationships in an explanation is not as effective as
referring to causes.

4. Explanations are social: They are presented in
a context through transfer knowledge, interactions,
and conversations. People interact differently to
explanations [8].

The divide between these two approaches towards
building xAI constitutes a research gap from computer
science to social science. It also leaves an impression
of xAI users being either lay-users or developers with
only a few articles discussing the different roles in the
processes of xAI.

Few papers address the different AI literacy levels
users may have and even fewer address the diversity
of stakeholders and their different needs for xAI.
While there are various levels of AI literacy and
diverse subject domains, researchers focus on
developing a user-centric conceptual framework [65].
This seems to be the norm, as the users are rarely
defined as anything else as ‘user’, though sometimes,
when case examples are presented, users are
mentioned as e.g. clinicians, doctors or experts [54],

Page 1288
[55], [65], [70]-{73]. Only a few papers discuss
different types of roles and stakeholders in regard to
the ecosystem of xAI, and argue that one solution
might not fit the purpose of all different types of users
but we need to include the context, background and
knowledge of the stakeholders to produce
understandable explanations [52], [74].

For identifying the different stakeholders Arrieta
et al. [22] presents a framework — or target audience,
in relation to xAI and acknowledge that stakeholders
have different explainability needs in ML models. For
example, domain experts may seek out xAI to gain
scientific knowledge, whereas users affected by the
model’s decision may seek to better understand and
verify a fair decision was made. Arrieta et al. [22]
continue to present a taxonomy of the different
techniques that produce xAI within images, text and
tabular data.

3.4. The technologies producing xAI

Many different approaches towards building more
transparent and explainable models have been
presented in recent years in the search for opening the
infamous black-box. Models such as PD plots (Partial
Dependencies), ALE plots (Accumulated Local
Effects) ICE (Individual Conditional Expectation),

SHAP values (SHapely Additive exPlanations) and
LIME (Local Interpretation) are amongst the most
popular groundworks for feature-based models,
excluding image-based algorithms such as CNN [30]-—
[33], [40], [75}1{78]. The above mentioned methods
each have their different approaches but can be
classified as follows:

Intrinsically transparent: ML Models that are of
a simpler character, but also less precise than other
more advanced models (Linear Regression, Logistic
Regression, Decision Trees RuleFit) [22], [32].

Model agnostic xAI frameworks: These are often
of a post-hoc character, meaning that they are designed
to fit any model type and rely on techniques that
simplifies the model (rule-based extraction from the
complex model), show feature relevance estimations
(e.g. SHAP produces an additive feature importance
score for single predictions), visualizes the model (e.g.
ICE which visualizes the estimated model of
supervised ML models) or produce a local surrogate
model of the output (e.g. LIME)[22], [32], [33], [79],
[80]. A common thread of these frameworks is that
they produce some kind of visual output for easier
understanding. However, a thorough comprehension of
the output of additive models, can still be a challenge
to laymen [49], [82], [83].

4. Discussion and Future Research
Agenda

The analysis of the xAI literature shows that due to
the rather new and interdisciplinary field of xAI [24], a
common understanding of key concepts has not been
fully established yet. However, more recent work [8],
[22] is providing more comprehensive definitions
having a greater impact and shaping the field.

The nascency of the field is also evident in the
approaches towards building and designing xAI
frameworks and outputs which were driven by
developers. Recently, researchers have come to
understand the need for closer interdisciplinary work,
as the explanations originally built for the developers
— by developers, can serve more purposes than just
ensuring sound and reliable models. xAI as a means to
minimize biases, ensure social responsibility and
fairness is as much about data preparation and model
design as it is about proving that it is taken into
account. As a tool for developers, xAI can be of great
importance to ensure a model is based on causations
and not correlations, data is evenly distributed, and
features used are relevant. However, the demand of
knowledge from programmers, data scientists and
computer scientists is increasing as the stack grows.
Moreover, to possess the domain knowledge a clinician
might have within his field will never be reasonable,
however necessary to build sound models. This again
calls for more interdisciplinary research in xAI to
ensure we can collaborate across expert domains and
obtain the expertise from all related fields. This is in
line with recent calls from IS researchers for
interdisciplinary research into AI phenomena [11],
[84].

Considering the application of xAI in AI based
systems, the four thematic debates indicate that
organizations need to be cautious when choosing to
use an xAI method to address a specific need. For
example, additive models generating instance-level
explanations might be misleading to some [49], which
does not suffice for ensuring compliance or justify
explanations - but could suffice more general
explanations. As xAI is often seen as an enabler of AJ,
their goals are different from AI/ML. AI is for
optimization, augmentation and automation of
decision making, utilizing the ever so strong power of
machine learning. xAI is for exploring and
understanding the decision made by AI and as a means
to validate models and the performance of ML-
models. However, validating explanations and xAI
outputs is a much more complex process as it includes
the perception of the stakeholder who might not
possess the technical literacy level required to follow
the chain of logic in the model and explanation
combined [66], [25]. Further, xAI should be aligned
with the decision objective, which should guide the
choice of the ML-model, variable selection, hyper
parameter settings and other adjustments of the ML-

Page 1289
model. Besides the fact that xAI is costly in terms of
computational power and time, this in turn adds to the
complexity of any Al-solution, which argues for re-
evaluating the need for xAI in a solution.

To generate xAI models that satisfy regulators
and ensures compliance, especially within healthcare
and finance, has also shown to be of high interest as
this is a great entry-barrier for implementing ML in
these highly regulated areas [21], [22], [85], [86]. The
literature greatly suggests that xAI will ease this
barrier, however, there has not yet been any significant
empirical studies showing how xAI in fact may satisfy
regulators need to ensure compliance or undergo
audits. Therefore, we greatly encourage researchers to
address this issue with more empirical studies such as
Lauritsen et al. [87].

Another emerging topic in xAI is how to validate
the explanations and related frameworks. On one
hand, researchers need to ensure the human
interpretation is accommodated and on the other hand,
that the frameworks show information as it is, without
skewing the measures and building persuasive outputs
instead. We found that different streams of research
seem to favor either building intrinsically transparent
models, or post-hoc local explanations. Future
research should focus on meeting both these
contradictory yet interrelated needs combining process
transparency and storytelling of the outcome [88],
[89].

Moreover, reviewing the demand for knowledge
and how much xAI stakeholders need to understand in
terms of ensuring reliability in using it, was discussed
as there seems to be only little research into how much
information we need to interpret and understand. The
debate of completeness versus interpretability will not
disappear but the nature of users’ expertise will
influence the level of sophistication in the explanation
[25]. However, empirical studies are currently needed
to further investigate the relationship between
completeness and interpretability, but also if less
interpretability could possibly satisfy stakeholders
needs without becoming suggestive [91].

The six sub-themes which emerged throughout the
literature review constituting the motivation for xAI,
are highly interlinked, but each has their own
underlying purpose. We argue for further research into
building — or defining existing methods that fits the
purpose of the motivated need for xAI. This could be
either ensuring compliance or minimizing biases in
model performance. Developing and implementing ML
and xAI we need to ensure the chain of logic persists
both in development and production, and that the
chosen xAI framework fits the ML model and supports
the underlying goal of the intended explanation. One
could argue that the same methods might be feasible in
many cases, however, the goals of the underlying ML

models, and the explanations themselves are different.
Therefore, we call for further research in understanding
what kinds of explanation techniques fit the different
purposes intended.

The thematic debate of human explanations is
currently dominated by social science perspectives with
little attention to what the technology is capable of
producing. This reinforces the social - technical
dichotomy making for a very divided xAlI field:
focusing either on how we as humans interpret
explanations or how we can technically extract
information from complex models. While the current
state is justified by the nascent stage the field is in,
further research is required in understanding the
different xAI stakeholders needs and how they can be
satisfied with building more targeted explanations than
the two dominant groups of either developers or users.

Techniques and frameworks developed to produce
explanations are continuously evolving, including more
and more complex ML models. However, these
techniques and frameworks seldomly consider other
stakeholders besides developers themselves. Therefore,
continuing the path paved by Ribeiro et al. [33] we call
for further empirical research on how well the
frameworks are understood by different stakeholders.
We further argue for research in understanding what
xAI frameworks would support each purpose the best,
and which would be inexpedient.

While this research agenda is not exhaustive in any
way, it revealed two future avenues for xAI research:

a) the need to account for the different
stakeholders and their different explanation needs.
This research avenue highlights the importance of
investigating the micropolitics of xAI in organizations
and its implications for work.

b) the need for a holistic approach in
investigating xAI, jointly considering the social and
technical aspects of xAI, the process and the outcome
aspects of xAI, as well as the factual and the
storytelling aspects of xAI. This research avenue
emphasizes the need to understand conceptually and
empirically the complex nature of xAI as a new kind
of sociotechnical system and its implications for AI
practices in business and society.

5. Conclusion

The quest for opening the famous ‘AI black box’
has attained great traction in the past decade, as ethical
concems, regulations, and the need for controlling
these models has increased. Responding to the calls by
Miller [8], and Lipton [9] to generate further
perspectives and critical writings on xAI we perform a
systematic literature review on the need for xAI. We
first identify four thematic debates central to how xAI
addresses the need for explainable AI. Second, taking
a sociotechnical perspective in assessing the debates

Page 1290
we identify two future research avenues: a) the need
for a stakeholder approach and, b) the need for a
holistic view on xAI. Third, we argue that to advance
theories and practice on xAl, the IS field is in need of
empirical studies that show how different xAI
frameworks address the different stakeholder needs.
Based on future empirical evidence, we as scholars
may be able to judge to what extent xAI meets its
expectations, both for its stakeholders seeking to
strategically benefit from AI, and society as a whole.

References

[1] T. Ha, S. Lee, and S. Kim, “Designing
Explainability of an Artificial Intelligence
System,” 2018.

[2] R. Kitchin, “Thinking critically about and
researching algorithms,” Program. City Work.

Pap. 5, vol. 4462, no. October, pp. 1-29, 2014.

[3] M. K. Lee, “Understanding perception of
algorithmic decisions: Fairness, trust, and emotion
in response to algorithmic management,” Big Data
Soc., vol. 5, no. 1, p. 205395171875668, 2018.

[4] S.S. Skiena, “The algorithm design manual,”

Choice Rev. Online, vol. 35, no. 11, pp. 35-6287-
35-6287, 2013.

[5] IEEE, “A Vision for Prioritizing Human Well-
being with Autonomous and Intelligent Systems,
First Edition,” 2019.

[6] C. O’Neil, Weapons of Math Destruction: How
Big Data Increases Inequality and Threatens
Democracy, vol. 272. 2016.

[7] W. Samek, T. Wiegand, and K.-R. Miller,
“Explainable Artificial Intelligence:

Understanding, Visualizing And
Interpreting Deep Learning Models,” 2019.

[8] T. Miller, “Explanation in artificial intelligence:
Insights from the social sciences,” Artificial
Intelligence, vol. 267. pp. 1-38, 2019.

[9] Z.C. Lipton, “The Mythos of Model
Interpretability,” Commun. ACM, vol. 61, no. Whi,
pp. 35-43, Jun. 2016.

[10] M. Turek, “DARPA: Explainable Artificial
Intelligence (XAI).” [Online]. Available:
https://www.darpa.mil/program/explainable-

artificial-intelligence. [Accessed: 08-Jul-2020].
[11] K. Lyytinen and P. A. Nielsen, “AIS Electronic
Library (AISeL ) PANEL 4: A NORDIC
SOCIOTECHNICAL PERSPECTIVE ON,” 2020.
[12] C. Zednik, “Solving the Black Box Problem: A
Normative Framework for Explainable Artificial
Intelligence,” Philos. Technol., pp. 1-29, 2019.
[13] B. Mittelstadt, C. Russell, and S. Wachter,
“Explaining explanations in AI,” in FAT* 2019 -
Proceedings of the 2019 Conference on Fairness,
Accountability, and Transparency, 2019, pp. 279-288.
[14] T. Kulesza, M. Burnett, W.-K. Wong, and S. Stumpf,
“Principles of Explanatory Debugging to Personalize
Interactive Machine Learning,” 2015.

[15] D. Arnott and G. Pervan, “A critical analysis of
decision support systems research,” J. Inf Technol.
vol. 20, no. 2, pp. 67-87, Jun. 2005.

[16] S.Sarker, S. Chatterjee, X. Xiao, and A. Elbanna, “The

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

sociotechnical axis of cohesion for the IS discipline: Its
historical legacy and its continued relevance,” MIS Q.
Manag. Inf. Syst., vol. 43, no. 3, pp. 695-719, 2019.
J. Webster and R. T. Watson, “Analyzing the Past to
Prepare for the Future: Writing A Literature Review,”
MIS Q., vol. 26, no. September, pp. 13-23, 2002.

O. Jones and C. Gatrell, “Editorial: The future of
writing and reviewing for IJMR,” Jniernational
Journal of Management Reviews, vol. 16, no. 3.
Blackwell Publishing Ltd, pp. 249-264, Jul-2014.

F. Rowe, “What literature review is not: diversity,
boundaries and recommendations,” Eur. J. Inf Syst.,
vol. 23, no. 3, pp. 241-255, 2014.

J. Webster and R. T. Watson, “Analyzing the past to
prepare for the future,” MZS Q., vol. 26, no. 2, pp. 13—
23, 2002.

A. Adadi and M. Berrada, “Peeking Inside the
Black-Box: A Survey on Explainable Artificial
Intelligence (XAI),” JEEE Access, vol. 6, pp.
52138-52160, 2018.

A.B. Arrieta e¢ al., “Explainable Artificial Intelligence
(XAI): Concepts, Taxonomies, Opportunities and
Challenges toward Responsible AI,” no. October, Oct.
2019.

R. Brandao, J. Carbonera, C. de Souza, J. Ferreira, B. N.
Gongalves, and C. F. Leitéo, “Mediation Challenges
and Socio-Technical Gaps for Explainable Deep
Learning Applications,” arXiv Prepr. arXiv ..., no.
Query date: 2020-04-16 13:43:28, pp. 1-39, 2019.

O. Biran and C. Cotton, “Explanation and
Justification in Machine Learning: A Survey,”

IJCAI Work. Explain. AI, no. August, pp. 8-14,
2017.

F. Doshi-Velez and B. Kim, “Towards A

Rigorous Science of Interpretable Machine
Learning,” no. Ml, pp. 1-13, 2017.

Q. V. Liao, D. Gruen, and S. Miller, “Questioning
the AI: Informing Design Practices for

Explainable AI User Experiences,” 2020, pp. 1—

15.

D. Gunning and D. W. Aha, “DARPA’s explainable
artificial intelligence program,” AJ Mag., 2019.

H. K. Dam, T. Tran, and A. Ghose, “Explainable
software analytics,” Proc. - Int. Conf. Softw. Eng.,
pp. 53-56, 2018.

S. T. Mueller, R. R. Hoffman, W. Clancey, A.
Emrey, and G. Klein, “Explanation in Human-AI
Systems: A Literature Meta-Review,” Def Adv.

Res. Proj. Agency, no. February 2019, p. 204,

2019.

P. Hall, “On the Art and Science of Machine
Learning Explanations,” Oct. 2018.

A. Goldstein, A. Kapelner, J. Bleich, and E.

Pitkin, “Peeking Inside the Black Box:

Visualizing Statistical Learning With Plots of
Individual Conditional Expectation,” J.

Comput. Graph. Stat., vol. 24, no. 1, pp. 44—

65, 2015.

C. Molnar, “Interpretable Machine Learning,”
Https://Christophm. Github Io/Interpretable-M1-
Boolky, 2018. [Online]. Available:
https://christophm.github.io/interpretable-ml-

book/. [Accessed: 06-Apr-2019].

Page 1291
[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why
should i trust you?’ Explaining the predictions of
any classifier,” Proc. ACM SIGKDD Int. Conf.
Knowl. Discov. Data Min., vol. 13-17-Augu, pp.
1135-1144, Aug. 2016.

L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M.
Specter, and L. Kagal, “Explaining explanations:
An overview of interpretability of machine
learning,” Proc. - 2018 IEEE 5th Int. Conf, Data
Sci. Adv. Anal. DSAA 2018, pp. 80-89, 2019.

R. Guidotti, A. Monreale, F. Giannotti, D.
Pedreschi, S. Ruggieri, and F. Turini, “Factual

and Counterfactual Explanations for Black Box
Decision Making,” IEEE Intell. Syst., vol. 34, no.

6, pp. 14-23, 2019.

R. Guidotti, A. Monreale, S. Ruggieri, F. Turini,

F. Giannotti, and D. Pedreschi, “A survey of
methods for explaining black box models,” ACM
Comput. Surv., vol. 51, no. 5, 2018.

W. Samek and K. R. Miller, “Towards

Explainable Artificial Intelligence,” in Lecture
Notes in Computer Science (including subseries
Lecture Notes in Artificial Intelligence and

Lecture Notes in Bioinformatics), vol. 11700
LNCS, 2019, pp. 5-22.

T. Miller, P. Howe, and L. Sonenberg,
“Explainable AI: Beware of Inmates Running the
Asylum Or: How I Learnt to Stop Worrying and
Love the Social and Behavioural Sciences,” 2017.
P. Madumal, T. Miller, and L. Sonenberg, “A
Grounded Interaction Protocol for Explainable
Artificial Intelligence,” no. Aamas, 2019.

Q. shi Zhang and S. chun Zhu, “Visual
interpretability for deep learning: a survey,”

Front. Inf. Technol. Electron. Eng., vol. 19, no.

1, pp. 27-39, 2018.

P. Madumal, T. Miller, F. Vetere, and L. Sonenberg,
“Towards a Grounded Dialog Model for Explainable
Artificial Intelligence,” 2018.

P. Madumal, L. Sonenberg, T. Miller, and F. Vetere,
“A grounded interaction protocol for explainable
artificial intelligence,” in Proceedings of the
International Joint Conference on Autonomous
Agents and Multiagent Systems, AAMAS, 2019.

Ni. Kuhl, J. Lobana, and C. Meske, “Do you comply
with AI? -- Personalized explanations of learning
algorithms and their impact on employees’
compliance behavior,” pp. 1-6, 2020.

B. Goodman and S. Flaxman, “European union
regulations on algorithmic decision making and a
‘right to explanation,’” AJ Mag., vol. 38, no. 3, pp. 50-
57, 2017.

M. E. Kaminski, “The Right to Explanation,
Explained,” SSRN Electron. J., 2018.

J. M. Schoenborn and K. Althoff, “Recent Trends in
XAI: A Broad Overview on current Approaches ,
Methodologies and Interactions,” Case-Based Reason.
Explan. Intell. Syst. Work., 2019.

J. Zenilli, A. Knott, J. Maclaurin, and C.

Gavaghan, “Transparency in Algorithmic and
Human Decision-Making: Is There a Double
Standard?,” Philos. Technol., 2019.

P. Hall, N. Gill, and N. Schmidt, “Proposed

[49]

[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

Guidelines for the Responsible Use of Explainable
Machine Learning,” no. NeurIPS, pp. I-11, 2019.

A. Gosiewska and P. Biecek, “Do Not Trust

Additive Explanations,” Mar. 2019.

C. Rudin, “Stop explaining black box machine
learning models for high stakes decisions and use
interpretable models instead,” Nat. Mach. Inteil., vol.

1, no. 5, pp. 206-215, 2019.

A. Abdul, J. Vermeulen, D. Wang, B. Y. Lim, and M.
Kankanhalli, “Trends and trajectories for explainable,
accountable and intelligible systems: An HCI research
agenda,” Conf: Hum. Factors Comput. Syst. - Proc.,
vol. 2018-Apmil, pp. 1-18, 2018.

A. Paez, “The Pragmatic Turn in Explainable

Artificial Intelligence (XAI),” Minds Mach.,

2019,

Y. Xie, X. A. Chen, and G. Gao, “Outlining the

design space of explainable intelligent systems for
medical diagnosis,” in CEUR Workshop

Proceedings, 2019, vol. 2327.

G. J. Katuwal and R. Chen, “Machine Learning
Model Interpretability for Precision Medicine,”

2016.

A. Holzinger, C. Biemann, C. S. Pattichis, and D. B.
Kell, “What do we need to build explainable AI systems
for the medical domain?,” no. MI, pp. 1-28, 2017.

E. van den Broek, “Hiring Algorithms : An
Ethnography of Fairness in Practice,” Fortieth Int.
Conf. Inf. Syst., pp. 1-9, 2019.

C. Rudin, “Stop explaining black box machine
learning models for high stakes decisions and use
interpretable models instead,” Nat. Mach. Inteil.,

vol. 1, no. 5, pp. 206-215, 2019.

B. Liao, M. Anderson, and 8S. L. Anderson,
“Representation, Justification and Explanation in a
Value Driven Agent: An Argumentation-Based
Approach,” pp. 1-24, 2018.

M. Ananny and K. Crawford, “Seeing without
knowing: Limitations of the transparency ideal and its
application to algorithmic accountability,” New Media
Soc., vol. 20, no. 3, pp. 1-17, 2016.

Jeffrey Dastin, “Amazon scraps secret AI recruiting
tool that showed bias against women - Reuters,”
Reuters, pp. 5-9, 2018.

J. Vincent, “Google ‘fixed’ its racist algorithm by
removing gorillas from its image-labeling tech,” The
Verge, pp. 2018-2020, 2018.

C. T. Wolf, “Explainability scenarios: Towards
scenario-based XAI design,” in International
Conference on Intelligent User Interfaces, Proceedings
TUT, 2019.

W. Samek, T. Wiegand, and K.-R. Miller, “Explainable
Artificial Intelligence: Understanding, Visualizing and
Interpreting Deep Learning Models,” Aug. 2017.

N. Tintarev and J. Masthoff, “Evaluating the
effectiveness of explanations for recommender systems:
Methodological issues and empirical studies on the
impact of personalization,” User Model. User-adapt.
Interact., vol. 22, no. 4-5, pp. 399-439, 2012.

D. Wang, Q. Yang, A. Abdul, B. Y. Lim, and U.
States, “Designing Theory-Driven User-Centric
Explainable AI.”

T. W. Kim, “Explainable artificial intelligence

Page 1292
[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

[76]

[7]

[78]

[79]

[80]

(XAI), the goodness criteria and the grasp-ability
test,” pp. 1-7, 2018.

H. F. Cheng ef al, “Explaining decision-making
algorithms through UI: Strategies to help non- expert
stakeholders,” in Conference on Human Factors in
Computing Systems - Proceedings, 2019, pp. 1-12.

B. Y. Lim, Q. Yang, A. Abdul, and D. Wang, “Why
these explanations? Selecting intelligibility types for
explanation goals,” in CEUR Workshop Proceedings,
2019,

S. M. Lundberg and S. I. Lee, “A unified approach to
interpreting model predictions,” Adv. Neural Inf.
Process. Syst., vol. 2017-Decem, no. Section 2, pp.
4766-4775, 2017.

R. R. Hoffman, S. T. Mueller, G. Klein, and J. Litman,
“Metrics for Explainable AI: Challenges and
Prospects,” pp. 1-50, 2018.

R. R. Hoffman, G. Klein, and 8. T. Mueller,
“Explaining explanation for ‘explainable AI,’”

Proc. Hum. Factors Ergon. Soc., vol. 1, no. 2, pp.
197-201, 2018.

S. M. Lauritsen e¢ a/., “Explainable artificial
intelligence model to predict acute critical illness from
electronic health records,” 2019.

Z. Che, 8. Purushotham, R. Khemani, and Y. Liu,
“Interpretable Deep Models for ICU Outcome
Prediction,” AMIA ... Annu. Symp. proceedings. AMIA
Symp., vol. 2016, no. August, pp. 371-380, 2016.

R. Tomsett, D. Braines, D. Harborne, A. Preece, and
S. Chakraborty, “Interpretable to Whom? A Role-
based Model for Analyzing Interpretable Machine
Learning Systems,” 2018.

A. Gosiewska and P. Biecek, “iBreakDown:
Uncertainty of Model Explanations for Non-
additive Predictive Models,” Adv. Neural Inf
Process. Syst., 2019.

K. Sokol, R. Santos-rodriguez, A. Hepburn, and P.
Flach, “Surrogate Prediction Explanations Beyond
LIME,” no. HCML, 2019.

B. Murray, M. Aminul Islam, A. J. Pinar, T. C.
Havens, D. T. Anderson, and G. Scott, “Explainable
Al for understanding decisions and data-driven
optimization of the Choquet integral,” in JEEE
International Conference on Fuzzy Systems, 2018.

J. M. Alonso and G. Casalino, “Explainable

Artificial Intelligence for Human-Centric Data
Analysis in Virtual Learning Environments,” in
Communications in Computer and Information
Science, 2019.

C. Molnar, “Interpretable Machine Learning. A Guide
for Making Black Box Models Explainable.,” Book, p.
247, 2019.

A. Datta, S. Sen, and Y. Zick, “Algorithmic

[81]

[82]

[83]

[84]

[85]

[86]

[87]

[88]

[89]

[90]

[91]

[92]

Transparency via Quantitative Input Influence: Theory
and Experiments with Learning Systems,” in
Proceedings - 2016 IEEE Symposium on Security and
Privacy, SP 2016, 2016.
P. Biecek and T. Burzykowski, “Explanatory Model
Analysis,” 2020.
N. Diakopoulos ef ai/., “Principles for Accountable
Algorithms and a Social Impact Statement for
Algorithms,” Fatmi.Org, 2018.
F. K. Dosilovic, M. Breic, and N. Hlupic,
“Explainable artificial intelligence: A survey,” in 20/8
41 st International Convention on Information and
Communication Technology, Electronics and
Microelectronics, MIPRO 2018 - Proceedings, 2018.
T. Gronsund and M. Aanestad, “Augmenting the
algorithm: Emerging human-in-the-loop work
configurations,” J. Strateg. Inf Syst., p. 101614, Jun.
2020.
N. Prentzas, A. Nicolaides, E. Kyriacou, A. Kakas,
and C. Pattichis, “Integrating machine learning with
symbolic reasoning to build an explainable ai model
for stroke prediction,” in Proceedings - 2019 IEEE
19th International Conference on Bioinformatics and
Bioengineering, BIBE 2019, 2019.
A. Hosny, C. Parmar, J. Quackenbush, L. H.
Schwartz, and H. J. W. L. Aerts, “Artificial
intelligence in radiology,” Nature Reviews Cancer,
vol. 18, no. 8. Nature Publishing Group, pp. 500-510,
01-Aug-2018.
S. M. Lauritsen e¢ a/., “Explainable artificial
intelligence model to predict acute critical illness from
electronic health records,” Dec. 2019.
B. Dykes, “Data Storytelling: The Essential Data
Science Skill Everyone Needs,” Forbes, vol. 234, p.
26, 2016.
M. F. Dahlstrom, “Using narratives and storytelling to
communicate science with nonexpert audiences,”
Proc. Natl. Acad. Sci. U.S. A., vol. 111, pp. 13614—
13620, 2014.
Wolfswinkel, J. & Furtmueller, E. & Wilderom, C.
(2013). Using Grounded Theory as a Method for
“Rigorously Reviewing Literature. European Journal
of Information Systems. 22”. 10.1057/ejis.2011.51.
C. J. Cai, S. Winter, D. Steiner, L. Wilcox, and M.
Terry, “‘Hello Ai’: Uncovering the onboarding needs
of medical practitioners for human—Al collaborative
decision-making,” Proceedings of the ACM on
Human-Computer Interaction, vol. 3, no. CSCW.
Association for Computing Machinery, pp. 1-24, 01-
Nov-2019.
Turban, E., & Watkins, P. R. (1986). Integrating
expert systems and decision support systems in Mis
Quarterly, 121-136.

Page 1293
Distributed Linguistic Representations in Decision Making: Taxonomy, Key
Elements and Applications, and Challenges in Data Science and Explainable
Artificial Intelligence
Yuzhu Wu?, Zhen Zhang’, Gang Kou*, Hengjie Zhang*, Xiangrui Chao‘,

Cong-Cong Li*, Yucheng Dong***, Francisco Herrera?"

4 School of Business Administration, Southwestern University of Finance and Economics, Chengdu 611130, China
‘Institute of Systems Engineering, Dalian University of Technology, Dalian 116024, China
° Business School, Hohai University, Nanjing 211100, China
4 Center for Network Big Data and Decision-Making, Business School, Sichuan University, Chengdu 610065,
China
* School of Economics and Management, Southwest Jiaotong University, Chengdu 610031, China
School of Information Management and Statistics, Hubei University of Economics, Wuhan 430205, China
® Andalusian Research Institute on Data Science and Computational Intelligence (DaSCI), University of Granada,
Granada 18071, Spain
4 Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia

 

Abstract: Distributed linguistic representations are powerful tools for modelling the
uncertainty and complexity of preference information in linguistic decision making.
To provide a comprehensive perspective on the development of distributed linguistic
representations in decision making, we present the taxonomy of existing distributed
linguistic representations. Then, we review the key elements and applications of
distributed linguistic information processing in decision making, including the
distance measurement, aggregation methods, distributed linguistic preference
relations, and distributed linguistic multiple attribute decision making models. Next,
we provide a discussion on ongoing challenges and future research directions from the
perspective of data science and explainable artificial intelligence.

Keywords: Linguistic decision making; distributed linguistic representation;
preference relation; multiple attribute decision making; computing with words

1. Introduction

Computing with words (CW) models [29], [56], [80], [94], [101] play a key role in
dealing with information linguistically. The classical CW models emphasize on
handling linguistic information based on the use of membership functions [101]-[104].
In CW, a mainstream is the linguistic symbolic computational models [14], [94]-[96],
among which the 2-tuple linguistic representation model [31] is widely used to handle
balanced linguistic information in the form of linguistic 2-tuples. Thereafter, different

 

* Corresponding author.

Email addresses: wuyuzhu@swufe.edu.cn (Y. Wu), zhen.zhang@dlut.edu.cn (Z. Zhang), kougang@swufe.edu.cn
(G. Kou), hengjiezhang@hhu.edu.cn (H. Zhang), chaoxr@scu.edu.cn (X. Chao), congcongli@swjtu.edu.cn (C. Li),
yedong@scu.edu.cn (Y. Dong), herrera@decsai.ugr.es (F. Herrera).

1
methods are proposed to process unbalanced linguistic information, including the
proportional 2-tuple linguistic model [82], unbalanced linguistic term set [30],
numerical scale [18], and nonuniform ordered qualitative scale [23]. Considering
individuality of decision makers in expressing and understanding words, personalized
individual semantics (PIS) based models [16], [38]-[41] are introduced following the
stream of linguistic symbolic computational models. Another mainstream are the
type-2 fuzzy sets, because words mean different things to different people [60], the
type-2 fuzzy sets are used to deal with this issue [56]-[59].

In the last decade has been proposed two different concepts for representing
cognitive complex information, in 2012 the hesitant fuzzy linguistic term set (HFLTS)
[70] and in 2014 the linguistic distribution (LD) [107], becoming two approaches in
modelling hesitant and uncertain linguistic information in decision making. (i)
HFLTSs are well qualified to represent decision makers’ hesitant preferences by using
comparative linguistic expressions. (ii) LDs provide certain symbolic proportion
information over linguistic terms to describe distributed preferences of decision
makers as distributed assessments. In [66], Pang et al. introduced the probabilistic
linguistic term set (PLTS), with a different name for the similar concept, and the use
of LD.

Compared with simple linguistic 2-tuples, complex expressions like HFLTS and
LD have improved the flexibility of expression and have shown to be very useful to
deal with complex decision making problems. In practical decision contexts with
complexity, decision makers are often uncertain and hesitant to make decisions due to
tight time pressure and lack of knowledge, and their linguistic preference information
may be presented in the form of distributed linguistic representations [17], [107].
These phenomena have raised the rapidly growing demand of modelling and
processing distributed preferences with efficiency, and advanced distributed linguistic
representations to become increasingly popular in decision making.

The aim of this paper is to present a review of the current hot topics of distributed
linguistic representations in decision making, including the taxonomy of distributed
linguistic representations, the key elements in distributed linguistic decision making,
and some challenges and future research directions from the perspective of data
science and explainable artificial intelligence (XAD).

The remainder of this paper is organized as follows. Section 2 introduces the origin,
basic concepts and taxonomy of existing distributed linguistic representations. Then
Section 3 presents the key elements and applications of distributed linguistic
representations in decision making. Next, the challenges and future research
directions are discussed in Section 4. Finally, the main conclusions are drawn in
Section 5.
2. Distributed linguistic representations: origin, basic concepts, and taxonomy

In this section, we present the origin, basic concepts, and taxonomy of distributed
linguistic representations.

2.1 Origin of distributed linguistic representation: linguistic 2-tuples and HFLTS

In the development of linguistic decision making, various distributed linguistic
representations have been proposed to model decision makers’ linguistic preferences.
These distributed linguistic representations are basically derived from the following
two types of basic linguistic expression formats:

(1) Linguistic 2-tuples, which show their convenience in preferences construction
via single or two successive linguistic terms;

(2) HFLTS, which is well qualified to elicit hesitant preferences by using several
consecutive linguistic terms.

This section reviews the basic concepts of linguistic 2-tuples and HFLTS.

Definition 1 (2-tuple linguistic representation model) [31], [55]: Let

L= Hoots afe} be an established linguistic term set with odd cardinality satisfying the
required characteristics: (i) The set is ordered: 4,>/, if i>; Gi) A negation operator:
Negti,)=1, such that j=g—i, and g+1 is the cardinality of L. Let fe[0,g] be a
numerical value representing the result of a symbolic aggregation operation. Then, a
linguistic 2-tuple (/,a@), where ¢L and qae[-0.5,0.5) that expresses the
equivalent information to f is obtained by means of a one to one mapping:
A:[0,g] > Lx[-0.5,0.5), and

M(B) = U;,0) (1)

: L,i=round(P)
with 17 = B-i,e[-0.5,0.5)

where round is the usual round operation.

Let Z={(,,0)|1,¢L,a€[-0.5,0.5)} be the set of all the linguistic 2-tuples
associated to Z. The inverse function of A is denoted by: A‘:Z—>[0,g] with
A’(L,a)=i+a. If a =0, the linguistic 2-tuple (4,@) is a simple term /,. And the

negation operator of a linguistic 2-tuple (1.@) is defined by

Neg(1,,at) =A( g-(A"G,,2)))

Linguistic 2-tuple and its computational model [31] have shown good advantages
in constructing simple linguistic preferences of decision makers in linguistic decision
making. An overview on the 2-tuple linguistic representation model for CW in
decision making can be found in [55].

Wang and Hao [82] proposed the proportional linguistic 2-tuples, which add
symbolic proportion information to two successive linguistic terms.

3
Definition 2 (Proportional 2-tuple linguistic representation model) [82]: Let
L ={hob.sl,} be an established linguistic term set, 7=[0,1] and IL=IxL={(y,,
L)|y¥, €[0,1],4 ¢L}. Given a pair (/,,/,.,) of two successive terms of L, any two
elements (y,,/,) and (y,,,,4,,) of JZ are called a symbolic proportion pair, and
(%>¥%;4,) are called a pair of symbolic proportions of the pair (/,,/,.,) if y,+7,, =1.
A symbolic proportion pair (y,,/,) 1—y,,/,,) can be denoted as (y/,,d—7,)4,..)-
and the proportional 2-tuple set generated by L is denoted by

L={(7l.d-7 ha) 7, €[01L |, €L}}. (2)

The elements of Z are called linguistic proportional 2-tuples.
The negation operator for proportional linguistic 2-tuples is defined by

Neg ((7%1,,(1 —¥ Mia )) = (d wh Ve-iaFiles) :
To improve the flexibility of linguistic expressions, HFLTS [70] was proposed,
which acts as a tool in fulfilling decision makers’ hesitant necessities and

requirements by applying comparative linguistic expressions (several consecutive
linguistic terms). The concept of HFLTS is introduced as Definition 3.

Definition 3 (HFLTS) [70]: Let L={/,,/,...,/,} be an established linguistic term

set. An HFLTS, A, , is an ordered finite subset of consecutive linguistic terms of Z.

The expression of f, is given by

A, ={Ishavsl, (OSIS J Sg}. (3)

The negation operation for h, is defined by Neg(h,)= Ves [2 eh, fe {0,1..., gh} .

An approach to generate HFLTS based preferences via comparative linguistic
expressions by using a context-free grammar was presented in [70]. The recent
developments for HFLTS in decision making can refer to [69].

The distributed linguistic representations in this review have two origins:

(1) Extension of the proportional linguistic 2-tuple based representation, referring
to Zhang et al. [107], Guo et al. [25], [26];

(2) HFLTS with distributed preference information, referring to Wu and Xu [90],
Chen et al. [9], Pang et al. [66], Zhang et al. [109], etc.

In particular, distributed linguistic representations can be formed in two ways:
linguistic preference expressions with distributed information in an individual context
({17], [107], etc.), and information fusion of linguistic terms or HFLTSs in a group
context ([46], [100], [118], [119], etc.), which can be described in Fig. 1.
 
  
 
 

  
    

Distributed information over
linguistic terms or HFLTSs in
individual contexts

Linguistic terms, Distributed linguistic
HFLTS representations

Information fusion of linguistic
terms or HFLTSs in group contexts

 

 

 

 

    
 

 

 

  
   
 

Fig.1 Formation of distributed linguistic representations in decision making

The distributed linguistic representations are reviewed in the following sections
in detail.
2.2 Linguistic distribution
Compared with HFLTS, LD provides some symbolic proportion information over
linguistic terms. The basic concept of LD is presented as Definition 4.

Definition 4 (LD) [107]: Let Z= aohseate} be an established linguistic term set.

AnLD over L is defined by
D, ={(1,,eG,))|i=0,1,...2}. (4)

where p(/,)20 is the symbolic proportion of /,, and ye p@)=1.

InanLD D,, 4 represents a linguistic term used by decision makers, and p(/,)

i

represents the associated symbolic proportion information of /, as a probabilistic
distribution associated to the linguistic terms (really it is a complete probabilistic
distribution).

The expectation of D, is defined by: EWD,)=A(D8,(A"G)x 0G))). and the

negation operator of D, was proposed [107]: Neg(D,)= {(4. PU.) I(4,00,)) eD,\

2.3 Incomplete versions of linguistic distributions

Distribution preference information provided by decision makers is not always
complete, and in the following we review the basic concepts of distributed linguistic
representations with incompleteness: proportional linguistic distribution (PLD), PLTS,
and incomplete linguistic distribution (ILD).

(1) PLD

Based on the proportional 2-tuple linguistic representation model, Guo et al. [26]
discussed the LD with partial symbolic information by introducing the concept of
PLD through a different mathematical representation.

Definition 5 (PLD) [26]: Let Z ={hobol,} be an established ordinal linguistic
term set, and /=[0,1] , MZ=IxL={(y,))|y, €[0.1].4 <L} be the set of

5
proportional 2-tuples of Z. Given a sequence of r+1 successive ordinal terms of Z, any
r+] elements (7,1). arly) o> “are4a,) Of i are called a symbolic

proportion sequence. The PLD is defined by
DP = {Hh Malas -Yardars®) [4 € 13} (5)
where 7j..%,,€0.1] , 0< 0" 7,<1, and O<i, itrsg . e=1->0"y,
represents the extent of incomplete information.
InaPLD D?, |, represents the preference judgment provided by decision makers,

and y, is the proportional coefficient of J,, which represents the decision maker’s

itr

confidence level that he/she believes a linguistic term fits an evaluation. If a!

itr

D? is called a complete PLD; if D. Vv, <1, DP is called an incomplete PLD.

The set of all the symbolic proportion sequences is called proportional linguistic
distribution set, denoted by 7: F ={(%h%oabea> MNiardbirpe€)(OSLI+K < gl, eL}.

In [25], Guo et al. discussed the PLD with interval symbolic proportions.

(2) PLTS

Pang et al. [66] proposed the PLTS to solve decision makers’ preferences with
hesitancy among several possible linguistic terms. Moreover, the incomplete
probabilistic distribution information of certain linguistic terms is considered.

Definition 6 (PLTS) [66]: Let Z={/,,4.../,} be an established ordinal linguistic

term set. A PLTS is defined by
L(p)={h (pI €L, p, 20,1 =1....nL(p)} (6)

where ee P, <1, and n(L(p)) denotes the number of element /, in L(p).

0

In a PLTS Lip), /, represents decision maker’s preference and p, is the

i

probability of 7. If 5°""’” p,=1, the probability distribution information of L(p)

Ey)

is complete; if an p, <1, partial ignorance exists because of decision makers’

knowledge limitation.
A computational model for handling incompleteness of L(p) is presented in [66].
Remark 1. A PLTS is transformed into an LD by using the normalization method
proposed in [66]. However, such transformation is questionable [40], which can be

demonstrated by using the following example. Let 1 ={/,,4,4,.4,,/,} be a linguistic
term set and L(p)={/,(0.3),/,(0.3)} be a PLTS. By using the normalization method
proposed in [66], an LD D, ={(4,0.5),(4,,0.5)$ can be obtained. Obviously,
L(p) = {1,(0.3),,0.3)} and D, ={(,,0.5),(4,0.5)} represent different preference
information. In fact, for two PLTSs L(p)={("(p’)|f’ EL, p? =0,i=1,2...,.0AL(pyy}
and E'(p)={U (py 2 EL, p; 20,7=1,2...nL py}. if nL’ (p)y=nL(p)), 7 =1, and
pi =Op, G=1,2,...nL(p)), then L’(p) and L’(p) will be transformed into the

same LD by using the normalization method proposed in [66], which is unreasonable.
(3) ILD
Zhang et al. [105] proposed the ILD, as shown in Definition 7.

Definition 7 (ILD) [85], [105]: Let L={hh,1,} be an established ordinal

linguistic term set. An ILD of LZ is given by
D, ={(4,.eG)).7|i=0.1....g} (7)
where eL, ))* ptl)+n=1, and pti), [0.1].
An ILD can be used to express a decision maker’s preferences with incompleteness.
Inan ILD D/, J, represents the linguistic term used by decision makers, and p(/,)

i

represents the relevant symbolic proportion information of /,. The variable 7
represents the extent of incompleteness and uncertainty in the preference D}. If
n=0, D; is complete and equivalent to the LD in Definition 4. Otherwise, the larger
n indicates the greater uncertainty in D/.

2.4 Flexible linguistic expression

HFLTSs and LDs have provided convenience in the construction of complex
linguistic expressions, but as a general format of linguistic preference expressions,
flexible linguistic expression (FLE) shows its advantages in linguistic decision
making, which is formally defined as follows.

Definition 8 (FLE) [86], [87]: Let L={hyh.-1,} be an established linguistic
term set, and S, be a set whose elements s, are the subsets of L. The decision
maker expresses his/her preferences by presenting distribution information of s,.
Then the decision maker’s preference is an FLE, denoted as /,, and it is formally
defined by

Sr ={(s,AS,)) 1s, € 5, } (8)
where p(s,)€[0,1].

Let the set of all the subsets of Z be S. The set S, is not fixed and S, cS. Inan
FLE f,. 5s, represents the preference expressed by decision makers, and p(s, )
represents the associated symbolic proportion information of s,. If s,¢S and
s,éS,, it means that s, is not used by the decision maker to represent his/her

preference. The negation operator of jf, is defined by [87]:
Neg(f,)={(Neg(s,).p(s,)) ls, €S,}, where Neg(s,)={I,_,|1,€5,}.

2.5 Other variants of LD

Moreover, there exist several representative distributed linguistic representations,
including LD with interval symbolic proportions (INLD) [17], possibility distribution
for HFLTS (PDHFLTS) [90], and proportional HFLTS (PHFLTS) [9], and hesitant
linguistic distribution (HLD) [109]. These variants are introduced below.

Dong et al. [17] proposed the version of LD with interval symbolic proportions.

Definition 9 (INLD) [17], [89]: Let L={hhsle} be an established linguistic
term set. An INLD is given by

D, ={(.pU))|, €L,i=0.1.....g} (9)

where (i) is the interval symbolic proportion of J _ satisfying

i

p= @).e" EC [0.1]. pe’) and p’(/,) are the lower bound and upper bound
of p,) respectively.

Inan INLD D , pt) represents the associated interval proportion information of

linguistic term /

i

, and the interval length of p(/,) reflects the confidence level of

decision maker when providing the preference |,. If p'(l)=p'(l), D, is
mathematically consistent with the LD in Definition 4.

Wu and Xu [90] proposed the PDHFLTS in group decision making (GDM) context,
in which decision makers provide their preferences via single terms and possibility
information is distributed over terms in an HFLTS.

Definition 10 (PDHFLTS) [90]: Let Z={/,.4...../,} be an established linguistic
term set, and A, be an HFLTS of LZ given by a decision maker. The PDHFLTS is
represented by

Dy, ={(L.P, 1h eh} (10)

1/n(h,) Leh,

where P=(p).p;,---P,) is the possibility distribution of fh, , r= {p otherwise

denotes the possibility assigned over the linguistic term /,, r p,;=1, and n(h,) is
the number of elements in A, .

Ina PDHFLTS Dy, , preference from a decision maker is represented by an HFLTS,
and each term /, in this HFLTS has the same possibility p, to become the

assessment value of an alternative.
Chen et al. [9] proposed the PHFLTS to solve HFLTS based preferences provided
by decision makers in GDM problems.
Definition 11 (PHFLTS) [9]: Let Z ={I,.h.-/,} be an established linguistic term

set, and Af (k=1,..,n) be n HFLTSs given by » decision makers. A PHFLTS based
on the union of Af is a set of ordered finite proportional pairs, represented by

Py, ={0,.P)\h €L} (11)
where (/,,p,) is called a proportional linguistic pair, P=(p,,p,...p,) iS a
proportional vector and p,¢[0,1] represents the possibility degree of J, provided by
n decision makers, and ye p=).

Chen et al. [9] discussed the case of PHFLTS with incomplete proportion
information, and mentioned that PHFLTS is mathematically consistent with LD in
Definition 4. Some associated computational approaches can be found in [10]-[12].

Zhang et al. [109] proposed the HLD, in which preferences of decision makers are
HFLTSs.

Definition 12 (HLD) [109]: Let Z ={h.h.-/,} be an established linguistic term
set, and #, be an HFLTS of LZ given by a decision maker. Let the set of all the
HFLTS of L be H,. The HLD is defined by

Dy, ={h,. Ph, h, €H,} (12)
where p(h,)e[0,1]U {null} and p(h,) is the symbolic proportion of A, if

Plh, ) # {null .
Inan HLD D,, , preference from a decision maker is a set of HFLTSs /, with

certain possibility p(h,), and the element #, is not used if p(h,)={null} . If

yn tt, pth, =1, D,, is complete; if > pel, pi) <1, D,, 8 with incompleteness.
Zhang et al. [109] proposed a normalization method for the case of > ell, pi,)>.

2.6 Taxonomy of distributed linguistic representations

We present the taxonomy of the existing distributed linguistic representations.
Based on the concepts of LD and its variants, their comparison results are listed in
Table 1.

Table 1. The analysis of distributed linguistic representations

 

 

Linguistic Mathematical Symbolic Distribution over Incomplete
. proportion linguistic information
expression format . . . . :
information information considered
LD [107] {(,. eG 14 eL} iw P=] Terms in L No
INLD [17] {(,.e@)) 12, eL} Le’). P" Terms in L No

 

 

 

 
 

 

PDHFLTS Terms in an
Le lleh, Dien =I Ne

[90] Wea dien) HELTS °
PHELTS [9] {Up )IL eZ} Dee Bal Terms in L Yes
PLTS [66] hp lt Ly SO ps1 Terms in L Yes
PLD [26] | (do Yierbnr@ 14 € LH} Sensi Terms in L Yes
HLD [109] {Ap} pth, )€[0.1] HFLTS of L Yes
ILD [105] {(.e@)).lf <2} Dye PEI =1 Terms in Z Yes
FLE [86] {(s,,e¢s.))} ts,)€[0.1] Subset of L Yes

 

 

 

From the above classifications, we can figure out the following characteristics:

(1) FLE is the generalization of almost all distributed linguistic representations
reviewed.

(2) LD, PDHFLTS, PHFLTS, PLTS, PLD, and ILD, are special HLDs.

(3) PLTS is mathematically consistent with ILD; and PLD shares high similarity
with PLTS and ILD, but in PLD symbolic proportions are distributed over successive
linguistic terms. Because this difference is minor, in this review we think that PLD is
equivalent to PLTS and ILD (approximately).

(4) PHFLTS is mathematically consistent with LD.

(5) PDHFLTS is the special case of LD.

(6) INLD is a generalization of LD.

The relationships among different distributed linguistic representations can be
described as Fig. 2.

PHFLTS

 

Fig.2 The taxonomy of distributed linguistic representations

Remark 2. To our knowledge, LD is the first attempt in the linguistic decision
making community to model the distributed linguistic representations. Although the
follow-up notions and concepts are mathematically consistent with LD in some sense,
they are designed with different purposes:

(1) The notions of PDHFLTS, PHFLTS, PLTS, HLD and FLE are designed for
10
extending HFLTS, while PLD, ILD and INLD are designed for extending proportional
linguistic 2-tuples.

(2) PDHFLTS is designed to the modelling of individual linguistic expression, and
PHFLTS is established for group linguistic representations as it involves the fusion of
individuals’ linguistic preference information.

3. Key elements and applications of distributed linguistic representations in
decision making

In this section, we show the key elements and applications of distributed linguistic
representations in decision making. Specifically, distance measurements and
aggregation methods under various distributed linguistic representations are
summarized in Section 3.1 and 3.2 respectively. Based on the use of distributed
linguistic representations, several distributed linguistic preference relations are
developed, which are introduced in Section 3.3. Following this, multiple attribute
decision making (MADM) models with distributed linguistic representations are
presented in Section 3.4. Finally, some real-life applications are reviewed in Section
3.5.

3.1. Distributed linguistic distance measurements

In this section, we review several widely used distance measurements in distributed
linguistic representations. To start with, in Table 2 we provide a summary of these
distance measurements (refer to column 4), which are the bases to support decision
making.

Table 2: A summary of distributed linguistic distance measurements and aggregation

 

 

 

 

Distributed Refe
8 " ue ee Features of Distance Aggregation Aggregation
linguistic rence... oe
. linguistic terms measurement method result
representations s
Symbolic
[107] Unified, proportions based LDWA LD
balanced extended Manhattan LDOWA
distance
LD Multi 1 Expectation based
ulti-granular.
[119] e > extended Manhattan LDWA LD
balanced .
distance
Unified, Maximum support
[88] Not discussed degree model with HFLTS
balanced
accuracy
Multi-granular. . INLDWA
INLD 17 ° Not d d INLD
[17] unbalanced Or emsenesse INLDOWA
Extended C
onsensus-
Unified Manhattan dist:
ILD [105] mee’ eee ON oriented ILD
balanced considering

symbolic

aggregation model

 

11
 

proportions and
uncertainty degree

 

 

 

 

PLISWA
Unified Extended Euclid
[66] mee’ enced SHCNCEAN PLTS weighted PLTS
balanced distance .
geometric operator
Weighted distance, PLISWA
. Weighted Hausdorff
Unified, . based on
[28] distance, . PLTS
PLTS unbalanced . . Archimedean
Hybrid weighted '
Hamming distance cope
Extended
[49] Unified, Manhattan distance PLTSWA PLTS
balanced considering PLTSOWA
probability
Unified HLDWA
HLD 109 ° Not di d HLD
[109] balanced OF USCS HLDOWA
Expectation based
PDHELTS [90] Unitied, t de 4 Ma h t PDHELWA PDHELTS
balanced 9 PDHEFLOWA
distance
A ti
Symbolic serge on
. . model with
FLE [86] Unified, proportions based d LD
balanced extended Manhattan aeeneny un
. minimum
distance

preference-loss

 

Next, the main distance measurements of LD, PDHFLTS, PLTS, ILD, and FLE are
reviewed.

(1) Distance measurements of LD and PDHFLTS

Let DI ={,p'h)|i=OL....g} and Di ={(L,p°(L)|i=0.1L...g} be two LDs.
Several methods for distance measurements in LDs were reported (see [33], [97],
[107], [119]).

The distance between Df and D; proposed by Zhang et al. [107] is provided as

follows:
r Ss 1 r Ss
AU(Di-Di)=F Dil @)-pG)I. (13)

Zhang et al. [119] pointed out that the above distance measurement defined by Eq.
(13) just calculates the deviation between symbolic proportions and ignores the
linguistic terms. To overcome this drawback, Zhang et al. [119] proposed another
distance measurement as follows:

d(D,.Di)=+1D% (0) 2°) NSU)
§ (14)
where NS(.) is the numerical scale [18], and NS(/,) is set to be /.

Ju et al. [33] proposed the following way to calculate the distance between LDs

12
Dr and Dr:

 

d(D; ,D;)= (XU) NSU)- P*(L,) NS)

Similar to Eq. (14), NSC) in Eq. (15) is set to be i.

(15)

Yao [97] used the following way to compute the distance between LDs D; and
Dp:

d(Dh.D)==S EIT e'I-S I
& (16)

Wu and Xu [90] discussed the distance measurement of PDHFLTSs. Let
Di’ ={d.p)|eL} and Di ={(,p*)|,<L} be two PDHFLTSs. The distance

between D’" and D/:° is defined by:

d(Di* Df) = : [De pl NSU)-Y, pr NS)| (17)
where NS(i,)=i, and Eq. (17) is equivalent to Eq. (14).
(2) Distance measurements of PLTS and ILD
Let L(p)={U (pf €L. pl 20i=1...nL(py} and L(p)={0 (pF €L.
p. >0,i=1,....n((pyy} be two PLTSs, and n(Z’(p))=n(Z'(p)). Pang et al. [66]
proposed the deviation degree between L'(p) and L’(p):

 

nL" (py)

dlp). =" (py sup (7 )— pi sus (EY /n(L(p)) (18)

where su’ and su° are the subscripts of linguistic terms /’ and /° respectively.
Zhang et al. [105] analyzed the distance measurement of ILDs. Let
Di” ={(L,e'(L)).17” |f=01,...g} and Di ={(h,p°()).7 |i=0,1,....2} be two ILDs.

Then, the distance measurement between ILDs D/?” and D/* is given by:

d(D" Di") = (EU) pl) -p'G)|+6 |" -9° ) (19)

where 5€(0,]] is a predefined distance parameter, and represents the importance

degree of incomplete information in the distance measurement.
(3) Distance measurements of FLE
Wu et al. [86] proposed the distance measurement for FLEs. Let

f, ={(8;,(s,))|8, €S,} be an FLE, and D;, ={(1,,e(,))|7=0.1,....g} be a PDHFLTS
of L. The distance between f, and Db; is defined by:

UM fisPh = Dog es, | Dies, PU) SL) | (20)
where (s,)¢[0,1] is the symbolic proportion of the subset s, in /f, , and

> hes, A) is the sum of symbolic proportions of all the simple terms in s, in Dj, .

13
3.2. Aggregation approaches of distributed linguistic representations

In this section, we review the commonly used aggregation methods for distributed
linguistic representations, which are summarized in Table 2 (columns 5 and 6).
(1) Aggregation approaches in LD, INLD
Let {D},D7,..,.D"} be aset of LDs of ZL, where Di ={(,,p'(,))|i=90,1.....2}, and
A=(AsAys-4,) be an associated weighting vector that satisfies 2,>0 and
4: =1. Zhang et al. [107] proposed the weighted averaging operator for LDs by:
LDWA,(D,,D7.....DP)= (hp ,)) |i =0.L....2} (21)
where p°()= 0" Ap") 7 = 0,L....2.
The ordered weighted averaging operator of LDs is computed by [107]:
LDOWA,(D) De... DY) =. pL) |i =0,1,-..2} (22)
where p°(/,)= vA pd) and {o(1),o(2),...00m} is a permutation of
{1,2,....m} such that De’? >pDe for k=2,3,...,.m.
Liang et al. [44] defined other operators for the aggregation of LDs. Let
{D),D7,..,D" be as above. The LD power average (LDPA) operator is defined by:

LDPA(D),,D?,...,DP) =40,, p° 4) |i =0,1,...,23 (23)
where
: mn 1+7(D}
p=" pe (24)
,.+7(D, ))
and T(Di)=)>)",, Sup(D}.Dr). Sup(D}.D;)=1-d(D},D7) is the support degree of

the elements Df and Dy.
Based on the LDPA operator, the LD weighted power average (LDWPA) operator
is defined by [44]:
LDWPA,(D),D7,...,D?") ={h.P°@))|i=90.1,....9} (25)
where
_AC+TD))

(26)
1% (+7 (D;))

pr) = yew)

Dong et al. [17] proposed the aggregation methods for INLDs. Let {D!D?,...,D™}

be a set of INLDs of L, where Dé ={(,p*(,))|i=0,1,...,g}. The weighted average
operator of INLDs is computed by:

INLDWA,(D}.D}....Di") = (0, Lp"). PUD |i =0.1,. 83 (27)

where pil=y 4 “pr(l). P= A -p'(l,). A, is the weighting vector of

14
Dé satisfying ve =1.
The ordered weighted average operator of INLDs is computed by [17]:

INLDOWA,(D}.,D}.....D?")= (Lp PED |i = 0d... 8} (28)

where p'(h)= Si A PP). PWD= Ae PU). {o(),0(2),...00m)} is a

~Ok-ly ok)

permutation of {1, 2,..,m} suchthat Di 2D: for k=2,3,....m.

Based on the use of LDs and HFLTSs, Wu et al. [88] proposed the maximum
support degree model (MSDM), aiming at maximizing the support degree as well as
guarantying the accuracy of group opinion, which is presented below:

max ¢(h; )
ph; ) = here ol,)
51.60) = AP)
n(h{)<T
h,¢H, (29)
where ¢(h;) is the support degree of the collective opinion Ay, ¢(/,) is the support
degree of the linguistic term 4 in D}. The constraint n(h’)<7 guarantees that the
number of linguistic terms in ff is less than a preset value 7, and constraint
hi eH, guarantees that hf is an HFLTS.

(2) Aggregation approaches in PLD, PLTS, and ILD
Let

1 1 1
Wb Vistar Vian den > & ld, eL),
R= see

2

In m In
Web Ke ab one Ve ntinan.> & |Z, € LD)

be a set of PLDs, and D?* ={(%4 Va ath ro 9 Vhs di on Fe), el} (k =1,...,.m). Then,

the weighted average of PLDs in ®, which is also a PLD denoted by
OL Maha Mads, & [h.--4,, €L), is defined by Guo et al. [26]:

{ft =" AY Je +L.wit

é,= ve ME, (30)
where i is the minimum index of starting labels and i+r is the maximum index of
ending labels of PLDs in % __ respectively, ic, i=minfi,..,i,} and
itr =max {i +h,..5_, tht -

In addition, the weighted average operator for PLDs defined above can also be
extended for the case where the weights are expressed by means of uncertain
linguistic weights instead of numerical values [26].

Different aggregation methods have been proposed for dealing with PLTSs. Pang et

15
al. [66] and Liu et al. [49] defined the PLTS weighted averaging (PLTSWA) operator
respectively, and Liu et al. [49] also proposed ordered weighted averaging operator of
PLTSs (PLTSOWA). In addition, Han et al. [28] presented the concepts of
Archimedean copula weighted probabilistic unbalanced linguistic arithmetic average
aggregation operator and Archimedean copula weighted probabilistic unbalanced
linguistic geometric average aggregation operator.

Let {D!',D!’,..,Dr"} be a set of ILDs, where Di‘ ={(,e°(L)),7* |i=0,1.....g}.
Let DP ={,,0°C),7' |i =0,1,...,.2} be the collective ILD, Zhang et al. [105]
proposed the following model to obtain D/*:

. 1 m n Cc Cc
min pet Dagat Daveal Py LP; E145 L775 —75 D GB)
where p*(/,) is the symbolic proportion information of /, provided by individual i
for alternative g. 6 represents the importance degree of incomplete information in
the distance measurement of Di* and D?°.

Meanwhile, since D}’ is an ILD, we have that n PU)+7 =1, pd ye[01]
and 7° €[0,1]. In order to get relatively precise collective opinion, the incomplete
degree of D/* is set to be less than the average incompleteness of all decision
makers’ opinions, ic. 77° €[0.2°" 7°].

(3) Aggregation approaches in PDHFLTS, PHFLTS, HLD, and FLE

In [90], Wu and Xu defined the hesitant fuzzy linguistic weighted average (HFLWA)
operator and hesitant fuzzy linguistic ordered weighted average (HFLOWA) operator
for the aggregation of PDHFLTSs.

Chen et al. [9] defined the proportional HFLWA (PHFLWA) operator and
proportional HEFLOWA (PHFLOWA) operator for the aggregation of PHFLTSs. Let
{Py, Pq, >--»P7} be aset of PHFLTSs. The PHFLWA operator is defined by [9]:

PHFLWA(Py, .Py, Pi) =TG" (A, Py, |k =1,2,...5m)

<1 (AP, \@TG""(A Ph, |= 2...4m)) 6?)
The PHFLOWA operator is defined as follows [9]:
PHFLOWA(Py, .P3, 5...Pit.) =TG" (Ay Po | k =1,2,...5) (33)

=T((A,,P1,) @TG"" (A, Pe | k =2,....m))

where {o(1),o(2),...,0(m)} is a permutation of {1, 2, .... m} such that Py > PZ.

T is a triangular norm [3], [71], and G(-) is a proportional convex combination
function [9].

Zhang et al. [109] proposed the weighted average operator and ordered weighted

average operator for the aggregation of HLDs. Let D,, ={(#,.p'(a,))|h, €H,} and

16
Di, ={Gz.P' (hy) hp €H,} be two HLDs. Let 4 and 4, be the corresponding
weights, where 4,,4,20 and 4,+4,=1. Then the weighted union of D,, and D;,
is defined by [109]:

U (Dy, Piz.) =U(AD yr, AD.) (34)
where U(ADy, A,Di,,) = (hPa) | hy, © H,}, with

(h, Ap (h,)+ Ap (h;)), if ph, ), phy) # null, hy, = h;
(i pity) = lt 0 Uy UE Aap? h3)), Hf pC). p* (ip) # mal, by #
POND (hs if p'(h,) # nul, p*(he)= null
(h, Ap (h;)), if p(y) # null, p'(h,) = null . (5)

Let {Dy Di,.--.Diz,} be a set of HLDs, where Dy, ={(hp,p'(h;))|Ap ¢H,}
(k =1,2,...,m). The weighted average operator for {Dy Dj Di} is defined by
[109]:

HLDWA, (Dy, Di, yD y= U(ADy, U(AD;, pe Ay Diy )). (36)

The ordered weighted average operator of {Dy Di, w2Dy, } 18 defined by [109]:

HLDOWA, (Dy, Diz, Dit, ) =U ADA? U ADE? 54g De”)) (37)
where (o(1),o(2),...0(m)) is a permutation of {1,2....,.m} suchthat D7“) >D;.

Wt et al. [86] proposed an optimization approach for the aggregation of FLEs. Let
Si =4(st ,p(st))| st eS#} be the FLE preference of individual k (4 =1,2,...,.m), and
DP* be the collective PDHFLTS. Af is the HFLTS associated with D?* [86]:

min)" Ad(f; Dr)

Pr —PD,
n(hi)<T (38)

where 4, is the weighting vector of individual &, and »(hf) is the number of
elements in hy.
3.3. Distributed linguistic preference relation

Based on the use of distributed linguistic representations in the pairwise
comparison method, several distributed linguistic preference relations including
linguistic distribution preference relation (LDPR), probabilistic linguistic preference
relation (PLPR), and flexible linguistic preference relation (FLPR) have been reported,
which are summarized in Table 3.

Table 3: A summary of distributed linguistic preference relations

 

 

Distributed Distributed
oo. ae Complete vs
linguistic preference linguistic References
. . Incomplete
relations representations
LDPR LD [107] Complete

 

17
 

[74] Incomplete

 

 

[92] Complete
[75] Complete
[76] Complete
[73] Complete
[49] Complete
[115] Complete
PLPR PLTS
[21] Incomplete
[52] Complete
[116] Complete
FLPR FLE [87] Complete

 

The details of these distributed linguistic distribution preference relations are
introduced in the rest of this section.

For the ease of illustration, all distributed linguistic preference relations involved in
this review are denoted as 4 =(a,)

mon *

Definition 13 (LDPR) [107]: An LDPR on the set of alternatives Y ={x,,x,...x,}

is represented by a matrix A=(a,),,,,, where a, ={(,.,(/))|i=0.1.....g} 18 an LD of

nxn?

L, and represents the preference degree of alternative x, over x,. A=(a,),,,, 1S

aH

reciprocal if Neg(a,)=a,,for r,j=1,2,...,n.

Particularly, if }0",9,(,)=1 forall r,j¢{1,2,....n}, then 4 is called LDPR with
complete symbolic proportions; otherwise, 4 is called LDPR with incomplete
symbolic proportions [74].

In many cases, words mean different things to different decision makers. In this
case, the LD preference relations are called PIS-based LD preference relations [75],
[76], [92].

Definition 14 (PLPR) [115], [116]: A PLPR on the set of alternatives

X={x.x),..x,} IS represented by a matrix A-=(a,) where

ay =the Pig) €L Pig 20,1 =1,..,.n(a,)} 18 a PLTS, and represents the preference
degree of alternative x, over x,, r,j¢{l,2,....n}.

Moreover, incomplete PLPR is developed in [21].
Definition 15 (FLPR) [87]: An FLPR on the set of alternatives Y ={x,,x,...x,}

is represented by a matrix 4A=(a,)

nxn >

where a, ={(s,,,(s,))|s, €S,,,} 18 an FLE,
Neg(a,)=a,, and a, represents the preference degree of alternative x, over x,.
S,,, 18 the set whose elements s, are the subsets of 1, and the decision maker uses

the elements of S,,, to express his/her preference of alternative x, over x,.

18
The transformations between different distributed linguistic preference relations are
investigated by several researchers [87], [112], [114].

3.4. Distributed linguistic MADM

The distributed linguistic representations have been utilized in MADM to model
the uncertain assessments of decision makers, and several distributed linguistic
MADM approaches are developed accordingly, which are summarized in Table 4.

Table 4: A summary of distributed linguistic MADM

 

 

 

 

 

 

 

 

 

 

Distributed References Decision context Features of linguistic terms
linguistic MADM
[97] Group Unified, balanced
[84] Group Unified, balanced
[88] Group Unified, balanced
MADM with LD [119] Large-scale group Multi-granular, balanced
[120] Large-scale group Multi-granular, unbalanced
[46] Group Multi-granular, balanced
[99] Large-scale group Multi-granular, unbalanced
MADM with INLD [17] Group Multi-granular, unbalanced
MADM with [90] Group Unitied, balanced
PDHFLTS [91] Group Unified, balanced
[8] Group PIS
rhe th [9] Group Unified, balanced
MADM with PLD [26] Individual Balanced
[28] Group Unified, unbalanced
[51] Group Unified, balanced
[50] Group Unified, balanced
MADM with PLTS [66] Group Unified, balanced
[77] Individual Balanced
[83] Group Unified, balanced
[81] Group Multi-granular, balanced
. [105] Group Unified, balanced
MADM with ILD .
[113] Group Unified, balanced
MADM with HLD [109] Group Unified, balanced
MADM with FLE [86] Group Unified, balanced

 

(1) MADM approaches with LD and INLD

The LD has been applied in MADM to represent uncertain assessments of decision
makers. Yao [97] proposed a consensus reaching model for multiple attribute group
decision making (MAGDM) with assessments represented by means of LDs, in which
a feedback mechanism is devised by combining an identification rule and an
optimization-based model. Wu et al. [84] presented a minimum adjustment cost
feedback mechanism based consensus model for MAGDM under social network, in
which the assessments information and the trust relationships among decision makers

19
are both represented by LDs. Based on the use of LDs and HFLTSs, Wu et al. [88]
proposed the MSDM to address linguistic MAGDM problem, which aims at
maximizing the support degree of group opinion as well as guarantying the accuracy
of group opinion. Zhang et al. [119] designed an approach to manage multi-granular
LDs in large-scale MAGDM, in which a linguistic computational model is developed
based on the extended linguistic hierarchies model and the transformation formulas
between a linguistic 2-tuple and an LD. Recently, Zhang et al. [120] developed a
large-scale MAGDM model with multi-granular unbalanced hesitant fuzzy linguistic
information. In their model, all unbalanced hesitant fuzzy linguistic information is
transformed into LDs defined on a balanced linguistic term set. Yu et al. [99]
proposed a method to deal with large-scale GDM problems with multi-granular
unbalanced linguistic information, in which the initial multi-granular unbalanced
linguistic information of decision makers is represented by unbalanced LDs and the
classical TODIM method is extended to derive a raking of alternatives. Liang et al.
[46] developed a consensus-based analysis model for MAGDM with multi-granular
LD preferences.

Dong et al. [17] proposed an MAGDM with INLD under multi-granular
unbalanced linguistic contexts. First, a basic linguistic term set is selected to
normalize the individual unbalanced INLDs matrices. Then, the collective unbalanced
INLDs matrix is obtained by aggregating the normalized individual unbalanced
INLDs matrices. Following this, the ranking of alternatives is obtained from the
collective unbalanced INLDs matrix.

(2) MADM approaches with PLD, PLTS, and ILD

In real-world MADM, due to the limitation of knowledge, problem complexity and
time pressure, the assessment information given by decision makers may not be
complete. Various distributed linguistic MADM methods to handle incomplete
assessment information have been proposed.

Guo et al. [26] proposed a PLD based model for MADM under linguistic
uncertainty, which is based on the nature of symbolic linguistic model combined with
distributed assessments. Moreover, this model is also able to deal with ILDs so that it
allows evaluators to avoid the dilemma of having to supply complete assessments
when not available.

Han et al. [28] proposed a new computational model based on Archimedean copula
for unbalanced PLTS and developed an MAGDM based on it. Liu et al. [51] presented
the bidirectional projection method for probabilistic linguistic MAGDM based on
power average operator. Liu and Li [50] developed an extended MULTIMOORA
method for probabilistic linguistic MAGDM based on prospect theory. Pang et al. [66]
developed an extended TOPSIS method and an aggregation-based method

respectively for MAGDM with probabilistic linguistic information. Tian et al. [77]
20
presented a probabilistic linguistic MADM based on evidential reasoning and
combined ranking methods considering decision-makers’ psychological preferences.
Wang et al. [83] proposed a distance-based MAGDM approach with PLTSs. Wang [81]
proposed a generalized distance measurements method between two PLTs with
multi-granular linguistic information, and applied them to deal with multi-granular
MAGDM problems. Zhou et al. [123] proposed particle swarm optimization method
for trust relationship based social network MAGDM under a probabilistic linguistic
environment.

Zhang et al. [105] developed a consensus-oriented aggregation model for MAGDM
with ILDs, which can obtain a collective opinion with maximum consensus, and
further developed a minimum-cost consensus model with variable unit consensus cost.
Zhang et al. [113] developed a deviation minimum-based optimization model to
manage ILDs in MAGDM by minimizing the opinion deviation among decision
makers, and proposed a consensus-reaching model with bounded confidences based
feedback adjustment mechanism to assist decision makers to gain a consensus.

(3) MADM approaches with PDHFLTS, PHFLTSs, HLD, and FLE

HFLTS based distributed linguistic MADM approaches have been proposed and
utilized to tackle a variety of decision problems.

Wu and Xu [90] proposed operation laws and aggregation operators and for
PDHFLTS and built a framework to deal with both consensus and selection processes
for MAGDM problems with PDHFLTSs. Wu et al. [91] examined an MAGDM
problem in which the linguistic information was represented by PDHFLTS, and
developed two approaches based on VIKOR and TOPSIS to find a compromise
solution.

Chen et al. [9] developed a proportional hesitant fuzzy linguistic MAGDM model,
in which a probability theory-based outranking method for PHFLTSs was proposed
and two fundamental aggregation operators for PHFLTSs were provided.

Zhang et al. [109] proposed an approach for MAGDM with HLD, in which the
transformation between HLDs and LDs and the basic comparison and aggregation
operations to perform on HLDs are developed.

Wu et al. [86] proposed MAGDM with FLE. In the proposed model, an FLE
aggregation process with accurate constraints is developed to improve the quality (i.c.,
accuracy) of the collective result as well as guarantee the principle of minimum
preference-loss through a mixed 0-1 linear programming model. Meanwhile, the
consensus rules with minimum preference-loss are designed to support the consensus
reaching process in the MAGDM with FLE.

3.5. Some real-life applications
In this section, some real-life applications of the distributed linguistic

21
representations are introduced. In Table 5, we provide a summary of them.

Table 5: A summary of distributed linguistic representations in real-life applications

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Applications References Distributed linguistic representations
[32] LD
[34] LD, PLTS
Failure mode and effect analysis 1 0 Mult granular ED
{111] LD with PIS
[113] ILD
[43] LD
[62] LD
Hotel selection [67] PLTS
[98] LD
[106] LD
Water security sustainability [63] LD
evaluation
Sustainability of constructed [52] PLTS
wetlands evaluating
Sustainable third-party reverse [53] PDHELTS
logistics provider selection
Renewable energy source [37] Interval-valued PLTS
selection/risk assessment [47] Multi-granular LD
Financial technologies selection [54] PLTS
Cloud-based ERP system selection [7] PLTS
Health-care waste disposal [33] Multi-granular LD
alternative selection
[22] PLTS
Emergency decision-making [42] PLTS
[44] LD
Cloud vendor prioritization [72] PLTS
Quality function deployment [78] LD

 

(1) Applications in failure mode and effect analysis

AS a proactive risk management instrument, failure mode and effect analysis
(FMEA) has been broadly utilized to recognize, evaluate, and eliminate failure modes
of products, processes, systems and services [24]. Due to various subjective and
objective conditions, it is often difficult for FMEA team members to provide precise
values for the assessment of failure modes. Instead, they prefer to utilize linguistic
labels to state their opinions regarding the risk of failure modes. Recently, the
distributed linguistic representations have been adopted to model the uncertain
opinions of FMEA team members. For example, Huang et al. [32] applied LDs to
represent FMEA team members’ risk evaluation information and employed an
improved TODIM (an acronym in Portuguese of interactive and multicriteria decision
making) method to determine the risk priority of failure modes. Ju et al. [34] proposed
new approaches for heterogeneous linguistic FMEA with incomplete weight

information as well as the idea to convert heterogencous linguistic information to LDs.
22
Nie et al. [64] developed a hybrid risk evaluation model within the FMEA framework
based on the use of multi-granular LDs and applied this model to supercritical water
gasification system. Zhang et al. [110] proposed a consensus-based MAGDM
approach for ordinal classification-based FMEA problem, in which FMEA
participants provide their preferences in a linguistic way using LDs. Zhang et al. [113]
integrated a consensus-reaching mechanism with bounded confidences into the FMEA
framework and adopts ILDs to represent risk assessment information. Zhang et al.
[111] presented the design of a PIS-based FMEA approach, in which members
express their opinions over failure modes and risk factors using LDs.

(2) Applications in hotel selection

With the considerable development of tourism market, as well as the expansion of
the e-commerce platform scale, increasing tourists often prefer to select tourism
products such as services or hotels online. Thus, it is necessary to develop efficient
decision support models for tourists to select tourism products. Liang et al. [43]
developed decision support model for hotel selection based on sentiment analysis and
LD-VIKOR method, where the text data are transformed into LDs via sentiment
analysis. Nie et al. [62] proposed a scientific hotel selection model to assist tourists in
choosing a satisfactory hotel and guiding hoteliers to gain competitive advantages in
the e-tourism era, in which LDs are used to summarize and denote evaluation values
under certain criteria associated with hotels. Peng et al. [67] developed an applicable
hotel decision support model for tourists utilizing online reviews on TripAdvisor.com,
in which PLTSs are introduced to summarize this information statistically by
considering a great deal of review information associated with hotels posted by
numerous tourists on TripAdvisor.com. Yu et al. [98] designed a mathematical model
to select appropriate hotels on websites based on LDs. Zhang et al. [106] proposed a
multi-stage MADM method based on online reviews for hotel selection considering
the aspirations with different development speeds, in which LDs are used to deal with
online reviews.

(3) Applications in sustainability evaluation and other selection problems

Water security sustainability plays an increasingly crucial role in maintaining the
balance between industrialization, urbanization and sustainability. Nie et al. [63]
established an observation data conversion standard and adopted LDs as information
representation and developed a multistage decision support framework by combining
several MADM techniques (such as, best-worst method, DEMATEL, and TOPSIS)
for water security sustainability evaluation. With the accumulation of practical
experience and the maturity of technology, constructed wetlands have gradually
become multi-functional ecological systems. Luo et al. [52] proposed GDM approach
for evaluating the sustainability of constructed wetlands with PLPRs.

The sustainable third-party reverse logistics provider selection, as the core of
23
sustainable supply chain management, has become paramount in research nowadays.
Luo and Li [53] utilized PDHFLTSs to deal with the situation in which decision
makers may hesitate in a few linguistic terms and have different partiality towards
each term in the actual evaluation process, and presented an MAGDM method by
combining PDHFLTSs and MULTIMOORA for the sustainable third-party reverse
logistics provider selection in the e-commerce express industry.

“No technology, no finance” has been the consensus in banking industry. Under the
background of financial technology (Fintech), how to select an appropriate
technology company to cooperate for the banks has become a key. Miao et al. [54]
developed an MAGDM model with PLTSs for financial technologies selection.
Krishankumar et al. [37] proposed a GDM framework for renewable energy source
selection under interval-valued PLTSs.

Cloud-based enterprise resource planning (ERP) is a combination of standard ERP
system and cloud flexibility. On the basis of extended probabilistic linguistic
MULTIMOORA method and Choquet integral operator, Chen et al. [7] introduced an
innovative two-step comparative method for the evaluation of cloud-based ERP
systems. Ju et al. [33] presented a framework incorporating the evaluation based on
distance from average solution method for selecting desirable health-care waste
disposal alternative(s), in which multi-granular LDs are adopted by decision makers
to assess the ratings of alternatives and subjective weights of criteria.

(4) Applications in emergency decision making and prioritization events

According to the characteristics of emergency management, Gao et al. [22]
proposed an emergency decision support method by using the PLPR, and a case study
about the emergency decision making in a petrochemical plant fire accident is
conducted to illustrate the proposed method. Li and Wei [42] proposed an emergency
decision-making method based on D-S evidence theory and PLTSs, and applied the
proposed method to an actual mine accident.

With the tremendous growth of cloud vendors, cloud vendor prioritization is a
complex decision-making problem. Sivagami et al. [72] proposed a scientific decision
framework for Cloud Vendor prioritization under PLTSs context with unknown/partial
weight information. Tian et al. [78] presented an improved quality function
deployment for prioritizing service designs, in which multi-granular unbalanced
linguistic term sets are used to capture evaluators’ ratings to cope with vague
information. Moreover, a unification method is proposed to convert multi-granular
linguistic information into LDs.

4, Summary, critical discussion and challenges from the perspective of decision
making and data science/XAI

In this section, we present the summary and critical discussion from a decision

24
making perspective, and also present some ongoing challenges and future research
directions from the perspective of data science and XAI.
4.1. Summary and critical discussion: A decision making perspective

The review about distributed linguistic representations is mainly summarized from
a triple perspective:

(1) Origin of distributed linguistic representations. The distributed linguistic
representations in the literature are mainly derived from: (i) extension of proportional
linguistic 2-tuple representation, including LD, INLD, PLD, and ILD; and (ii)
extending HFLTSs preferences, including PDHFLTS, PHFLTS, PLTS, HLD, and FLE.
Particularly, these distributed linguistic representations can be formed in two ways: (i)
distributed linguistic preference expressions in an individual context; and (ii)
information fusion of linguistic terms or HFLTSs in a group context.

(2) Taxonomy of LD and its variants. We present the taxonomy of distributed
linguistic representations. The LD is the first attempt to model the distributed
linguistic representations among the decision-making community, and the follow-up
notions and concepts are mathematically consistent with LD in some sense: FLE is
the generalization of all the distributed linguistic representations reviewed; LD,
PDHFLTS, PHFLTS, PLTS, PLD, and ILD are special HLDs; PLTS and ILD are
mathematically consistent with PLD; PHFLTS is mathematically consistent with LD;
PDHFLTS is the special case of LD; and INLD is a generalization of LD.

(3) Applications in decision making. We review different applications of distributed
linguistic representations in decision making from four aspects. (i) Distributed
linguistic distance measurements and aggregation methods. Most of them are based
on classical distance measurements and aggregation operators. Moreover,
optimization methods are also developed for dealing with accuracy problems in some
decision contexts. (i) Distributed linguistic pairwise comparison methods, including
three types of distributed linguistic preference relations (i.c., DLPR, PLPR, and
FLPR). (iii) Distributed linguistic MADM approaches. Several researchers revisited
this issue based on classical MADM methods in distributed linguistic contexts. (iv)
Real-life applications, in which participants involved prefer to adopt distributed
linguistic representations to express their preferences to deal with practical decision
problems.

Distributed linguistic representations have been widely studied to model the
uncertainty and complexity of preference information in decision making, but there
still exist some limitations to be further highlighted:

(1) Although distributed linguistic representations have been analyzed from
multiple aspects, it still lacks a systematic research on the transformations among
them. Relevant discussions on transformations from an axiom-based perspective are

of significant necessity, and rational minimum information-loss based transformation
25
models are needed to be designed as well. Notably, normalization methods have been
proposed to transform PLTSs with incompleteness to LDs, but they are questionable.
Thus, reasonable normalization methods from PLTSs to LDs are worth studying.

(2) In distributed linguistic representations many similar concepts have been put
forward, and there is much parallel research in decision-making being undertaken,
which lead to lots of repeated discussions and confusion of concepts. Therefore, it is
necessary to focus on the original one and undertake valuable research and
comparisons. Particularly, the FLE will be a potential tool in distributed linguistic
representations to form a unified framework.

4.2. Challenges in data science and artificial intelligence: An explainable
linguistic approach

The development of data science and artificial intelligence, particularly the efforts
currently being made in the area of XAI [4], has provided enormous opportunities as
well as arising challenges. How to fix the idea of XAI, and the opportunity to use
linguistic information for representing the cognitive complex expressions from
decision makers/experts or data driven approaches getting linguistic labels as
knowledge, is needed to be addressed.

(1) Natural language processing (NLP) method. NLP is a significant artificial
intelligence-based tool to process information linguistically. The potential in using
NLP approaches in decision making has been partially shown in the literature [124],
and the research on opinion mining from user data is being undertaken, such as
accurate recognition of specific behaviors [5], credit risk assessments [61], etc. The
advent of data science has brought the chance that NLP-based techniques can be used
to handle a wider range of linguistic data [27], [36]. Particular attention should be
paid to the data driven decision making using distributed linguistic representations as
a representation approach for getting the information, experts’ assessments from the
data, and using NLP approaches for getting linguistic assessments from linguistic
opinions. Essentially, challenges under this perspective rely on how to apply these
NLP-based techniques (¢.g., sentiment analysis) as well as data-driven technologies to
analyze decision makers’ distributed linguistic data, and emphasize on the necessity to
figure out the characteristics of complexity of distributed linguistic information, and
the difficulty to effectively handle these complexity with data-driven NLP tools.

(2) Data fusion with data-driven and/or artificial intelligent approaches. In the
extent reviewed literature about the fusion methods of distributed linguistic
information in decision making problems, the popular methods are based on the
aggregation operators and mathematical optimization modelling. The development of
mass media and internet technologies has witnessed more and more direct or indirect
participation from different groups in the process of decision making. Complex

26
distributed linguistic preferences are provided by decision makers with varying social
backgrounds, self-confidence levels, knowledge structures, etc., and some key
elements begin to appear in the decision making at a large scale [15], such as
interactions with behaviors among decision makers/experts [6], [65], [93].

The challenges in this perspective centralize on coping with enormous amounts of
distributed linguistic information from disparate data sources with data-driven and/or
intelligent approaches. The analysis or processing of such information is no longer
using traditional decision support tools or simple aggregation techniques. In contrast
to the black models provided by the neural network based approaches, whose utility in
data analysis have been limited because of the interpretation difficulty, linguistic
based models with explainability are much more suitable. How to fuse these data and
how to acquire significant insights from these data are still the future research needed
to be investigated.

(3) Data-driven preference learning methods. Preference learning is a new
research field of the intersection of machine learning and decision making. The
preference learning mainly focuses on the analysis of individual and group
characteristics, and modelling group/multi-attribute preference learning functions by
learning historical data [2], [13]. It is worth pointing out that the advent of data
science and artificial intelligence highlights the importance of preference learning
methodology driven by large scale data sets [68]. The potential of applying
data-driven preference learning methods in linguistic decision making is still under
development. For example, how to learn the word encoding (semantic analysis),
which is the core of CW, and model PIS-based semantic learning from distributed
linguistic data at a large scale, is a problem worthy of further study.

Recent studies [1], [108] suggested that deep learning approaches are playing vital
roles in parameter estimating and parameter settings for correlated features of
decision models. Thereby, this learning method could be used to estimate optimal
weights of multiple attributes in distributed linguistic MADM, and to determine
suitable parameter settings for aggregation functions in the fusion process of
distributed linguistic data. The potential of these machine learning methods can be
detected in various practical applications, such as financial risk analysis [35], online
social recommendation [122], etc.

(4) Social network analysis (SNA) applications. With the development of social
media and communication technologies, it is easier to obtain the access to information
about social networks, which makes interactions among decision makers become
increasingly common. As a powerful tool, SNA has been investigated to support the
decision process from many aspects such as opinion dynamics [20], trust/distrust
relationships [79], [117], GDM [19], and LSDM [15], in which relationships among

decision makers are modelled in a social network. Combining the distributed
27
trust/distrust relationships among decision makers analyzed by SNA, the preference
characteristics of decision makers are identified, which can further support the
management of decision makers’ behaviors and the approaches for generating
recommendations to improve the efficiency of decision process. It would also be
interesting to apply these SNA-based decision models into real-life decision problems.
For example, in the hotel selection driven by online textual reviews [62], customers’
sentiments can be analyzed to extract both preferences and distributed trust
relationship information as an input for decision models.

(5) Online customer reviews application. Mass media and platforms have been
vested with significant power in society. For example, it is noted that the spread of
e-commerce platforms and mobile apps have improved customers’ ability to assess
products online. Online reviews on various e-commerce platforms have become an
important part of the electronic word of mouth and the important references for
potential customers to make decisions [48]. These online reviews/data have
significant distribution characteristics and effective analysis or processing of these
data can help enterprises/government understand the consumer preference much better,
and make rational decision accordingly.

In future research, a comprehensive analysis of the complexity of consumption data
sources is needed. In particular, NLP-based, emotion recognition-based [121],
SNA-based, and PIS-based approaches [45] are powerful tools to better and
accurately understand the sociality and individuality of customers through these
distributed linguistic data.

5. Conclusion

We review the distributed linguistic representations in decision making from a
triple perspective of taxonomy, key elements and applications, and ongoing
challenges. Specifically, we analyze the origin of existing distributed linguistic
representations in the literature, which are classified into two types: extension of
proportional linguistic 2-tuple representation, and HFLTS based _ extended
representation. Particularly, these distributed linguistic representations can be formed
in two ways: linguistic preference expressions in an individual context, and
information fusion of linguistic terms or HFLTSs in a group context. Furthermore, we
summarize the key elements and applications of distributed linguistic representations
in various decision problems, including distributed linguistic distance measurements
and aggregation methods; distributed linguistic preference relations; distributed
linguistic MADM; and some real-life applications. Finally, we critically discuss the
concept confusion issues of distributed linguistic representations, and propose the
challenges and future directions from the perspective of data science and XAI.

Acknowledgments
28
We thank Dr. Cuiping Wei (Yangzhou University) and Dr. Zhen-Song Chen
(Wuhan University) for their valuable suggestions to improve this review. This work
was supported by the grants (Nos. 71971039, 71421001, 71910107002, 71771037,
71874023, and 71871149) from NSF of China, and the grants (Nos. sksy1201705 and
2018hhs-58) from Sichuan University.

References

[1] M. Aggarwal, Learning of aggregation models in multi-criteria decision making,
Knowledge-Based Systems 119 (2017) 1-9.

[2] M. Aggarwal, A. Fallah Tehrani, Modelling human decision behaviour with preference
learning, INFORMS Journal on Computing 31(2) (2019) 318-334.

[3] C. Alsina, M.J. Frank, B. Schweizer, Associative Functions: Triangular Norms and Copulas,
World Scientific, Singapore, 2006.

[4] A.B. Arrieta, N. Diaz-Rodriguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. Garcia, S.
Gil-Lopez, D. Molina, R. Benjamins, R. Chatila, F. Herrera, Explainable artificial
intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible
AI, Information Fusion 58 (2020) 82-115.

[5] X.R. Chao, G. Kou, Y. Peng, F.E. Alsaadi, Behavior monitoring methods for trade-based
money laundering integrating macro and micro prudential regulation: A case from China,
Technological and Economic Development of Economy, in press,
doi: 10.3846/tede.2019.9383.

[6] X.R. Chao, G. Kou, Y. Peng, E. Herrera-Viedma, Large-scale group decision-making with
non-cooperative behaviors and heterogeneous preferences: an application in financial
inclusion, European Journal of Operational Research, in press, doi:
10.1016/j.ejor.2020.05.047.

[7] SX. Chen, J.Q. Wang, T.L. Wang, Cloud-based ERP system selection based on extended
probabilistic linguistic MULTIMOORA method and Choquet integral operator,
Computational and Applied Mathematics 38 (2019) 88.

[8] Z.S. Chen, K. Chin, L. Martinez, K. Tsui, Customizing semantics for individuals with
attitudinal HFLTS possibility distributions, IEEE Transactions on Fuzzy Systems 26 (2018)
3452-3466.

[9] Z.S. Chen, K.S. Chin, Y.L. Li, Y. Yang, Proportional hesitant fuzzy linguistic term set for
multiple criteria group decision making, Information Sciences 357 (2016) 61-87.

[10] Z.S. Chen, K.S. Chin, N.Y. Mu, S.H. Xiong, J.P. Chang, Y. Yang, Generating HFLTS
possibility distribution with an embedded assessing attitude, Information Sciences 394-395
(2017) 141-166.

[11] Z.S. Chen, L. Martinez, K.S. Chin, K.L. Tsui, Two-stage aggregation paradigm for HFLTS
possibility distributions: A hierarchical clustering perspective, Expert Systems with
Applications 104 (2018) 43-66.

[12] Z.S. Chen, M. Xu, X. Wang, K. Chin, K. Tsui, L. Martinez, Individual semantics building for
HFLTS possibility distribution with applications in domain-specific collaborative decision
making, IEEE Access 6 (2018) 78803-78828.

[13] S. Corrente, S. Greco, M. Kadzinski, R. Stowinski, Robust ordinal regression in preference
learning and ranking, Machine Learning 93 (2-3) (2013) 381-422.

29
[14] M. Delgado, J. L. Verdegay, and M. A. Vila, On aggregation operations of linguistic labels,
International Journal of Intelligent Systems 8 (1993) 351-370.

[15] R.X. Ding, I. Palomares, X.Q. Wang, G.-R. Yang, B.S. Liu, Y.-C. Dong, E. Herrera- Viedma, F.
Herrera, Large-Scale decision-making: Characterization, taxonomy, challenges and future
directions from an Artificial Intelligence and applications perspective, Information Fusion 59
(2020) 84-102.

[16] Y.C. Dong, E. Herrera-Viedma, Consistency-driven automatic methodology to set interval
numerical scales of 2-tuple linguistic term sets and its use in the linguistic GDM with
preference relation. IEEE Transactions on Cybernetics 45 (4) (2015) 780-792.

[17] Y.C. Dong, Y.Z. Wu, HJ. Zhang, G.Q. Zhang, Multi-granular unbalanced linguistic
distribution assessments with interval symbolic proportions, Knowledge-Based Systems 82
(2015) 139-151.

[18] Y.C. Dong, Y. Xu, S. Yu, Computing the numerical scale of the linguistic term set for the
2-tuple fuzzy linguistic representation model. IEEE Transactions on Fuzzy Systems 17 (6)
(2009) 1366-1378.

[19] Y.C. Dong, Q.B. Zha, H.J. Zhang, G. Kou, H. Fujita, F. Chiclana, E. Herrera-Viedma,
Consensus reaching in social network group decision making: research paradigms and
challenges, Knowledge-Based Systems 162 (2018) 3-13.

[20] Y.C. Dong, M. Zhan, G. Kou, Z.G. Ding, H.M. Liang, A survey on the fusion process in
opinion dynamics, Information Fusion 43 (2018) 57-65.

[21] J. Gao, Z.S. Xu, Z.L. Liang, H.C. Liao, Expected consistency-based emergency decision
making with incomplete probabilistic linguistic preference relations, Knowledge-Based
Systems 176 (2019) 15-28.

[22] J. Gao, Z.S. Xu, P.J. Ren, H.C. Liao, An emergency decision making method based on the
multiplicative consistency of probabilistic linguistic preference relations, International
Journal of Machine Learning and Cybernetics 10 (2019) 1613-1629.

[23] J. Garcia-Lapresta, D. Perez-Roman, Aggregating opinions in nonuniform ordered qualitative
scales. Applied Soft Computing 67(2018) 652-657.

[24] H.H. Guerrero, J.R. Bradley, Failure modes and effects analysis: An evaluation of group
versus individual performance, Production and Operations Management 22 (2013)
1524-1539,

[25] W.T. Guo, V.N. Huynh, Y. Nakamori, An interval linguistic distribution model for multiple
attribute decision making problems with incomplete linguistic information, International
Journal of Knowledge Systems Sciences 6 (4) (2015) 16-34.

[26] W.T. Guo, V.-N. Huynh, S. Sriboonchitta, A proportional linguistic distribution based model
for multiple attribute decision making under linguistic uncertainty, Annals of Operations
Research 256 (2017) 305-328.

[27] M. Guo, X. Liao, J. Liu, Q. Zhang, Consumer preference analysis: A data-driven multiple
criteria approach integrating online information. Omega, in press, doi:
10.1016/j.omega.2019.05.010.

[28] B. Han, Z.F. Tao, H.Y. Chen, L.G. Zhou, J.P. Liu, A new computational model based on
Archimedean copula for probabilistic unbalanced linguistic term set and its application to
multiple attribute group decision making, Computers & Industrial Engineering 140 (2020)
106264.

30
[29] F. Herrera, S. Alonso, F. Chiclana, and E. Herrera-Viedma, Computing with words in decision
making: Foundations, trends and prospects, Fuzzy Optimization & Decision Making 8 (4)
(2009) 337-364.

[30] F. Herrera, E. Herrera-Viedma, L. Martinez, A fuzzy linguistic methodology to deal with
unbalanced linguistic term sets, IEEE Transactions on Fuzzy Systems 16 (2) (2008) 354-370.

[31] F. Herrera and L. Martinez, A 2-tuple fuzzy linguistic representation model for computing
with words, IEEE Transactions on Fuzzy Systems 8 (6) (2000) 746-752.

(32] J. Huang, Z.J. Li, H.C. Liu, New approach for failure mode and effect analysis using
linguistic distribution assessments and TODIM method, Reliability Engineering & System
Safety 167 (2017) 302-309.

[33] Y.B. Ju, YY. Liang, L. Martinez, E.D.R. Santibanez Gonzalez, M. Giannakis, P.W. Dong,
A.H. Wang, A new framework for health-care waste disposal alternative selection under
multi-granular linguistic distribution assessment environment, Computers & Industrial
Engineering, in press, doi: 10.1016/j.cie.2020. 106489.

[34] Y.B. Ju, YY. Liang, L. Martinez, A-H. Wang, C.-F. Chien, P-W. Dong, E.D.R. Santibanez
Gonzalez, A new approach for heterogeneous linguistic failure mode and effect analysis with
incomplete weight information, Computers & Industrial Engineering, in press, doi:
10.1016/}.cie.2020.106659.

[35] G. Kou, X.R. Chao, Y. Peng, FE. Alsaadi, E. Herrera-Viedma, Machine learning methods for
systemic risk analysis in financial sectors, Technological and Economic Development of
Economy, in press, doi:10.3846/tede.2019.8740.

[36] G. Kou, P. Yang, Y. Peng, F. Xiao, Y. Chen, F. Alsaadi, Evaluation of feature selection
methods for text classification with small datasets using multiple criteria decision-making
methods, Applied Soft Computing, in press, doi: 10.1016/}.asoc.2019.105836.

[37] R. Krishankumar, A.R. Mishra, K.S. Ravichandran, X. Peng, E.K. Zavadskas, F. Cavallaro, A.
Mardani, A group decision framework for renewable energy source selection under
interval-valued probabilistic linguistic term set, Energies 13 (2020) 986.

[38] C.C. Li, Y.C. Dong, F. Herrera, A consensus model for large-scale linguistic group decision
making with a feedback recommendation based on clustered personalized individual
semantics and opposing consensus groups, IEEE Transactions on Fuzzy Systems 27 (2)
(2019) 221-233.

[39] C.C. Li, ¥.C. Dong, F. Herrera, E. Herrera-Viedma, L. Martinez, Personalized individual
semantics in computing with words for supporting linguistic group decision making. An
application on consensus reaching, Information Fusion 33 (2017) 29-40.

[40] C.C. Li, Y. Gao, Y.C. Dong, Managing ignorance elements and personalized individual
semantics under incomplete linguistic distribution context in group decision making, Group
Decision and Negotiation, to appear.

[41] C.C. Li, R.M. Rodriguez, L. Martinez, Y.C. Dong, F. Herrera, Personalized individual
semantics based on consistency in hesitant linguistic group decision making with
comparative linguistic expressions, Knowledge-Based Systems 145 (2018) 156-165.

[42] P. Li, C.P. Wei, An emergency decision-making method based on D-S evidence theory for
probabilistic linguistic term sets, International Journal of Disaster Risk Reduction 37 (2019)
101178.

[43] X. Liang, P.D. Liu, Z. Wang, Hotel selection utilizing online reviews: a novel decision

31
support model based on sentiment analysis and DL-VIKOR method, Technological and
Economic Development of Economy 25 (2019) 1139-1161.

[44] X. Liang, F. Teng, Y. Sun, Multiple group decision making for selecting emergency
alternatives: a novel method based on the LDWPA operator and LD-MABAC, International
Journal of Environmental Research and Public Health 17 (2020) 2945.

[45] HM. Liang, C.C. Li, YC. Dong, F. Herrera, Linguistic opinions dynamics based on
personalized individual semantics, IEEE Transactions on Fuzzy Systems, in press, dot:
10.1109/TFUZZ .2020.2999742.

[46] Y.Y. Liang, J.D. Qin, L. Martinez, Consensus-based multi-criteria group preference analysis
model with multi-granular linguistic distribution information, IEEE Transactions on Fuzzy
Systems, in press (2020), doi: 10.1109/TFUZZ.2020.3002389.

[47] Y.Y. Liang, Y.B. Ju, J.D. Qin, W. Pedryez, Multi-granular linguistic distribution evidential
reasoning method for renewable energy project risk assessment, Information Fusion, in press,
doi: 10.1016/j.inffus.2020.08.010.

[48] Y. Liu, J.-W. Bi, Z.-P. Fan, Ranking products through online reviews: A method based on
sentiment analysis technique and intuitionistic fuzzy set theory, Information Fusion 36 (2017)
149-161.

[49] A. Liu, H. Qiu, H. Lu, X. Guo, A consensus model of probabilistic linguistic preference
relations in group decision making based on feedback mechanism, IEEE Access 7 (2019)
148231-148244.

[50] PD. Liu, Y. Li, An extended MULTIMOORA method for probabilistic linguistic
multi-criteria group decision-making based on prospect theory, Computers & Industrial
Engineering 136 (2019) 528-545.

[51] PD. Liu, Y. Li, F. Teng, Bidirectional projection method for probabilistic linguistic
multi-criteria group decision-making based on power average operator, International Journal
of Fuzzy Systems 21 (2019) 2340-2353.

[52] S.Z. Luo, H-Y. Zhang, J.Q. Wang, L. Li, Group decision-making approach for evaluating the
sustainability of constructed wetlands with probabilistic linguistic preference relations,
Journal of the Operational Research Society 70 (12) (2019) 2039-2055.

[53] Z.H. Luo, Z.X. Li, A MAGDM method based on possibility distribution hesitant fuzzy
linguistic term set and its application, Mathematics 7 (2019) 1063.

[54] X.B. Mao, M. Wu, J.Y. Dong, S.P. Wan, Z. Jin, A new method for probabilistic linguistic
multi-attribute group decision making: Application to the selection of financial technologies,
Applied Soft Computing 77 (2019) 155-175.

[55] L. Martinez, R.M. Rodriguez, F. Herrera, The 2-tuple linguistic model: Computing with
words in decision making, New York: Springer, 2015.

[56] J.M. Mendel, Computing with words and its relationships with fuzzistics, Information
Science 177 (4) (2007) 988-1006.

[57] J.M. Mendel, D. Wu, Perceptual computing: aiding people in making subjective judgments,
TEEE-Wiley press, John Wiley & Sons, Inc., Hoboken, New Jersey, 2010.

[58] J.M. Mendel, H. Wu, Type-2 fuzzistics for symmetric interval type-2 fuzzy sets: Part 1,
porward problems, IEEE Transactions on Fuzzy Systems 14 (6) (2006) 781-792.

[59] J.M. Mendel, H. Wu, Type-2 fuzzistics for symmetric interval type-2 fuzzy sets: part 2,
inverse problems, IEEE Transactions on Fuzzy Systems 15 (2) (2007) 301-308.

32
[60] J. Mendel, L.A. Zadeh, E. Trillas, J. Lawry, H. Hagras, S. Guadarrama, What computing with
words means to me. IEEE Computational Intelligence Magazine 5 (1) (2010) 20-26.

[61] S. Moradi, F. Mokhatab Rafiei, A dynamic credit risk assessment model with data mining
techniques: evidence from Iranian banks, Financial Innovation 5 (2019) 15.

[62] R.X. Nie, Z.P. Tian, J.Q. Wang, K.S. Chin, Hotel selection driven by online textual reviews:
Applying a semantic partitioned sentiment dictionary and evidence theory, International
Journal of Hospitality Management 88 (2020) 102495.

[63] R.X. Nie, Z.P. Tian, J.Q. Wang, H.Y. Zhang, T.L. Wang, Water security sustainability
evaluation: Applying a multistage decision support framework in industrial region, Journal of
Cleaner Production 196 (2018) 1681-1704.

[64] R.X. Nie, Z.P. Tian, X.K. Wang, J.Q. Wang, T.L. Wang, Risk evaluation by FMEA of
supercritical water gasification system using multi-granular linguistic distribution assessment,
Knowledge-Based Systems 162 (2018) 185-201.

[65] I. Palomares, L. Martinez, F. Herrera, A consensus model to detect and manage
noncooperative behaviors in large-scale group decision making, IEEE Transactions on Fuzzy
Systems 22 (3) (2014) 516-530.

[66] Q. Pang, H. Wang, Z.S. Xu, Probabilistic linguistic term sets in multi-attribute group decision
making, Information Sciences 369 (2016) 128-143.

[67] H.G. Peng, H.-Y. Zhang, J.Q. Wang, Cloud decision support model for selecting hotels on
TripAdvisor.com with probabilistic linguistic information, International Journal of
Hospitality Management 68 (2018) 124-138.

[68] V.C. Raykar, R. Duraiswami, B. Krishnapuram, A fast algorithm for learning a ranking
function from large-scale data sets. IEEE Transactions on Pattern Analysis and Machine
Intelligence 30 (7) (2008) 1158-1170.

[69] R.M. Rodriguez, B. Bedregal, H. Bustince, Y.C. Dong, B. Farhadinia, C. Kahraman, L.
Martinez, V. Torra, Y. Xu, Z. Xu, F. Herrera, A position and perspective analysis of hesitant
fuzzy sets on information fusion in decision making. Towards high quality progress,
Information Fusion 29 (2016) 89-97.

[70] R.M. Rodriguez, L. Martinez and F. Herrera, Hesitant fuzzy linguistic term sets for decision
making, IEEE Transactions on Fuzzy Systems 20 (1) (2012) 109-119.

[71] B. Schweizer, A. Sklar, Associative functions and abstract semi-groups, Publ. Math.
Debrecen 10 (1963) 69-81.

[72] R. Sivagami, K.S. Ravichandran, R. Krishankumar, V. Sangeetha, S. Kar, X.-Z. Gao, D.
Pamucar, A scientific decision framework for cloud vendor prioritization under probabilistic
linguistic term set context with unknown/partial weight information, Symmetry 11 (2019)
682.

[73] Y.M. Song, J. Hu, Large-scale group decision making with multiple stakeholders based on
probabilistic linguistic preference relation, Applied Soft Computing 80 (2019) 712-722.

[74] X. Tang, Q. Zhang, Z.L. Peng, W. Pedryez, S.L. Yang, Distribution linguistic preference
relations with incomplete symbolic proportions for group decision making, Applied Soft
Computing 88 (2020) 106005.

[75] X.A. Tang, Z.L. Peng, Q. Zhang, W. Pedryez, S.L. Yang, Consistency and consensus-driven
models to personalize individual semantics of linguistic terms for supporting group decision

making with distribution linguistic preference relations, Knowledge-Based Systems 189

33
(2020) 105078.

[76] X.A. Tang, Q. Zhang, Z.L. Peng, S.L. Yang, W. Pedryez, Derivation of personalized
numerical scales from distribution linguistic preference relations: an expected
consistency-based goal programming approach, Neural Computing and Applications 31
(2019) 8769-8786.

[77] Z.P. Tian, R.X. Nie, J.Q. Wang, Probabilistic linguistic multi-criteria decision-making based
on evidential reasoning and combined ranking methods considering decision-makers’
psychological preferences, Journal of the Operational Research Society 71 (2020) 700-717.

[78] Z.P. Tian, R.X. Nie, J.Q. Wang, L. Li, Group multigranular linguistic QFD for prioritizing
service designs with combined weighting method, Expert Systems 36 (2019) e12419.

[79] R. Urefia, G. Kou, Y. Dong, F. Chiclana, E. Herrera-Viedma, A review on trust propagation
and opinion dynamics in social networks and group decision making frameworks,
Information Sciences 478 (2019) 461-475.

[80] PP. Wang (Ed.), Computing with words, Wiley Series on Intelligent Systems, John Wiley &
Sons, Inc., 2001.

[81] JX. Wang, A MAGDM algorithm with multi-granular probabilistic linguistic information,
Symmetry 11 (2019) 127.

[82] J.H. Wang and J. Hao, A new version of 2-tuple fuzzy linguistic representation model for
computing with words, IEEE Transactions on Fuzzy Systems 14 (3) (2006) 435-445.

[83] X.K. Wang, J.Q. Wang, H.-Y. Zhang, Distance-based multicriteria group decision-making
approach with probabilistic linguistic term sets, Expert Systems 36 (2019) €12352.

[84] J. Wu, L.F. Dai, F. Chiclana, H. Fujita, E. Herrera-Viedma, A minimum adjustment cost
feedback mechanism based consensus model for group decision making under social network
with distributed linguistic trust, Information Fusion 41 (2018) 232-242.

[85] Y.Z. Wu, Y.C. Dong, An optimization-based approach with minimum preference loss to fuse
incomplete linguistic distributions in group decision making, 2017 IEEE International
Conference on Fuzzy Systems (FUZZ-IEEE), Naples, 2017, 1-6.

[86] Y.Z. Wu, Y.C. Dong, J.D. Qin, W. Pedryez, Flexible linguistic expressions and consensus
reaching with accurate constraints in group decision-making, IEEE Transactions on
Cybernetics, 50 (6) (2020) 2488-2501.

[87] Y.Z. Wu, Y.C. Dong, J.D. Qin, W. Pedryez, Linguistic distribution and_priority-based
approximation to linguistic preference relations with flexible linguistic expressions in
decision making, JEEE Transactions on Cybernetics, in press (2020) doi:
10.1109/TCYB.2019.2953307.

[88] Y.Z. Wu, C.C. Li, X. Chen, Y.C. Dong, Group decision making based on linguistic
distributions and hesitant assessments: Maximizing the support degree with an accuracy
constraint, Information Fusion 41 (2018) 151-160.

[89] Y.Z. Wu, H.J. Zhang, Y.C. Dong, Linguistic distribution assessments with interval symbolic
proportions, Proceedings of the Eighth International Conference on Management Science and
Engineering Management. Advances in Intelligent Systems and Computing, 280, 163-173,
2014.

[90] Z.B. Wu, J.P. Xu, Possibility distribution-based approach for MAGDM with hesitant fuzzy
linguistic information, IEEE Transactions on Cybernetics 46 (2016) 694-705.

[91] Z.B. Wu, J.P. Xu, X.L. Jiang, L. Zhong, Two MAGDM models based on hesitant fuzzy

34
linguistic term sets with possibility distributions: VIKOR and TOPSIS, Information Sciences
473 (2019) 101-120.

[92] J. Xiao, X.L. Wang, H.J. Zhang, Managing personalized individual semantics and consensus
in linguistic distribution large-scale group decision making, Information Fusion 53 (2020)
20-34,

[93] W.J. Xu, X. Chen, YC. Dong, F. Chiclana, Impact of decision rules and non-cooperative
behaviors on minimum consensus cost in group decision making, Group Decision and
Negotiation in press (2020), doi: 10.1007/s10726-020-09653-7.

[94] R.R. Yager, On the retranslation process in Zadeh’s paradigm of computing with words, IEEE
Transactions on Systems Man and Cybernetics Part B-Cybernetics 34 (2) (2004) 1184-1195.

[95] R.R. Yager, Aggregation of ordinal information, Fuzzy Optimization and Decision Making 6
(2007) 199-219.

[96] R.R. Yager, A new methodology for ordinal multiobjective decisions based on fuzzy sets,
Decision Sciences 12 (1981) 589-600.

[97] S.B. Yao, A new distance-based consensus reaching model for multi-attribute group
decision-making with linguistic distribution assessments, International Journal of
Computational Intelligence Systems 12 (2019) 395-409.

[98] S.M. Yu, J. Wang, J.Q. Wang, L. Li, A multi-criteria decision-making model for hotel
selection with linguistic distribution assessments, Applied Soft Computing 67 (2018)
TA1-755.

[99] W.Y. Yu, Z. Zhang, Q.Y. Zhong. A TODIM-based approach to large-scale group decision
making with multi-granular unbalanced linguistic information. In Proceedings of the 2017
IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), Naples, 2017, 1-8.

[100] W.Y. Yu, Q.Y. Zhong, Z. Zhang. Fusing multi-granular unbalanced hesitant fuzzy linguistic
information in group decision making. In Proceedings of the 2016 IEEE International
Conference on Fuzzy Systems (FUZZ-IEEE), Vancouver, Canada, 2016, 872-879.

[101] L.A. Zadeh, Fuzzy logic = computing with words, IEEE Transactions on Fuzzy Systems 4
(1996) 103-111.

[102] L.A. Zadeh, A computational approach to fuzzy quantifers in natural languages, Computers
and Mathematics with Applications 9 (1983) 149-184.

[103] L.A. Zadeh, The concept of a linguistic variable and its applications to approximate
reasoning, Information Sciences Part I, 8: 1199-249; Part II, 8: 301-357; Part II, 9: 43-80,
1975.

[104] L.A. Zadeh, Outline of a new approach to the analysis of complex systems and decision
processes, IEEE Transactions on Systems, Man, and Cybernetics (SMC3) (1973) 28-44.

[105] B.W. Zhang, H.M. Liang, Y. Gao, G.Q. Zhang, The optimization-based aggregation and
consensus with minimum-cost in group decision making under incomplete linguistic
distribution context, Knowledge-Based Systems 162 (2018) 92-102.

[106] C.X. Zhang, M. Zhao, M.Y. Cai, Q.R. Xiao, Multi-stage multi-attribute decision making
method based on online reviews for hotel selection considering the aspirations with different
development speeds, Computers & Industrial Engineering 143 (2020) 106421.

[107] G.Q. Zhang, Y.C. Dong, Y.F. Xu, Consistency and consensus measures for linguistic
preference relations based on distribution assessments, Information Fusion 17 (2014) 46-55.

[108] W. Zhang, Y. Du, Y. Yang, T. Yoshida, DeRec: A data-driven approach to accurate

35
recommendation with deep learning and weighted loss function, Electronic Commerce
Research and Applications 31 (2018) 12-23.

[109] G.Q. Zhang, Y.Z. Wu, Y.C. Dong, Generalizing linguistic distributions in hesitant decision
context, International Journal of Computational Intelligence Systems 10 (2017) 970-985.
[110] H.J. Zhang, Y.C. Dong, I. Palomares, H.W. Zhou, Failure mode and effect analysis in a
linguistic context: A consensus-based multi-attribute group decision-making approach, IEEE

Transactions on Reliability 68 (2019) 566-582.

[111] H.J. Zhang, Y.C. Dong, J. Xiao, F. Chiclana, E. Herrera-Viedma, Personalized individual
semantics-based approach for linguistic failure modes and effects analysis with incomplete
preference information, IISE Transactions, in press (2020), doi:
10.1080/24725854.2020.1731774.

[112] H.J. Zhang, C.C. Li, Y.T. Liu, Y¥.C. Dong, Modelling personalized individual semantics and
consensus in comparative linguistic expression preference relations with self-confidence: An
optimization-based approach, IEEE Transactions on Fuzzy Systems, in press (2019) dot:
10.1109/TFUZZ.2019.2957259.

[113] H.J. Zhang, J. Xiao, Y.C. Dong, Integrating a consensus-reaching mechanism with bounded
confidences into failure mode and effect analysis under incomplete context,
Knowledge-Based Systems 183 (2019) 104873.

[114] H.J. Zhang, J. Xiao, I. Palomares, H.M. Liang, Y.C. Dong, Linguistic distribution-based
optimization approach for large-scale GDM with comparative linguistic information: An
application on the selection of wastewater disinfection technology, IEEE Transactions on
Fuzzy Systems 28 (2020) 376-389.

[115] YX. Zhang, Z.S. Xu, H.C. Liao, A consensus process for group decision making with
probabilistic linguistic preference relations, Information Sciences 414 (2017) 260-275.

[116] YX. Zhang, Z.S. Xu, H. Wang, H.C. Liao, Consistency-based risk assessment with
probabilistic linguistic preference relation, Applied Soft Computing 49 (2016) 817-833.

[117] Z. Zhang, Y. Gao, Z.L. Li, Consensus reaching for social network group decision making by
considering leadership and bounded confidence, Knowledge-Based Systems, 204 (2020)
106240.

[118] Z. Zhang, C. Guo. New operations of hesitant fuzzy linguistic term sets with applications in
multi-attribute group decision making. In Proceedings of the 2015 IEEE International
Conference on Fuzzy Systems (FUZZ-IEEE 2015), Istanbul, Turkey, 2015, 1-8.

[119] Z. Zhang, C. Guo, L. Martinez, Managing multigranular linguistic distribution assessments
in large-scale multiattribute group decision making, IEEE Transactions on Systems, Man, and
Cybernetics: Systems 47 (2017) 3063-3076.

[120] Z. Zhang, W.Y. Yu, L. Martinez, Y. Gao, Managing multigranular unbalanced hesitant fuzzy
linguistic information in multiattribute large-scale group decision making: A linguistic
distribution-based approach, IEEE Transactions on Fuzzy Systems, in press (2019) doi:
10.1109/TFUZZ.2019.2949758.

{121] JH. Zhang, Z. Yin, P. Chen, S. Nichele, Emotion recognition using multi-modal data and
machine learning techniques: A tutorial and review, Information Fusion 59 (2020) 103-126.

[122] Z. Zhao, H. Lu, D. Cai, X. He, Y. Zhuang, User preference learning for online social
recommendation. IEEE Transactions on Knowledge and Data Engineering 28 () (2016)
2522-2534.

36
[123] X.Y. Zhou, FP. Ji, L.Q. Wang, YF. Ma, H. Fujita, Particle swarm optimization for trust
relationship based social network group decision making under a probabilistic linguistic
environment, Knowledge-Based Systems 200 (2020) 105999.

[124] C. Zuheros, E. Martinez-Camara, E. Herrera-Viedma, F. Herrera, Decision making model
based on expert evaluations extracted with sentiment analysis, Proc.2019 Int. Alan-Turing
Conf. Decis. Support Recommend. Syst. DSRS-Turing 1st Ed (2019).

37
The Methods and Approaches of Explainable
Artificial Intelligence

Mateusz Szczepariski?, Michal Chorag!?, Marek Pawlickib?, and Aleksandra
Pawlicka!

! ITTI Sp. z 0.0. Poznan, Poland
2 UTP University of Science and Technology, Bydgoszcz, Poland

Abstract. Artificial Intelligence has found innumerable applications,
becoming ubiquitous in the contemporary society. From making unno-
ticeable, minor choices to determining people’s fates (the case of predic-
tive policing). This fact raises serious concerns about the lack of explain-
ability of those systems. Finding ways to enable humans to comprehend
the results provided by AI is a blooming area of research right now.
This paper explores the current findings in the field of Explainable Ar-
tificial Intelligence (xAI), along with xAI methods and solutions that
realise them. The paper provides an umbrella perspective on available
xAI options, sorting them into a range of levels of abstraction, starting
from community-developed code snippets implementing facets of xAI re-
search all the way up to comprehensive solutions utilising state-of the-art
achievements in the domain.

Keywords: xAI - AI - Intelligent Systems - Explainability

1 Introduction

Since Artificial Intelligence (AI) models have become sophisticated enough
to outclass many competing approaches in their respective fields, their popularity
has been on the rise [1]. With initiatives such as autonomous vehicles, various
recommendation systems (e.g., used by Netflix or Google Sybil), personal
assistants and many more, intelligent systems are being instilled in everyone’s
lives.

This increasing ubiquity, along with the black-box nature of the best perform-
ing solutions, has led to some serious concerns [1][2][3], such as the questions of
finding whether the model is unbiased [3], guaranteeing the security of the AI
models [4], ensuring the model’s decisions are right [5], or deciding whether to
trust a system, the decisions of which cannot be understood [1].

The need to answer those questions has initiated the concept of Explainable
Artificial Intelligence (xAI) [1]. Its main concern is to deliver the tools and
methods that allow human operators to understand the driving forces behind
the decisions made by AI [6]. The field also relies on the achievements of other
disciplines, such as psychology or sociology [2].

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
2 M. Szczepaniski et al.

Following the expansion of deep learning solutions, the search for rational
explanations to the decisions taken by Artificial Intelligence has gained wider
recognition [6]. This very year, a number of papers in the field have been pub-
lished. Some of them present a general overview of the concept [7][8], while others
focus on specific, particular features of the Explainable AI [9][10]. Finally, scien-
tific papers which recommend using xAI in a particular field, or prove how ben-
eficial this kind of application would be, have been published, e.g., [11][12][13],
etc.

At present, the discipline is expanding in a dynamic way, enjoying its renais-
sance [3] and attracting the attention of the biggest corporations, such as Google
[14] and IBM [15].

In other words, the accuracy obtained by AI is not the only factor that must
be considered at this moment. The ability to understand the decision processes
driving AI seems to be of crucial importance, too [5]. This subject has recently
started to attract a wider audience [2]. Therefore, the following paper aims to
become a starting point for exploring Explainable Artificial Intelligence, the
main approaches and available solutions . It is structured as follows: firstly, the
notion of Explainable Artificial Intelligence is introduced, with the criteria for
explanations and some practical issues. Then, an overview of xAI taxonomies
solutions is performed, and lastly, an umbrella perspective of the solutions that
utilise x AI is given. The above approach is summarised in the conclusion section
that follows.

2 Explainable Artificial Intelligence

The following subsection goes into the details of explainable artificial intelligence,
and its advantages over the classical, black box approach to AI are illustrated.

2.1 The issue about the black-box Artificial Intelligence

In psychology, there is the term of the ” Clever Hans effect” [16]. The name
comes from a horse which was famous for its ability to answer questions and solve
arithmetic equations, communicating the results by tapping its hoof. However, it
later turned out that instead of being a genius, the animal could simply read the
cues from the body language of the person asking questions, and stopped tapping
accordingly [1]. Today, the “Clever Hans effect” refers to a situation when, in
the course of a flawed experiment, the questioner cues the desired behaviour in
an unintentional way.

As scientists have learned, this effect is not limited to animals and humans,
but also applies to artificial intelligence models as well. There have been observed
the cases of models that were successful in performing their tasks only when very
specific conditions were met (e.g., a model recognised boats provided that there
was water in the picture, too) [1]. This issue may carry adverse implications.

One of the main concerns of today is related to the application of AI in pre-
dictive policing. For example, it has been brought to the public’s attention that

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
The Methods and Approaches of Explainable Artificial Intelligence 3

some discriminatory practices generated ”dirty data”. The data, having been
directly ingested by the predictive policing system, posed the risk of reinforcing
and amplifying deeply ingrained biases [17]. This in turn might easily have led to
disrespecting individual rights, human dignity and undermining justice [18]. In
fact, it has indeed been observed that the intelligent criminal justice system had
been deciding whether a person deserved parole or not based on their ethnicity
[19]. This particular incident has since become a valid argument illustrating the
need for artificial intelligence to be transparent, especially in high stake decision
processes. An unexplainable system is unverifiable, and therefore untrustworthy.
Probably no end user would wish to trust such a system with their lives. Actually,
the matter caused so much controversy that a few jurisdictions in the US have
ceased their use of predictive policing, whilst in Europe it is being argued that
it would be better to pause the use of it until the systems become explainable
and transparent enough [17].

2.2. Exploring Explainable artificial intelligence

As stated before, the Al-based systems need to be transparent. So much so, in
some cases the transparency has been required by law [3]. Therefore, new solu-
tions needed to be found. Thus, the essence of Explainable Artificial intelligence
has become that, given an audience, an explainable Artificial Intelli-
gence is one that produces details or reasons to make its functioning
clear or easy to understand [20]. In order to start discussing explainabil-
ity, one should then first define the term. In the literature, there exist several
terms which, in the context of AI are often used interchangeably to describe a
very similar concept, i.e. ”explainability”, ”interpretability”, ”understandabil-
ity”, "comprehensibility”, ”intelligibility” and ”transparency”. However, there
are slight differences between them, or rather, the terms have somewhat differ-
ent undertones, and there is still an ongoing discussion concerning what they
actually mean and what they differ in [1][2][3] [20].

In order to clarify this issue, Table 1 presents the meaning of the synonyms
in detail. For the sake of this paper, the term ”explainability” was selected, due
to its broadest scope, active nature and its already established position in the
subject literature.

2.3. The criteria for explanations

Generally, all of those considerations lead to the objective of determining what
constitutes a good explanation. To begin with, as Carvalho et al. highlight,
an important distinction must be made between the aim of achieving a cor-
rect explanation and the best explanation. Generally speaking, there are non-
pragmatic and pragmatic theories of explanation. The former group is
concentrated on achieving correct explanations, while the latter searches for
good explanations [3].

The non-pragmatic theories usually assume that there is only one, true reason
behind the actions of an intelligent system. Their aim is to unveil this reason,

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
4 M. Szczepaniski et al.

term definition
Refers to the extent to which human users are able to comprehend and literally
explain the mechanisms that drive the learning of an AI/ML system [21].
It is an active feature of a model; the term refers to the actions taken by the model
to clarify its inner working [22]
Related to the aspects concerning observing the outputs of an AI system. The more
predictable the changes of the system outputs when having switched algorithmic
interpretability parameters, the higher the system’s interpretability. Otherwise stated, it concerns
the extent to which humans are able to forecast the results produced by an AI system,
relying on various inputs [21]. It is a passive feature of the model [22]
Used to describe the situation where the user is able to comprehend and generate
understandability explanations of how the model works (its way of functioning), without being offered
any description of the processes within the learning model [22]([23].
intelligibility | In the context of AI, it is understood in a very similar way to understandability [22][23].
Is used to describe the capability of the learning model to outline the knowledge
it has learnt in a manner that the user can understand [22].
A transparent model is one which does not need any other interface or process
to be understood, i.e. it is understandable by itself [22].

Table 1. The terms used when discussing explainability in the context of AI

 

explainability

 

 

 

 

comprehensibility

 

transparency

 

but whether or not it is understandable for an audience, is beyond their concern.
On the other hand, the pragmatic theories include the listener as an important
part of the whole process. Explanation must be formulated in the manner that
the audience can understand and use.

The pragmatic theory adds a powerful tool to the theoretical arsenal of xAI
researchers and designers: the Rashomon effect [3]. It states that an event
can have multiple explanations; i.e., more than one explanation can actually
be found, and a person can select the one that fits their goals best, while still
keeping some level of ” truthfulness”. However, though certainly useful, it still
leaves the matter of selecting the” best” explanation from all of the ” good” ones.

There have been a number of attempts to solve this issue [1]. General guide-
lines, as well as more objective measures of quality have been suggested. For ex-
ample, Hansen and Rieger present the ”#AI Desiderata”, proposed by Swartout
and Moore in 1993:

1. Fidelity: the explanation must be a reasonable representation of what the
system actually does.

2. Understandability: Involves multiple usability factors including terminol-
ogy, user competencies, levels of abstraction and interactivity.

3. Sufficiency: Should be able to explain function and terminology, and be
detailed enough to justify decision.

4. Low Construction Overhead: The explanation should not dominate the
cost of designing AT.

5. Efficiency: The explanation system should not slow down the AI signifi-
cantly[1].

Though developed in the context of expert systems, it still remains true for mod-
ern Al systems. It also presents a challenge for the community, because designing
a solution that adheres to all the principles is not an easy task. As regards the
Quantitative Interpretability Indicators [24](25], that is the indicators that

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
The Methods and Approaches of Explainable Artificial Intelligence 5

can be measured and compared, there have been the attempts to formulate those,
preferably in a universal manner. The Axiomatic Explanation Consistency
Framework(25][26] is one of such endeavours. It measures to what degree an
explanation method achieves the objective of attaining explanation consistency,
and is based upon three axioms [3]:

— Identity - Identical objects must have identical explanations.
— Separability - Nonidentical objects cannot have identical explanations.
— Stability - Similar objects must have similar explanations.

2.4 A range of practical issues

Besides the above-mentioned theoretical aspects, there exist a number of other
practical issues. At present, most top performing models are Artificial Neural
Networks (ANN). These work by utilising layers of connected computation units
called neurons [27]. Though each one on its own is only able to solve simple
mathematical problems, together they form complex equations capable of diag-
nosing cancer, for instance [28]. This ability to generate more abstract concepts
based on the simpler ones [29] is what gives Neural Networks their power, but is
also the main reason for why achieving their explainability is a non-trivial task.
There can be thousands or millions of neurons that interact with one another.
Somehow, they are able to form some sort of representations that allow perform-
ing advanced tasks. How can those concepts be grasped, though? And even if
one is able to frame the concept, the question remains of how to present it to
people in an understandable way. Finally, there are also the issues of accuracy
loss and a drastic increase of additional overhead.

3 An overview of xAI taxonomies

In the recent years, many approaches to explainability have been developed.
Many attempts at taxonomising the domain have also been undertaken. One
of those attempts can be found in [30]. A comprehensive and in-depth survey
on xAI can be found in [31], where authors place considerable effort to handle
the formalisms and multidisciplinarity of the field. A brief attempt at a user-
centered taxonomy was placed in [32]. A preliminary taxonomy of human subject
evaluation can be found in [33]. There is also a comprehensive taxonomy of
xAJI presented in [20], which includes the methods for both shallow and Deep
Learning (DL).

To begin with, the main division present within xAI should be pointed out,
i.e., the distinction between the models that inherently have some level of ex-
plainability and the ones that need to utilise external means to achieve it. Arrieta
et al. present further decomposition of the first category based on the domain,
within which the model is transparent [20]. They highlight three main classes:

1. Simulatable models - the models that can be fully comprehended and
simulated by humans,

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:

DOI: 10. 1007/978-3-030-77970-2_1
6 M. Szczepaniski et al.

2. Decomposable models - the models that every part of which, ie., input,
parameter and calculation, can be explained,

3. Algorithmically transparent models - the process that generates the
output can be understood by a man [20].

Generally, linear models, decision trees rule-base systems etc. are inherently
transparent, with the degree varying across the mentioned domains. Neverthe-
less, with the increasing complexity, these explainable properties can be lost. For
example, in case of decision trees, when they get too deep and wide, it becomes
quite difficult to follow the paths that a system uses to generate predictions [34].

Unfortunately, most models do not possess this natural transparency; there-
fore, external methods are needed. Those techniques fall into the wide category of
the post-hoc explanations. They ”’aim at communicating understand-
able information about how an already developed model produces its
predictions for any given input” [20]. In other words, they make opaque
system explainable to some degree.

The post-hoc methods are further split into the model agnostic and model
specific ones. The former means that a method can be used by different Ma-
chine Learning models, while the latter marks those designed to explain specific
algorithms. Of course, those can be divided even further. The authors of [20]
propose to organise the agnostic methods as follows:

— Feature relevance explanation - the techniques based on measuring the
importance that each feature has for the model’s prediction,

— Explanation by simplification - the methods where a new, simpler model
is built. It resembles the original and keeps a similar performance score, but
the level of its complexity has been lowered,

— Visual Explanation - as the name suggests, the algorithms belonging to
this category employ some form of graphical representation to explain an
opaque model.

A good example of an agnostic method is the Local Interpretable Model-
Agnostic Explanation (LIME) [35], which trains an interpretable linear model
around the prediction. It falls into the category of ” explanation by simplification”
and has achieved a significant popularity [1]. Another popular agnostic method
is Shapley Additive exPlanations (SHAP)/36]. It is a game-theory based
framework that calculates an additive feature importance score for each predic-
tion using the Shapley values [20].

As already mentioned, the model specific approaches are designed for par-
ticular algorithms. Although they lose the flexibility offered by the agnostic
approaches, they may allow for a higher level of fidelity and accuracy. All in all,
they were made to leverage the traits of the model they explain. Though the
tools are being searched for which can explain shallow models, such as Sup-
port Vector Machines (SVM), the main focus is on something else. Since
the top performing artificial intelligence systems are usually based on deep learn-
ing, it should be no surprise that the methods designed to explain them attract
the most attention [20]. There is a variety of approaches dedicated to them. It

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
The Methods and Approaches of Explainable Artificial Intelligence 7

should be mentioned though that many agnostic methods prove useful for ex-
plaining various aspects of deep networks, e.g., the SHAP [36] or LIME [20].
Nonetheless, there are the methods that make sense only with ANN. Layer-
wise relevance propagation (LRP) [37] is an example of such a method []].
Founded theoretically on Deep Taylor decomposition, it propagates the output
backwards through the network in order to calculate the impact of the input.
Like in the case of image recognition, it is expected that the pixels representing
the object one wants to detect have a higher score than the others. Of course,
it is not the only one. In addition, there are the attribution methods, such as
Grad-CAM, hybrid approaches, the systems which combine other deep learning
algorithms to automatically generate textual explanation, and many other ways
to achieve explainability of DL systems [20]. The final section of this paper will
present several of them.

4 An overview of xAI Solutions

4.1 xAI Methods

Developers and scientists have been looking for practical solutions that will fulfil
the pressing need for xAI in modern intelligent systems [1]. This search has
ultimately led to the creation of many new algorithms, together with the ways
to use them in practice.

To begin with, there are standalone methods developed that are available to
the community. Those usually take the form of a source code which the developer
can download from the portals like GitHub. In some cases, standard copy-paste
procedures are enough to use them as part of the program. This is a rather
” low-level” approach. When there is the need for more of them, it can quickly
become cumbersome and unpractical; even more so if each one of them has its
own set of dependencies.

4.2 xAI Libraries and Frameworks

One level of abstraction above the code fragments there are modules, libraries
and frameworks. Those provide the practitioners with whole collections of meth-
ods in a single package. iNNvestigate [38] is a good example. This library can be
simply imported using Python’s package manager pip. It allows a developer to
quickly use algorithms such as PatternNet, PatternAttribution [39], and differ-
ent variants of LRP [40]. Another representative for this category is Skater [41].
It provides completely different methods from iNNvestigate, like bfPartial De-
pendence or LIME.

The last example for this category is the AI Explainability 360 Open
Source Toolkit from IBM. It presents itself as one of the best frameworks
currently available for the practitioners. It offers a diverse selection of algorithms
like ProtoDash [42] or Contrastive Explanation Method (CEM) (43), even
improving some of them [15]. Additionally, in contrast to the libraries mentioned

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
8 M. Szczepaniski et al.

earlier, it also provides some metrics to evaluate the quality of the explanation,
though it is still quite limited. All of this is backed up by an extensive amount of
materials, tutorials and guidelines, which makes it easier to start working with

xAl.

4.3 xAlI as Part of the System

A popular alternative for frameworks is designing and implementing solutions
integrated into a specific system. The main benefit of this approach comes with
full customisation, allowing to cater for the specific needs of stakeholders and
their product. Explainability is therefore a natural part of the whole and should
seamlessly integrate with the rest of the solution. On the other hand, the main
disadvantage is the need for additional resources necessary to develop an xAI
module from scratch. Additionally, this solution requires the personnel to have
expert knowledge about the subject. Therefore, it is suggested to follow this path
only if there is a viable reason to do so.

An example from the financial technology market is Flowcast [44]. The solu-
tion offers machine learning products for money lending companies. Smartcredit
is one of them and is supposed to help in making decisions about financing thin-
file small and medium-size enterprises (SMEs), i.e., companies with small
amount of traditional financial data used by banks in classic loan application
process. This often leads to the rejection of such applicants, although some of
them are potential good clients. The creators of the solution claim that this
market offers 540 billion dollars’ worth of financeable opportunities. Therefore,
their system was designed to collect information from non-traditional sources
like transaction data, to help the lender get a better picture of an SME com-
pany and assess the risks more accurately. Their platform supports a selection of
ML algorithms, one of them being a variant of the boosted trees algorithm [45].
As explainability is crucial in the finance sector and the mentioned algorithm is
naturally opaque, they had to find a way to clearly explain system assessments.
Thus, they use SHAP along with Natural Language Processing (NLP) to
generate plain-text sentences explaining the output in layman’s terms. This is
supposed to provide the description of why the system made such a decision,
what must be done to change it and the level of confidence in it. They highlight
it that the risk professionals employing their platform can access up to top ten
reasons why each decision was made. The quality of those explanations is tested
by focus groups comprised of risk management professionals and consumers.

The concluding examples of system with an integrated xAI module come
from the area of cybersecurity. There is work in the domain of xAI geared to-
wards explaining the decisions of Artificial Neural Networks used as an intrusion
detection system. The solution leverages aggregations of decision trees to find the
closest explanations for a classified sample [46] To protect network environments
from unwanted, malevolent activity, intrusion detection systems (IDS) are
deployed. As mentioned earlier, the systems that employ some form of ML have
become very popular. The ones with best performance usually utilise some form
of deep learning. As it was explained in [3], this opaqueness raises concerns and

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
The Methods and Approaches of Explainable Artificial Intelligence 9

fosters lack of trust. This is a serious issue in the field of cybersecurity, where
a wrong decision can lead to dramatic consequences. An expert needs clear un-
derstanding, in order to be able to make the right decisions. The authors of
[6] present a way to help with that. On a sample dataset, they have trained
two deep neural networks to act as IDS. Then, they attached an explainabil-
ity module that uses the earlier-mentioned SHAP algorithm. The explanation
is provided using simple charts that clearly show the features and their contri-
butions. Additionally, the paper introduces a new way to show global relations
between feature values and classes. It still needs extensive testing to prove both
feasibility and resistance to sophisticated types of attacks. As a final note, it
should be clarified that the authors of [6] present their solution as a framework.
In this paper, the framework is treated as a collection of ready-made algorithms
and tools that support some way of developing a piece of software. Therefore,
because this solution would still have to be implemented and integrated into an
IDS by a developer, it was placed in this subsection.

As all of the examples above illustrate, ’rAI as part of the system” is, even
with its shortcomings, a valid and fairly popular approach. However, it is not a
proper solution if one does not have the knowledge and resources necessary to
use xAJ this way. Similarly, this is not the best solution for those who only want
to validate a model or gain some additional insights into the data without a
*deep dive’ into the domain. The last subsection proposes solutions to this issue.

4.4 xAlIas a Service

In this section, a promising way of delivering xAI to the companies, develop-
ers and scientists is discussed. It is called *xAI as a Service” (xAI-S). As
mentioned earlier, implementing the explainable part manually has its unique
benefits. However, in most cases it would need excessive resources and would
not prove to be as worthy in the long run. Following, there are several examples
of zAI lending services.

One company offering such service is called DarwinAI. On their website
[47], they present The Gensynth Platform. It is designed to help developers
build deep learning models faster, by automatic generation of high performance
neural networks that can be deployed in many environments. The fact that it also
offers explainability is even more important from the point of view of this work.
Their materials show that this is achieved by Generative Synthesis. The crux
of it is to use another AI model, which will learn how the observed ANN works
and generate a compact version of it. Thanks to this, a mathematical model
explaining the decision process can be constructed. So far, it has been applied
by companies such as Intel, Nvidia and Audi.

Fiddler Labs also offer their own system that helps to achieve explainability
[48]. However, while the DarwinAI tool seems to focus more on supporting quick
development of deep learning models, Fiddler is all about xAI. While it offers a
way to understand AI predictions using methods such as SHAP or Integrated
Gradients[49], it does not limit itself to them. The official materials highlight
other capabilities of the platform, like continuous monitoring of the deployed

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
10 M. Szczepaniski et al.

models. It can be utilised to detect abnormalities in deployed models or catch
data anomalies by rising settable alerts. The system also investigates feature
relationships, for example by comparing distributions across dataset splits or
explaining performance within a specific subset. Last but not least, it allows to
test ”What-if scenarios” i.e. check how different values of input features impact
model’s decisions [48]. All of that is complemented by the inclusion of human
feedback in the workflow, and modern user interface.

The final example illustrates that even the biggest corporations are devel-
oping an interest in xAI and the possibilities it offers. Google Explainable AI is
a part of the Google Cloud platform and has been released in beta version. It
is a collection of ready-made tools and frameworks, rather than a streamlined
solution, providing a supplement for other products offered by the Google AI
Platform. Nevertheless, some of the solutions, like the What-If Tool, can be
used within a range of environments. Owing to its diverse nature, it is hard to un-
ambiguously classify this whole collection into one category. Nonetheless, these
tools are developed to support an existing development service, so the platform
roughly falls into the same category as Fiddler and Darwin AI. The mentioned
frameworks and tools offer a range of advantages. The mentioned What-If Tool,
for example, allows checking feature attribution, test different scenarios to see
their impact on the model, examine it for fairness, compare it with others and
more; all of that delivered in the form of an interactive dashboard. The official
website presents the full list of features and tools available, along with in-depth
descriptions, guidelines and tutorials [14]. The platform integrates xAI imple-
mentations of Axiomatic Attribution for Deep Networks [49], Sampled Shapley
[50], eXplanation with Ranked Area Integrals (XRAI) [51] and others.

There are of course many more startups, products and frameworks that either
offer or utilise explainability, which are not included in this this section; Rulex
[52], Kyndi [53], H20.ai [54], to name just a few [55].

4.5 Current Initiatives and Research Projects

Apart from business and development solutions mentioned in previous sections,
there are also several research initiatives and projects looking into the future of
Al and xAl.

Obviously, most research projects worked on explainability for image recog-
nition and image retrieval tasks. However, there are projects that touch upon
many other domains, like physics etc.

Explainability is one of the challenges recognized by the SPARTA, a Hori-
zon 2020 cybersecurity pilot project, funded by the European Commission. In
particular, SAFAIR Programme (Secure and Fair AI Systems for Citizens) of
the H2020 SPARTA project focuses on security, explainability, and fairness of
AI/ML systems, especially in the cybersecurity domain [56]. Explainability is
also one of the factors closely interlinked with ELSA (ethical, legal, societal)
activities of SPARTA. Both the SPARTA project and the SAFAIR Programme
have started in 2019, and the results are expected by 2022.

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
The Methods and Approaches of Explainable Artificial Intelligence 11

Explainability in cybersecurity domain is very challenging (not as visually
comprehensible as heatmaps of images), but such aspects are also in the agenda
of SIMARGL (Secure Intelligent Methods for Advanced Recognition of Malware
and Stegomalware) project working on malware and stegomalware detection
mechanisms.

Another H2020 project dealing with the explainability of AI solutions is the
Transparent, Reliable and Unbiased Smart Tool for AI (TRUST). It aims at
creating an AI platform which is going to be trustworthy and collaborative, and
employ explainable by design models and learning models. All the while, the
learning process that is going to be adopted is said to be ”human-centric” and
integrate cognition [57].

In her plenary talk, [58] explored the research field of xAI, used to "overcome
the shortcomings of pure statistical learning” and provide the results in the form
that could be comprehended by human users [58].

5 Conclusions

This paper discusses the concept of xAI and describes some of its noteworthy
solutions.
Explainability is worth being brought to AI models for a variety of reasons:

— Explainability helps to root out the ” Clever Hans” models [1]. An
opaque model is by its nature difficult to debug or verify. In that case, only
the results are visible, not the process. It forces a developer to follow tedious
and inefficient approaches in order to find possible inconsistencies. This slows
down the whole development and makes it unstable, which in turns increases
the risk of obtaining faulty models. However, if the decision process is clear
to the designer, many potential problems immediately become apparent.

— Explainability is a cornerstone of reliability. This statement results
directly from the previous one. Deployed models face challenges such as
Concept Drift (changes in the hidden context that can induce more or less
radical changes in the concept of interest [59]) and Data Decay([60]. These
are caused by the change of data relevance and its dynamics over time. To
alleviate those, both the model and data have to be regularly verified to stay
relevant, and, consequently, reliable.

— Explainability brings trust in the system’s decisions [1]. People will
not use the tools they do not trust. It is especially true when the stakes are
high. A physician deciding about the treatment needs to know the reasons for
the system reaching such a diagnosis in order to verify it and decide whether
they should agree with it or not. Either way, a clear picture is necessary to
make a decision based on AI system’s output.

— Explainability reduces bias and supports fairness [3]. By understand-
ing the principles behind system’s decision, it is possible to identify unwanted
biases that are present in a dataset. This helps to build models that sup-
port our modern ethics, instead of deepening unfair treatment based on race,
gender or orientation.

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
12 M. Szczepaniski et al.

— Explainability allows to gain additional insights into the domain
[1][31]. An explainable system can detect and unveil unknown relations
present in the data. This may lead to new discoveries and studies, making
the transparent AI models valuable for the scientific community.

Raising awareness about AI explainability and implementing it across vari-
ous sectors is still an ongoing process. Even though the questions of explaining
AI models to people without losing their accuracy are not easy, the scientific
community keeps searching for answers. Since xAI enjoys its renaissance, many
new approaches were developed in the recent years [20]. Though a perfect one
does not exist, a lot of them show promise and have already proved to be useful.

We hope, this work may serve as a reference point to understand the various
tools and xAJI solutions/problems better.

Acknowledgment

This work is funded under the SPARTA project, which has received funding
from the European Union’s Horizon 2020 research and innovation programme
under grant agreement No 830892.

References

1. Samek W., Montavon G., Vedaldi A., Hansen L., Miller KR., “Explainable AI: In-
terpreting, Explaining and Visualizing Deep Learning,” Lecture Notes in Computer
Science, vol. 11700, 2019.

2. Miller T., “Machine learning interpretability: A survey on methods and metrics,”
Electronics, vol. 8, 2019.

3. Carvalho D. V., Pereira E. M., Cardoso J. 8., “Explanation in artificial intelligence:
insights from the social sciences,” Artificial Intelligence, vol. 267, 2019.

4. Pawlicki M., Choras M., Kozik R., “Defending network intrusion detection systems
against adversarial evasion attacks,” FGCS, vol. 110, 2020.

5. Choras M., Pawlicki M., Puchalski D., Kozik R., “Machine Learning - the re-
sults are not the only thing that matters! What about security, explainability
and fairness?,” International Conference on Computer Recognition Systems, LNCS,
vol. 12140, 2020.

6. Wang M., Zheng K., Yang Y., Wang X., “An explainable machine learning frame-
work for intrusion detection systems,” [EEE Access, vol. 8, 2020.

7. Vilone G., Longo L., “Explainable Artificial Intelligence: a Systematic Review,”
2020.

8. Xie, N., Ras, G., van Gerven, M., Doran, D. , “Explainable Deep Learning: A Field
Guide for the Uninitiated,” 2020.

9. Stoyanovich, J. and Van Bavel, Jay J., West, Tessa V., “The imperative of inter-
pretable machines,” Nature Machine Intelligence, 2020.

10. Roscher R., Bohn B., Duarte M.F., Garcke J. , “Explainable Machine Learning for
Scientific Insights and Discoveries,” CoRR, 2019.

11. Tjoa E., Guan E., “ A Survey on Explainable Artificial Intelligence: Toward Med-
ical XAI,” IEEE Transactions on Neural Networks and Learning Systems.

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
12.

13.

14.
15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

26.

27.
28.

29.

30.

31.

32.

The Methods and Approaches of Explainable Artificial Intelligence 13

Ghosh A., Kandasamy D., “Interpretable Artificial Intelligence: Why and When,”
American Journal of Roentgenology.

Reyes, M., Meier, R., Pereira, S., Silva, C. A., Dahlweid, F-M., von Tengg-Kobligk,
H., Summers, R. M., Wiest, R., “On the Interpretability of Artificial Intelligence
in Radiology,” Radiology: Artificial Intelligence.

https: //cloud.google.com/explainable-ai.

Arya V., Bellamy R.K.E., Chen P.Y., Dhurandhar A., Hind M., Hoffman S8.C.,
Houde S., Liao Q.V., Luss R., Mojsilovic A., Mourad S., Pedemonte P., Raghaven-
dra R., Richards J., Sattigeri P., Shanmugam K., Singh M., Varshney K.R., Wei
D., Zhang Y., “One explanation does not fit all: A toolkit and taxonomy of ai
explainability techniques,” 2019.

Samhita L., Gross H., “The “Clever Hans Phenomenon” revisited,” Communicative
integrative biology, 2013.

Greene T., “AI Now: Predictive policing systems are flawed because they replicate
and amplify racism,” TVW, 2020.

Asaro, P.M., “AI Ethics in Predictive Policing: From Models of Threat to an Ethics
of Care,” IEEE TSM, 2019.

Wexler R., “When a computer program keeps you in jail: How computers are
harming criminal justice,” New York Times, 13.06.2017.

Arrieta A.B., Diaz-Rodriguez N., Del Ser J., Bennetot A., Tabik S., Barbado A.,
Garcia S., Gil-Lopez S., Molina D., Benjamins R., Chatila R., Herrera F., “Ex-
plainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and
challenges toward responsible AI,” Information Fusion, vol. 58, 2020.

Choras M., Pawlicki M., Puchalski D., Kozik R., “Machine Learning - The Results
Are Not the only Thing that Matters! What About Security, Explainability and
Fairness?,” I[CC'S, vol. 4, 2020.

Gandhi, M., “What exactly is meant by explainability and interpretability of AI?,”
Analytics Vidhya, 2020.

Taylor, M. E., “Intelligibility is a key component to trust in machine learning,”
Borealis AI, 2019.

Doshi-Velez, F., Been K., “Considerations for evaluation and generalization in in-
terpretable machine learning.,” Explainable and Interpretable Models in Computer
Vision and Machine Learning, 2018.

Doshi-Velez, F., Been K., “Towards a rigorous science of interpretable machine
learning.,” arXiv preprint: 1702.08608, 2017.

Honegger, M., “Shedding Light on Black Box Machine Learning Algorithms: Devel-
opment of an Axiomatic Framework to Assess the Quality of Methods that Explain
Individual Predictions.,” arXiv preprint:1808.05054, 2018.

Russel 8., Norvig P. Artificial Intelligence: A Modern Approach, 2010.

Liu S., Zheng H., Feng Y., Li W., “Prostate cancer diagnosis using deep learning
with 3D multiparametricMRI,” Medicallmaging2017: Computer-Aided Diagnosis,
2017.

Goodfellow I., Bengio Y., Courville A. Deep Learning, 2016.

Lipton, Z. C., “The mythos of model interpretability,” Int. Conf. "In Machine
Learning: Workshop on Human Interpretability in Machine Learning”, 2016.
Adadi A., Berrada M., “Peeking Inside the Black-Box: A Survey on Explainable
Artificial Intelligence (XAI),” [CC'S, vol. 6, 2018.

Weina J., Carpendale S., Hamarneh G., Gromala D., “Bridging AI Developers and
End Users: an End-User-Centred Explainable AI Taxonomy and Visual Vocabu-
laries,” [EEE Vis, 2019.

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
34.

35.

36.

37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.

. https://www.fiddler.ai, Accessed: 30.12.2020.
49.

M. Szczepaniski et al.

Chromik, M., Schuessler, M., “A Taxonomy for Human Subject Evaluation of
Black-Box Explanations in XAI,” £xSS-ATEC@ IUI, 2020.

Blanco-Justicia, A., Domingo-Ferrer, J., “Machine learning explainability through
comprehensible decision trees,” Machine Learning and Knowledge Extraction. Lec-
ture Notes in Computer Science, vol. 11713, 2019.

Ribeiro, M., Singh, S., Guestrin, C., “Why should I trust you?: explaining the
predictions of any classifier,” 2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Demonstrations, San Diego, CA.
Lundberg, 5, M., Su-In L., “A unified approach to interpreting model predictions. .,”
Advances in neural information processing systems, 2017.

Bach, S., Binder A., Montavon G., Klauschen F., Miller K.R., Samek W., “On
pixel-wise explanations for non-linear classifier decisions by layer-wise relevance
propagation.,” PloS one 10, vol. 7, 2015.

Alber M., Lapuschkin S., Seegerer P., Hagele M., Schutt K., Montavon G., Samek
W., MullerK., Dahne S., KindermansP., “iNNvestigate neural networks!,” arXiv,
2018.

P.-J. Kindermans, K. T. Schtitt, M. Alber, K.-R. Miller, D. Erhan, B. Kim, and
S. Dahne, “Learning how to explain neural networks: Patternnet and patternattri-
bution,” 2017.

G. Montavon, A. Binder, 8. Lapuschkin, W. Samek, and K.-R. Miller, Layer- Wise
Relevance Propagation: An Overview, pp. 193-209. Cham: Springer International
Publishing, 2019.

https: //github.com/oracle/Skater, Accessed: 30.12.2020.

Gurumoorthy, K.S., Dhurandhar A., Cecchi G, Aggarwal C., “Efficient Data Rep-
resentation by Selecting Prototypes with Importance Weights,” [C’D, IEEE, 2019.
Dhurandhar, A., Chen,P., R. Luss, Tu,C., Ting, P., Shanmugam, K., Das, P., “Ex-
planations based on the missing: Towards contrastive explanations with pertinent
negatives,” Advances in Neural Information Processing Systems, 2018.

https: //flowcast.ai, Accessed: 30.12.2020.

https: //resources.flowcast.ai/resources/big-data-smart-credit-white-paper/, Ac-
cessed: 18.03.2021.

Szczepanski, M., Choras M., Pawlicki M., Kozik R., “Achieving Explainability of
Intrusion Detection System by Hybrid Oracle-Explainer Approach,” L/C'NN, 2020.
https://darwinai.com, Accessed: 30.12.2020.

Sundararajan, M., Taly, A., Yan, Q., “Axiomatic attribution for deep networks,”
arXiv preprint arXtv:1703.01365, 2017.

. Maleki, 8., Tran-Thanh, L., Hines, G., Rahwan, T., Rogers, A., “Bounding the es-

timation error of sampling-based Shapley value approximation,” arXiv:1306.4265.

. Kapishnikov, A., Bolukbasi, T., Viégas, F., Terry, M., “Xrai: Better attributions

through regions,” IEEE International Conference on Computer Vision, 2019.

. https://www.rulex.ai, Accessed: 30.12.2020.

. https://kyndi.com, Accessed: 30.12.2020.

. https://www.h2o.ai, Accessed: 30.12.2020.

. https://www.ventureradar.com, Accessed: 30.12.2020.

. https://www.sparta.eu/programs/safair/, Accessed: 18.03.2021.

. https: //cordis.europa.eu/project/id/952060, Accessed: 30.12.2020.

. Zanni-Merk, C., “On the Need of an Explainable Artificial Intelligence,” 2020.

. Widmer, G., Kubat M., “Learning in the presence of concept drift and hidden

contexts.,” Machine learning 23, vol. 1, 1996.

. https://peterasaro.org/writing/Asaro_PredictivePolicing.pdf, 30.12.2020.

ICCS Camera Ready Version 2021
To cite this paper please use the final published version:
DOT: 10. 1007/978-3-030-77970-2_1
COVER FEATURE HYBRID HUMAN-ARTIFICIAL INTELLIGENCE

| A Research Agenda for
Hybrid Intelligence:
Augmenting Human
Intellect With Collaborative,
Adaptive, Responsible,

    
 

  

   
 

and Explainable Artificial

Intelligence

Zeynep Akata, University of Amsterdam and
University of Tubingen

Dan Balliet, Vrije Universiteit Amsterdam
Maarten de Rijke, University of Amsterdam
Frank Dignum, Utrecht University

Virginia Dignum, TU Delft

Guszti Eiben and Antske Fokkens, Vrije
Universiteit Amsterdam

Digital Object Identifier 10.1109/MC.2020.2996587
Date of current version: 30 July 2020

18 COMPUTER PUBLISHED BY THE IEEE COMPUTER SOCIETY

Davide Grossi, University of Groningen

Koen Hindriks, Vrije Universiteit Amsterdam
Holger Hoos, Leiden University

Hayley Hung and Catholijn Jonker, TU Delft
Christof Monz, University of Amsterdam
Mark Neerincx and Frans Oliehoek, TU Delft
Henry Prakken, Utrecht University

OOI8-SIG2/2OOG2IO2OIEEE
Stefan Schlobach, Vrije Universiteit Amsterdam

TU Delft

Linda van der Gaag, Utrecht University and Dalle

Molle Institute

Frank van Harmelen, Vrije Universiteit Amsterdam

Herke van Hoof, University of Amsterdam

Birna van Riemsdijk and Aimee van Wynsberghe,

Rineke Verbrugge and Bart Verheij, University

of Groningen

Piek Vossen, Vrije Universiteit Amsterdam

Max Welling, University of Amsterdam

We define hybrid intelligence (HI) as the combination of human
and machine intelligence, augmenting human intellect and
capabilities instead of replacing them and achieving goals

that were unreachable by either humans or machines. Hl is an
important new research focus for artificial intelligence, and we
set a research agenda for HI by formulating four challenges.

ver the course of history,

the use of tools has played

a crucial role in enabling

human civilizations, cul-
tures and economies: fire, the wheel,
the printing press, the computer, and
the Internet are just a few of human-
ity’s crucial innovations. Such tools
have augmented human skills and
thought to previously unachievable lev-
els. Over the past several decades, artifi-
cial intelligence (AI) techniques, which
allow humans to “scale up” by providing
increasingly intelligent decision sup-
port, have become the latest addition to
this toolset. Until now, however, these
tools have been mostly used by experts.
Hybrid intelligence (HI) can go well
beyond this by creating systems that
operate as mixed teams, where humans
and machines cooperate synergistically,
proactively, and purposefully to achieve
shared goals, showing Al's potential for
amplifying instead of replacing human
intelligence. This perspective on Al as

HI is critical to our future understand-
ing of Al as a way to augment human
intellect as well as to our ability to apply
intelligent systems in areas of crucial
importance to society.

Contemporary societies face prob-
lems that have a weight and scale novel
to humanity, such as global pandemics,
resource scarcity, environmental con-
servation, climate change, and main-
taining democratic institutions. To
solve these problems, humans need
help to overcome some of their limita-
tions and cognitive biases: poor han-
dling of probabilities, entrenchment,
short termism, confirmation bias, func-
tional fixedness, stereotypes, in-group
favoritism, and so forth. We need help
from intelligent machines that chal-
lenge our thinking and support our
decision making, but we do not want
to be ruled by machines and their deci-
sions, nor dowe want to supplanthuman
biases with those of machines. Instead,
we need cooperative problem-solving

approaches in which machines and
humans contribute through a collab-
orative conversation, where machines
engage with us, explain their reason-
ing, behave responsibly, and learn from
their mistakes.

Al systems tend to be “idiots savants,”
reaching or exceeding the performance
of human experts in a very narrow
range. There is a danger that users (be
they individuals or organizations) will
overestimate the range of expertise
of an automated system and deploy it
for tasks at which it is not competent,
with potentially catastrophic conse-
quences. Human experts are needed
in the loop to ensure that this does not
happen. This is an urgent problem; at
present, there are deployed AI systems
that were not designed with societal
values such as fairness, accountability,
and transparency in mind. This con-
tributes to today’s problems of “fake
news,” Facebook messages leading to
ethnic and religious violence, and the

AUGUST 2020 19
HYBRID HUMAN-ARTIFICIAL INTELLIGENCE

large-scale manipulation of elections.
This lack of alignment with human val-
ues is impacting us more frequently.
Now that AI technologies affect our
everyday lives at an ever-increasing
pace, there is a greater need for Al sys-
tems that work synergistically with
humans rather than ones that simply
replace them. Thought leaders in Al
increasingly share the conviction that,
for Al systems to augment our abilities
and compensate for our weaknesses,
we need anew understanding of Al that
takes humans and humanity explicitly
into account. It is better to view Al sys-
tems not as “thinking machines” but
as cognitive prostheses that can help
humans think and act better.”

WHAT IS HYBRID
INTELLIGENCE?
We define HI as the combination of hu-
man and machine intelligence, aug-
menting human intellect and capabili-
ties instead of replacing them, to make
meaningful decisions, perform appro-
priate actions, and achieve goals that
were unreachable by either humans or
machines alone. HI requires interaction
between artificial intelligent agents
and humans, taking human exper-
tise and intentionality into account,
together with ethical, legal, and socie-
tal (ELS) considerations. The main HI
research challenge is as follows: how to
build adaptive intelligent systems that
augment rather than replace human
intelligence, leverage our strengths,
and compensate for our weaknesses
while taking into account ethical, legal,
and societal considerations.
Developing HI requires fundamen-
tally new solutions to core research
problems in AI. Modern Al technology
surpasses humans in many pattern
recognition, machine learning, rea-
soning, and optimization tasks, but it

20 COMPUTER

falls short on general world knowledge;
common sense; and the human capabil-
ities of collaboration, adaptability, and
responsibility in terms of norms and
values and explanation. Humans, on
the other hand, excel in collaboration,
flexibly adapting to changing circum-
stances during the execution of a task.
An essential element in our collabora-
tion is the capability to explain motiva-
tions, actions, and results. And humans
always operate in a setting where norms
and values (often implicitly) delineate
which goals and actions are desirable or
even permissible. We therefore unpack
the challenge of building HI systems
into four research challenges:

» Collaborative HI: How do we
develop Al systems that work in
synergy with humans?

) Adaptive HI: How can these
systems learn from and adapt to
humans and their environment?

) Responsible HI: How do we ensure
that they behave ethically and
responsibly?

) Explainable HI: How can Al
systems and humans share and
explain their awareness, goals,
and strategies?

In the following sections, we dis-
cuss the state of the art for each of these
challenges, leading to a set of research
questions to be addressed to achieve
hybrid intelligent systems as envis-
aged previously.

COLLABORATIVE HI

State of the art

Collaboration in human teams is vital,
pooling different skills to solve more dif
ficult problems than any of the members
could alone. The skills that computer
systems excel in are different from those

of humans. A key question is therefore
how to best exploit this complementar-
ity in human-machine collaboration.
Early results in successful complemen-
tary human-machine collaboration in
cognitive tasks are known from negoti-
ation tasks, planning, behavior change
support systems, and “centaur” chess.
There are key challenges when promot-
ing machines from tools to partners: a
computational understanding of human
actors, a theory of mind, an understand-
ing of joint actions in teams, and social
norms such as reciprocity, which are
crucialin such teamwork. Hybrid intelli-
gent machines will need to both perceive
social behavior by collaborators and
communicate with their collaborators
using multiple modalities. Our notion of
collaborative HI goes beyond the estab-
lished notions of human-in-the-loop
machine learning? or interactive Al by
aiming for reciprocity between human
and computer agents, as discussedin the
following sections.

Understanding human actors. To
exploit skill differences, we need mod-
els that make machines aware of these
differences and enable them to proac-
tively provide support by exploiting
skill complementarity. In addition,
machines can help prevent common
human biases and limitations, such
as a bias toward short-term rewards,
a confirmation bias, entrenchment,
in-group favoritism, a limited atten-
tion span, and limited short-term
memory. Solutions can build on the
substantial research of how to miti-
gate cognitive biases.*

Theory of mind. Maintaining the
beliefs, goals, and other mental atti-
tudes of other people ina theory of mind
(ToM) is essential for effective coopera-
tion. In complex social interactions,

WWW.COMPUTER.ORG/COMPUTER
people also need to apply a second-or-
der ToM (“She thinks that I plan to go
right”). There is substantial theory on
people’s use of and difficulties with
ToM. A relatively unexplored area
is the use of recursive ToM in hybrid
groups containing humans, robots, and
software agents, allowing an agent to
recursively apply a ToM to detect anom-
alies in its state of mind. de Weerd
et al.? show how second-order ToM is
beneficial in competitive, cooperative,
and mixed-motive situations and how
software agents of different ToM levels
can support humans to achieve better
negotiation outcomes.

Teamwork, joint actions, plans, and
tasks. In multiagent systems (MASs)},
substantial work has been performed
on distributing tasks and monitoring
plan progression. Frequently used sys-
tems suchas TAEMS consider only soft-
ware agent teams, not hybrid teams of
humans and agents. Thus, many results
might not carry over to hybrid teams
because humans typically react differ-
ently from agents in unexpected situa-
tions and are not likely to accept orders
from agents in all circumstances and
so on. Recent work on an agreement
framework proves to support human-
agent teams when they dynamically
adapt their task allocation and coor-
dination. Cooperation and teamwork
have been extensively studied in eco-
nomic disciplines and specifically in
game theory, including within MASs.®
Game theory has already had several
high-impact ramifications in the MAS
field and will provide ways to inform
artificial agents in hybrid teams of the
tradeoffs involved in collaborative tasks
and how to best manage them.

Reciprocity, social norms, and cul-
ture. The social and biological sciences

have converged on a common under-
standing that kinship, direct reciproc-
ity, indirect reciprocity, and the social
learning of norms can explain why and
how humans cooperate.” Further, peo-
ple can quickly and efficiently interpret
social situations along various parame-
ters (for example, mutual dependence,

communication, human-computer
interfacing, and other component
technologies, such as facial expres-
sion analysis and gesture detection,
that show the importance of multi-
modal interaction for collaboration.”
The same can be said about multi-
modal dialogue systems and, more

RECENT WORK ON AN AGREEMENT
FRAMEWORK PROVES TO SUPPORT
HUMAN-AGENT TEAMS WHEN THEY
DYNAMICALLY ADAPT THEIR TASK
ALLOCATION AND COORDINATION.

power, and conflict), and this can
shape their willingness to cooperate.
Computational theories of reciprocity
show that the effect of reciprocity has
similar effects on artificial agents. For
such agents to interact with humans
in ways that promote collaboration, HI
systems should be aware of these traits
in humans and use this knowledge to
engage in actions that can positively
influence human collaboration. Ini-
tial work has been done to incorporate
social norms in agents and develop
new architectures for social agents.
That designing for interdependencies
and coactivity makes the system more
effective was proved by the success of
the Florida Institute for Human and
Machine Cognition team that secured
second place in the DARPA challenge,®
where its team capabilities and interac-
tion design were based on the coactive
design method.

Multimodal interaction. There is a long
tradition of research on multimodal

recently, chatbot systems using neu-
ral networks. In all these studies, the
assumption is made that systems pro-
cess signals correctly. They also con-
sider tasks separately and not systems
as a whole. There are few systems that
combine natural language commu-
nication and perception for the pur-
pose of task-oriented learning. She
and Chai! describe a system that is
instructed through multimodal inter-
action to perform a physical task. This
system deals with the uncertainties of
perceived sensor data and interpreta-
tion of the instructions, but it does not
assume that humans and AI systems
work together and is limited to very
basic physical actions.

Machine perception of social and
affective behavior. In the growing
branch of multimodal interaction con-
cerned with human social behavior,
the fields of affective computing and
social signal processing have made
great leaps with respect to the machine

AUGUST 2020 2i
HYBRID HUMAN-ARTIFICIAL INTELLIGENCE

perception, modeling, and synthesis of
social cues; individual and social con-
structs; and emotion. There has been
a paradigm shift in research on the
perception of human behavior, going
away from training machine learn-
ing models using data collected in
the lab to settings in controlled, real-
life settings. However, moving from
controlled laboratory studies to real-
life settings requires a fundamental
change in experimental approaches.
As argued by Hung et al.,"! we need to
transition from expecting clearly vis-
ible video footage of frontal faces and
use other sensing modalities to exploit
the arsenal of social signals that are
emitted by humans.

Research questions
Theaforementioned state ofthe art leads
to the following research questions for
collaboration in hybrid systems:

~

What are the appropriate models
for negotiation, agreements,
planning, and delegation in
hybrid teams?

How cana computational ToM
(based on social and psychologi-
cal concepts) be designed to plan
collaboration between humans
and artificial agents?

How can HI exploit experience
sharing for the purpose of estab-
lishing common ground, resolv-
ing uncertainties and conflicts,
adjusting tasks and goals, and
correcting actions?

Which specific challenges and
advantages arise when groups of
humans and agents collaborate,
given the complementarities in
their skills and capabilities?
How can multimodal messages,
expressions, gestures, and semi-
or unstructured representations

~

~

~

~

22 COMPUTER

be understood and generated for
the purpose of collaboration?

ADAPTIVE HI

In HI settings, artificial and human
agents work together in complex envi-
ronments. Such environments are sel-
dom static: team composition and tasks
can change, interpersonal relations
evolve, preferences can shift, and exter-
nal conditions (for example, available
resources and environment) can vary
over time. Thus, competences cannot
be fixed before deployment, and agents
will have to adapt and learn during
operation. As such, the ability of HI sys-
tems to adapt or learn is a prerequisite
not only to perform well but to func-
tion at all. To accomplish such adap-
tivity, agents need to deploy machine
learning techniques to learn from data,
experiences, and dialogues with other
agents (human or artificial).

State of the art

There is an inherent tension between
the adaptive nature of HI systems and
the desire for their safety and reliabil-
ity. Constraints on the adaptivity of a
system are needed to avoid adaptations
that are undesirable from the point of
view of safety, either for the agent or
the environment, or from the stand-
point ofethical and social acceptability.
Such constraints may be encoded in the
reward/loss functions of the learning
system, symbolically encoded, or imple-
mented through the modification of the
adaptive exploration process. Highly
adaptive systems also pose a challenge
to the transparency and explainability
of a system’s actions or advice. Data,
settings, concepts, and competences all
interact in the decision-making process.
The system's architecture thus needs to
keep track of all these changes to trace
back why a specific decision was made

at a specific point in time. Furthermore,
these systems must not only keep track
of such information but also be able to
effectively communicate it to a variety
of users to elicit necessary feedback.

Several research directions within
Al have focused on learning models
that can adapt to either changing users,
tasks, resources, or environments. For
instance, multitask learning aims to
find models for a range of tasks. Trans-
fer learning approaches try to adapt
learned models from source tasks to
target tasks that could differ in either
environment or objective. A growing
body of work has also studied the use
of metalearning for rapid adaptation.
Metalearning methods attempt to learn
a solution strategy from a collection of
previously solved tasks to, for example,
discover optimal exploration strate-
gies. Adapting to the changing pref
erences of the user can be addressed
using multiobjective models and
methods, which model different
reward functions for different desir-
able features of a solution. Recently,
so-called automated machine learning
methods have been developed to select
and optimize learning algorithms for
specific tasks or data sets.

Various aspects and subproblems
of the challenge of adaptive HI have
already been addressed in the liter-
ature. For example, to handle user
preferences that change over time,
different preference-elicitation strat-
egies have been compared, and multi-
objective optimization has been used
to adapt an information retrieval sys-
tem to the current user preferences.
Incomplete knowledge about the pref
erences of negotiation parties has also
been used to inform multiattribute
negotiation systems. However, none
of these approaches combine tech-
niques for learning from data streams

WWW.COMPUTER.ORG/COMPUTER
or dialogues. Furthermore, there is no
explicit strategic reasoning on what
the best learning techniques would be,
given the task and circumstances. The
subproblem of adaptivity to changes in
the environment has been studied in
the form of robot controllers that adapt
depending on the environmental con-
ditions, and even the morphology of
robots can be adapted to the environ-
ment. Finally, fully automated proce-
dures have been developed for selecting
and configuring algorithms for a given
supervised machine learning task”
and are rapidly gaining traction.

Research questions

The state of the art discussed in the
previous section leads to the following
research questions for adaptivity in
hybrid systems:

~

How can interaction ina mixed
group of agents (humans and
machines) be used to improve
learning systems, for example,
by communicating intent and
asking for and handling com-
plex feedback?

How can learning systems respect
the societal, legal, ethical, safety,
and resource constraints that
might be expressed symbolically?
How can learning systems
accommodate changes in user
preferences, environments, tasks,
and available resources without
having to completely relearn each
time something changes?

How can the learning mecha-
nism itself be adapted to improve
efficiency and effectiveness in
highly dynamic HI settings based
on task experience as wellas
human guidance?

How can the adaptivity of
machine learning techniques

~

~

~

~

be integrated with the precision
and interpretability of symbolic
knowledge representation
and reasoning?

RESPONSIBLE HI

Modern AI techniques often put users
in situations in which information
about their decisions is unknown or
unclear, and the ability to dispute a
decision is not possible. Advances in
Alincreasingly lead to concerns about
the ability of such systems to behave
according to legal constraints and
moral values. Models and techniques
are needed to evaluate, analyze, and
design AI systems with the capability to
reason about and act according to legal
constraints and moral values as well as
to understand the consequences of their
decisions. The urgency of these ques-
tions is increasingly acknowledged by
researchers and policy makers alike, as
shown from recent reports by the IEEE
Ethically Aligned Design of Autono-
mous Systems; the United Nations Edu-
cational, Scientific and Cultural Orga-
nization; the French government; the
U.K. House of Lords; and the European
Commission. In the following sections,
we describe a dual approach for dealing
with the challenges concerning legal
and ethical HI systems.

State of the art

Ethical reasoning about HI systems.
Where it concerns the legal and reg-
ulatory governance of HI systems,
current research focuses on whether
existing legal systems can deal with
the consequences of introducing arti-
ficial systems. However, the liabil-
ity of and for any (semijautonomous
system remains a challenge, requir-
ing a better understanding between
lawyers and computer scientists

of concepts such as legal person-
hood (which does not require moral
agency), human autonomy (which
does not stand in the way of strict lia-
bility), and machine autonomy (which
does not imply self-consciousness, let
alone moral agency).

Many different solutions have been
developed and discussed: from strict
liability for manufacturers, to revers-
ing the burden of proof, to compulsory
certification or automated compensa-
tion in the case of smart contracts. This
relates to the position of Al systems: are
they tools or (anthropocentric) moral
entities with moral patience and dis-
tribution of responsibility? To ensure
responsibility, deliberation should ide-
ally include a grounding in moral con-
cepts, allowing for explanations based
in and coordinated over values (such
as privacy), social norms and relation-
ships, commitments, habits, motives,
and goals. Underlying all of these is
the need to analyze the social, ethical,
and legal characteristics ofthe domain.
The “design for values” approaches!
and methods used to identify and align
the possibly conflicting values of all
stakeholders“ are well-known can-
didates for these tasks. Translating
abstract values to more concrete design
requirements is an important area
where more research is needed to make
these approaches effective in designing
responsible HI.

Ethical reasoning by HI systems.
Ethical reasoning is an even more con-
troversial issue. When creating artificial
moral agents, that is, machines that
are embedded with ethical reasoning
capabilities, the following questions
arise: Can machines comprehend the
world of ethics? Which ethics should
be programmed? Can machines be
assigned moral roles or capacities?

AUGUST 2020 23
HYBRID HUMAN-ARTIFICIAL INTELLIGENCE

Should machines be made accountable
or responsible for consequences? The
methods and tools used to design the
ethical behavior of intelligent agents
are either descriptive or focus on
modeling moral reasoning as a direct
translation of some well-known moral
theory, modeling moral agency in a
general way, or designing an ethical
agent architecture. Other approaches
take a fundamentally interactive
approach to normative reasoning by
HI systems, allowing users to express
their norms and values to the system at
runtime. Ethical decision making then
emerges from the resulting human-
machine interaction. This is motivated
by the observation that, in particular
for personal and intimate technolo-
gies, the choice of how to support a per-
sonis highly context dependent.

On the other hand, research in Al
and the law on artificial legal reasoning
is reasonably well developed. Deduc-
tive techniques have been practically
successful, especially in the applica-
tion of knowledge-based systems in
the large-scale processing of adminis-
trative law, such as social benefit law
and tax law, and, more recently, for
legal advice and regulatory compli-
ance. Such systems apply computa-
tional representations of legislation to
the facts as interpreted by the human
user. However, such systems often suf
fer from the well-known “knowledge
acquisition bottleneck,” which has
proved a major barrier to the practical
exploitation of intelligent techniques
in many domains. The recent success
of deep learning and natural language
processing applied to huge corpora of
unstructured legal information may
provide opportunities, but employing
them in the right way to obtain the
necessary knowledge to overcome this
barrier is highly challenging. Finally,

24 COMPUTER

most approaches to AI and the law and
Al and ethics do not clearly take the
collective and distributed dimension
of interaction into account. Work on
norms and institutions in multiagent
systems~" can be used to prove that
specific rules of behavior are observed
when making decisions.

Research questions
The aforementioned state of the art leads
to the following research questions:

~

How can ELS considerations be
included in the HI development
process (ethics in design)?

What is the best way to verify the
agent's architecture and behav-
ior to prove their ethical “scope”
(ethics in design)?

What is the best way to measure
ELS performance and compare
designed versus learning sys-
tems (ethics in design)?

What are the ELS concerns
around the development of sys-
tems that can reason about ELS
consequences of their decisions
and actions (ethics by design)?
Which methodology can ensure
ELS alignment during the
design, development, and use of
ELS-aware HI systems (ethics
by design)?

What new computational tech-
niques are required for ELS in
the case of HI systems where
humans and artificial agents
work together?

~

~

~

~

~

EXPLAINABLE HI

People look for explanations to improve
their understanding of someone or
something so that they can derive a
stable model to be used for prediction
and control. By building more trans-
parent, interpretable, or explainable

artificial agents, human agents will be
better equipped to understand, trust,
and work with intelligent agents. A
recent trend is to distinguish between
interpretation and explanation. In the
case of interpretation, abstract con-
cepts are translated into insights that
are useful for domain knowledge (for
example, identifying correlations
between layers in a neural network
for language analysis and linguistic
knowledge). An explanation provides
information that gives insights to users
as to how a model came to a decision or
interpretation. Models of how humans
explain decisions and behavior can be
used to design and implement intelli-
gent agents that provide explanations,
including how people employ biases
and social expectations when they gen-
erate and evaluate an explanation.

de Graaf and Malle! argue that
the anthropomorphization of agents
causes users to expect explanations uti-
lizing the same conceptual framework
used to explain human behaviors. This
suggests a focus on everyday explana-
tions, that is, explanations of why par-
ticular facts (events, properties, deci-
sions, and so on) occurred rather than
of more general relationships, such
as in a scientific explanation. Trust is
lost when users cannot understand
observed behavior or decisions, which
necessitates effective solutions that
must combine AI with insights from
the social sciences and human-com-
puter interactions.

Everyday explanations are contras-
tive; people do not ask why an event
happened but rather why it happened
instead of another event. Moreover,
explanations are selective (in a biased
manner); people rarely expect a com-
plete causal chain of events as expla-
nation. Humans are adept at selecting
one or two causes from a large chain of

WWW.COMPUTER.ORG/COMPUTER
them to be the explanation; however,
this selection is influenced by certain
cognitive biases. In addition, expla-
nations are social, that is, they are a
transfer of knowledge as part of an
interaction, and thus are presented rel-
ative to the explainer’s beliefs about the
explainee’s beliefs.

State of the art

Al has a long history of work on expla-
nation. In early work on expert sys-
tems, users rated the ability to explain
decisions as the most desirable fea-
ture of a system design to assist in
decision making. Studies consistently
show that explanations significantly
increase users’ trust as well as their
ability to correctly assess whether an
algorithmic decision is accurate. The
need for explaining the decisions of
expert systems was discussed as early
as the 1970s, with early work already
stressing the importance of explana-
tions that are not merely traces but
also contain justifications. Lacave
and Diez!® survey methods of expla-
nation for Bayesian networks and
distinguish between the reasoning,
model, and evidence for the decision.
Recommender systems have long had
facilities to produce justifications to
help users decide whether to follow
a recommendation.

Studies from the early 2000s show
that users are much more satisfied with
systems that contain some form of jus-
tification. Early work on explanations
in machine learning focused on visu-
alizing predictions to support experts
in assessing models. This line of work
continues to this day, for example, with
techniques for producing visualizations
of the hidden states of neural networks.
Another line of work on explainability
in machine learning develops models
that are intrinsically interpretable and

can be explained through reasoning,
such as decision lists or trees. Other
approaches have created sparse mod-
els via feature selection or extraction to
optimize interpretability.

Today, considerable work is focused
on interpreting and explaining the
predictions of complex (“black box”)
models. Methods for improving the
interpretability of neural networks
aim at identifying what information
is captured in various layers of the
neural network. Diagnostic prob-
ing methods, for instance, inves-
tigate which properties can be pre-
dicted from individual layers of a
neural network by testing whether
these properties can be predicted by
a regression model. These methods
have shown, for example, that lower
layers of models used for interpreting
natural language perform reasonably
well on syntactic categories such as
part-ofspeech tasks whereas higher
layers are more successful for more
semantic-oriented properties.

Correlation-based methods such as
singular value canonical correlation
analysis and representation similarity
analysis can be used to identify cor-
relations between layers in different
models. Here, the inner layers ofa more
complex model under investigation are
typically compared to the output layer
of a model trained on a more basic task
that identifies information likely to be
relevant for the complex task as well.
Examples of methods that support
explanation of the output of a neural
network include layerwise relevance
propagation, which uses the gradients
of the network to determine the rele-
vance of previously seen input. Con-
textual decomposition, on the other
hand, computes how information from
a specific input propagates throughout
the model. The insights provided by

these methods help identify how the
model arrives at specific decisions and
are thus typical examples of explana-
tory features.

Previously, many studies that focused
on the explainability of machine learn-
ing algorithms were conducted from
a human-computer interaction angle,
that is, questions such as how users
interact with the system and how
explanations can help with this are
asked. These studies do not focus on
how to construct faithful explanations
to describe the underlying decisions of
the algorithm. Recently, the focus has
shifted toward 1) describing the train-
ing process, 2) explaining the outcomes
and the relationship to the training
material, and 3) the underlying algo-
rithm. As to the first, Ross et al.” use
the gradients of the output probabil-
ity of a model with respect to the input
to define feature importance in a pre-
dictive model, but this is restricted
to differentiable models. Concerning
the second, Koh and Liang!® deal with
finding the most influential training
objects so as to make a model’s pre-
diction more understandable. And
concerning the third, Ribeiro et al.
introduce LIME, a method used to
locally explain the classifications of
any classifier.

Research questions

The state of the art described in the
previous section leads to the following
research questions for explainability in
hybrid systems:

)» How can shared representations
be built and used as the basis for
explanations, covering both the
external world and the internal
problem-solving process?

>» What are the different types
of explanations that make the

AUGUST 2020 25
HYBRID HUMAN-ARTIFICIAL INTELLIGENCE

decision-making process more
transparent and understandable?
How can explanations be com-
municated to users such that
they improve the user’s trust and
leads to a successful agent-user
collaboration?

How can explanations be person-
alized so that they align with the
users’ needs and capabilities?
How can the quality and strength
ofthe explanations be evaluated?

~

~

~

EXAMPLE APPLICATIONS

OF HI

HI techniques can be applied across
many domains, and we expect them
to bring major economic and societal
benefits to those applications. In the
following sections, we outline three
potential scenarios that illustrate the
use of HI (namely, health care, educa-
tion, and science) in demonstrating
its potential, and we direct the inter-
ested reader to additional sources for
more details. Although implementa-
tions of all of these scenarios have been
tested, HI is a new research focus, and
the results described are preliminary
examples of what future HI systems
may look like.

Education. A child with learning diffi-
culties is supported by a team in which
the child’s remedial teacher, an edu-
cational therapist, and a Nao robot
collaborate. Together, they design a
targeted learning program, monitor
progress, and provide encouragement.
The robot combines expertise from
the human team members with its
own observations and gives advice on
possible adjustments to the program.
Interacting with the Nao robot helps
the child to stay focused and have fun
for a longer period of time. (Visit www
robotsindeklas.nl for an early example

26 COMPUTER

of how robots can be deployed in
the classroom.)

Health care. A teenage leukemia
patient is accompanied 24/7 by a robot
dog during multiple prolonged stays in
the hospital. A large medical team col-
laborates with this HI agent to answer
the patient’s questions. Simple ques-
tions, for example, on diet and daily
schedule, are autonomously answered
by the embodied agent. More complex
medical questions are routed to med-
ical staff members according to their
medical discipline, available knowl-
edge, and rapport with the patient. The
dog explains the inevitable medical ter-
minology, remembering what has been
explained before. It monitors the teen-
ager’s mood and advises the specialists
on the patient’s psychological well-be-
ing. (Visit https://goo.gl/CNN8iM for
an early example of how robots can
support children during long-term
hospital stays.)

Science. A scientist in a commercial
pharmaceutical lab is investigating a
chemical compound expected to have
an inhibitory effect on neurodegener-
ation. Overwhelmed by the enormous
amounts of data available online, the
scientist turns to the lab’s HI virtual
assistant. Data volume is not a problem
for this assistant, who searches through
dozens of databases, scans the recent
literature, and fires off a few emails to
authors of relevant papers while mak-
ing sure not to include scientists who
work at competing big pharma compa-
nies and consulting with the HI system
of a sister lab in China. The scientist
and the HI agent analyze the findings
and conclude that the compound has
been investigated before and failed to
show the required inhibitory activity.
Thanks to HI, all ofthis could be done in

a day rather than weeks. (Visit https://
goo.gl/CajqnM for an early example of
our work.}

Based on these case studies, we are
formulating generalizable design pat-
terns that capture reusable patterns
in both the HI architecture as well as
reusable interaction patterns with
these systems. For example, Ligthartet
al.2° identify five interaction patterns
for the “getting acquainted” phase of
an HI system, including open-ended
and closed questions and prechoreo-
graphed turn taking. An evaluation of
75 8-11-year-old children shows sub-
stantially different efficacy between
the various behaviors of the HI sys-
tem. Similarly, van Harmelen and Ten
Teije?! describe how a large number
of hybrid system architectures can
be captured in a limited number of
design patterns.

n this article, we argued that Al

research should include the quest

for systems that collaborate with
people instead of focusing mainly
on systems that replace people. We
defined the notion of HI and formu-
lated the main research challenges
to be faced. We identified four cen-
tral properties that are required for
such hybrid intelligent systems: col-
laborative, adaptive, responsible, and
explainable. For each of these, we dis-
cussed the state of the art and formu-
lated a number of key research ques-
tions to be addressed. We also briefly
illustrated the use of hybrid intelli-
gent systems in three example appli-
cation scenarios. fl

REFERENCES
1. S.Kambhampati, Challenges of
human-aware AI systems. 2019.
[Online]. Available: arXiv:1910.07089

WWW.COMPUTER.ORG/COMPUTER
 

 

ABOUT THE AUTHORS

ZEYNEP AKATA is a professor of computer science at the Uni-
versity of Tubingen. Contact her at zeynepakata@gmail.com.

DAN BALLIET is a professor of experimental and applied psy-
chology and the head of the Amsterdam Cooperation Lab at
the Vrije Universiteit Amsterdam. Contact him at d.p.balliet@
vu.nl.

MAARTEN DE RIJKE is a professor of artificial intelligence at
the University of Amsterdam. Contact him at derijke@uva.nl.

FRANK DIGNUM isa professor of socially aware artificial intel-
ligence at Umea University. Contact him at f.p.m.dignum@
uu.nl.

VIRGINIA DIGNUN is a professor at Umea University. Contact
her at mv.dignum@tudelft.nl.

GUSZTI EIBEN is a professor of artificial intelligence at the
Vrije Universiteit Amsterdam. Contact him at a.e.eiben@vu.nl.

ANTSKE FOKKENS is an assistant professor at the Vrije Uni-
versiteit Amsterdam. Contact her at antske.fokkens@vu.nl.

DAVIDE GROSSI is an associate professor of artificial intelli-
gence at the University of Groningen. Contact him at d.grossi@
rug.ni.

KOEN HINDRIKS is a professor of social artificial intelli-
gence at the Vrije Universiteit Amsterdam. Contact him at kv
-hindriks@vu.nl.

HOLGER HOOS is a professor of machine learning at Leiden
University. Contact him at hh@liacs.nl.

HAYLEY HUNG is an associate professor at the Technical Uni-
versity Delft. Contact her at HHung@tudelft.nl.

CATHOLIJN JONKER is a professor of interactive intelligence
at the Technical University Delft. Contact her at c.m.jonker@
tudelft.nl.

CHRISTOF MON is an associate professor at the University
of Amsterdam. Contact him at c.monz@uva.nl.

MARK NEERINCX is a professor of human-centered computing at
the Technical University Delft. Contact him at mark.neerincx@

tno.nl and m.a.neerincx@tudelft.nl.

FRANS OLIEHOEK is an associate professor at the Technical
University Delft. Contact him at f.a.cliehoek@tudelft.nl.

HENRY PRAKKEN is a professor of legal informatics at the
University of Groningen. Contact him at h.prakken@uu.n.

STEFAN SCHLOBACH is an associate professor at the Vrije
Universiteit Amsterdam. Contact him at k.s.schlobach@vu.nl.

LINDA VAN DER GAAG is a senior researcher at the Dalle
Molle Institute, Lugano, Switzerland. Contact her at I.c.vander
Gaag@uu.nl.

FRANK VAN HARMELEN is a professor of artificial intelligence
at the Vrije Universiteit Amsterdam. Contact him at frank.van

-harmelen@vu.nl.

HERKE VAN HOOF is an assistant professor at the University
of Amsterdam. Contact him at h.c.vanhoof@uva.nl.

BIRNA VAN RIEMSDIJK is an associate professor at the
University of Twente. Contact her at m.b.vanriemsdijk@

utwente.nl.

AIMEE VAN WYNSBERGHE is an associate professor of eth-
ics and technology at the Technology University Delft. Contact
her at A.L.Robbins-vanWynsberghe@tudelft.nl.

RINEKE VERBRUGGE is a professor of logic and cognition at
the University of Groningen. Contact her at I.c.verbrugge@rug.nl.

BART VERHEIJ is a professor of artificial intelligence and argu-
mentation at the University of Groningen. Contact him at bart

verheij@rug.nl.

PIEK VOSSEN is a professor of computational lexicology at the
Vrije Universiteit Amsterdam. Contact him at piek.vossen@vu.nl.

MAX WELLING is a professor of machine learning at the Uni-
versity of Amsterdam. Contact him at m.welling@uva.nl.

 

 

AUGUST 2020

27
HYBRID HUMAN-ARTIFICIAL INTELLIGENCE

2.

28

J. Guszcza, “Smarter together:
Why artificial intelligence needs
human-centered design,” Deloitte,
London, no. 22, 2018. [Online].
Available: https://www2.deloitte

.com/content/dam/insights/us/ 10.

articles/4214_Smarter-together/
DI_Smarter-together.pdf

. §S.Amershi, M. Cakmak, W.B. Knox,

and T. Kulesza, “Power to the people:
The role of humans in interactive
machine learning,” AI Mag., vol. 35,
no. 4, pp. 105-120, 2014. doi: 10.1609/

aimag.v35i4.2513. 11.
. M.B.Cook andH. S. Smallman,

“Human factors of the confirma-
tion bias in intelligence analysis:
Decision support from graphical
evidence landscapes,” Hum. Factors,
vol. 50, no. 5, pp. 745-754, 2008.
doi: 10.1518/001872008X354183.

. H. de Weerd,R. Verbrugge, and B. 12.

Verheij, “How much does it help to
know what she knows you know?
Anagent-based simulation study,”
Artif. Intell., vol. 199-200, pp. 67-92,
June-July 2013. doi: 10.1016/j.

artint.2013.05.004. 13.
. D. Grossi and P. Turrini, “Dependence

in games and dependence games,”
J. Auton. Agents Multi-Agent Syst.,
vol. 25, no. 2, pp. 284-312, 2012. doi:
10.1007/s10458-011-9176-3.

. A.Romano and D. Balliet, “Reciproc-

ity outperforms conformity to pro-
mote cooperation,” Psychol. Sci.,

vol. 28, no. 10, pp. 1490-1502, 2017. 14.

doi: 10.1177/0956797617714828.

. M. Johnson etal., “Team IHMC’s les-

sons learned from the DARPA robot-
ics challenge trials,” J. Field Robot.,
vol. 32, no. 2, pp. 192-208, 2015. doi:
10.1002/rob.21571.

. I. Rauschert, P. Agrawal, R. 15.

Sharma, S. Fuhrmann, I. Brewer,
and A. MacEachren, “Designing a
human-centered, multimodal GIS

COMPUTER

interface to support emergency
management,” in Proc. 10th ACM Int.

Symp. Advances Geographic Informa- 16.

tion Systems, 2002, pp. 119-124. doi:
10.1145/585147.585172.

L. She and J. Y. Chai, “Interactive
learning of grounded verb semantics

towards human-robot communi- 17.

cation,” in Proc. 55th Annu. Meeting
Assoc. Computational Linguistics (ACL),
2017, pp. 1634-1644. [Online]. Avail-
able: https://dblp.uni-trier.de/pers/
hd/s/She:Lanbo}

H. Hung, E. Gedik, and L. Cabre- 18.

ra-Quiros, “Complex conversational
scene analysis using wearable sens-
ing,” in Multi-modal Behavior Analysis

in the Wild: Advances Challenges, X. 19.

Alameda-Pineda, E. Ricci, N. Sebe,
Eds. New York: Academic, 2018,

pp. 225-245.

F. Hutter, L. Kotthoff, and J. Van-
schoren, Eds., Automated Machine
Learning Methods, Systems, Challenges.
New York: Springer Verlag 2019.
[Online]. Available: https://jwww
-automLorg/book/

J. van den Hoven, P. Vermaas, and I.
van de Poel, “Sources, theory, values
and application domains,” in Hand-
book of Ethics, Values, and Technological
Design. Dordrecht: Springer-Verlag,
2015. [Online]. Available: https: //

link.springer.com/reference- ai.

work/10.1007%2F978-94-007-
6994-6

I. Verdiesen, V. Dignum, and J. Van
Den Hoven, “Measuring moral accept-
ability in E-deliberation: A practical
application of ethics by participa-
tion,” ACM Trans. Internet Technol.,
vol. 18, no. 4, Art. no. 43. 2018, doi:
10.1145/3183324.

M.A. de Graaf, and B F. Malle,
“How people explain action (and
Autonomous Intelligent Systems
should too},” in Proc. Association

20.

22.

Advancement Artificial Intelligence
Fall Symp. (AAAI), 2017, pp. 19-26.
C. Lacave and F. Diez, “A review of
explanation methods for Bayesian
networks,” Knowl. Eng. Rev., vol. 17,
no. 2, pp. 107-127, 2002. doi: 10.1017/
$026988890200019X.

5. Ross, M. C. Hughes, and F. Doshi-
Velez, Right for the right reasons:
Training differentiable models

by constraining their explana-
tions. 2017. [Online]. Available:
arXiv:1703.03717

P. W. Koh and P. Liang, Under-
standing black-box predictions via
influence functions. 2017. [Online].
Available: arXiv:1703.04730

M. T. Ribeiro, S. Singh and C. Gues-
trin, “Why should I trust you?:
Explaining the predictions of any
classifier,” in Proc. 22nd ACM SIG
KDD Int. Conf. Knowledge Discovery
and Data Mining, 2016, pp. 1135-1144.
doi: 10.1145 /2939672.2939778.

M. Ligthart, T. Fernhout, M.A.
Neerincx, L. A. Van Bindsbergen,
M.A. Grootenhuis, and K. V. Hin-
driks, “A child and a robot getting
acquainted: Interaction design for
eliciting self-disclosure,” in Proc.
18th Int. Conf. Autonomous Agents
and MultiAgent Systems, May 2019,
pp. 61-70.

F. Van Harmelen and A. Ten Teije,
“A boxology of design patterns for
hybrid learning and reasoning
systems,” J. Web Eng., vol. 18, no.

1, pp. 97-124, 2019. doi: 10.13052/
jwel540-9589.18133.

22. V. Dignum and F. Dignum, "Mod-
elling agent societies: Co-ordination
frameworks and institutions." in
Portuguese Conference on Artificial
Intelligence (EPIA 2001) (Lecture Notes
in Computer Science series 2258}.
Berlin: Springer-Verlag, 2001,

pp. 191-204.

WWW.COMPUTER.ORG/COMPUTER
2101.09429v1 [cs.AI] 23 Jan 2021

arXiv

Explainable Artificial Intelligence Approaches: A
Survey

Sheikh Rabiul Islam, University of Hartford, William Eberle, Tennessee Tech University, Sheikh Khaled
Ghafoor, Tennessee Tech University, Mohiuddin Ahmed, Edith Cowan University

Abstract—The lack of explainability of a decision from an Artificial Intelligence (Al) based “black box” system/model, despite its
superiority in many real-world applications, is a key stumbling block for adopting Al in many high stakes applications of different domain
or industry. While many popular Explainable Artificial Intelligence (XAl) methods or approaches are available to facilitate a
human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate
popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple
perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or
human-centered Al using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate
competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or
human-centric Al systems, which is crucial to adopt Al in high stakes applications.

Index Terms—Explainable Artificial Intelligence, Explainability Quantification, Human-centered Artificial Intelligence, Interpretability.

 

1 INTRODUCTION

RTIFICIAL Intelligence (AI) has become an integral
Apa of many real-world applications. Factors fueling
the proliferation of Al-based algorithmic decision making
in many disciplines include: (1) the demand for process-
ing a variety of voluminous data, (2) the availability of
powerful computing resources (e.g., GPU computing, cloud
computing), and (3) powerful and new algorithms. How-
ever, most of the successful Al-based models are “black
box” in nature, making it a challenge to understand how
the model or algorithm works and generates decisions.
In addition, the decisions from AI systems affect human
interests, rights, and lives; consequently, the decision is
crucial for high stakes applications such as credit approval
in finance, automated machines in defense, intrusion detec-
tion in cybersecurity, etc. Regulators are introducing new
laws such as European Union’s General Data Protection
Regulation (GDPR) 1 [1] aka “right to explanation” [2], US
government’s “Algorithmic Accountability Act of 2019” 2
[3], or U.S. Department of Defense’s Ethical Principles for
Artificial Intelligence * [4] ) to tackle primarily fairness, ac-
countability, and transparency-related risks with automated
decision making systems.

XAI is a re-emerging research trend, as the need to ad-
vocate these principles/laws, and promote the explainable
decision-making system and research, continues to increase.
Explanation systems were first introduced in the early ’80s
to explain the decisions of expert systems. Later, the focus of
the explanation systems shifted towards human-computer
systems (e.g., intelligent tutoring systems) to provide better
cognitive support to users. The primary reason for the
renewed interest in XAI research has stemmed from recent
advancements in AI and ML, and their application to a

1. https:/ /www.eugdpr.org
2. https: / /www.senate.gov
3. https: / /www.defense.gov

wide range of areas, as well as prevailing concerns over the
unethical use, lack of transparency, and undesired biases
in the models. Many real-world applications in the Indus-
trial Control System (ICS) greatly increase the efficiency of
industrial production from the automated equipment and
production processes [5]. However, in this setting, the use
of ‘black box’ is still not in a favorable position due to the
lack of explainability and transparency of the model and
decisions.

According to [6] and [7], XAI encompasses Machine
Learning (ML) or AI systems/tools for demystifying black
models internals (e.g., what the models have learned)
and/or for explaining individual predictions. In general,
explainability of an AI model’s prediction is the extent of
transferable qualitative understanding of the relationship
between model input and prediction (i.e., selective /suitable
causes of the event) in a recipient friendly manner. The
term “explainability” and “interpretability” are being used
interchangeably throughout the literature. To this end, in
the case of an intelligent system (ie., Al-based system), it
is evident that explainability is more than interpretability
in terms of importance, completeness, and fidelity of pre-
diction. Based on that, we will use these terms accordingly
where appropriate.

Due to the increasing number of XAI approaches, it has
become challenging to understand the pros, cons, and com-
petitive advantages, associated with the different domains.
In addition, there are lots of variations among different XAI
methods, such as whether a method is global (i.e., explains
the model’s behavior on the entire data set), local (ie.,
explains the prediction or decision of a particular instance),
ante-hoc (i.e. involved in the pre training stage), post-hoc
(i.e. works on already trained model), or surrogate (i.e.
deploys a simple model to emulate the prediction of a
“black box” model). However, despite many reviews on XAI
methods, there is still a lack of comprehensive analysis of
XAI when it comes to these methods and perspectives.

Some of the popular work/tools on XAI are LIME,
Deep Vis Toolbox, TreeInterpreter, Keras-vis, Microsoft Inter-
pretML, MindsDB, SHAP, Tensorboard WhatlIf, Tensorflow’s
Lucid, Tensorflow’s Cleverhans, etc. However, a few of these
work/tools are model specific. For instance, Deep Vis, keras-
vis, and Lucid are for a neural network’s explainability, and
TreeInterpreter is for a tree-based model's explainability. At
a high level, each of the proposed approaches have similar
concepts, such as feature importance, feature interactions,
shapely values, partial dependence, surrogate models, coun-
terfactual, adversarial, prototypes and knowledge infusion.
However, despite some visible progress in XAI methods,
the quantification or evaluation of explainability is under-
focused, and in particular, when it comes to human study-
based evaluations.

In this paper, we (1) demonstrate popular meth-
ods/approaches towards XAI with a mutual task (i.e., credit
default prediction) and explain the working mechanism in
layman’s terms, (2) compare the pros, cons, and competitive
advantages of each approach with their associated chal-
lenges, and analyze those from multiple perspectives (e.g.,
global vs local, post-hoc vs ante-hoc, and inherent vs emu-
lated /approximated explainability), (3) provide meaningful
insight on quantifying explainability, and (4) recommend a
path towards responsible or human-centered AI using XAI
as a medium. Our survey is only one among the recent ones
(See Table 1) which includes a mutual test case with useful
insights on popular XAI methods (See Table 4).

TABLE 1
Comparison with other Surveys

 

 

Survey Reference Mutual test case
Adadi et al., 2018 [8] x
Mueller et al., 2019 [9] x
Samek et al., 2017 [6] x
Molnar et al., 2019 [10] x
Staniak et al., 2018 [11] x
Gilpin et al., 2018 [12] x
Collaris et al., 2018 [13] x
Ras et al., 2018 [1] x
Dosilovic et al., 2018 [14] x
Tjoa et al., 2019 [15] x
Dosi-Valez et al., 2017 [16] x
Rudin et al., 2019 [17] x
Arrieta et al., 2020 [18] x
Miller et al., 2018 [19] x
Zhang et al., 2018 [20] x
v

This Survey

 

We start with a background of related works (Section
2), followed by a description of the test case in Section
3, and then a review of XAI methods in Section 4. We
conclude with an overview of quantifying explainability
and a discussion addressing open questions and future
research directions towards responsible for human-centered
Al in Section 5.

2 BACKGROUND

Research interests in XAI are re-emerging. The earlier works
such as [21], [22], and [23] focused primarily on explain-
ing the decision process of knowledge-based systems and
expert systems. The primary reason behind the renewed

2

interest in XAI research has stemmed from the recent ad-
vancements in AI, its application to a wide range of areas,
the concerns over unethical use, lack of transparency, and
undesired biases in the models. In addition, recent laws
by different governments are necessitating more research
in XAI. According to [6] and [7], XAI encompasses Machine
Learning (ML) or AI systems for demystifying black models
internals (e.g., what the models have learned) and/or for
explaining individual predictions.

In 2019, Mueller et al. presents a comprehensive review
of the approaches taken by a number of types of “explana-
tion systems” and characterizes those into three generations:
(1) first-generation systems—for instance, expert systems
from the early 70’s, (2) second generation systems—for in-
stance, intelligent tutoring systems, and (3) third generation
systems—tools and techniques from the recent renaissance
starting from 2015 [9]. The first generation systems attempt
to clearly express the internal working process of the system
by embedding expert knowledge in rules often elicited
directly from experts (e.g., via transforming rules into natu-
ral language expressions). The second generations systems
can be regarded as the human-computer system designed
around human knowledge and reasoning capacities to pro-
vide cognitive support. For instance, arranging the interface
in such a way that complements the knowledge that the
user is lacking. Similar to the first generation systems, the
third generation systems also attempt to clarify the inner
workings of the systems. But this time, these systems are
mostly “black box” (e.g., deep nets, ensemble approaches).
In addition, nowadays, researchers are using advanced
computer technologies in data visualizations, animation,
and video, that have a strong potential to drive the XAI
research further. Many new ideas have been proposed for
generating explainable decisions from the need of primarily
accountable, fair, and trust-able systems and decisions.

There has been some previous work [10] that mentions
three notions for quantification of explainability. Two out
of three notions involve experimental studies with humans
(e.g., domain expert or a layperson, that mainly investigate
whether a human can predict the outcome of the model)
[24], [25], [26], [27], [28]. The third notion (proxy tasks) does
not involve a human, and instead uses known truths as a
metric (e.g., the less the depth of the decision tree, the more
explainable the model).

Some mentionable reviews on XAI are listed in Table
1. However, while these works provide analysis from one
or more of the mentioned perspectives, a comprehensive
review considering all of the mentioned important perspec-
tives, using a mutual test case, is still missing. Therefore,
we attempt to provide an overview using a demonstration
of a mutual test case or task, and then analyze the various
approaches from multiple perspectives, with some future di-
rections of research towards responsible or human-centered
Al.

3 TEST CASE

The mutual test case or task that we use in this paper
to demonstrate and evaluate the XAI methods is credit
default prediction. This mutual test case enables a better
understanding of the comparative advantages of different
XAI approaches. We predict whether a customer is going
to default on a mortgage payment (ie., unable to pay
monthly payment) in the near future or not, and explain the
decision using different XAI methods in a human-friendly
way. We use the popular Freddie Mac [29] dataset for the
experiments. Table 2 lists some important features and their
descriptions. The description of features are taken from the
data set’s [29] user guide.

We use well-known programming language R’s package
“iml” [30] for producing the results for the XAI methods
described in this review.

4 EXPLAINABLE ARTIFICIAL INTELLIGENCE

METHODS

This section summarizes different explainability methods
with their pros, cons, challenges, and competitive advan-
tages primarily based on two recent comprehensive surveys:
[31] and [16]. We then enhance the previous surveys with a
multi-perspective analysis, recent research progresses, and
future research directions. [16] broadly categorize methods
for explanations into three kinds: Intrinsically Interpretable
Methods, Model Agnostic Methods, and Example-Based
Explanations.

4.1 Intrinsically Interpretable Methods

The convenient way to achieve explainable results is to
stick with intrinsically interpretable models such as Lin-
ear Regression, Logistic Regression, and Decision Trees by
avoiding the use of “black box” models. However, usually,
this natural explainability comes with a cost in performance.

In a Linear Regression, the predicted target consists
of the weighted sum of input features. So the weight or
coefficient of the linear equation can be used as a medium of
explaining prediction when the number of features is small.

y=bo +b) eat... +b) ean te (1)

In Formula 1, y is the target (e.g., chances of credit default),
bo is a constant value known as the intercept (e.g., .33), b;
is the learned feature’s weight or coefficient (e.g., .33) for
the corresponding feature x; (e.g., credit score), and € is a
constant error term (e.g., .0001). Linear regression comes
with an interpretable linear relationship among features.
However, in cases where there are multiple correlated fea-
tures, the distinct feature influence becomes indeterminable
as the individual influences in prediction are not additive to
the overall prediction anymore.

Logistic Regression is an extension of Linear Regression
to the classification problems. It models the probabilities for
classification tasks. The interpretation of Logistic Regression
is different from Linear Regression as it gives a probability
between 0 and 1, where the weight might not exactly rep-
resent the linear relationship with the predicted probability.
However, the weight provides an indication of the direction
of influence (negative or positive) and a factor of influence
between classes, although it is not additive to the overall
prediction.

Decision Tree-based models split the data multiple
times based on a cutoff threshold at each node until it
reaches a leaf node. Unlike Logistic and Linear Regression,

3

it works even when the relationship between input and
output is non-linear, and even when the features interact
with one another (i.e., a correlation among features). In a
Decision Tree, a path from the root node (i.e., starting node)
(e.g., credit score in Figure 1) to a leaf node (e.g., default)
tells how the decision (the leaf node) took place. Usually, the
nodes in the upper-level of the tree have higher importance
than lower-level nodes. Also, the less the number of levels
(i.e., height) a tree has, the higher the level of explainability
the tree possesses. In addition, the cutoff point of a node in
the Decision Trees provides counterfactual information—for
instance, increasing the value of a feature equal to the cutoff
point will reverse the decision /prediction. In Figure 1, if the
credit score is greater than the cutoff point 748, then the
customer is predicted as non-default. Also, tree-based ex-
planations are contrastive, i.e.,a “what if” analysis provides
the relevant alternative path to reach a leaf node. According
to the tree in Figure 1, there are two separate paths (credit
score — delinquency — non-default; and credit score >
non-default) that lead to a non-default classification.

However, tree-based explanations cannot express the
linear relationship between input features and output. It
also lacks smoothness; slight changes in input can have a big
impact on the predicted output. Also, there can be multiple
different trees for the same problem. Usually, the more the
nodes or depth of the tree, the more challenging it is to
interpret the tree.

    

credit score

>0

non-default

non-default

default

Fig. 1. Decision Trees

Decision Rules (simple IF-THEN-ELSE conditions) are
also an inherent explanation model. For instance, “IF credit
score is less than or equal to 748 AND if the customer is
delinquent on payment for more than zero days (condition),
THEN the customer will default on payment (prediction)”.
Although IF-THEN rules are straightforward to interpret,
it is mostly limited to classification problems (i-e., does not
support a regression problem), and inadequate in describing
linear relationships. In addition, the RuleFit algorithm [32]
has an inherent interpretation to some extent as it learns
sparse linear models that can detect the interaction effects
in the form of decision rules. Decision rules consist of the
combination of split decisions from each of the decision
paths. However, besides the original features, it also learns
some new features to capture the interaction effects of
TABLE 2
Dataset description

 

 

Feature Description

creditScore A number in between 300 and 850 that indicates the creditworthiness of the borrowers.
originalU/PB Unpaid principle balance on the note date.

originallnterestRate Original interest rate as indicated by the mortgage note.
currentLoanDelinquencyStatus Indicates the number of days the borrower is delinquent.

numberOfBorrower Number of borrower who are obligated to repay the loan.

currentInterestRate Active interest rate on the note.

originalCombinedLoanToValue Ratio of all mortgage loans and apprised price of mortgaged property on the note date.
currentActualUPB Unpaid principle balance as of latest month of payment.

defaulted Whether the customer was default on payment (1) or not (0.)

 

original features. Usually, interpretability degrades with an
increasing number of features.

Other interpretable models include the extension of lin-
ear models such as Generalized Linear Models (GLMs)
and Generalized Additive Models (GAMs); they help to
deal with some of the assumptions of linear models (e.g.,
the target outcome y and given features follow a Gaussian
Distribution; and no interaction among features). However,
these extensions make models more complex (i.e., added
interactions) as well as less interpretable. In addition, a
Naive Bayes Classifier based on Bayes Theorem, where the
probability of classes for each of the features is calculated
independently (assuming strong feature independence), and
K-Nearest Neighbors, which uses nearest neighbors of a
data point for prediction (regression or classification), also
fall under intrinsically interpretable models.

4.2 Model-Agnostic Methods

Model-agnostic methods separate explanation from a ma-
chine learning model, allowing the explanation method to
be compatible with a variety of models. This separation has
some clear advantages such as (1) the interpretation method
can work with multiple ML models, (2) provides different
forms of explainability (e.g., visualization of feature impor-
tance, linear formula) for a particular model, and (3) allows
for a flexible representation—a text classifier uses abstract
word embedding for classification but uses actual words
for explanation. Some of the model-agnostic interpretation
methods include Partial Dependence Plot (PDP), Individual
Conditional Expectation (ICE), Accumulation Local Effects
(ALE) Plot, Feature Interaction, Feature Importance, Global
Surrogate, Local Surrogate (LIME), and Shapley Values
(SHAP).

4.2.1 Partial Dependence Plot (PDP)

The partial Dependence Plot (PDP) or PD plot shows the
marginal effect of one or two features (at best three features
in 3-D) on the predicted outcome of an ML model [33]. It is
a global method, as it shows an overall model behavior, and
is capable of showing the linear or complex relationships
between target and feature(s). It provides a function that
depends only on the feature(s) being plotted by marginal-
izing over other features in such a way that includes the
interactions among them. PDP provides a clear and causal
interpretation by providing the changes in prediction due
to changes in particular features. However, PDP assumes
features under the plot are not correlated with the remaining

features. In the real world, this is unusual. Furthermore,
there is a practical limit of only two features that PD plot
can clearly explain at a time. Also, it is a global method, as it
plots the average effect (from all instances) of a feature(s) on
the prediction, and not for all features on a specific instance.
The PD plot in Figure 2 shows the effect of credit score on
prediction. Individual bar lines along the X axis represent
the frequency of samples for different ranges of credit scores.

0.48-
>
wy O4s-
oS
2
2
o

0.42-

0.39-

| LT TT nh, ||
500 600 700 800
creditScore

Fig. 2. Partial Dependence Plot (PDP)

4.2.2 Individual Conditional Expectation (ICE)

Unlike PDP, ICE plots one line per instance showing how
a feature influences the changes in prediction (See Figure
3. The average on all lines of an ICE plot gives a PD plot
[34] (i-e., the single line shown in the PD plot in Figure 2).
Figure 4, combines both PDP and ICE together for a better
interpretation.

Although ICE curves are more intuitive to understand
than a PD plot, it can only display one feature meaningfully
at a time. In addition, it also suffers from the problem of
correlated features and overcrowded lines when there are
many instances.

4.2.3 Accumulated Local Effects (ALE) Plot

Similar to PD plots (Figure 2, ALE plots (Figure 5 describe
how features influence the prediction on average. However,
1.00-

0.75 -

 

0.50 -

Predicted y

0.25-

0.00 -

 

creditScore

Fig. 3. Individual Conditional Expectation (ICE)

1.00 -

0.75-

 

0.50 -

 

Predicted y

0.25 -

0.00 -

 

creditScore

Fig. 4. PDP and ICE combined together in the same plot

unlike PDP, ALE plot reasonably works well with correlated
features and is comparatively faster. Although ALE plot is
not biased to the correlated features, it is challenging to in-
terpret the changes in prediction when features are strongly
correlated and analyzed in isolation. In that case, only plots
showing changes in both correlated features together make
sense to understand the changes in the prediction.

4.2.4 Feature Interaction

When the features interact with one another, individual
feature effects do not sum up to the total feature effects
from all features combined. An H-statistic (i.e., Friedman’s
H-statistic) helps to detect different types of interaction,
even with three or more features. The interaction strength
between two features is the difference between the partial
dependence function for those two features together and the sum

0.00-
N

0.04 -
3s
Ww
a
=

-0.02-

| TAT ||
500 600 700 800

creditScore

Fig. 5. Accumulated Local Effects (ALE) Plot

of the partial dependence functions for each feature separately.
Figure 6 shows the interaction strength of each partici-
pating feature. For example, current Actual UPB has the
highest level of interaction with other features, and credit
score has the least interaction with other features. However,
calculating feature interaction is computationally expensive.
Furthermore, using sampling instead of the entire dataset
usually shows variances from run to run. 6,

 

currentActualUPB ~

currentLoanDelinquencyStatus ~

originalUPB -

currentinterestRate -

Features

originalinterestRate -_ }§5 ——————————*
originalGombinedLoanToValue- ©.————*
creditScore- =——_*
0.00 0.05 0.10

Overall interaction strength

Fig. 6. Feature interaction

4.2.5 Feature importance

Usually, the feature importance of a feature is the increase
in the prediction error of the model when we permute
the values of the feature to break the true relationship
between the feature and the true outcome. After shuffling
the values of the feature, if errors increase, then the feature
is important. [35] introduced the permutation-based feature
importance for Random Forests; later [36] extended the
work to a model-agnostic version. Feature importance pro-
vides a compressed and global insight into the ML model’s
behavior. For example, Figure 7 shows the importance of
each participating feature, current Actual UPB possess the
highest feature importance, and credit score possess the low-
est feature importance. Although feature importance takes
into account both the main feature effect and interaction,
this is a disadvantage as feature interaction is included in
the importance of correlated features. We can see that the
feature current Actual UPB possesses the highest feature
importance (Figure 7), at the same time it also possesses the
highest interaction strength 6. As a result, in the presence
of interaction among features, the feature importance does
not add up to total drop-in of performance. Besides, it is
unclear whether the test set or training set should be used
for feature importance, as it demonstrates variance from run
to run in the shuffled dataset. It is necessary to mention that
feature importance also falls under the global methods.

currentActualUPB - —

currentLoanDelinquencyStatus ~ -

originalUPB - *.

currentinterestRate ~ *

Feature

originallnterestRate - *
originalCombinedLoanToValue- ©

creditScore-  @

o 10 20 30 40
Feature Importance (loss: mae)

Fig. 7. Feature importance

4.2.6 Global Surrogate

A global surrogate model tries to approximate the overall
behavior of a “black box” model using an interpretable ML
model. In other words, surrogate models try to approxi-
mate the prediction function of a black-box model using
an interpretable model as correctly as possible, given the
prediction is interpretable. It is also known as a meta-model,
approximate model, response surface model, or emulator.
We approximate the behavior of a Random Forest using
CART decision trees (Figure 8). The original black box

6

model could be avoided given the surrogate model demon-
strates a comparable performance. Although a surrogate
model comes with interpretation and flexibility (ie., such
as model agnosticism), diverse explanations for the same
“black box” such as multiple possible decision trees with
different structures, is a drawback. Besides, some would
argue that this is only an illusion of interpretability.

currentActualUPB <=0 & currentActualUPB <= 0 &
currentLoanDelinquencyStatus <= 0 currentLoanDelinquencyStatus = 0

1.00 - ~~...

0.75-

 

0.50 -

0.25-

 

 

 

 

0.00- I

currentActualUPB > 0 &
currentLoanDelinquencyStatus > 3

=< currentActualUPB = 0 &
currentLoanDelinquencyStatus <= 3

1.00-
0.75-
0.50-

0.25-

0.00 - —————_

Fig. 8. Global surrogate

4.2.7 Local Surrogate (LIME)

Unlike global surrogate, local surrogate explains individ-
ual predictions of black-box models. Local Interpretable
Model-Agnostic Explanations (LIME) was proposed by [37].
Lime trains an inherently interpretable model (e.g., Decision
Trees) on a new dataset made from the permutation of
samples and the corresponding prediction of the black box.
Although the learned model can have a good approximation
of local behavior, it does not have a good global approxi-
mation. This trait is also known as local fidelity. Figure 9
is a visualization of the output from LIME. For a random
sample, the black box predicts that a customer will default
on payment with a probability of 1; the local surrogate
model, LIME also predict that the customer will default
on the payment, however, the probability is 0.99, that is
little less than the black box models prediction. LIME also
shows which feature contributes to the decision making
and by how much. Furthermore, LIME allows replacing the
underlying “black box” model by keeping the same local
interpretable model for the explanation. In addition, LIME
works for tabular data, text, and images. As LIME is an ap-
proximation model, and the local model might not cover the
complete attribution due to the generalization (e.g., using
shorter trees, lasso optimization), it might be unfit for cases
where we legally need complete explanations of a decision.
Furthermore, there is no consensus on the boundary of the
neighborhood for the local model; sometimes, it provides
very different explanations for two nearby data points.
Actual prediction: 1.00
LocalModel prediction: 0.99

originalUPB=177340 483883077 -

 

currentLoanDelinquencyStatus=122.778044932027 -

feature value

 

currentActualUPB=0 -

0.0 01 02 0.3 04
effect

Fig. 9. Local Interpretable Model-Agnostic Explanations (LIME)

4.2.8 Shapley Values

Shapley is another local explanation method. In 1953, Shap-
ley [38] coined the Shapley Value. It is based on coalitional
game theory that helps to distribute feature importance
among participating features fairly. Here the assumption is
that each feature value of the instance is a player in a game,
and the prediction is the overall payout that is distributed
among players (i.e., features) according to their contribution
to the total payout (ie., prediction). We use Shapely val-
ues (See Figure 10) to analyze the prediction of a random
forest model for the credit default prediction problem. The
actual prediction for a random sample is 1.00, the average
prediction from all samples in the data set is 0.53, and
their difference .47 (1.00 — 0.53) consists of the individual
contributions from the features (e.g., Current Actual UPB
contributes 0.36). The Shapely Value is the average contri-
bution in prediction over all possible coalition of features,
which make it computationally expensive when there is a
large number of features—for example, for k number of
features, there will be 2 number of coalitions. Unlike LIME,
Shapely Value is an explanation method with a solid theory
that provides full explanations. However, it also suffers from
the problem of correlated features. Furthermore, the Shapely
value returns a single value per feature; there is no way
to make a statement about the changes in output resulting
from the changes in input. One mentionable implementation
of the Shapely value is in the work of [39] that they call
SHAP.

4.2.9 Break Down

The Break Down package provides the local explanation and
is loosely related to the partial dependence algorithm with
an added step-wise procedure known as “Break Down”
(proposed by [11]). It uses a greedy strategy to identify

7

and remove features iteratively based on their influence
on the overall average predicted response (baseline) [40].
For instance, from the game theory perspective, it starts
with an empty team, then adds feature values one by one
based on their decreasing contribution. In each iteration, the
amount of contribution from each feature depends on the
features values of those are already in the team, which is
considered as a drawback of this approach. However, it is
faster than the Shapley value method due to the greedy
approach, and for models without interactions, the results
are the same [31]. Figure 11 is a visualization of break down
for a random sample, showing contribution (positive or
negative) from each of the participating features towards
the final prediction.

4.3 Example-Based Explanations

Example-Based Explanation methods use particular in-
stances from the dataset to explain the behavior of the model
and the distribution of the data in a model agnostic way.
It can be expressed as “X is similar to Y and Y caused Z,
so the prediction says X will cause Z”. According to [31],
a few explanation methods that fall under Example-Based
Explanations are described as follows:

4.3.1 Counterfactual

The counterfactual method indicates the required changes
in the input side that will have significant changes (e.g.,
reverse the prediction) in the prediction/output. Coun-
terfactual explanations can explain individual predictions.
For instance, it can provide an explanation that describes
causal situations such as “If A had not occurred, B would
not have occurred”. Although counterfactual explanations
are human-friendly, it suffers from the “Rashomon effect”,
where each counterfactual explanation tells a different story
to reach a prediction. In other words, there are multiple
true explanations (counterfactual) for each instance level
prediction, and the challenge is how to choose the best one.
The counterfactual methods do not require access to data
or models and could work with a system that does not use
machine learning at all. In addition, this method does not
work well for categorical variables with many values. For
instance, if the credit score of customer 5 (from Table 3) can
be increased to 749 (similar to the credit score of customer 6)
from 748, given other features values remain unchanged, the
customer will not default on a payment. In short, there can
be multiple different ways to tune feature values to make
customers move from non-default to default, or vice versa.

Traditional explanation methods are mostly based on
explaining correlation rather than causation. Moraffah et al.
[41] focus on the causal interpretable model that explains
the possible decision under different situations such as
being trained with different inputs or hyperparameters. This
causal interpretable approach share concept of counterfac-
tual analysis as both work on causal inference. Their work
also suggests possible use in fairness criteria evaluation of
decisions.

4.3.2 Adversarial

An adversarial technique is capable of flipping the decision
using counterfactual examples to fool the machine learner
currentActualUPB=0 -

currentLoanDelinquencyStatus=122 778044932027 -

currentinterestRate=1.6569990232168 -

feature.value

originalUPB=177340.483883077 -

Actual prediction: 1.00
Average prediction: 0.53

originallnterestRate=5.47856243895105 -

originalCombinedLoanToValue=81.627 9960928672 -

creditScore=669.068731685315 -

Fig. 10. Shapely values

TABLE 3
Example-Based Explanations

 

 

Customer Delinquency Credit score Defaulted
1 162 680 yes
2 149 691 yes
3 6 728 yes
4 6 744 yes
5 0 748 yes
6 0 749 no
7 0 763 no
8 0 790 no
9 0 794 no
10 0 806 no

 

(ie., small intentional perturbations in input to make a false
prediction). However, adversarial examples could help to
discover hidden vulnerabilities as well as to improve the
model. For instance, an attacker can intentionally design
adversarial examples to cause the AI system to make a
mistake (i.e., fooling the machine), which poses greater
threats to cyber-security and autonomous vehicles. As an
example, the credit default prediction system can be fooled
for customer 5, just by increasing the credit score by 1 (see
Table 3), leading to a reversed prediction.

Hartl et al. [42] emphasize on understanding the implica-

0.0 041 02 03

tions of adversarial samples on Recurrent Neural Network
(RNNs) based IDS because RNNs are good for sequential
data analysis, and network traffic exhibits some sequential
patterns. They find that adversarial the adversarial train-
ing procedure can significantly reduce the attack surface.
Furthermore, [43] apply an adversarial approach to finding
minimum modification of the input features of an intrusion
detection system needed to reverse the classification of the
misclassified instance. Besides satisfactory explanations of
the reason for misclassification, their approach work pro-
vide further diagnosis capabilities.

4.3.3 Prototypes

Prototypes consist of a selected set of instances that rep-
resent the data very well. Conversely, the set of instances
that do not represent data well are called criticisms [44].
Determining the optimal number of prototypes and crit-
icisms are challenging. For example, customers 1 and 10
from Table 3 can be treated as prototypes as those are strong
representatives of the corresponding target. On the other
hand, customers 5 and 6 (from Table 3) can be treated as a
criticism as the distance between the data points is minimal,
and they might be classified under either class from run to
run of the same or different models.
Breakdown of feature contributions

final_prognosis 4

- ofiginalCombinedLoanToValue = 78.970177187739 7

- currentinterestRate = 4.42292345824355 7

- creditScore = 747.961672548133 4

- originalinterestRate = 5.43703401855842 9

- originalUPB = 193444.082227014 7

- currentLoanDelinquencyStatus = 14.6240680371 168 7

- currentActualUPB =07

 

(Intercept) 7

 

0.946

-0.001

0.003

-0.004

0.013

0.03.

0.192

0.251

 

0.0

Fig. 11. Breakdown

4.3.4 Influential Instances

Influential instances are data points from the training set
that are influential for prediction and parameter determina-
tion of the model. While it helps to debug the model and un-
derstand the behavior of the model better, determining the
right cutoff point to separate influential or non-influential
instances is challenging. For example, based on the values
of feature credit score and delinquency, customers 1, 2, 9,
and 10 from Table 3 can be treated as influential instances as
those are strong representatives of the corresponding target.
On the other hand, customers 5 and 6 are not influential in-
stances, as those would be in the margin of the classification
decision boundary.

4.3.5 k-nearest Neighbors Model

The prediction of the k-nearest neighbor model can be
explained with the k-neighbor data points (neighbors those
were averaged to make the prediction). A visualization of
the individual cluster containing similar instances provides
an interpretation of why an instance is a member of a
particular group or cluster. For example, in Figure 12, the
new sample (black circle) is classified according to the other
three (3-nearest neighbor) nearby samples(one gray, two
white). This visualization gives an interpretation of why a
particular sample is part of a particular class.

Table 4 summarizes the explainability methods from the
perspective of (A) whether the method approximates the

O4 O18

 

 

a O default
© non-default
O @ new sample
e OC Oe
2] ©
o
ua
OC)
©
QO Oo .
credit score
Fig. 12. KNN

model behavior (i.e., creates an illusion of interpretability)
or finds actual behavior, (B) whether the method alone is
inherently interpretable or not, (C) whether the interpre-
tation method is ante-hoc, that is, it incorporates explain-
ability into a model from the beginning, or post-hoc, where
explainability is incorporated after the regular training of
the actual model (i.e., testing time), (D) whether the method
is model agnostic (i.e., works for any ML model) or specific
to an algorithm, and (E) whether the model is local, provid-
ing instance-level explanations, or global, providing overall
model behavior.

Our analysis says there is a lack of an explainability
method (i.e., a gap in the literature), which is, at the same
time actual and direct (i.e., does not create an illusion of
explainability by approximating the model), model agnostic,
and local, such that it utilizes the full potential of the
explainability method in different applications. There are
some recent works that bring external knowledge and infuse
that into the model for better interpretation. These XAI
methods have the potential to fill the gap to some extent by
incorporating domain knowledge into the model in a model
agnostic and transparent way (i.e., not by illusion).

4.4 Other Techniques

Chen et al. [45] introduce an instance-wise feature selection
as a methodology for model interpretation where the model
learns a function to extract a subset of most informative
features for a particular instance. The feature selector at-
tempt to maximize the mutual information between selected
features and response variables. However, their approach is
mostly limited to posthoc approaches.

In a more recent work, [46] study explainable ML us-
ing information theory where they quantify the effect of
an explanation by the conditional mutual information be-
tween the explanation and prediction considering user back-
ground. Their approach provides personalized explanation
based on the background of the recipient, for instance, a
different explanation for those who know linear algebra and
those who don’t. However, this work is yet to be considered
as a comprehensive approach which considers a variety of
user and their explanation needs. To understand the flow
of information in a Deep Neural Network (DNN), [47]
analyzed different gradient-based attribution methods that
assign an attribution value (i.e., contribution or relevance) to
each input feature (i.e., neuron) of a network for each out-
put neurons. They use a heatmap for better visualizations
where a particular color represents features that contribute
positively to the activation of target output, and another
color for features that suppress the effect on it.

A survey on the visual representation of Convolutional
Neural Networks (CNNs), by [20], categorizes works based
on a) visualization of CNN representations in intermediate
network layers, b) diagnosis of CNN representation for
feature space of different feature categories or potential
representation flaws, c) disentanglement of “the mixture of
patterns” encoded in each filter of CNNs, d) interpretable
CNNs, and e) semantic disentanglement of CNN represen-
tations.

In the industrial control system, an alarm from the
intrusion /anomaly detection system has a very limited role
unless the alarm can be explained with more information.
[5] design a layer-wise relevance propagation method for
DNN to map the abnormalities between the calculation pro-
cess and features. This process helps to compare the normal

10

samples with abnormal samples for better understanding
with detailed information.

4.5 Knowledge Infusion Techniques

[48] propose a concept attribution-based approach (ie.,
sensitivity to the concept) that provides an interpretation
of the neural network’s internal state in terms of human-
friendly concepts. Their approach, Testing with CAV (TCAV),
quantifies the prediction’s sensitivity to a high dimensional
concept. For example, a user-defined set of examples that
defines the concept ‘striped’, TCAV can quantify the in-
fluence of ‘striped’ in the prediction of ‘zebra’ as a single
number. However, their work is only for image classification
and falls under the post-modeling notion (i.e., post-hoc) of
explanation.

[49] propose a knowledge-infused learning that mea-
sures information loss in latent features learned by the
neural networks through Knowledge Graphs (KGs). This
external knowledge incorporation (via KGs) aids in super-
vising the learning of features for the model. Although
much work remains, they believe that (KGs) will play a
crucial role in developing explainable AI systems.

[50] and [51] infuse popular domain principles from the
domain in the model and represent the output in terms of
the domain principle for explainable decisions. In [50], for
a bankruptcy prediction problem they use the 5C’s of credit
as the domain principle which is commonly used to analyze
key factors: character (reputation of the borrower/firm),
capital (leverage), capacity (volatility of the borrower’s earn-
ings), collateral (pledged asset) and cycle (macroeconomic
conditions) [52], [53]. In [51], for an intrusion detection and
response problem, they incorporate the CIA principles into
the model; C stands for confidentiality—concealment of infor-
mation or resources, I stands for integrity—trustworthiness
of data or resources, and A stands for availability—ability
to use the information or resource desired [54]. In both
cases, the infusion of domain knowledge leads to better
explainability of the prediction with negligible compromises
in performance. It also comes with better execution time and
a more generalized model that works better with unknown
samples.

Although these works [50], [51] come with unique com-
binations of merits such as model agnosticism, the capability
of both local and global explanation, and authenticity of
explanation—simulation or emulation free, they are still
not fully off-the-shelf systems due to some domain-specific
configuration requirements. Much work still remains and
needs further attention.

5 QUANTIFYING EXPLAINABILITY AND FUTURE
RESEARCH DIRECTIONS

5.1 Quantifying Explainability

The quantification or evaluation of explainability is an open
challenge. There are two primary directions of research to-
wards the evaluation of explainability of an AI/ML model:
(1) model complexity-based, and (2) human study-based.
TABLE 4
Comparison of different explainability methods from a set of key perspectives (approximation or actual; inherent or not; post-hoc or ante-hoc;
model-agnostic or model specific; and global or local)

 

 

Method Approx. Inherent Post/Ante Agnos./Spec. Global/Local
Linear /Logistic Regression No Yes Ante Specific Both
Decision Trees No Yes Ante Specific Both
Decision Rules No Yes Ante Specific Both
k-Nearest Neighbors No Yes Ante Specific Both
Partial Dependence Plot (PDP) Yes No Post Agnostic Global
Individual Conditional Expectation (ICE) Yes No Post Agnostic Both
Accumulated Local Effects (ALE) Plot Yes No Post Agnostic Global
Feature Interaction No Yes Both Agnostic Global
Feature Importance No Yes Both Agnostic Global
Global Surrogate Yes No Post Agnostic Global
Local Surrogate (LIME) Yes No Post Agnostic Local
Shapley Values (SHAP) Yes No Post Agnostic Local
Break Down Yes No Post Agnostic Local
Counterfactual explanations Yes No Post Agnostic Local
Adversarial examples Yes No Post Agnostic Local
Prototypes Yes No Post Agnostic Local
Influential instances Yes No Post Agnostic Local

 

5.1.1 Model Complexity-based Explainability Evaluation

In the literature, model complexity and (lack of) model
interpretability are often treated as the same [10]. For in-
stance, in [55], [56], model size is often used as a measure of
interpretability (e.g., number of decision rules, depth of the
tree, number of non-zero coefficients).

[56] propose a scalable Bayesian Rule List (i.e., proba-
bilistic rule list) consisting of a sequence of IF-THEN rules,
identical to a decision list or one-sided decision tree. Unlike
the decision tree that uses greedy splitting and pruning,
their approach produces a highly sparse and accurate rule
list with a balance between interpretability, accuracy, and
computation speed. Similarly, the work of [55] is also rule-
based. They attempt to evaluate the quality of the rules
using a rule learning algorithm by: the observed coverage,
which is the number of positive examples covered by the
rule, which should be maximized to explain the training
data well; and consistency, which is the number of negative
examples covered by the rule, which should be minimized
to generalize well to unseen data.

According to [57], while the number of features and the
size of the decision tree are directly related to interpretabil-
ity, the optimization of the tree size or features (i.e., feature
selection) is costly as it requires the generation of a large
set of models and their elimination in subsequent steps.
However, reducing the tree size (i.e., reducing complexity)
increases error, as they could not find a way to formulate
the relation in a simple functional form. More recently, [10]
attempts to quantify the complexity of the arbitrary machine
learning model with a model agnostic measure. In that
work, the author demonstrates that when the feature in-
teraction (i.e., the correlation among features) increases, the
quality of representations of explainability tools degrades.
For instance, the explainability tool ALE Plot (see Figure
5 starts to show harsh lines (ie., zigzag lines) as feature
interaction increases. In other words, with more interaction
comes a more combined influence in the prediction, induced
from different correlated subsets of features (at least two),

which ultimately makes it hard to understand the causal
relationship between input and output, compared to an
individual feature influence in the prediction. In fact, from
our study of different explainability tools (e.g., LIME, SHAP,
PDP), we have found that the correlation among features is
a key stumbling block to represent feature contribution in
a model agnostic way. Keeping the issue of feature inter-
actions in mind, [10] propose a technique that uses three
measures: number of features, interaction strength among
features, and the main effect (excluding the interaction part)
of features, to measure the complexity of a post-hoc model
for explanation.

Although, [10] mainly focuses on model complexity for
post-hoc models, their work was a foundation for the ap-
proach by [58] for the quantification of explainability. Their
approach to quantify explainability is model agnostic and
is for a model of any notion (e.g., pre-modeling, post-hoc)
using proxy tasks that do not involve a human. Instead,
they use known truth as a metric (e.g., the less number of
features, the more explainable the model). Their proposed
formula for explainability gives a score in between 0 and 1
for explainability based on the number of cognitive chunks
(i.e., individual pieces of information) used on the input side
and output side, and the extent of interaction among those
cognitive chunks.

5.1.2 Human Siudy-based Explainability Evaluation

The following works deal with the application-level and
human-level evaluation of explainability involving human
studies.

[26] investigate the suitability of different alternative
representation formats (e.g., decision tables, (binary) deci-
sion trees, propositional rules, and oblique rules) for clas-
sification tasks primarily focusing on the explainability of
results rather than accuracy or precision. They discover that
decision tables are the best in terms of accuracy, response
time, the confidence of answer, and ease of use.

[24] argue that interpretability is not an absolute con-
cept; instead, it is relative to the target model, and may or
may not be relative to the human. Their finding suggests
that a model is readily interpretable to a human when it uses
no more than seven pieces of information [59]. Although,
this might vary from task to task and person to person.
For instance, a domain expert might consume a lot more
detailed information depending on their experience.

The work of [27] is a human-centered approach, focus-
ing on previous work on human trust in a model from
psychology, social science, machine learning, and human-
computer interaction communities. In their experiment with
human subjects, they vary factors (e.g., number of features,
whether the model internals are transparent or a black box)
that make a model more or less interpretable and measures
how the variation impacts the prediction of human subjects.
Their results suggest that participants who were shown a
transparent model with a small number of features were
more successful in simulating the model’s predictions and
trusted the model’s predictions.

[25] investigate interpretability of a model based on two
of its definitions: simulatability, which is a user’s ability to
predict the output of a model on a given input; and “what
if” local explainability, which is a user’s ability to predict
changes in prediction in response to changes in input, given
the user has the knowledge of a model’s original prediction
for the original input. They introduce a simple metric called
runtime operation count that measures the interpretability,
that is, the number of operations (e.g., the arithmetic opera-
tion for regression, the boolean operation for trees) needed
in a user’s mind to interpret something. Their findings
suggest that interpretability decreases with an increase in
the number of operations.

Despite some progress, there are still some open chal-
lenges surrounding explainability such as an agreement of
what an explanation is and to whom; a formalism for the
explanation; and quantifying the human comprehensibility
of the explanation. Other challenges include addressing
more comprehensive human studies requirements and in-
vestigating the effectiveness among different approaches
(e.g., supervised, unsupervised, semi-supervised) for var-
ious application areas (e.g., natural language processing,
image recognition).

5.2 Future Research Directions

The long term goal for current Al initiatives is to contribute
to the design, development, and deployment of human-
centered artificial intelligent systems, where the agents col-
laborate with the human in an interpretable and explainable
manner, with the intent on ensuring fairness, transparency,
and accountability. To accomplish that goal, we propose a
set of research plans/directions towards achieving respon-
sible or human-centered AI using XAI as a medium.

5.2.1 A Generic Framework to Formalize Explainable Arti-
ficial Intelligence

The work in [50] and [51], demonstrates a way to collect
and leverage domain knowledge from two different do-
mains, finance and cybersecurity, and further infused that
knowledge into black-box models for better explainability.
In both of these works, competitive performance with en-
hanced explainability is achieved. However, there are some

12

open challenges such as (A) a lack of formalism of the
explanation, (B) a customized explanation for different types
of explanation recipients (e.g., layperson, domain expert,
another machine), (C) a way to quantify the explanation,
and (D) quantifying the level of comprehensibility with
human studies. Therefore, leveraging the knowledge from
multiple domains, a generic framework could be useful
considering the mentioned challenges. As a result, mission-
critical applications from different domains will be able to
leverage the black-box model with greater confidence and
regulatory compliance.

5.2.2 Towards Fair, Accountable, and Transparent Al-
based Models

Responsible use of Al is crucial for avoiding risks stemming
from a lack of fairness, accountability, and transparency in
the model. Remediation of data, algorithmic, and societal
biases is vital to promote fairness; the AI system/adopter
should be held accountable to affected parties for its deci-
sion; and finally, an AI system should be analyzable, where
the degree of transparency should be comprehensible to
have trust in the model and its prediction for mission-
critical applications. Interestingly, XAI enhances understat-
ing directly, increasing trust as a side-effect. In addition,
the explanation techniques can help in uncovering potential
risks (e.g., what are possible fairness risks). So it is crucial to
adhere to fairness, accountability, and transparency princi-
ples in the design and development of explainable models.

5.2.3. Human-Machine Teaming

To ensure the responsible use of AI, the design, devel-
opment, and deployment of human-centered AI, that col-
laborates with the humans in an explainable manner, is
essential. Therefore, the explanation from the model needs
to be comprehensible by the user, and there might be some
supplementary questions that need to be answered for a
clear explanation. So, the interaction (e.g., follow-ups after
the initial explanation) between humans and machines is
important. The interaction is more crucial for adaptive ex-
plainable models that provide context-aware explanations
based on user profiles such as expertise, domain knowledge,
interests, and cultural backgrounds. The social sciences and
human behavioral studies have the potential to impact
XAI and human-centered AI research. Unfortunately, the
Human-Computer Interaction (HCI) community is kind of
isolated. The combination of HCI empirical studies and
human science theories could be a compelling force for the
design of human-centered AI models as well as furthering
XAI research. Therefore, efforts to bring a human into the
loop, enabling the model to receive input (repeated feed-
back) from the provided visualization/explanations to the
human, and improving itself with the repeated interactions,
has the potential to further human-centered AI. Besides
adherence to fairness, accountability, and transparency, the
effort will also help in developing models that adhere to our
ethics, judgment, and social norms.

5.2.4 Collective intelligence from Multiple Disciplines

From the explanation perspective, there is plenty of research
in philosophy, psychology, and cognitive science on how
people generate, select, evaluate, and represent explana-
tions and associate cognitive biases and social expectations
in the explanation process. In addition, from the interac-
tion perspective, human-computer teaming involving social
science, the HCI community, and social-behavioral stud-
ies could combine for further breakthroughs. Furthermore,
from the application perspective, the collectively learned
knowledge from different domains (e.g., Health-care, Fi-
nance, Medicine, Security, Defense) can contribute to fur-
thering human-centric AI and XAI research. Thus, there is a
need for a growing interest in multidisciplinary research to
promote human-centric Al as well as XAI in mission-critical
applications from different domains.

6 CONCLUSION

We demonstrate and analyze mutual XAI methods using
a mutual test case to explain competitive advantages and
elucidate the challenges and further research directions.
Most of the available works on XAI are on the post-hoc
notion of explainability. However, the post-hoc notion of
explainability is not purely transparent and can be mis-
leading, as it explains the decision after it has been made.
The explanation algorithm can be optimized to placate
subjective demand, primarily stemming from the emulation
effort of the actual prediction, and the explanation can be
misleading, even when it seems plausible [60], [61]. Thus,
many suggest not to explain black-box models using post-
hoc notions, instead, they suggest adhering to simple and
intrinsically explainable models for high stakes decisions
[17]. Furthermore, from the literature review, we find that
explainability in pre-modeling is a viable option to avoid
the transparency related issues, albeit, under-focused. In
addition, knowledge infusion techniques have the potential
to enhance explainability greatly, although, also an under-
focused challenge. Therefore, we need more focus on the
explainability of “black box” models using domain knowl-
edge. At the same time, we need to focus on the evaluation
or quantification of explainability using both human and
non-human studies. We believe this review provides a good
insight into the current progress on XAI approaches, eval-
uation and quantification of explainability, open challenges,
and a path towards responsible or human-centered AI using
XAI as a medium.

ACKNOWLEDGMENTS

Our sincere thanks to Christoph Molnar for his open E-
book on Interpretable Machine Learning and contribution
to the open-source R package “iml”. Both were very useful
in conducting this survey.

REFERENCES

[1] G. Ras, M. van Gerven, and P. Haselager, “Explanation methods
in deep learning: Users, values, concerns and challenges,” in
Explainable and Interpretable Models in Computer Vision and Machine
Learning. Springer, 2018, pp. 19-36.

[2] B. Goodman and 5S. Flaxman, “Eu regulations on algorithmic
decision-making and a “right to explanation”,” in ICML workshop
on human interpretability in machine learning (WHI 2016), New York,
NY. http://arxiv. org/abs/1606.08813 v1, 2016.

[3]

[4]

[5]

[6]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

13

B. Wyden, “Algorithmic accountability,” https://www.wyden.
senate.gov /imo/media/doc/Algorithmic%,20Accountability%
20Act%200f%202019%20Bill%20Text.pdf, (Accessed on
11/21/2019).

M. T. Esper, “Ai ethical principles,” https://www.defense.gov/
Newsroom /Releases /Release /Article/2091996 / dod-adopts-
ethical-principles- for-artificial-intelligence /, February
(Accessed on 03/07/2020).

Z. Wang, Y. Lai, Z. Liu, and J. Liu, “Explaining the attributes of
a deep learning based intrusion detection system for industrial
control networks,” Sensors, vol. 20, no. 14, p. 3817, 2020.

W. Samek, T. Wiegand, and K.-R. Miiller, “Explainable artificial
intelligence: Understanding, visualizing and interpreting deep
learning models,” arXiv preprint ar Xiv:1708.08296, 2017.

A. Fernandez, F. Herrera, O. Cordon, M. J. del Jesus, and F. Mar-
celloni, “Evolutionary fuzzy systems for explainable artificial in-
telligence: why, when, what for, and where to?” ieee Computational
intelligenCe magazine, vol. 14, no. 1, pp. 69-81, 2019.

A. Adadi and M. Berrada, “Peeking inside the black-box: A survey
on explainable artificial intelligence (xai),” IEEE Access, vol. 6, pp.
52 138-52 160, 2018.

5. T. Mueller, R. R. Hoffman, W. Clancey, A. Emrey, and G. Klein,
“Explanation in human-ai systems: A literature meta-review, syn-
opsis of key ideas and publications, and bibliography for explain-
able ai,” arXiv preprint ar Xiv:1902.01876, 2019.

C. Molnar, G. Casalicchio, and B. Bischl, “Quantifying model
complexity via functional decomposition for better post-hoc in-
terpretability,” in Joint European Conference on Machine Learning and
Knowledge Discovery in Databases. Springer, 2019, pp. 193-204.
M. Staniak and P. Biecek, “Explanations of model predictions with
live and breakdown packages,” arXiv preprint arXiv:1804.01955,
2018.

L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Ka-
gal, “Explaining explanations: An overview of interpretability of
machine learning,” in 2018 IEEE 5th International Conference on data
science and advanced analytics (DSAA). IEEE, 2018, pp. 80-89.

D. Collaris, L. M. Vink, and J. J. van Wijk, “Instance-level ex-
planations for fraud detection: A case study,” arXiv preprint
ar Xiv:1806.07129, 2018.

F. K. DoSilovié, M. Bréié, and N. Hlupi¢, “Explainable artificial
intelligence: A survey,” in 2018 41st International convention on
information and communication technology, electronics and microelec-
tronics (MIPRO). TEEE, 2018, pp. 0210-0215.

E. Tjoa and C. Guan, “A survey on explainable artificial intelli-
gence (xai): towards medical xai,” arXiv preprint ar Xiv:1907.07374,
2019.

F. Doshi-Velez and B. Kim, “Towards a rigorous science of inter-
pretable machine learning,” arXiv preprint ar Xiv:1702.08608, 2017.
C. Rudin, “Stop explaining black box machine learning models
for high stakes decisions and use interpretable models instead,”
Nature Machine Intelligence, vol. 1, no. 5, pp. 206-215, 2019.

A.B. Arrieta, N. Diaz-Rodriguez, J. Del Ser, A. Bennetot, S. Tabik,
A. Barbado, 5. Garcia, 5. Gil-Lépez, D. Molina, R. Benjamins et al.,
“Explainable artificial intelligence (xai): Concepts, taxonomies,
opportunities and challenges toward responsible ai,” Information
Fusion, vol. 58, pp. 82-115, 2020.

T. Miller, “Explanation in artificial intelligence: Insights from the
social sciences,” Artificial Intelligence, 2018.

Q.-s. Zhang and 5.-C. Zhu, “Visual interpretability for deep
learning: a survey,” Frontiers of Information Technology & Electronic
Engineering, vol. 19, no. 1, pp. 27-39, 2018.

B. Chandrasekaran, M. C. Tanner, and J. R. Josephson, “Explaining
control strategies in problem solving,” IEEE Intelligent Systems,
no. 1, pp. 9-15, 1989.

W. R. Swartout and J. D. Moore, “Explanation in second generation
expert systems,” in Second generation expert systems. Springer,
1993, pp. 543-585.

W. R. Swartout, “Rule-based expert systems: The mycin experi-
ments of the stanford heuristic programming project: Bg buchanan
and eh shortliffe,(addison-wesley, reading, ma, 1984); 702 pages,”
1985.

A. Dhurandhar, V. Iyengar, R. Luss, and K. Shanmugam, “Tip:
Typifying the interpretability of procedures,” arXiv preprint
ar Xiv:1706.02952, 2017.

5. A. Friedler, C. D. Roy, C. Scheidegger, and D. Slack, “Assess-
ing the local interpretability of machine learning models,” arXiv
preprint ar Xiv:1902.03501, 2019.

2020,
[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]
[34]
[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens,
“An empirical evaluation of the comprehensibility of decision
table, tree and rule based predictive models,” Decision Support
Systems, vol. 51, no. 1, pp. 141-154, 2011.

F. Poursabzi-Sangdeh, D. G. Goldstein, J. M. Hofman, J. W.
Vaughan, and H. Wallach, “Manipulating and measuring model
interpretability,” arXiv preprint arXiv:1802.07810, 2018.

Q. Zhou, F. Liao, C. Mou, and P. Wang, “Measuring interpretability
for different types of machine learning models,” in Pacific-Asia
Conference on Knowledge Discovery and Data Mining. Springer, 2018,
pp. 295-308.

“Single family loan level dataset - freddie mac.” [Online].
Available:  http:/ /www.freddiemac.com/research/ datasets /sf_
loanlevel_dataset.page

“TIml-cran package.” [Online]. Available: https:/ /cran.r-project.
org /web/packages /iml/index.html

C. Molnar et al., “Interpretable machine learning: A guide for mak-
ing black box models explainable,” E-book at; https://christophm.
github. iofinterpretable-ml-book/;, version dated, vol. 10, 2018.

J. H. Friedman, B. E. Popescu et al., “Predictive learning via rule
ensembles,” The Annals of Applied Statistics, vol. 2, no. 3, pp. 916-
954, 2008.

J. H. Friedman, “Greedy function approximation: a gradient boost-
ing machine,” Annals of statistics, pp. 1189-1232, 2001.

A. Goldstein, A. Kapelner, J. Bleich, and M. A. Kapelner, “Package
‘icebox’,” 2017.

L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.
5-32, 2001.

A. Fisher, C. Rudin, and F. Dominici, “Model class re-
liance: Variable importance measures for any machine learning
model class, from the “rashomon” perspective,” arXiv preprint
arXiv:1801.01489, 2018.

M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should i trust you?:
Explaining the predictions of any classifier,” in Proceedings of the
22nd ACM SIGKDD international conference on knowledge discovery
and data mining. ACM, 2016, pp. 1135-1144.

L. 8. Shapley, “A value for n-person games,” Contributions to the
Theory of Games, vol. 2, no. 28, pp. 307-317, 1953.

5. Lundberg and 5.-I Lee, “An unexpected unity among
methods for interpreting model predictions,” arXiv preprint
arXiv:1611.07478, 2016.

B. B. . B. Greenwell, “Chapter 16 interpretable machine learning
— hands-on machine learning with 1,” https: / /bradleyboehmke.
github.io/ HOML/iml-html, (Accessed on 11/28/2019).

R. Moraffah, M. Karami, R. Guo, A. Raglin, and H. Liu, “Causal
interpretability for machine learning-problems, methods and eval-
uation,” ACM SIGKDD Explorations Newsletter, vol. 22, no. 1, pp.
18-33, 2020.

A. Hartl, M. Bachl, J. Fabini, and T. Zseby, “Explainability and
adversarial robustness for mms,” arXiv preprint ar Xiv:1912.09855,
2019.

D. L. Marino, C. S. Wickramasinghe, and M. Manic, “An adversar-
ial approach for explainable ai in intrusion detection systems,” in
IECON 2018-44th Annual Conference of the IEEE Industrial Electronics
Society. TEEE, 2018, pp. 3237-3243.

B. Kim, R. Khanna, and O. O. Koyejo, “Examples are not enough,
learn to criticize! criticism for interpretability,” in Advances in
Neural Information Processing Systems, 2016, pp. 2280-2288.

J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan, “Learning to
explain: An information-theoretic perspective on model interpre-
tation,” arXiv preprint ar Xiv:1802.07814, 2018.

A. Jung and P. H. J. Nardelli, “An information-theoretic approach
to personalized explainable machine learning,” IEEE Signal Pro-
cessing Letters, 2020.

M. Ancona, E. Ceolini, C. Oztireli, and M. Gross, “Towards better
understanding of gradient-based attribution methods for deep
neural networks,” arXiv preprint arXiv:1711.06104, 2017.

B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, and
R. Sayres, “Interpretability beyond feature attribution: Quantita-
tive testing with concept activation vectors (teav),” arXiv preprint
arXio:1711.11279, 2017.

U. Kursuncu, M. Gaur, and A. Sheth, “Knowledge infused learn-
ing (k-il): Towards deep incorporation of knowledge in deep
learning,” arXiv preprint arXiv:1912.00512, 2019.

5. R. Islam, W. Eberle, 5. Bundy, and S. K. Ghafoor, “Infusing
domain knowledge in ai-based” black box” models for better
explainability with application in bankruptcy prediction,” ACM

[51]

[52]

[53]
[54]
[55]

[56]

[57]
[58]

[59]

[60]

[61]

14

SIGKDD Conference on Knowledge Discovery and Data Mining, 2019,
Anomaly Detection in Finance Workshop, 2019.

5. R. Islam, W. Eberle, S$. K. Ghafoor, A. Siraj, and M. Rogers,
“Domain knowledge aided explainable artificial intelligence for
intrusion detection and response,” arXiv preprint ar Xiv:1911.09853,
2019.

E. Angelini, G. di Tollo, and A. Roli, “A neural network approach
for credit risk evaluation,” The quarterly review of economics and
finance, vol. 48, no. 4, pp. 733-755, 2008.

J. Segal, “Five cs of credit.” [Online]. Available:
/ /www.investopedia.com/terms /f/five-c-credit.asp

B. Matt et al., Introduction to computer security. Pearson Education
India, 2006.

J. Firnkranz, D. Gamberger, and N. Lavraé, “Rule learning in a
nutshell,” in Foundations of Rule Learning. Springer, 2012, pp. 19-
55.

H. Yang, C. Rudin, and M. Seltzer, “Scalable bayesian rule lists,” in
Proceedings of the 34th International Conference on Machine Learning-
Volume 70. JMLR. org, 2017, pp. 3921-3930.

S. Riiping et al., “Learning interpretable models,” 2006.

5. R. Islam, W. Eberle, and S. K. Ghafoor, “Towards quantification
of explainability in explainable artificial intelligence methods,”
arXiv preprint arXiv:1911.10104, 2019.

G. A. Miller, “The magical number seven, plus or minus two: Some
limits on our capacity for processing information.” Psychological
review, vol. 63, no. 2, p. 81, 1956.

Z.C. Lipton, “The mythos of model interpretability,” arXiv preprint
ar Xiv:1606.03490, 2016.

P. Gandhi, “Explainable artificial intelligence.” [Online]. Available:
https: / /www.kdnuggets.com /2019/01/explainable-ai-html

https:
Evolutionary Fuzzy Systems
for Explainable Artificial
Tel rete ee AT
Em Cd me emt a Coes

 

 

i)
oa
nN
N

= 7

2014
a
ai s © Y
Bw

1995 sa = un 2010 6
as a =

y=

LY

~0)
® ——

 

 

 

IMAGE LICENSED BY INGRAM PUBLISHING FOR ING IMAGES.

Alberto Fernandez, Francisco Herrera, and Oscar Cordon
DaSClI Andalusian Institute of Data Science and Computational Intelligence,
University of Granada, Granada, SPAIN

Maria Jose del Jesus
DaSClI Andalusian Institute of Data Science and Computational Intelligence,
Department of Computer Science, University of Jaen, Jaen, SPAIN

Francesco Marcelloni

Department of Information Engineering, University of Pisa, Pisa, ITALY

t Identifier 10.1109/MCL 2018.2.
Date of publication: 10 January 2019

1556-603X/19©2019TEEE FEBRUARY 2019 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE 69

 

rized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
“

Abstract—Evolutionary fuzzy systems are one of the great-
est advances within the area of computational intelligence.
They consist of evolutionary algorithms applied to the
design of fuzzy systems. Thanks to this hybridization,
superb abilities are provided to fuzzy modeling in many
different data science scenarios. This contribution is intend-
ed to comprise a position paper developing a comprehen-
sive analysis of the evolutionary fuzzy systems research field.
To this end, the “4 W” questions are posed and addressed
with the aim of understanding the current context of this
topic and its significance. Specifically, it will be pointed out
why evolutionary fuzzy systems are important from an
explainable point of view, when they began, what they are
used for, and where the attention of researchers should be
directed to in the near future in this area. They must play
an important role for the emerging area of eXplainable
Artificial Intelligence (XAI) learning from data.

I. Why Evolutionary Fuzzy Systems?
nowledge itself is power” [1]. This simple sentence has
led to a wave of continuous interest in developing
models that are able to extract the maximum
amount of information from data. In this context, the
use of machine learning (ML) techniques allows stakeholders to
obtain useful insights, predictions, and decisions from datasets of
many different sources in an automatic fashion [2]. Current
applications come with novel data characteristics as big dimen-
sion and non-standard classification problems, and both research-
ers and practitioners actually aim to understand how the models
work. Therefore, a movement is being witnessed from traditional
data mining towards a more profitable and challenging scenario
known as data science [3]. It brings novel technologies, frame-
works, methodologies, and skills that are designed to ease the
management of the challenges related to big data applications [4].

Nevertheless, to be able to reach the deepest level when
considering all the information available, the knowledge domain
and the data analysis must have a strong synergy. From a data
viewpoint, one must be aware of the quality associated with it.
Additionally, representation of the expert knowledge available
on the tackled application domain must also be considered.

In this scenario, fuzzy set theory might be regarded as a valu-
able tool. Proposed by Lofti A. Zadeh in the mid 60s [5], a lin-
guistic representation of numerical variables is possible by means
of a membership degree being assigned to each of them, which
could vary from 0 (full non-membership) to 1 (full member-
ship). Therefore, models that use fuzzy sets as a tool for data han-
dling provide some important advantages. In terms of semantics,
the use of linguistic labels in the fuzzy model structure is a natu-
ral knowledge representation allowing for a direct human inter-
action [6]. In addition, from a learning perspective, translating the
input features into fuzzy variables with fuzzy membership func-
tions permits obtaining smoothed descriptive models that adapt

well to data with a certain degree of uncertainty.

 

Corresponding Author: Alberto Fernandez (Email: alberto@decsai.ugr.es)

7O IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | FEBRUARY 2019

Based on the former observations, and without lacking gen-
erality, the focus of this contribution will be set on fuzzy rule-
based systems (FRBSs) [7]. As their name suggests, FRBSs are
composed of fuzzy IF-THEN rules where both antecedents
and consequents usually contain fuzzy sets. The main compo-
nents of any FRBS are the knowledge base (KB) and the infer-
ence engine module. The KB comprises all the fuzzy rules
within a rule base (RB), and the definition of the fuzzy sets in
the data base (DB).The inference engine includes a fuzzification
interface, an inference system, and a defuzzification interface.

The learning procedure for FRBSs involves a search for the
model that, based on the observations, best approximates a
given performance metric [2]. In particular, finding the best
approach is related to the so-called “empirical risk—loss” func-
tion, which depends on the data the user works with. In other
words, any learning algorithm has the ultimate goal of optimiz-
ing a mapping function between inputs and outputs. In this
sense, the clear advantages of evolutionary algorithms (EAs) [8]
for developing this task must be highlighted.

EAs are a type of metaheuristic based optimization tech-
niques that are based on a population of solutions. As the name
suggests, EAs are concerned with biological evolution, so that
each solution is encoded as an individual of the population.
The search for the optimal values of the function that is being
optimized (the fitness value) is carried out by means of the
repeated application of operators such as reproduction, muta-
tion, recombination and selection. Among the different imple-
mentations of EAs, the most popular is certainly Genetic
Algorithms (GAs) [9], where solutions (also known as chromo-
somes) are encoded into strings of “genes”, and evolve by
means of recombination and/or mutation. EAs are not specifi-
cally designed as ML techniques. However, it is well-known
that a learning task can be modeled as an optimization problem
and thus effectively solved through evolution. EAs’ powerful
search in complex, ill-defined problem spaces has allowed for
them to be successfully applied to a huge variety of ML and
knowledge discovery tasks [10]. When the accuracy perfor-
mance is set as the fitness function of this optimization process,
the predictive quality of the system is expected to be boosted.

Considering the issues on predictive performance, the main
goal traditionally pursued is to make the model matching reality,
Le. effectively summarizing the underlying data. Hence, this
“quest for accuracy” is the most important requirement when
selecting a solution, and has probably supported the current
explosion of black-box ML approaches. This type of system is
known for having an excellent ability to learn accurately from
the input data. On the contrary, most of these accurate models
are highly non-transparent, i.e. it is no clear what information in
the input data makes them arrive at their decisions [11], [12].
Nowadays, there is an important need for safety, ethics, and sci-
entific understanding systems that provide the right to an expla-
nation where necessary [13]. They must be optimized not only
for accuracy but also for other criteria as fairness or unbiased-
ness, privacy, reliability, robustness, causality and/or trust, among

others. Most of these criteria often cannot be completely

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
 

quantified, but if the system is interpretable,

Le. if it can explain its reasoning, it can be

verified whether that reasoning is sound with

respect to these auxiliary criteria.

There is little consensus on the definition of
interpretability, explainability and some of the
auxiliary criteria. In the ML context, the inter-
pretability is defined as the ability to explain or
to present in understandable terms to a human
[14]. In recent years a broader concept known as
explainability has emerged in what has been
denoted as eXplanaible Artificial Intelli-
gence (XAJ). It encompasses ML/AI systems for opening black
box models, for improving the understanding of what the models
have learned and/or for explaining individual predictions [11].

The design of this kind of systems includes different aspects
to be taken into account:

QO) Designers and developers must have the chance of analyzing
the generated model and discerning its meaning. i.e. to
understand the structure of the system.

QO) End-users must use the system as a decision support, so that
the model must explain the phenomena under study.
Interpretability is probably the traditional buzzword, in

the context of ML, and explainability in the broader context

of AI (XAI). Both with equivalent meaning and behind the
desideratum for accomplishing all the former issues. However,
different sub-concepts can be distinguished associated with
terms “interpretability” and “explainability”. Among others, we
should refer to “understandability”, “intelligibility” or “compre-

hensibility” [12], [15].

QO) Understandability and intelligibility can be viewed as
synonyms that are associated to a functional understanding of
the model in ML. In other words, it refers to grasp how the
model works [16], without trying to elucidate its inner proce-
dure or to shed light on its internal representation [17]. For
FRBSs, intelligibility is primarily associated with inference.

QO) Comprehensibility was defined as the learning algo-
rithm ability for encoding its model in such a way that it
may be inspected and understood by humans [15], [18].
This definition narrows the focus to the model itself. It is
based on the comprehensibility postulate argued by
Michalski [19]: “The results of computer induction should be
symbolic descriptions of given entities, semantically and structural-
ly similar to those a human expert might produce observing the
same entities. Components of these descriptions should be compre-
hensible as single ‘chunks’ of information, directly interpretable in
natural language, and should relate quantitative and qualitative
concepts in an integrated fashion”. In the FRBS context, the
comprehensibility is related with the fuzzy rules, and in
general with the KB.

The final objective related to explainability is boosting the
transparency in the solutions proposed for data science applica-
tions, and therefore the ability to trust the system output [20].

One straightforward way for combining two of the most

important concepts in ML (accuracy and interpretability/

One straightforward way for combining two of
the most important concepts in Machine Learning
(accuracy and interpretability/explainability) in

a natural way, to obtaining eXplainable Artificial
Intelligence models, is by means of the synergy
between Fuzzy Rule-Based Systems and
Evolutionary Algorithms.

explainability) in a natural way, to obtaining XAI models, is by
means of the synergy between FRBSs and EAs. Specifically, the
intrinsic understandability, comprehensibility, and explainability
associated with FRBSs, and the potential of EAs as the optimi-
zation technique for improving FRBSs, leads to what is known
as evolutionary fuzzy systems (EFSs) [21].

In short, an EPS is a methodology that utilizes EAs for a
“fine-setting” of the components of the FRBS, with the aim of
achieving a more reliable solution in terms of any desired
objective function, based on accuracy, interpretability, or a
combination of both.The EA may be applied a priori during
the building stage of the FRBS, leading to a “learning” stage, or
rather to be considered a posteriori for a finer adjustment of
the FRBS, which is known as a “tuning” stage. Different per-
spectives for the application of both approaches may be found
in the specialized literature.To set up the fuzzy sets, the param-
eterization of the fuzzy membership functions, and the selec-
tion of the fuzzy rules, among others, should be noted. In
addition, a very interesting extension uses multi-objective evo-
lutionary algorithms (MOEAs) [22], which can consider several
design criteria to be optimized concurrently, thus composing
the so-called multi-objective evolutionary fuzzy systems
(MOEFSs) [23].

In this contribution, a better insight on the topic of EFS is
provided by posing the 4 W questions. The first one has been
addressed throughout this section, namely Why are EFSs so
significant in data science tasks? The main reason has been
discussed above: they have a great potential for obtaining a
good trade-off between accurate and explainable models,
which is extremely important in most of the current social and
engineering applications.

The remaining 3 W questions are analyzed in depth to set
the past, present and future scenarios. Whereas the historical
review (past and present) is intended to set the current context
of EFSs, the main novelty in this position paper is related to
future insights. Specifically, special attention is paid to analyze
the capabilities of EFSs, in particular those areas that could
benefit the most from their application. Additionally, some
novel scenarios, namely data science, big data and the develop-
ment of explainable models, that EPS researchers should focus
on for the near future are described. Finally, the paper is con-
cluded by a thorough discussion on the need for XAI models
in ML, which is intended to go beyond the old-fashioned

FEBRUARY 2019 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE 71

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
dialectical exercise on the interpretability-accuracy trade-off,
very extended in FRBSs.

To carry out these tasks, the remainder of this paper is orga-
nized as follows. First, Section II considers When did EFSs
begin?, introducing the pioneer works on the topic and the
current taxonomy for EFSs. Section II analyzes What should
EFSs be used for?, presenting the good capabilities of EPSs,
namely robustness and explainability, for solving current prob-
lems and applications. Section IV is devoted to establishing
Where are EFSs going to?, considering some challenges for
future work, and focusing in particular on the big data scenario
due to its significance for current research. To summarize all
thoughts considered in this position paper, Section V presents
an overview of the need for XAI. Finally, Section VI points out
some concluding remarks.

Il. When Did Evolutionary Fuzzy Systems Begin?

Past and Present

In order to understand the present and to be able to forecast
the future of a specific topic, the history and state of the art
must be acknowledged. In this regard, this section highlights
the main progresses made in EPSs from their proposal to pres-
ent. To this aim, the initial milestones on the topic and some
well-known reviews are first introduced (Section I-A). Then, a
complete taxonomy is presented to frame the EFS techniques

in their different categories (Section II-B).

A. EFSs: Pioneering Approaches

The pioneering work by Lofti A. Zadeh on fuzzy sets supposed
a significant contribution in many research areas of control and
modeling [5]. Specifically, the application of fuzzy set theory as
a knowledge representation structure based on fuzzy rules
began in the mid-1970s with the work of Mamdani [7]. Practi-
cally at the same time, the basis and theoretical studies on EAs
were established, in particular regarding one of its branches, that
of GAs [9]. Despite the early appearance of both approaches,
only in the early 1990s the hybridization between these two
computational intelligence techniques was introduced.

Throughout this section, four of the pioneering works in
the field are discussed in no particular order. They adopted
both tuning and learning methodologies of the components of
the FRBS KB by means of GAs.

Karr was the first to investigate the genetic tuning of the
DB for fuzzy controllers [24]. In his study, the complete defini-
tion of the DB was intended to be optimized, i.e. the best val-
ues of the parameters that define the fuzzy partitions were
automatically determined. This was obtained thanks to the ease
of encoding information on the GA chromosome. In particu-
lar, the evolution of the input and output fuzzy sets was joint-
ly considered.

One of the first models of evolutionary learning of a lin-
guistic RB was proposed by Valenzuela~-Rendon in [25]. This
proposal used a learning scheme where each solution (chromo-
some) represented a single rule over the entire RB. The first

version of the methodology used a reward distribution scheme.

72° IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | FEBRUARY 2019

Later, the original proposal was extended to allow reinforce-
ment learning.

A different scheme for learning the RB was defined by
Thrift in [26]. The author used a decision matrix to represent
the RB, in that particular case study only with two dimen-
sions. This matrix collected the linguistic labels of the output
variable according to the corresponding input fuzzy labels. It
considered an integer coding scheme, using “0” to represent a
null value, thus allowing for the automatic learning of the
optimal number of rules. The GA encoded different fuzzy
tules, i.e. the whole RB definition, on a single chromosome.

Pham and Karaboga proposed a similar approach but using a
fuzzy relation R instead of the classical crisp relation (decision
table) [27]. The GA was used to modify the fuzzy relational
matrix of a single-input/single-output fuzzy model. The chro-
mosome was obtained by concatenating the M:N elements of
R, where M and N were the number of linguistic terms asso-
ciated with the input and output variables, respectively. The
main difference with the work in [26] is that the elements of R
were real numbers in [0, 1] instead of integer values.

All these contributions were considered as milestones for the
work on EFSs. From that point on, the interest of researchers on
the topic has significantly increased. As a short description of
developments throughout the history of EPSs, four surveys are
considered. Ten years after the publication of the first approach-
es, the main achievements in the field were compiled in the
2001 monograph by Cordon, Herrera, Hoffmann, and Magda-
lena [21]. After a period of another ten years, Cordén proposed
a historical review focusing on the interpretability viewpoint in
2011 [28]. In 2013, Fazzolari, Alcala, Nojima, Ishibuchi, and
Herrera published an overview focused on the MOEFSs topic,
which was intended to summarize the main contributions in
this particular field [23]. Finally, in 2015 Fernandez, Lopez, del
Jesus, and Herrera revisited the topic of EPSs by presenting a
complete taxonomy of the existing proposals, and also posing
some new trends and challenges to suggest some potential

research directions [29].

B, EFSs: Current Taxonomy

As previously explained at the beginning of this paper, any EFS
is developed on top of an FRBS. In this way, the components of
the FRBS are learned or optimized using an evolutionary pro-
cess commonly taken from available data, as illustrated in Fig-
ure 1.The final goal is being able to contextualize the behavior
of these systems in a given scenario.

The main reason for using EAs as a tool for the design of
FRBSs is related to the possibility of addressing the optimiza-
tion of their components as a search problem. Rule sets, mem-
bership functions, and many other features of an FRBS can be
easily encoded inside a chromosome. The optimization proce-
dure can be viewed from a double perspective: learning and/or
tuning. In addition, the classical trade-off between accuracy and
interpretability must be taken into account [30], [31]. For this
task, the use of an MOEA is probably the option that is best
suited. Finally, different ways of representing fuzzy sets may be

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
considered as another aspect to be embedded in the optimiza-
tion approach.

Taking all these aspects into account, in [29] authors pro-
posed a complete taxonomy of EFSs, which is illustrated in
Figure 2. Specifically, three large groups were highlighted
depending on several aspects, such as how the FRBSs’ elements
are optimized, the trade-off among the different learning crite-
ria, and the use of new fuzzy set representations.

In what follows, a brief description of the EFS approaches
enumerated in the taxonomy is provided. First, the learning
and tuning of FRBSs are introduced. Next, the use of multi-
objective optimization in this framework is presented. Finally,
several comments are given on the parameterized construction

for new fuzzy representations.

1) Evolutionary Learning and Tuning

of FRBSs’ Components

When using an FRBS for modeling a given problem, researchers
and practitioners must decide whether a simple system is enough
for the existing requirements, or if a more complex computa-
tional solution is required. In such a case, the use of an EFS is

mandatory in order to achieve a robust and accurate model.

There are two main alternatives for developing EFSs: (a)
using EAs in the learning procedure of FRBSs and (b) using
EAs for tuning the elements of FRBSs. Specifically, learning
can be carried out either on the KB components or in con-
junction with the inference engine parameters. The post-pro-
cessing tuning is devoted to refining a preliminary definition of
the FRBSs. In the following, a short description of the different
approaches for both alternatives is provided.

a) Evolutionary learning. The specialized literature distin-
guishes between two main cases, depending on whether
the learning is applied just to the KB or to the inference
engine parameters as well.

QO) Evolutionary KB learning. Four different learning

options have been proposed to obtain the KB:

i) Evolutionary rule selection. The goal is to remove
useless rules in the final RB, i.e. those that may degrade
the FRBS accuracy. Thus, a compact and accurate
subset of fuzzy rules is intended to be the output for
these methods.

ii) Simultaneous evolutionary learning of KB com-

EB:

ponents. As its name suggests, several FRBS ele-

ments may be obtained at once. The hitch in this case

 

  

“Ry: If Xp is
6 Ayy and Xpp
} iS Ayo then Y |. . _—
< is By

Rule oe "

 

E>

Fuzzy Set
Numerical Data

   

Fuzzyfication
(Input Interface)

    
    
 
         
 

Data Base

™ a |

  
 

SW sie cia10|
Learning Process

  
 
 
  

_

 
         
    
   
  

Fuzzy Set
Numerical Data

 

 

 

 

FIGURE I How EFSs are built on top of an FRBS. Inspired from [29].

FEBRUARY 2019 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE 73

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
 

   

Nol UL Cevarcl ay

Fuzzy Systems

 

  

 

Evolutionary Learning/ Fava
Tuning of FRBS ee
Components (via Single-
Objective or (via MOEFS)

  

 

Evolutionary
Learning of KB
Components and
Inference Engine
Parameters

Evolutionary|

Evolutionary
KB Learning Tuning

—~__
Versus

  

Performance
Versus

Performance
Interpretability

(Control Problems)

 

 

    

Evolutionary |] Simultaneous
Rule Selection] | Evolutionary
(a Priori Rule | Learning of KB

Extraction) |} Components

Evolutionary
Rule Learning
(a Priori DB)

  
   
   
   
   

Evolutionary
DB Learning

     
         

Evolutionary
Adaptive

Evolutionary
Tuning of
KB Parameters] |!nference Engine

 

    
     

Evolutionary
Learning of
Approximative/
TSK-Rules

Evolutionary
Learning of
Linguistic
Models

Embedded
Evolutionary

 
 

DB Learning

 

NS

A Priori
Evolutionary
DB Learning

 

   

   
  
  
  

Evolutionary
Adaptive

Defuzzification

Methods

   
 

Evolutionary
Adaptive
Inference

System

   
  
 

 

 

FIGURE 2 Evolutionary Fuzzy Systems Taxonomy. First presented in Fernandez et al. [29].

is that a larger search space is to be considered, imply-
ing a slower and harder learning process.

iti) Evolutionary rule learning. This is by far the pre-
ferred approach in the specialized literature. Starting from
a predefined DB (in most cases, composed of equally dis-
tributed fuzzy partitions), the fuzzy rules of the RB are
generated by means of the evolutionary process.

iv) Evolutionary DB learning. In this case, several
parameters of the DB are considered. This includes
the granularity degree, the shape of the membership
functions, and the scaling functions, among other DB
components. This DB learning can be carried out
either after the RB is obtained or at the same time.

QO Evolutionary learning of KB components and
inference engine parameters. This comprises a special
case, in which both the adaptive inference engine and
the KB components are optimized. The main idea is to
achieve the best synergy between the former elements,
including both in a simultaneous learning process.

b) Evolutionary tuning. Tuning involves an a-posteriori

optimization of the DB or the inference engine parame-

74 IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | FEBRUARY 2019

ters. The RB is initially obtained by using a predefined

DB and inference engine. Two different approaches have

been proposed:

i) Evolutionary tuning of KB parameters. The
parameters of the fuzzy sets are tuned in the evolution-
ary process.

ii) Evolutionary adaptive inference engine. It is divided
into two groups depending on whether it is applied to the
inference system or the defuzzification method. In the
former case, the final objective is to obtain a higher coop-
eration among fuzzy rules by acting on the inference
engine. The linguistic rule interpretability is maintained as
the DB remains homogeneous. In the latter case, if a
weighted average operator is used in the defuzzifier, its
parameters can be optimized by means of EAs.

2) Objectives Trade-off: Approaches for Jointly

Optimizing Several Objectives

As seen in Section I, there is a growing interest in developing
FRBSs that are both accurate and interpretable [30], [31]. How-

ever, this goal is not easy to achieve, as these criteria are usually in

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
conflict. A smart solution is to carry out the learning procedure
by means of MOEAs [22]. MOEAs generate a set of FRBSs
with different trade-offs between the different learning objectives
being considered in the optimization process. This solution is
known as MOEFPS [23], which can consider any metric of per-
formance to carry out the optimization of the FRBSs, namely
the cost, or the simplicity or comprehensibility, among others.

A short description of these techniques based on the multi-
objective nature of the problem is given in what follows. In
case of using MOEFSs for learning or tuning FRBS compo-
nents, the reader should refer to those models introduced in the
previous section.

QO) Performance vs. Interpretability. Fuzzy systems in gen-
eral, and fuzzy linguistic models in particular, are well suit-
ed to understanding the nature of the problem that they
represent. For this reason, it is quite important to consider
different interpretability measures as an estimation of the
FRBS potential.

An FRBS is not interpretable per se. Instead, there are
many different issues which must be taken into account in
order to obtain a human-interpretable structure. Among
others, the rule base compactness or the semantic compre-
hensibility of the fuzzy partitions should be considered.
Hence, the ML method used to learn the fuzzy model, clas-
sifier, or decision support systems must be properly designed
to obtain the desired trade-off for the problem at hand.

Specifically, two different options can be taken into
account. On the one hand, those which are based on the
simplicity could be considered, i.e. the dimensionality, of the
system (the simpler, the better). On the other hand, seman-
tic-based metrics could be considered, i.e. the comprehensi-
bility of the derived system.

QO Performance vs. Performance. When addressing a con-
trol system problem, there are more constraints that must be
taken into account than there are for standard data mining
tasks. Specifically, there is a need to obtain an efficient con-
troller, with proper stability and, if possible, to have a com-
pact and interpretable structure. Therefore, the use of
MOEAs for designing fuzzy controllers has become a very
successful approach.

In this case, both the structure of the controller and its
parameters must be obtained. This implies a direct adaptation
for all methods previously introduced for general EFSs. Spe-
cifically, a post-processing tuning step is the most common
approach due to its simplicity and reduced search space.

3) New Representations: Fuzzy Sets Extensions
In several applications, where the degree of uncertainty is very
high, traditional fuzzy sets are not able to provide an appropri-
ate representation of the hidden knowledge. To cope with this
aspect, some extensions such as type-2 fuzzy sets and interval-
valued fuzzy sets are used instead.

Being special cases of fuzzy set representations, there is no
established way of obtaining their parameter values and/or their

structure. For this reason, EAs are a proper tool for the design

strategy and/or for optimizing these fuzzy models. For example,
the parameters of type-2 fuzzy sets may be optimized from a
given standard fuzzy set, or by considering the direct generation
from data. Recent approaches are aimed at tuning rules and

conditions using this kind of fuzzy representation [32], [33].

IIL What Evolutionary Fuzzy

Systems Should Be Used For?

In the introduction of this manuscript, several interesting prop-
erties of fuzzy systems were highlighted. In short, they provide
two interesting features making them very useful for knowl-
edge representation tasks:

1) On the one hand, the inherent interpretability of the sys-
tem. This refers to two different aspects. First, the intrinsic
comprehensibility associated with the use of a simple
description mechanism in the form of fuzzy linguistic rules,
very close to natural language. And second, the comprehen-
sibility and understandability of the rule-based system and
the inference procedure.

2) On the other hand, the robustness of adapting to and learn-
ing from complex problems, i.e. to model scenarios which
are difficult to represent with other types of paradigms. In
particular, it is a very interesting tool to apply when users
must deal with the lack of data or uncertainty in the defini-
tion of the input data.

In regards to the aspects associated with the interpretability
of FRBSs, first the “cointension” term must be highlighted,
defined between fuzzy sets and regular concepts [34]. In partic-
ular, Lofti A. Zadeh emphasized that this semantic cointension
was the key for understanding the success of the application of
fuzzy models, namely the importance of the human compo-
nent in the data science process [15]. This is what can be
referred to as human-centric modeling and decision making,
which implies the need to provide descriptive models and
decision support systems able to comprehend the underlying
human mental processes in order to manage the information in
a more human-oriented style [35]. Human-centric models and
descriptive support systems allow the designer to use the com-
prehensibility of the designed solutions to both understand the
underlying human reasoning processes and uncover knowledge
about the system at hand, as well as to enhance the problem
solving [15].

Although the use of linguistic labels provides a solid basis
for achieving both facets, many model induction techniques
that use fuzzy systems can impose certain interpretability con-
straints (linguistic structure, rule length, and rule set size, among
others) in the search for greater accuracy. It is at this point
where EFS algorithms can be used to find a good trade-off
between interpretability and accuracy [30], [31].

As previously discussed in detail in Section II-B, this can be
done in one out of two phases. On the one hand, in the con-
struction of the model itself, i.e. during the learning process.
On the other hand, a posteriori, i.e. once the model has been
obtained, a component tuning can be carried out. In both

cases, the properties associated with EAs in regards to the

FEBRUARY 2019 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE 75

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
 

Analyzing these properties in detail, the key ability

of Evolutionary Fuzzy Systems is to obtain a Machine
Learning system incorporating the fundamental
property of what was defined as eXplainable Artificial
Intelligence. It is not only about understanding the
composition of the model, but also about the fact that
the system is able to explain to the user the process it

followed to make the output decision.

coding of different types of information provide a clear benefit:
flexibility [29]. In fact, they permit optimizing from simple
parameters of fuzzy systems to complete sets of rules. The prac-
titioner will therefore be able to directly pair the necessary
interpretability constraints while generating a model with a
high predictive power [30], [31].

Analyzing these properties in detail, the key ability of EFS
is to obtain an ML system incorporating the fundamental
property of what was defined as XAI [12]. It is not only about
understanding the composition of the model, but also about
the fact that the system is able to explain to the user the pro-
cess it followed to make the output decision. Therefore, the
synergy between fuzzy systems and the ability of EAs to learn
them makes EFSs a very appealing tool for a large number of
problems. As an example of these developments, one may
refer to the EFS design that allows the fusion mechanism for
a classifier ensemble to be interpreted by the human designer
thanks to a hierarchical fuzzy rule-based structure [36].

Focusing on the robustness properties associated with EFSs,
the first aspect to be taken into account is their effectiveness in
extracting information that resides in small datasets with low
density in the domain, the so-called “lack of data” problem [37].
The reason is simple: by defining the universe of discourse of
the fuzzy variables along the overall domain of the attributes
that represent the problem, an integral coverage of the input
space is allowed. Furthermore, when there is some overlap
between the fuzzy sets, a smoother transition among the mod-
eled information granules is expected to be obtained.

In addition to the lack of data, the benefits of EFSs in han-
dling imprecise and uncertain data is also of high priority. In
these cases, the flexibility of the definition of fuzzy partitions as
well as the membership functions must be taken into account.
In this sense, it is natural to use different extensions to the tra-
ditional type-1 fuzzy sets to add an extra level of freedom in
the representation. However, defining the exact values accord-
ing to the problem may not be a trivial task performed by a
human operator. Thus, it is of great interest to be able to rely
on the use of EAs to learn or adjust the components of the
fuzzy DB.

It should be stressed that social network analysis or social
mining are application areas that may benefit the most from
EFSs [38]. It has become a hot topic in the last few years due to

76 IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | FEBRUARY 2019

the increase of social media interactions. Cor-
porations and academia are very interested in
conceptualizing, modeling, analyzing, explain-
ing, and predicting these relationships. There is
a natural connection between graph theory,
on which social network analysis is based, and
fuzzy set theory. This allows providing an
easier and more robust way to express rela-
tionships among nodes in these networks.
Furthermore, some research has already dis-
cussed the theoretical and conceptual models
for social data based on fuzzy sets [39], [40].

Another area in which the inherent uncer-
tainty of the data imposes a difficult restriction, and also requires
interpretable models in order to be truly useful to the end-users,
is finance. In this environment, the comprehension of how inputs
and outputs are related to each other is crucial in order to be
able to make operative and strategic decisions. Therefore, EFSs
have been successfully applied to many financial domains [41].

Decisions in medical applications are also considered to be
critical. Therefore, any action taken by experts must be taken
confidently. This implies that any decision support system used
in this context must be trustworthy and transparent. In other
words, it must explain to both the doctor and the patient, the
reasons behind a particular diagnosis. In this sense, an EFS
based on fuzzy linguistic rules might be the proper choice [42].

Finally, current solutions focused on intrusion detection sys-
tems are high priority. The reason for this is clear, namely the
vast use of information systems and the need of establishing
security policies and rules that allow an undesired system access
to be discriminated. In particular, EFS-based approaches are
very interesting for several reasons. First of all, this type of prob-
lem has a common structure. Indeed, they are described by
numeric data and therefore crisp thresholds can lead to low
detection accuracy. Additionally, the boundary between legit
and abnormal behavior is inherently fuzzy. In other words,
small changes in an intrusion behavior may not be recognized,
whereas a small deviation in a normal profile can generate a
false alarm. In accordance with the former, several relevant EFS
approaches based on fuzzy linguistic variables may be found in
the specialized literature [43].

IV. Where are Evolutionary Fuzzy

Systems Going? Future Prospects

From Section II, the reader might acknowledge that EFS-based
methods have come a long way since the pioneering proposal
of the first EFS-based methods more than 25 years ago. Exten-
sive research has been carried out in this field, mainly due to
the versatility associated with the learning and tuning of the
different components of the FRBS.

Now, researchers and practitioners must look ahead to discern
what the future objectives and challenges in the EFS field could
be. The open directions for novel research are particularly associat-
ed with two basic pillars: (1) efforts focused on the novel optimi-
zation. of the internal components of the FRBS (Section IV-A);

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
and (2) analysis of the application of EPSs to emerging work sce-
narios in data science (Section IV-B). Finally, the particular char-
acteristics of this big data scenario imply paying special attention
to the design and development of EFSs (Section IV-C).

A. Optimization of Novel FRBS’ Components
There is a wide variety of FRBS elements whose values can be
optimized using an EFS. Enumerating some possibilities, we
should stress the choice of relevant inputs, scaling factors, member-
ship functions, shape functions, granularity, fuzzy rules, inference
parameters, number of rules, and so on. In addition, as previously
described the power of EAs permits the joint learning of several of
these components so as to obtain a much more robust FRBS.
However, it must be highlighted once again that for current
applications it is not useful to generate models with excessively
complex components. In other words, special attention must be
paid to maintain the comprehensibility of the models. For this
reason, the evolution of the multiple components of an FRBS
must always be carried out considering the semantic properties

and simplicity of the obtained system.

B. Emerging Data Science Scenarios for EFS

Data science is a quite recent field of study, and it is still rapidly
expanding. New problems and practical applications arise and
require the development of robust techniques to address new
complex paradigms. Specifically, novel problems are usually
characterized by: i) uncertainty, the available information is
often imprecise, uncertain, noisy, or there might be an acute lack
of information; the objective of the modeling or decision prob-
lem is ambiguous and the problem structure might be loosely
specified; and the problem environment may be not stationary
but changing; ii) the impressively increasing problem dimen-
sions, requiring prohibitive computational times to achieve
problem tractability; and iti) the need to provide descriptive
models and decision support systems able to comprehend the
underlying human mental processes to manage the information
in a more human-oriented style (numan-centric modeling and
decision making) [44]. EFSs have excelled in many different sce-
narios and, in accordance with this general good behavior, a
bright future for their use in most of the incoming data science
areas can be predicted.

A first case-study is related to non-standard classification
problems, such as those based on multi-label and multi-instance
learning. In the former case, the model must classify a query
instance in a set of non-exclusive categories [45]. In the latter,
incomplete information about the classes or instances implies a
handicap during the categorization [46]. Thus, instances are
coupled into a single “bag”, being unaware of which is the one
that provides the true label. Finally, both problems can be com-
bined adding a higher degree of complexity to the design of
future solutions. Taking into account this type of relationship
between labels and/or instances, the nature of both problems is
well suited for the use of fuzzy sets and systems, and thus for
EFSs. However, at present just a few attempts have been made
to solve this task using these tools [47].

The novel topic related to ordinal and monotonic classifica-
tion [48] should be noted. This case study comprises those prob-
lems where both the input attributes and/or the class have a
monotonicity constraint. The first study that extracts fuzzy rules
satisfying these constraints without the need of a preprocessing
stage can be found in [49], where authors incorporate several
mechanisms based on monotonicity with EPS algorithms.

Supervised descriptive rule discovery [50], and in particular
subgroup discovery [51], [52], is also another interesting area of
study. The goal is to locate subgroups which are statistically
“most interesting” for the user. The obtained model must fulfill
some properties such as simplicity (understandable structure
with few variables), and both a high significance and support.
Another particular case of this scenario is the so-called Emerg-
ing Pattern Extraction [53], in which frequency changes signif-
icantly from one dataset to another. In this context, the use of
MOEFSs has shown that obtained rules allow the descriptions
of the emerging phenomena to be simpler than those in the
state of the art [54].

A topic that has gained much attention in the research
community of data science is multi-view learning [55], where
examples are described by multiple distinct feature sets. This is
related to the way several current-day problems are defined,
such as multimedia-content, web page classification, and bioin-
formatics. The approaches which address multi-view learning
via data integration are quite diverse.

Another interesting area of study is semi-supervised learning.
It is based on problems in which only a subset of the instances
are labeled and the algorithm can modify the output of the
training data [56]. Considered as an extension to unsupervised
learning, clustering techniques have received much attention. In
this context, the use of an FRBS can represent a sort of linguis-
tic description of the dissimilarity relation among patterns [57],
thus helping the recognition of the clusters with fewer data.

Finally, there are novel domains of application arising every-
day that share problems similar to the ones already solved in
other domains. Therefore, it can be of interest to exploit previ-
ously acquired knowledge to manage new tasks in a quick and
effective way. This is the premise of the area of research known
as transfer learning [58]."Taking advantage of fuzzy learning
methods for this task is evident, regarding the uncertainty that
is found in these dynamic environments [59]. They also allow
leveraging knowledge from some referring scenes when there
is little data available [60].

These are some of the new data science scenarios without
the aim of being very exhaustive. The previous analysis illus-
trates that there are not enough scientific studies using EPSs for
such novel and significant topics. Therefore, this must be
regarded as a “call-to-action” for current researchers in the area
of EPSs to open the way in such promising lines of study, from
both the theoretical and practical points of view.

C. EFSs in Big Data: A Significant Topic for the Near Future

One of the hottest topics for current research is related to data
science and big data problems [4]. An in-depth analysis of the

FEBRUARY 2019 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE 77

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
 

In spite of the impressive capacity of black-box
solutions to obtain accurate models, this virtue is
often associated with a higher system complexity.
Designers are often unable to understand how the
systems work, and also to decipher why they

produce a certain output.

current state of this framework was carried out in both [61]
and [62], where authors reported the good properties of fuzzy
systems when dealing with such types of applications. However,
focusing on the case of EPSs for big data, so far few studies
have been developed in this area of research [63]-[65].

The main reason for the lack of proposals in the special-
ized literature is the difficulty in reaching solutions that are
scalable within the EFS paradigm. Indeed, this constraint is
associated with the computation of an accuracy-based fitness
function from data at the core of the evolutionary procedure.
In other words, it is not straightforward to develop method-
ologies that allow the whole dataset to be handled in a rea-
sonable time.

Por this reason, the development of learning and tuning
methods must be redirected within a distributed environment.
To this end, MapReduce has established as a de-facto solution
to simplify the implementation of these techniques [66].

It is basically an execution environment which lays over
a distributed and fault-tolerant file system, HDFS being
the most common option. By means of two simple func-
tions, Map and Reduce, any implementation can be auto-
matically parallelized in a transparent way for the programmer.
The first function allows operating on independent “chunks”
of data, by applying the same procedure. The second merg-
es the outputs of the Map functions to produce a single
final output.

According to the new MapReduce paradigm of distributed
programming, any ML solution can be categorized into two
main types [67]:

1) Local approaches (also known as approximate models)
that work directly on the distributed chunks of data by
creating partial models that are then aggregated.

2) Global approaches (also known as exact models) that iter-
ate over all examples to generate the model or build the
system iteratively by optimal merging.

By analyzing the properties of each of these types of meth-
odologies some very interesting insights could be achieved.

Local approaches are a priori easier to develop than the
original algorithm would be embedded in the different Map
tasks. However, it would imply a greater effort in the merging
phase of the independent sub-models (Reduce function).
Another advantage is an expected efficiency increase when
augmenting the number of partitions. The main hitch, in this
case, would be to work with a smaller number of data in each

of the sub-processes. Fortunately, the good behavior of

78 IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | FEBRUARY 2019

FRBSs and EFSs with respect to such an
eventuality has been previously highlighted
[62] (see Section III).

Global approaches, on the other hand, have
as their main virtue the learning of more
robust models. This is a theoretical issue as
they comprise the analysis of the complete set
of data. However, this is precisely their major
disadvantage. Specifically, more effort must be
devoted to design the methodology to meet
the conditions of correctness. In addition to the former, the
efficiency can also be reduced as the process must be iterated
several times.

A very interesting option in this case is to take advantage of
the features of novel environments such as Spark [68]. It is a dis-
tributed computing platform that provides a memory-intensive
scheme, being very suitable for ML algorithms [67]. Specifically, it
supports very versatile data structures, together with a wide range
of operations for transforming them, such as filtering, grouping, or
set operations, among others.

When referring to big data constraints for the development
of novel ML approaches, the efforts are mainly focused on
finding scalable solutions considering only one-side of the Vol-
ume, namely the “Big Instance Size”. However, there is anoth-
er significant issue in this regard, which is known as the “Big
Dimensionality” explosion [69]. There are diverse ways to face
this problem, including smart feature selection [70], fuzzy
ensemble models based on bagging for vertically partitioning
the training set [71], [72], and the use of fuzzy decision trees
that internally consider a feature ranking mechanism [73].

As it has been discussed throughout this section, the
task of addressing big data problems with EFSs is still far
from being fully covered. Many state-of-the-art learning
and tuning approaches need to be redesigned and translat-
ed into the new MapReduce paradigm. The objective is to
take advantage of the power and quality that these algo-
rithms have already shown, but using the new applications
and current-day data sets. Furthermore, researchers must
be aware of this circumstance to be more ambitious and
develop new robust methodologies by taking into account
the new functionalities available in the current program-
ming environments. As a final remark, the difficulties asso-
ciated with this framework imply the development of
much more ingenious and effective ideas to achieve greater

milestones in this area of research.

V. Remarks on the Need for Interpretable

and Explainable Artificial Intelligence

Throughout this paper, it has been stressed that there are two
main criteria any practitioner must consider when determining
what kind of ML techniques should be used to address a spe-
cific regression, classification, or decision-making problem:
accuracy and interpretability/explainability. Of course, the ideal
situation would be to obtain jointly high degrees of both but,

since they are in conflict, this is a complex task. Figure 3 shows

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
 

 
   
   

Neural Nets

  
    

Deep

 

 

 

 

 

 

 

Learning

 
 
 
    
  

 

Bayesian
Belief Models

  

Statistical

Models SVMs

     
  

    
   

AOGs

Markov
Models

  

 

Dp
oO
Graphical g A © Current Value
Mosels 3 O Desired Value
<} 50—»0
Ensemble Challenge
Methods

   
 
 
     

Decision
Trees

(Fuzzy) Rule Based
Systems

 

mol > i
_
La

 

$$$ >
Explainability

     
   

 

 

FIGURE 3 Accuracy vs. explainability: Venn diagram for several ML models (partially inspired from [74]).

this fact with a trade-off for some well-known ML techniques
and the current challenge to increase the explainability without
reducing accuracy. For the most interpretable models the chal-
lenge is to increase both axes.

In practice, one of the two properties usually prevails over
the other (use of black-box vs. white- or gray-box models/
classifiers). Driven by the need for accuracy, we are witnessing
an emerging trend to embrace “black-box” solutions, in partic-
ular those based on neural networks and mainly Deep Learning
(DL) [75], [76]. In short, these systems are based on a number
of layers of fully-connected neurons. The first layers are devoted
to extracting simpler attributes from the data, which are then
combined in the subsequent layers to form more complex and
thus more representative attributes. In spite of the impressive
capacity of these black-box solutions to obtain accurate models,
this virtue is often associated with a higher system complexity.
Designers are often unable to understand how the systems
work, and also to decipher why they produce a certain output.
In fact, the effectiveness of the existing ML methods could be
limited by the machines’ inability to explain its thoughts and
actions to human users in some application fields, and thus may
lead to unsafe and incorrect decisions’.

Interpreting and explaining deep networks is a young and
emerging field of research. In [17] can be found a study of
techniques for interpreting complex machine learning models,
with focus on DL. Most of the proposals have focused on

 

*hetps://www.statnews.com/2018/07/25 /ibm-watson-recom mended-unsafe-incorrect-
treatments/

post-hoc interpretability but greater efforts are required in the
near future to understand the ML models.

In this scenario, rule-based systems [77] allow to audit the
extracted knowledge with a double objective. On the one
hand, obtaining a clear explanation of the cognition process
carried out by the system. On the other hand, being able to
trust in the description of the rules and their relationship with
the problem that is aimed to be solved. In particular, EFSs
combine the ability to represent knowledge in natural way for
human understanding (with fuzzy rules), the strength of fuzzy
reasoning and the ability of EAs for search in complex and ill-
defined problems. However, it must be emphasized that
FRBSs must remain simple and understandable, since they are
not interpretable per se [78]. It is important to take account of
different issues in order to obtain FRBSs that represent
knowledge easily understood by humans. Among others, the
tule base compactness or the semantic comprehensibility of
the fuzzy partitions must be stressed. Moreover, the EFSs must
be properly designed to obtain the desired trade-off between
accuracy and explainability for the problem at hand. When in
the EFS design more attention is paid to the accuracy than to
explainability, the skills of the fuzzy system obtained are hardly
comparable with other preferable and more complex solutions
such as DL.

DL is a powerful paradigm that can effectively capture rele-
vant features, in particular for those problems from which a
higher level of abstraction is needed to describe the hidden
knowledge [76]. In any way, DL solutions must not be consid-
ered to be rivals for FRBSs and EFSs, but rather as comple-

mentary approaches, each one with their strengths and

FEBRUARY 2019 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE 79

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
 

Machine Learning methods based on Evolutionary
Fuzzy Systems preserve the original essence of
comprehensibility exposed by Zadeh, also boosting

 

ies.

 

their modeling abi

drawbacks. A promising step forward the achievement of this
objective could be to make use of the good interpretability
properties of EFSs in conjunction with DL.

There are some interesting examples of the positive synergy
between both paradigms. First, a fuzzy classifier based on stack-
ing is proposed in [79]. It embeds a zero-order TSK FRBS
with a limited number of linguistic labels within a neuron-lay-
ered representation. Thus, each layer becomes a single model
within the “ensemble”. After the learning stage using a DL
procedure, rules can be simply extracted from each “neuron” to
keep the original interpretability. A different approach is pro-
posed in [80]. It comprises a hierarchical system composed of a
fuzzy rule layer, to manage the ambiguity of the data, and a DL
layer, to reduce the noise and to create higher order variables
for a more accurate representation. Then, both parts are fused
leading to a more robust classification. Finally, an interpretable
structure for DL networks is also proposed in [81].‘To achieve
the required explainability, zero-order TSK fuzzy linguistic
rules are encoded and learned in the final layer of the net to
perform the final classification, just after the feature extraction.

In summary, EFSs are a very significant tool in many fields of
application where the explainability of the decision must be
taken into account.The reader should refer to areas such as med-
ical diagnosis, financial problems, or security systems, among oth-
ers. It is straightforward to acknowledge that, in these scenarios,
the same importance must be given to the accuracy/confidence
of the output and the explainability of the decision made. In
other words, ML models based on EFSs are a desirable choice
when the output is intended to be trusted by the human user.

VI. Concluding Remarks
The world of data science has changed the way applications are
approached. At present, the core of the model not only aims to
achieve the highest possible accuracy but also to make it explain-
able for researchers and practitioners. In this sense, ML methods
based on EFSs preserve the original essence of comprehensibility
exposed by Zadeh, also boosting their modeling abilities. It is
straightforward to acknowledge that this provides several advan-
tages over other paradigms toward handing XAI learning models
including transparency, understanding and comprehensibility.
Throughout this work, a variety of perspectives for under-
standing the virtues of EFSs have been identified. A series of “4
W questions” (why, when, what for, and where to) have
been posed. The objective was that the answers provide some
insight into the capabilities that EPSs have shown when being
adapted to different research areas, and to promote new devel-

opments in the discipline.

80 IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE | FEBRUARY 2019

It has allowed us to notice the lack of una-
nimity in the literature about XAI concepts,
their formal definition, quantitative measures
and relations among them. There are currently
numerous coexisting approaches to interpret-
ability/explainability. Por this reason, there is a
need for a debate within the Al, ML and
fuzzy communities to standardize and to assess
these concepts and the auxiliary properties than let to interpret
and explain properly the AI models and outputs.

Acknowledgments
The authors would like to ennoble Prof. Lofti A. Zadeh’s figure,
both from a personal and a professional viewpomt. They would
like to acknowledge the pioneering development of the fuzzy sets
and systems research area and the strong positive influence Prof.
Zadeh played for its development during more than
50 years. The authors are very proud of having met and collabo-
rated with Prof. Zadeh in many different activities both at the
University of Granada and the European Centre for Soft Com-
puting. They had the chance to recognize how Lotfi was not only
an impressive researcher with a visionary mind but, and even
more important, an outstanding human being. He will be missed.
This work have been partially supported by the Spanish
Ministry of Science and Technology under projects TIN2015-
68454-R, TIN2015-67661-P, and TIN2017-89517-P, including
European Regional Development Funds.

References

[1] F. Bacon, Meditationes Sacrae. Excusum impensis Humfredi Hooper, 1597.

[2] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and pros-
pects,” Science, vol. 349, no. 6245, pp. 255-260, 2015.

[3] V. Dhar, “Data science and prediction,” Commun. Assoc. Comput. Mach., vol. 56, pp.
64-73, 2013.

[4] A. Fernandez, S. Rio, V. Lopez, A. Bawakid, M. J. del Jesus, J. M. Benitez, and F.
Herrera, “Big data with cloud computing: An insight on the computing environment,
MapReduce and programming framework,” WIREs Data Mining Knowl. Discovery, vol.
4, no. 5, pp. 380-409, 2014.

[5] L. A. Zadeh, “Fuzzy sets,” Inf Control, vol. 8, pp. 338-353, 1965.

[6] H. Ishibuchi, T. Nakashima, and M. Nii, Classification and Modeling with Linguistic
Information Granules. New York, NY, USA: Springer, 2005.

[7] E. H. Mamdani and S. Assilian, “An experiment in linguistic synthesis with a fuzzy
logic controller,” Iat. J. Man-Mach. Stud., vol. 7, no. 1, pp. 1-13, 1975.

[8] A. E. Eiben and J. E. Smith, Introduction to Evolutionary Computation. Berlin, Germany:
Springer-Verlag, 2003.

[9] J. H. Holland, Adaptation ia Natural and Artificial Systems. Ann Arbor, MI, USA: Uni-
versity of Michigan Press, 1975.

[10] A. Freitas, Data Mining and Knowledge Discovery with Evolutionary Algorithms. ser. Natu-
ral Computing Series. Berlin: Springer, 2002.

[11] W. Samek, T. Wiegand, and K. R. Miiller, “Explainable artificial intelligence:
Understanding, visualizing and interpreting deep learning models,” CoRR, vol.
abs/1708.08296, 2017. [Online]. Available: http://www.arxiv.org/abs/1708.08296. Ac-
cessed on: Nov. 21, 2018.

[12] D. Castelvecchi, “Can we open the black box of AI?” Nature, vol. 538, no. 7623, pp.
20-23, 2016.

[13] S. F. Bryce Goodman, “European Union regulations on algorithmic decision-making
and a right to explanation,” Artif. Intell. Mag., vol. 38, no. 3, 2017. [Online]. Available:
https://arxiv.org/abs/1606.08813v3. Accessed on: Nov. 21, 2018.

[14] F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable machine
learning,” CoRR, vol. abs/1702.08608, 2017. [Online]. Available: https://arxiv.org/
abs/1702.08608. Accessed on: Nov. 21, 2018.

[15] M. Gleicher, “A framework for considering comprehensibility in modeling,” Big
Data, vol. 4, no. 2, pp. 75-88, 2016.

[16] Z. C. Lipton, “The mythos of model interpretability,” CoRR, vol. abs/1606.03490,
2017. [Online]. Available: https://arxiv.org/abs/1606.03490. Accessed on: Nov. 21,
2018.

[17] G. Montavon, W. Samek, and K. R. Miiller, “Methods for interpreting and under-
standing deep neural networks,” Dig. Signal Process., vol. 73, pp. 1-15, 2018.

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
[18] M. W. Craven, “Extracting comprehensible models from trained neural networks,”
Thesis, Univ. Wisconsin, 1996.

[19] R. S. Michalski, “A theory and methodology of inductive learning,” Artif. Intell., vol.
20, pp. 111-161, 1983.

[20] M. Ribeiro, S. Singh, and C. Guestrin, “Why should I trust you?” explaining the
predictions of any classifier,” in Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and
Data Mining, 2016, pp. 1135-1144.

[21] ©. Cordon, F. Herrera, F. Hoffmann, and L. Magdalena, Genetic Fuzzy Systems Evo-
lutionary Tuning and Learning of Fuzzy Knowledge Bases. Singapore: World Scientific, 2001.
[22] C. A. Coello-Coello, G. Lamont, and D. van Veldhuizen, Evolutionary Algorithms
for Solving Multi-Objective Problems, 2nd ed., ser. Genetic and Evolutionary Computation.
Berlin: Springer, 2007.

[23] M. Fazzolari, R. Alcala, Y. Nojima, H. Ishibuchi, and F. Herrera, “A review of the
application of multi-objective evolutionary systems: Current status and further direc-
tions,” IEEE Trans. Fuzzy Syst., vol. 21, no. 1, pp. 45-65, 2013.

[24] C. Karr, “Genetic algorithms for fuzzy controllers,” Artif. Intell. Expert, vol. 6, no.
2, pp. 26-33, 1991.

[25] M. Valenzuela~-Rendon, “The fuzzy classifier system: A classifier system for con-
tinuously varying variables,” in Proc. 4th Int. Conf. Genetic Algorithms, 1991, pp. 346-353.
[26] P. Thrift, “Fuzzy logic synthesis with genetic algorithms,” in Proc. 4th Int. Conf.
Genetic Algorithms, 1991, pp. 509-513.

[27] D. T. Pham and D. Karaboga, “Optimum design of fuzzy logic controllers using
genetic algorithms,” J. Syst. Exg., vol. 1, pp. 114-118, 1991.

[28] ©. Cordon, “A historical review of evolutionary learning methods for Mamdani-type
fuzzy rule-based systems: Designing interpretable genetic fuzzy systems,” Int. J. Approx.
Reason., vol. 52, no. 6, pp. 894-913, 2011.

[29] A. Fernandez, V. Lopez, M. J. del Jesus, and F. Herrera, “Revisiting evolutionary
fuzzy systems: Taxonomy, applications, new trends and challenges,” Knowl. Based Syst.,
vol. 80, pp. 109-121, 2015.

[30] J. M. Alonso, L. Magdalena, and G. Gonzilez-Rodriguez, “Looking for a good fuzzy
system interpretability index: An experimental approach,” Int. J. Approx. Reason., vol. 51,
pp. 115-134, 2009.

[31] M. J. Gacto, R. Alcala, and F. Herrera, “Interpretability of linguistic fuzzy rule-
based systems: An overview of interpretability measures,” Inf. Sei., vol. 181, no. 20, pp.
4340-4360, 2011.

[32] J. Sanz, D. Bernardo, F. Herrera, H. Bustince, and H. Hagras, “A compact evolution-
ary interval-valued fuzzy rule-based classification system for the modeling and prediction
of real-world financial applications with imbalanced data,” IEEE Trans. Fuzzy Syst., vol.
23, no. 4, pp. 973-990, 2015.

[33] K. Tai, A.-R. El-Sayed, M. Biglarbegian, C. I. Gonzalez, ©. Castillo, and S.
Mahmud, “Review of recent type-2 fuzzy controller applications,” Algorithms, vol. 9,
no. 2, 2016.

[34] L. A. Zadeh, “Is there a need for fuzzy logic?” Inf. Sdi., vol. 178, no. 13, pp. 2751—
2779, 2008.

[35] P. Guo and W. Pedrycz, Eds., Human-Centric Decision-Making Models for Social Sciences,
vol. 502, ser. Studies in Computational Intelligence. New York, NY, USA: Springer,
2014.

[36] K. Trawinski, O. Cordon, L. Sanchez, and A. Quirin, “A genetic fuzzy linguistic
combination method for fuzzy rule-based multi-classifiers,” IEEE Trans. Fuzzy Syst., vol.
21, no. 5, pp. 950-965, 2013.

[37] V. Lopez, A. Fernandez, S. Garcia, V. Palade, and F. Herrera, “An insight into classi-
fication with imbalanced data: Empirical results and current trends on using data intrinsic
characteristics,” Inf. Sci., vol. 250, no. 20, pp. 113-141, 2013.

[38] G. Bello-Orgaz, J.J. Jung, and D. Camacho, “Social big data: Recent achievements
and new challenges,” Inf. Fusion, vol. 28, pp. 45-59, 2016.

[39] S. Bastani, A. K. Jafarabad, and M. H. F. Zarandi, “Fuzzy models for link prediction
in social networks,” Int. J. Intell. Syst., vol. 28, no. 8, pp. 768-786, 2013.

[40] M. Dragoni and G. Petrucci, “A fuzzy-based strategy for multi-domain sentiment
analysis,” Int. J. Approx. Reason., vol. 93, pp. 59-73, 2018.

[41] M. Antonelli, D. Bernardo, H. Hagras, and F. Marcelloni, “Multiobjective evolu-
tionary optimization of type-2 fuzzy rule-based systems for financial data classification,”
IEEE Trans. Fuzzy Syst., vol. 25, no. 2, pp. 249-264, 2017.

[42] S. A. Mokeddem, “A fuzzy classification model for myocardial infarction risk assess-
ment,” Appl. Intell., vol. 48, no. 5, pp. 1233-1250, 2018.

[43] S. Elbag, A. Fernandez, A. Altalhi, S. Alshomrani, and F. Herrera, “A multi-objective
evolutionary fuzzy system to obtain a broad and accurate set of solutions in intrusion
detection systems,” Soft Comput., in press.

[44] M. Chen, F. Herrera, and K. Hwang, “Cognitive computing: Architecture, tech-
nologies and intelligent applications,” IEEE Access, vol. 6, pp. 19774-19783, 2018.

[45] F. Herrera, F. Charte, A. J. Rivera, and M. J. del Jesus, Multilabel Classification: Problem
Analysis, Metrics and Techniques. New York, NY, USA: Springer, 2016.

[46] F. Herrera, S. Ventura, R. Bello, C. Cornelis, A. Zafra, D. S. Tarragé, and S. Vluy-
mans, Multiple Instance Learning: Foundations and Algorithms. New York, NY, USA: Spring-
er, 2016.

[47] S. Vluymans, D. Sanchez-Tarrago, Y. Saeys, C. Cornelis, and F. Herrera, “Fuzzy
multi-instance classifiers,” IEEE Trans. Fuzzy Syst., vol. 24, no. 6, pp. 1395-1409, 2016.
[48] W. Kotlowski and R. Slowinski, “On nonparametric ordinal classification with
monotonicity constraints,” IEEE Trans. Knowl. Data Eng., vol. 25, no. 11, pp. 2576-2589,
2013.

[49] J. Alcala-Fdez, R. Alcala, S. Gonzalez, Y. Nojima, and S. Garcia, “Evolutionary
fuzzy rule-based methods for monotonic classification,” IEEE Trans. Fuzzy Syst., vol. 25,
no. 6, pp. 1376-1390, 2017.

[50] P. K. Novak, N. Lavrac, and G. I. Webb, “Supervised descriptive rule discovery: A
unifying survey of contrast set, emerging pattern and subgroup mining,” J. Mach. Learn.
Res., vol. 10, pp. 377-403, 2009.

[51] W. Klésgen, “Explora: A multipattern and multistrategy discovery assistant,” in Proc.
Advances Knowledge Discovery and Data Mining American Association for Artificial Intelligence,
1996, pp. 249-271.

[52] F. Herrera, C. J. Carmona, P. Gonzalez, and M. J. Del Jesus, “An overview on sub-
group discovery: Foundations and applications,” Knowl. Inf. Syst., vol. 29, no. 3, pp.
495-525, 2011.

[53] G. Dong and J. Li, “Mining border descriptions of emerging patterns from dataset
pairs,” Knowl. Inf. Syst., vol. 8, no. 2, pp. 178-202, 2005.

[54] A. M. Garcia-Vico, C. J. Carmona, P. Gonzalez, and M. J. del Jesus, “MOEA-EFEP:
Multi-objective evolutionary algorithm for the extraction of fuzzy emerging patterns,”
IEEE Trans. Fuzzy Syst., vol. 26, no. 5, pp. 2861-2872.

[55] J. Zhao, X. Xie, X. Xu, and S. Sun, “Multi-view learning overview: Recent progress
and new challenges,” Inf. Fusion, vol. 38, pp. 43-54, Nov. 2017.

[56] I. Triguero, S. Garcia, and F. Herrera, “Self labeled techniques for semi-supervised learn-
ing: Taxonomy, software and empirical study,” Knowl. Inf Syst., vol. 42, no. 2, pp. 245-284,
2013.

[57] M. G. C. A. Cimino, B. Lazzerini, and F. Marcelloni, “A novel approach to fuzzy
clustering based on a dissimilarity relation extracted from data using a T'S system,” Pattern
Revog., vol. 39, no. 11, pp. 2077-2091, 2006.

[58] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans. Knowl. Data Eng.,
vol. 22, no. 10, pp. 1345-1359, 2010.

[59] J. Shell and S. Coupland, “Fuzzy transfer learning: Methodology and application,”
Inf. Sai., vol. 293, pp. 59-79, 2015.

[60] H. Zuo, G. Zhang, W. Pedrycz, V. Behbood, and J. Lu, “Fuzzy regression transfer
learning in Takagi-Sugeno fuzzy models,” IEEE Trans. Fuzzy Syst., vol. 25, no. 6, pp.
1795-1807, 2017.

[61] A. Fernandez, C. J. Carmona, M. J. del Jesus, and F. Herrera, “A view on fuzzy
systems for big data: Progress and opportunities,” Int. J. Comput. Intell. Syst., vol. 9, no.
1, pp. 69-80, 2016.

[62] A. Fernandez, A. Altalhi, S. Alshomrani, and F. Herrera, “Why linguistic fuzzy rule
based classification systems perform well in big data applications?” Int. J. Comput. Intell.
Syst., vol. 10, pp. 1211-1225, 2017.

[63] 1. Rodriguez-Fdez, M. Mucientes, and A. Bugarin, “S-FRULER: Scalable fuzzy rule
learning through evolution for regression,” Knowl.-Based Syst., vol. 110, pp. 255-266, 2016.
[64] A. Ferranti, F. Marcelloni, A. Segatori, M. Antonelli, and P. Ducange, “A distributed
approach to multi-objective evolutionary generation of fuzzy rule-based classifiers from
big data,” Inf Sci., vol. 415-416, pp. 319-340, 2017.

[65] F. Pulgar-Rubio, A. J. Rivera-Rivas, M. D. Pérez-Godoy, P. Gonzalez, C. J. Car-
mona, and M. J. del Jesus, “MEFASD-BD: Multi-objective evolutionary fuzzy algorithm
for subgroup discovery in big data environments: A MapReduce solution,” Knowl.-Based
Syst., vol. 117, pp. 70-78, 2017.

[66] K. H. Lee, Y. J. Lee, H. Choi, Y. D. Chung, and B. Moon, “Parallel data processing
with MapReduce: A survey,” SIGMOD Rec., vol. 40, no. 4, pp. 11-20, 2011.

[67] S. Ramirez-Gallego, A. Fernandez, S. Garcia, M. Chen, and F. Herrera, “Big data:
Tutorial and guidelines on information and process fusion for analytics algorithms with
MapReduce,” Inf Fusion, vol. 42, pp. 51-61, 2018.

[68] M. Hamstra, H. Karau, M. Zaharia, A. Konwinski, and P. Wendell, Learning Spark:
Lightning-Fast Big Data Analytics. O’Reilly Media, 2015.

[69] Y. Zhai, YS. Ong, and I. W. Tsang, “The emerging “big dimensionality,” IEEE
Comput. Intell. Mag., vol. 9, no. 3, pp. 14-26, 2014.

[70] S. Ramirez-Gallego, H. Mourifio-Talin, D. Martinez-Rego, V. Bolén-Canedo, J.
M. Benitez, A. Alonso-Betanzos, and F. Herrera, “An information theory-based feature
selection framework for big data under apache spark,” IEEE Trans. Syst. Man, Cybern.
Syst., vol. 48, no. 9, pp. 1441-1453, 2018.

[71] P. Bonissone, J. M. Cadenas, M. Carmen Garrido, and R. Andrés Diaz-Valladares, “A
fuzzy random forest,” Int. J. Approx. Reason., vol. 51, no. 7, pp. 729-747, 2010.

[72] K. Trawiski, O. Cordén, and A. Quirin, “On designing fuzzy rule-based multiclas-
sification systems by combining FURIA with bagging and feature selection,” Int. J. Un-
certainty Fuzziness Knowl.-Based Syst., vol. 19, no. 4, pp. 589-603, 2011.

[73] A. Segatori, F. Marcelloni, and W. Pedrycz, “On distributed fuzzy decision trees for
big data,” IEEE Trans. Fuzzy Syst., vol. 26, no. 1, pp. 174-192, 2018.

[74] D. Gunning, “Explainable artificial intelligence (XAI),” Defense Advanced Research
Projects Agency, DAR PA/120, 2017.

[75] Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553,
pp. 436-444, 2015.

[76] Z. C. P. L. Q. Zhang and L. T. Yang, “A survey on deep learning for big data,” Inf.
Fusion, vol. 42, pp. 146-157, 2018.

[77] ]. Fiirnkranz, D. Gamberger, and N. Lavrac, Foundations of Rule Learning. New York,
NY, USA: Springer, 2012.

[78] L. I. Kuncheva, “How good are fuzzy if-then classifiers?” IEEE Trans. Syst. Man,
Cybern. B, vol. 30, no. 4, pp. 501-509, 2000.

[79] T. Zhou, F.-L. Chung, and S. Wang, “Deep TSK fuzzy classifier with stacked gener-
alization and triplely concise interpretability guarantee for large data,” IEEE Trans. Fuzzy
Syst., vol. 25, no. 5, pp. 1207-1221, 2017.

[80] Y. Deng, Z. Ren, Y. Kong, F. Bao, and Q. Dai, “A hierarchical fused fuzzy deep neural
network for data classification,” IEEE Trans. Fuzzy Syst., vol. 25, no. 4, pp. 1006-1012, 2017.
[81] P. Angelov and X. Gu, “A cascade of deep learning fuzzy rule-based image classifier
and SVM,” in Proc. IEEE Int. Conf. Systems Man and Cybernetics, 2017, pp. 746-751. J

FEBRUARY 2019 | IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE 81

Authorized licensed use limited to: Universidad de Jaen. Downloaded on May 12,2021 at 10:47:08 UTC from IEEE Xplore. Restrictions apply.
The Thirty-Third International
FLAIRS Conference (FLAIRS-33)

Towards Quantification of Explainability
in Explainable Artificial Intelligence Methods

Sheikh Rabiul Islam, William Eberle, Sheikh K. Ghafoor
Department of Computer Science, Tennessee Technological University
Cookeville, Tennessee, United States
sislam42 @ students.tntech.edu, weberle @tntech.edu, sghafoor@tntech.edu

Abstract

Artificial Intelligence (AI) has become an integral part of
domains such as security, finance, healthcare, medicine, and
criminal justice. Explaining the decisions of AI systems in
human terms is a key challenge—due to the high complex-
ity of the model, as well as the potential implications on hu-
man interests, rights, and lives. While Explainable AI is an
emerging field of research, there is no consensus on the def-
inition, quantification, and formalization of explainability. In
fact, the quantification of explainability is an open challenge.
In our previous work (Islam et al. 2019), we incorporated do-
main knowledge for better explainability, however, we were
unable to quantify the extent of explainability. In this work,
we (1) briefly analyze the definitions of explainability from
the perspective of different disciplines (e.g., psychology, so-
cial science), properties of explanation, explanation methods,
and human-friendly explanations; and (2) propose and formu-
late an approach to quantify the extent of explainability. Our
experimental result suggests a reasonable and model-agnostic
way to quantify explainability.

Introduction

The use of Artificial Intelligence (AI) has spread into a
spectrum of high-stakes applications from different domains
such as security, finance, healthcare, medicine, and criminal
justice, impacting human interests, rights, and lives. Follow-
ing the pace of the demand, the successful AI-based mod-
els have also become so complex that their decisions have
become too complicated to express in human terms. This
further complicates their adoption in many sensitive disci-
plines, raising concerns from the ethical, privacy, fairness,
and transparency perspectives. The root cause of the prob-
lem is the lack of explainability and/or interpretability of the
decision.

We have seen the very confusing use of the terms “ex-
plainability” and “interpretability” throughout the literature.
Some treat these as the same and stick to one, while some
differentiate among the two, and others use them ambigu-
ously. Research in Explainable Artificial Intelligence (XAD
is seeing a resurgence after three decades of slowed progress

Copyright © 2020, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

75

since some preliminary work on expert systems in the early
1980’s. Currently, there is no consensus on the definition
these terms. In addition, the quantification of explainability
is another open challenge, which will be difficult until there
is a consensus on the concrete definition of those terms (e.g.,
explainability, interpretability).

In our previous work (Islam et al. 2019) and (Islam et
al. 2020), we proposed an approach to incorporate domain
knowledge in the model to make the prediction more ex-
plainable for two different domain. However, we were un-
able to quantify the quality of explanations (i.e., validation
of explainability). There has been some previous work (Mol-
nar, Casalicchio, and Bischl 2019) that mentions three no-
tions for quantificiaiton of explainability. The first two no-
tions involve experimental studies with humans (e.g., do-
main expert, layperson) that mainly investigate whether a
human can predict the outcome of the model (Dhurandhar
et al. 2017), (Friedler et al. 2019), (Huysmans et al. 2011),
(Poursabzi-Sangdeh et al. 2018). However, the third notion
(proxy tasks) does not involve a human. Instead they use
known truth as a metric (e.g., the less the depth of the deci-
sion tree, the more explainable the model).

In this work, our contributions are as follows:

1. We define explainability and interpretability from a multi-

domain perspective and clarify their resemblance.

2. We analyze the properties of explanation, explanation

methods, and human friendly explanations.

3. We present a potential way to formalize and quantify ex-

plainability with experimental results using proxy tasks.

Explainability, and Interpretabiliy

(Miller 2018) argue that most of the work on XAI focuses
on the researcher’s intuition of what constitutes a good ex-
planation. However, there exists a vast area of research in
philosophy, psychology, and cognitive science on how peo-
ple generate, select, evaluate, and represent explanations and
associated cognitive biases and social expectations towards
the explanation process. Therefore, the author emphasizes
that the research on Explainable AI should incorporate stud-
ies from these different domains.
From the social science perspective, according to (Miller
2018) and (Lombrozo 2006), explanation is both a process
and product:

e Cognitive explanation process: identifies the causes for an
event, perhaps concerning particular counterfactual cases,
and a subset of these causes is selected as the explanation.

e Product: the explanation that results from the cognitive
explanation process.

e Social process: the process of transferring knowledge be-
tween explainer and explainee, generally an interaction
between groups of people, in which the goal is to provide
the explainee with enough information to understand the
causes of the event.

This definition of explanation insists that it is both a process
and product (i.e., outcome), and focuses on understanding
the causes of the event.

Furthermore, according to psychologist (Lombrozo
2006), “Explanations are ... the currency in which we ex-
changed belief’’. This definition stresses the need for high fi-
delity for the explanation, in other words, gaining trust from
the explanation recipient. Moreover, according to physicist
(Deutsch 1998) who is the pioneer of quantum computation:
explanations consist of interpretations of how the world
works and why. This definition suggests that explanations
are roughly equal to or a superset of interpretations. Inter-
pretations are the building blocks of explanations; without
interpretation, explanation is incomplete. Furthermore, the
terms “explainability” and “interpretability” are simply the
extent of explanation and interpretation accordingly.

There are other mentionable definitions too. According to
(Miller 2018), “interpretability is the degree to which a hu-
man can understand the cause of a decision”. Besides, ac-
cording to (Kim et al. 2017) “Interpretability is the degree to
which a human can consistently predict the model’s result”.
Furthermore, according to (Ribeiro, Singh, and Guestrin
2016) “By ‘explaining a prediction’, we mean presenting
textual or visual artifacts that provide qualitative understand-
ing of the relationship between the instance’s components
(e.g. words in text, patches in an image) and the model’s
prediction”. In other words, (Ribeiro, Singh, and Guestrin
2016) put an emphasis on the qualitative understanding of
the relationship between input and output (Le., a selec-
tive/suitable set of information pieces that together can re-
fer to a cause to an event, in contrast to the complete causal
attributions) .

To this end, in the case of an intelligent system (i.e.,
AI/ML-based system), it is evident that explainability is
more than interpretability in terms of importance, complete-
ness, and fidelity of prediction. Based on that, we choose
to keep our focus entirely on explainability instead of inter-
pretability. Finally, analyzing different definitions from the
literature, we come up with following:

Definition: Explainabiliry of an AI model’s prediction is the
extent of transferable qualitative understanding of the re-
lationship between model input and prediction (i.e., selec-
tive/suitable causes of the event) in a recipient friendly man-
ner.

76

We proceed further based on this informal definition of ex-
plainability.

Background

There are two primary directions of research towards eval-
uation of explainability of an AIYML model: (1) model
complexity-based and (2) human study-based.

Model Complexity-based Explainability Evaluation

In the literature, model complexity and (lack of) model in-
terpretability are often treated as the same (Molnar, Casalic-
chio, and Bischl 2019). For instance, in (Fiirnkranz, Gam-
berger, and Lavraé 2012) and (Yang, Rudin, and Seltzer
2017), model size is often used as a measure of interpretabil-
ity (e.g., number of decision rules, depth of tree, number of
non-zero coefficients).

(Yang, Rudin, and Seltzer 2017) propose a scalable
Bayesian Rule List (i.e., probabilistic rule list) consisting
of a sequence of IF-THEN rules, identical to a decision
list or one-sided decision tree. Unlike the decision tree that
uses greedy splitting and pruning, their approach produces a
highly sparse and accurate rule list with a balance between
interpretability, accuracy, and computation speed.

Furthermore, according to (Rtiping and others 2006),
while the number of features and the size of the decision tree
are directly related to the interpretability, the optimization of
the tree size or features (1.e., feature selection) is costly as it
requires generation of a large set of models and their elimi-
nation in subsequent steps. Besides, reducing the tree size
(i.e., reducing complexity) increases error; however, they
could not find a way to formulate the relation in simple func-
tional form.

More recently, (Molnar, Casalicchio, and Bischl 2019) at-
tempt to quantify the complexity of the arbitrary machine
learning model with a model agnostic measure. In that work,
the author demonstrates that when the feature interaction
(i.e., the correlation among features) increases, the quality
of representations of explainability tools degrades. For in-
stance, the explainability tool ALE Plot starts to show harsh
lines (i.e., zigzag lines) as feature interaction increases. In
other words, with more interaction comes a more combined
influence in the prediction, induced from different correlated
subsets of features (at least two), which ultimately makes it
hard to understand the causal relationship between input and
output, compared to an individual feature influence in the
prediction. In fact, from our study of different explainabil-
ity tools (e.g., LIME, SHAP, PDP), we have found that the
correlation among features is a key stumbling block to rep-
resent feature contribution in a model agnostic way. Keeping
the issue of feature interactions in mind, (Molnar, Casalic-
chio, and Bischl 2019) proposes a technique that uses three
measures: number of features, interaction strength among
features, and the main effect (excluding the interaction part)
of features to measure the complexity of a post-hoc model
for interpretation. Although, their work mainly focuses on
model complexity for post-hoc models, that acted as a pre-
cursor to formulate our approach of explainability quantifi-
cation. Our approach to quantify explainability is model ag-
nostic and is for a model of any notion (e.g., pre-modeling,
post-hoc).

Human Study-based Explainability Evaluation

The following work deals with the application and human-
level evaluation of explainability involving human studies.

(Huysmans et al. 2011) investigate the suitability of dif-
ferent alternative representation formats (e.g., decision ta-
bles, (binary) decision trees, propositional rules, and oblique
rules) for classification tasks primarily focusing on the ex-
plainability of results rather than accuracy or precision. They
discover that decision tables are the best in terms of accu-
racy, response time, confidence of answer, and ease of use.

Furthermore, (Dhurandhar et al. 2017) argue that inter-
pretability is not an absolute concept; rather, it is relative
to the target model, and may or may not be relative to the
human. Their finding suggests that a model is readily inter-
pretable to a human when it uses no more than seven pieces
of information (Miller 1956). Although, this might vary in
task to task and person to person. For instance, a domain ex-
pert might consume a lot more detail information depending
on their experience.

The work of (Poursabzi-Sangdeh et al. 2018) is a human-
centered approach, focusing on previous work on human
trust in a model from psychology, social science, machine
learning, and human-computer interaction communities. In
their experiment with human subjects, they vary factors
(e.g., number of features, whether the model internals is
clear or a black box) that makes a model more or less in-
terpretable and measures how the variation impacts the pre-
diction of human subjects. Their result suggests that partic-
ipants who were shown a clear model with a small number
of features were more successful in simulating the model’s
predictions and trusted the model’s predictions.

Furthermore, (Friedler et al. 2019) investigate inter-
pretability of a model based on two of its definitions: sim-
ulatability, which is a user’s ability to predict the output of
a model on a given input; and ”what if” local explainabil-
ity, which is a user’s ability to predict changes in predic-
tion in response to changes in input, given the user has the
knowledge of a model’s original prediction for the original
input. They introduce a simple metric called runtime op-
eration count that measures the interpretability, that is, the
number of operations (e.g., the arithmetic operation for re-
gression, the boolean operation for trees) needed in a user’s
mind to interpret something. Their findings suggest that the
interpretability decreases with an increase in the number of
operations.

Properties of Explanations and Explanation
Methods

Although our main interest lies in explainable prediction, it
depends on the model that generates the explanations. So,
it is crucial to analyze the properties of both the explana-
tion and the explanation method that generates the explana-
tions. (Robnik-Sikonja and Bohanec 2018) and (Molnar and
others 2018) attempt to define the properties of explanation

77

methods and individual explanations, which we present in
the following sections.

Properties of Explanation Methods
Some properties of explanation methods include:

e Expressive Power: Refers to the structure (e.g., decision
trees, IF-THEN rules, weighted sum, and natural lan-
guage) of the explanation method.

e Translucency: The level in which the explanation method
relies on looking into (e.g., coefficient for a linear regres-
sion, node splitting point in the tree-based approach) the
ML model. For instance, explanation methods that rely on
intrinsically interpretable methods like linear regression
are highly translucent; however, counterfactual model ag-
nostic explanation methods that leverage the changes in
output in response to input for explanation have zero
translucency as it does not look at model at all.

e Portability: The number of ML models that the explana-
tion method covers. Usually low translucency (i.e., ML
model usually remains black box) comes with more porta-
bility.

e Algorithmic Complexity: The required time to generate
the explanation.

Properties of Individual Explanations
Some properties of individual explanations include:

e Accuracy: The explanation needs to be accurate enough
when fidelity is essential. Although, low accuracy of ex-
planation might be a cause of the low accuracy of the
ML/AI model.

e Fidelity: How well the explanation approximates the pre-
diction of the black-box model. Usually a highly accurate
model with high fidelity leads to highly accurate explana-
tions. Thus, fidelity and accuracy are related.

e Consistency: Refers to the extent of consistency among
explanations for different ML models on the same task.

e Stability: Refers to the extent of similarity of explanations
for similar instances. While consistency compares expla-
nations between different models, stability compares the
explanations of similar instances for a particular model.

e Comprehensibility: The extent to which the recipient of
explanation understands the explanation, which is very
hard to measure. A few measures could be the number of
features with non-zero weights in a linear model, or the
number of rules in a tree. Usually a human can compre-
hend 7+-2 pieces of information at a time (Miller 1956)

e Certainty: Many ML models provide the probability of the
target class. Similarly, explanations with a certainty value
are expected to be useful.

e Degree of Importance: How well the explanation covers
the important features or how well the explanation reflects
parts of the explanations. For example, from the decision
rules, we can understand which of the rules are more im-
portant (e.g., important features are at the beginning of a
tuleset, or the top of the tree).
Recent laws (Goodman and Flaxman 2016) (Wyden ) fo-
cus on the impact of an algorithmic decision on human
tights, interests, and lives. In that sense, we need a human-
friendly explanation, which has slightly different properties
than the explanation that does not consider humans as a key
factor.

Properties of Human-friendly Explanation

According to (Miller 2018), humans usually prefer short ex-
planations that contrast the current situation with a situation
in which that event would not have occurred (a.k.a., coun-
terfactual explanations). Furthermore, the human friendly
explanation does not consider all factors (i.e., selective in
nature) for a particular prediction or behavior. However, if
one needs to legally specify all influencing factors or need
to debug the machine learning model, there is a need for a
complete causal attribution (Molnar and others 2018), which
is out of the scope of human-friendly explanation. That is
the reason behind mentioning the term qualitative instead of
quantitative in our definition of explanation (see Definition
of Explainability).

In summary, according to (Strumbelj and Kononenko
2011), (Miller 2018) (Lipton 1990), and (Molnar and others
2018), human friendly explanations are: contrastive—why
a prediction (e.g., loan was rejected) was made instead
of the alternative prediction (e.g., loan was accepted), se-
lected—does not cover the complete list of causes of an
event, social—social context (e.g., explaining to a layperson
or domain expert) determines the nature of the explanations,
abnormal behavior focused—when a criteria (e.g., a particu-
lar feature value) is rare and has influence in the prediction,
then it should be included (i.e., should have higher prece-
dence in case of other criteria with the same influence) in the
explanation, truthful—explanation should be as truthful as
possible, although selectiveness comes first which might ex-
clude some of the true reasons, consistent—consistent with
prior belief, and general and probable—good explanations
are general and probable, although this contradicts with the
claims that abnormal causes make good explanations, abnor-
mal causes have higher preference over general and probable
explanations (see abnormal behavior focused point).

Explainability Quantification Method

Usually, humans can relate and process 7+-2 pieces of in-
formation (i.e., cognitive chunks) to understand something
(Miller 1956). For instance, suppose that, in the most gener-
alized form, the quality of an explanation is dependent upon
the number of cognitive chunks that the recipient has to re-
late to in order to understand an explanation (i.e., the less,
the better). Lets assume, E = explainability; N, = number
of cognitive chunks; I = interaction; Nj = number of input
cognitive chunks; and N, = number cognitive chunks in-
volved in the explanation representation (i.e., output cogni-

tive chunks).
1
k= 1
N. (D
However, sometimes, these cognitive chunks are correlated
and their influence/contribution/abilities are not mutually

78

exclusive. This interaction among cognitive chunks compli-
cates the explainability. So we penalize Formula 1 for having
an interaction among cognitive chunks, resulting in Formula
2.

1
B=5 +0-D) (2)

c
Where, the interaction / ranges in between 0 and 1, and the
less the interaction, the better the explainability, so we take
the complement of that.

Formula 2 is in a form that can be applied to any of the
application, domain, or proxy level explanation evaluations
described before. However, from the perspective of an ap-
plication and domain level evaluation of explainability, to
progress further, we need human studies that are out of the
scope of this work. Instead, in this work, we focus on the
proxy level evaluation of explainability that considers dif-
ferent properties of output representation (e.g., depth of de-
cision tree, length of rule list) as a metric for evaluation.

Furthermore, we need to breakdown the number of cog-
nitive chunks more to get a better evaluation. Both the num-
ber of input cognitive chunks in the model and the number
of output cognitive chunks involved in the representation of
output are important to understand the causal relationship,
which is vital for explanation. While the ideal explainability
case would be when there is only one input and one output
cognitive chunk (no chance of interaction), that is unusual
in real-world situations. Following the segregation of input
and output cognitive chunks, Formula 2 can be re-written as
Formula 3:

1 1

E N, + N, +0-T)

where N; refers to the number of input cognitive chunks

and N, refers to the number cognitive chunks involved in the

explanation representation (i.e., output cognitive chunks).

Usually, the more these cognitive chunks, the more com-

plicated the explanation becomes. So, the ratio of the best

case (i.e., one cognitive chunk) and observed case is added
towards total explainability.

Also, Formula 3 has three predicates, which might have
different influences on the quantification of explainability,
and different domains might have different implications
(e.g., accuracy vs explainability trade off). So, we add a
weight term with each of the predicates, considering the
weights are constant (e.g., .3333) by default, and their sum-
mation is equal to 1: w, + W2 + w3 = 1. However, these
weights can be set to a different distribution, perhaps depen-
dant upon a particular domain (e.g., healthcare, finance).

After the addition of the weight terms, Formula 3 be-
comes Formula 4:

(3)

Bas wl — 1)

N,N, (4)

Formula 4 can then be used to quantify the explainabil-
ity of the explanation method (i.e., global explainability).
We can use Formula 4 to also quantify the explainability
of an instance level prediction (i.e., local explainability).
In that case, the first predicate of Formula 4 (including the
final_prognosis

- conditions = -0.36389)
- character = -0.0701777 753691933
- collateral = 0.297050602129171
- capacity = 0.0162271441718871

- capital = 0.0277945971515959
(Intercept)

 

  

Figure 1: Breakdown of feature contributions for a random
sample.

weight term) remains the same (i.e., the same number of in-
put chunks). However, predicate 2 and predicate 3 will be
different from instance to instance as a different set of cog-
nitive chunks with different interaction strengths might be
involved in the representation of explanation for a particular
instance as explanations are selective.

Experimental Results

We provide a brief overview of the experimental settings
of our previous work (Islam et al. 2019) that we will then
use for our proposed explainability quantification method.
In this example, we incorporate domain knowledge in the
model for mortgage bankruptcy prediction, and apply dif-
ferent supervised ML algorithms in three different ways:

1. Using original features: we use all original features that
have a non-zero effect on prediction (features with zero
effect were removed in a data pre-processing stage by ob-
serving the Random Forest’s feature importance).

2. Using domain-related features: we select a subset of fea-
tures that match with extracted domain knowledge (e.g.,
5 C’s of credit) from the domain. Traditionally, credit risk
is assessed using the 5C’s of credit (character, capital, ca-
pacity, collateral, and cycle), a popular domain principle
to for determining credit risk. We extract necessary do-
main knowledge and map to the 5 C’s of credit to get the
domain features (Islam et al. 2019).

3. Using newly constructed features: we construct a very
concise set of new features (i.e., five features—one feature
for each element of the domain principle 5C’s of credit)
from the domain-related features using the quantitative
measure of gain/compromises (i.e., the cumulative sum
of related feature values times the correlation coefficient)
associated with each element of the domain principle.

Furthermore, we represent the predicted output as a com-
position of individual elements of the domain principle 5C’s
of credit (i.e., newly constructed features) (Figure 1).

One will notice that the representation (Figure 1) of the
prediction in terms of the newly constructed features pro-
vides better explainability as the final prediction is segre-
gated into the individual influences of a very concise set of
intuitive features (i.e., five compared to 30). However, there
is no way to quantify the level of perceived explainability.
To use our proposed formula in this work (Formula 4) to
quantify explainability, we need to calculate the interaction
strength (1) too. We measure the interaction strength among

79

 

 

Original Domain Constructed
Input chunks (Nj) 30 7 7
Output chunks (N,) 30 7 5
Int. Strength (1) 0.556 0.5233 0.5251
Explainability (E) 0.1701 0.2539 0.2723

 

Table 1: Comparison of explainability

0.15
0.10
0.05
0.00

-0.05

Original - Domain

-0.10

0.15
Recall

-0.02
0.12
-0.10

Precision
0.05
0.05
0.02

Accuracy Fscore ROC-AUC
0.00
0.00

0.00

MANN
mSVM
mRF

mET 0.00

0.00

0.02
-0.03

0.03

=GB -0.02

MANN @SVM @&RF MET &GB

Figure 2: Dispersion in performance—original features mi-
nus domain-related features

features using R’s iml package that uses the partial depen-
dence of an individual features as the basis for calculating
interaction strength (1).

Applying Formula 4, on metadata (Table 1) of three dif-
ferent feature settings, we see that newly constructed fea-
tures (5’C of credit) provide the best explainability score of
-2723, which is an improvement of 60.14% compared to the
0.17 that we get using the original features (Table 1). In fact,
even if we apply the state-of-the-art methods of post-hoc
interpretability/explainability like SHAP, the explainability
will be still limited to 0.17 as it does not reduce the num-
ber of cognitive chunks to represent output. Besides, using
domain related features, the explainability score is .2538,
which is better than using the original features, although a
little worse than using the newly constructed features. We
also applied Principal Component Analysis (PCA) to see
the extent of compromise in information in exchange for
achieved explainability in domain mapped and newly con-
structed features. We found that PCA takes the first 29 prin-
cipal components (i.e., new features) to get a similar perfor-
mance to when we use the original features (30 cognitive
chunks). Therefore we can say that the compromise in in-
formation for explainability is very negligible. Besides, the
principal components of PCA lacks explainability.

Furthermore, this explainability gain comes with a negli-
gible cost of performance (Figure 2, Figure 3) which is ex-
pected and a known trade off for explainability methods. In
fact, for a few algorithms (e.g., Random Forest (RF), Gra-
dient Boosting (GB)), the newly constructed features, and
domain related features lead to better recall, which is cru-
cial for anomaly detection where the target class instances
are very few compared to non-target class instances. In that
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
-0.10
-0.20

Hala”
Recall
-0.14
-0.12
-0.10
-0.12
-0.02

Original - Constructed

ROC-AUC
-0.02
0.00
-0.03
-0.03
0.02

Fscore
0.57
0.31

Precision
0.69
0.24

Accuracy
0.01
0.01

mANN
SVM
@RF 0.01
0.01

0.04

0.00
0.00
0.00

0.10
0.11
0.08

meET
=GB

ANN @SVM @RF BET &GB

Figure 3: Dispersion in performance—original features mi-
nus newly constructed features

sense, besides a better explainability, the domain knowledge
also benefits us in terms of improved performance (e.g., bet-
ter recall, less features leads to less computation time).

Conclusion

Explainable decisions from commercial AI systems are go-
ing to be a standard imposed by regulators to eliminate
bias and discrimination, and ensure trust. Our work attempts
to establish a concrete definition of explainability, proper-
ties of explanations and explanation methods, and a way
to quantify explainability which is transferable to a vari-
ety of explainability methods or tools. For the quantifica-
tion of explainability, we only present an approach for the
proxy method. As an extension of this work, we would like
to address human studies, and investigate the effectiveness
among different approaches (e.g., supervised, unsupervised,
semi-supervised) for different application areas (e.g., natural
language processing, image recognition).

Acknowledgment

Thanks to Tennessee Tech’s College of Engineering (CoE)
and Cyber-security Education, Research and Outreach Cen-
ter (CEROC) for supporting this research.

References
Deutsch, D. 1998. The fabric of reality. Penguin UK.

Dhurandhar, A.; Iyengar, V.; Luss, R.; and Shanmugam,
K. 2017. Tip: Typifying the interpretability of procedures.
arXiv preprint arXiv:1706.02952.

Friedler, S. A.; Roy, C. D.; Scheidegger, C.; and Slack, D.
2019. Assessing the local interpretability of machine learn-
ing models. arXiv preprint arXiv:1902.03501.

Fiirnkranz, J.; Gamberger, D.; and Lavrat, N. 2012. Rule
learning in a nutshell. In Foundations of Rule Learning.
Springer. 19-55.

Goodman, B., and Flaxman, S. 2016. Eu regulations on
algorithmic decision-making and a “right to explanation”.
In ICML workshop on human interpretability in machine
learning (WHI 2016), New York, NY.

80

Huysmans, J.; Dejaeger, K.; Mues, C.; Vanthienen, J.; and
Baesens, B. 2011. An empirical evaluation of the compre-
hensibility of decision table, tree and rule based predictive
models. Decision Support Systems 51(1):141-154.

Islam, S. R.; Eberle, W.; Bundy, S.; and Ghafoor, S. K. 2019.
Infusing domain knowledge in ai-based” black box” models
for better explainability with application in bankruptcy pre-
diction. arXiv preprint arXiv:1905.11474.

Islam, S. R.; Eberle, W.; Ghafoor, S. K.; Siraj, A.; and
Rogers, M. 2020. Domain knowledge aided explainable
artificial intelligence for intrusion detection and response.
In AAAI Spring Symposium: Combining Machine Learning
with Knowledge Engineering.

Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C.; Wexler, J.; Vie-
gas, F.; and Sayres, R. 2017. Interpretability beyond feature
attribution: Quantitative testing with concept activation vec-
tors (tcav). arXiv preprint arXiv:1711.11279.

Lipton, P. 1990. Contrastive explanation. Royal Institute of
Philosophy Supplements 27:247-266.

Lombrozo, T. 2006. The structure and function of explana-
tions. Trends in cognitive sciences 10(10):464-470.

Miller, G. A. 1956. The magical number seven, plus or
minus two: Some limits on our capacity for processing in-
formation. Psychological review 63(2):81.

Miller, T. 2018. Explanation in artificial intelligence: In-
sights from the social sciences. Artificial Intelligence.

Molnar, C., et al. 2018. Interpretable machine learning: A
guide for making black box models explainable. E-book at;
https://christophm. github. io/interpretable-ml-book/; 10.

Molnar, C.; Casalicchio, G.; and Bischl, B. 2019. Quan-
tifying interpretability of arbitrary machine learning mod-
els through functional decomposition. arXiv preprint
arXiv: 1904.03867.

Poursabzi-Sangdeh, F.; Goldstein, D. G.; Hofman, J. M.;
Vaughan, J. W.; and Wallach, H. 2018. Manipulat-
ing and measuring model interpretability. arXiv preprint
arXiv: 1802.07810.

Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. Why
should i trust you?: Explaining the predictions of any classi-
fier. In 22nd ACM SIGKDD, 1135-1144. ACM.

Robnik-Sikonja, M., and Bohanec, M. 2018. Perturbation-
based explanations of prediction models. In Human and Ma-
chine Learning. Springer. 159-175.

Rtiping, S., et al. 2006. Learning interpretable models.

Strumbelj, E., and Kononenko, I. 2011. A general method
for visualizing and explaining black-box regression models.
In International Conference on Adaptive and Natural Com-
puting Algorithms, 21-30. Springer.

Wyden, B. Algorithmic accountability.

Yang, H.; Rudin, C.; and Seltzer, M. 2017. Scalable
bayesian rule lists. In Proceedings of the 34th International

Conference on Machine Learning-Volume 70, 3921-3930.
JMLR. org.
References
Deutsch, D. 1998. The fabric of reality. Penguin UK.

Dhurandhar, A.; Iyengar, V.; Luss, R.; and Shanmugam,
K. 2017. Tip: Typifying the interpretability of procedures.
arXiv preprint arXiv:1706.02952.

Friedler, S. A.; Roy, C. D.; Scheidegger, C.; and Slack, D.
2019. Assessing the local interpretability of machine learn-
ing models. arXiv preprint arXiv:1902.03501.

Fiirnkranz, J.; Gamberger, D.; and Lavra¢é, N. 2012. Rule
learning in a nutshell. In Foundations of Rule Learning.
Springer. 19-55.

Goodman, B., and Flaxman, S. 2016. Eu regulations on
algorithmic decision-making and a “right to explanation”.

In ICML workshop on human interpretability in machine
learning (WHI 2016), New York, NY.

Huysmans, J.; Dejaeger, K.; Mues, C.; Vanthienen, J.; and
Baesens, B. 2011. An empirical evaluation of the compre-
hensibility of decision table, tree and rule based predictive
models. Decision Support Systems 51(1):141-154.

Islam, S. R.; Eberle, W.; Bundy, S.; and Ghafoor, S. K. 2019.
Infusing domain knowledge in ai-based” black box” models
for better explainability with application in bankruptcy pre-
diction. arXiv preprint arXiv:1905.11474.

Islam, S. R.; Eberle, W.; Ghafoor, S. K.; Siraj, A.; and
Rogers, M. 2020. Domain knowledge aided explainable
artificial intelligence for intrusion detection and response.
In AAAI Spring Symposium: Combining Machine Learning
with Knowledge Engineering.

Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C.; Wexler, J.; Vie-
gas, F; and Sayres, R. 2017. Interpretability beyond feature
attribution: Quantitative testing with concept activation vec-
tors (tcav). arXiv preprint arXiv:1711.11279.

Lipton, P. 1990. Contrastive explanation. Royal Institute of
Philosophy Supplements 27:247-266.

Lombrozo, T. 2006. The structure and function of explana-
tions. Trends in cognitive sciences 10(10):464—-470.

Miller, G. A. 1956. The magical number seven, plus or
minus two: Some limits on our capacity for processing in-
formation. Psychological review 63(2):81.

Miller, T. 2018. Explanation in artificial intelligence: In-
sights from the social sciences. Artificial Intelligence.

Molnar, C., et al. 2018. Interpretable machine learning: A
guide for making black box models explainable. E-book at;
hitps://christophm. github. io/interpretable-ml-book/; 10.

Molnar, C.; Casalicchio, G.; and Bischl, B. 2019. Quan-
tifying interpretability of arbitrary machine learning mod-
els through functional decomposition. arXiv preprint
arXiv: 1904.03867.

Poursabzi-Sangdeh, F.; Goldstein, D. G.; Hofman, J. M.;
Vaughan, J. W.; and Wallach, H. 2018. Manipulat-
ing and measuring model interpretability. arXiv preprint
arXiv: 1802.07810.

Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. Why
should i trust you?: Explaining the predictions of any classi-
fier. In 22nd ACM SIGKDD, 1135-1144. ACM.

81

Robnik-Sikonja, M., and Bohanec, M. 2018. Perturbation-
based explanations of prediction models. In Human and Ma-
chine Learning. Springer. 159-175.

Rtiping, S., et al. 2006. Learning interpretable models.

Strumbelj, E., and Kononenko, I. 2011. A general method
for visualizing and explaining black-box regression models.
In International Conference on Adaptive and Natural Com-
puting Algorithms, 21-30. Springer.

Wyden, B. Algorithmic accountability.

Yang, H.; Rudin, C.; and Seltzer, M. 2017. Scalable
bayesian rule lists. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, 3921-3930.
JMLR. org.
UU Seb
Mts Le

 

Taylor & Francis
Taylor & Francis Group.

Information Systems Management

ISSN: (Print) (Online) Journal homepage: https:/Avww.tandfonline.com/loi/uism20

Explainable Artificial Intelligence: Objectives,
Stakeholders, and Future Research Opportunities

Christian Meske, Enrico Bunde, Johannes Schneider & Martin Gersch

To cite this article: Christian Meske, Enrico Bunde, Johannes Schneider & Martin Gersch (2022)
Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities,
Information Systems Management, 39:1, 53-63, DOI: 10.1080/10580530.2020.1849465

To link to this article: https://doi.org/10.1080/10580530.2020.1849465

a © 2020 The Author(s). Published with
license by Taylor & Francis Group, LLC.

ssa Published online: 08 Dec 2020.

 

o
(sg Submit your article to this journal @

lil Article views: 12238

 

»
ey View related articles @

® View Crossmark data@

GaussMark

 

) Citing articles: 14 View citing articles 7

 

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journallnformation?journalCode=uism20
INFORMATION SYSTEMS MANAGEMENT
2022, VOL. 39, NO. 1, 53-63
https://doi.org/10.1080/10580530.2020.1849465

Taylor & Francis
Taylor & Francis Group

 

RESEARCH NOTE

@ OPEN ACCESS

Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research

Opportunities

Christian Meske@®?, Enrico Bunde?, Johannes Schneider, and Martin Gersch*

Department of Information Systems, Freie Universitat Berlin and Einstein Center Digital Future, Berlin, Germany; "Institute of Information
Systems, University of Liechtenstein, Vaduz, Liechtenstein; ‘Department of Information Systems, Freie Universitat Berlin, Einstein Center Digital

Future as Well as Digital Entrepreneurship Hub, Berlin, Germany

ABSTRACT

Artificial Intelligence (Al) has diffused into many areas of our private and professional life. In this
research note, we describe exemplary risks of black-box Al, the consequent need for explainability,
and previous research on Explainable Al (XAl) in information systems research. Moreover, we discuss
the origin of the term XAl, generalized XAI objectives, and stakeholder groups, as well as quality

KEYWORDS

Artificial Intelligence;
explainability; accountability;
transparency; trust;
managing Al

criteria of personalized explanations. We conclude with an outlook to future research on XAI.

Introduction

Artificial Intelligence (AI), a research area initiated in
the 1950ies (Mccarthy et al., 2006), has received signifi-
cant attention in science and practice. Global spending
on AI systems is expected to more than double from
38 billion USD in 2019 to 98 billion USD by 2023 (Shirer
& Daquila, 2019). Emphasizing on machine learning,
and thereby connecting to what is meant by “intelli-
gent”, AI can be defined, for instance, as the “system’s
ability to correctly interpret external data, to learn from
such data, and to use those learnings to achieve specific
goals and tasks through flexible adaptation” (Kaplan &
Haenlein, 2019, p. 15).

In combination with increasing IT-processing cap-
abilities, especially machine learning approaches includ-
ing artificial neural networks have led to a task
performance of AI that has never been seen before.
Hence, advanced technologies of today make increas-
ingly use of ‘bio-inspired paradigms’ in order to eftec-
tively tackle complex real-world problems (Zolbanin
et al., 2019). We still speak of such systems as “weak
Al” or “narrow AJ” - since they are only used for very
specific tasks and, in contrast to “strong AI”, are not
universally applicable (Searle, 1980; Watson, 2017).
However, today’s algorithms already reached or even
surpassed the task performance of humans in different
domains. For example, corresponding applications out-
performed professional human players in complex
games such as Go and Poker (Blair & Saffidine, 2019;
Silver et al., 2017) or proved to be more accurate in

breast cancer detection (McKinney et al., 2020). In con-
sequence, these advances in socio-technical systems will
significantly affect the future of work (Dewey & Wilkens,
2019; Elbanna et al., 2020).

AI is thus increasingly applied in use cases with
potentially severe consequences for humans. This holds
true not only in medical diagnostics, but also in pro-
cesses of job recruitment (Dastin, 2018), credit scoring
(Wang et al., 2019), prediction of recidivism in drug
courts (Zolbanin et al., 2019), or as autopilots in aviation
(Garlick, 2017) and autonomous driving (Grigorescu
et al., 2020). Furthermore, corresponding technology is
more and more integrated into our everyday private
lives in the form of intelligent agents like Google
Home or Siri (Bruun & Duka, 2018). However, due to
the growing complexity of underlying models and algo-
rithms, AJ appears as a “black box”, because the internal
learning processes as well as the resulting models are not
completely comprehensible. This trade-off between per-
formance and explainability can have a significant
impact on individual beings, businesses, and society as
a whole (Alt, 2018).

Research on information systems, so we argue, needs
to respond to this challenge by fostering research on
Explainable Artificial Intelligence (XAI), which to date
has been mostly investigated with a method-oriented
focus for developers in computer science. Yet, explain-
ability is a prerequisite for fair, accountable, and trust-
worthy AI (Abdul et al., 2018; Fernandez et al., 2019;
Miller, 2019), eventually affecting how we manage, use,
and interact with it. For instance, the absence of

 

CONTACT Christian Meske @ christian. meske@fu-berlin.de Department of Information Systems, Freie Universitat Berlin and Einstein Center Digital Future,

Berlin 14195, Germany.
© 2020 The Author(s). Published with license by Taylor & Francis Group, LLC.

This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License (http://creativecommons.org/licenses/by-nc-
nd/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited, and is not altered, transformed, or built
upon in any way.
54 @ C. MESKE ETAL.

explainability implies that humans cannot conduct a risk
or threat analysis, increasing the probability of undesir-
able behavior of the system. Further, our community’s
“collective research efforts should advance human wel-
fare” (Malhotra et al., 2013, p. 1270), which may be
jeopardized by such non-explainable and hence possibly
uncontrollable AI. Also, as future automation and deci-
sion support systems will be increasingly based on com-
plex algorithms, information systems may use machine
learning more often as an additional method for scien-
tific research.

In this research note, we will first discuss exemplary
risks and the “dark side” of AI in Section 2, followed by
a short overview of previous research on explainability in
information systems in Section 3. In Section 4, we outline
the terminology and origin as well as objectives and
stakeholders of XAJI, and list quality criteria of persona-
lized explanations. In Section 5, we provide future
research opportunities for behavioral as well as design
science researchers, followed by a conclusion in Section 6.

Risks and dark sides of Al usage

Different risks exist regarding the use of AI systems.
A major potential problem is “bias”, which comes in
different facets. In certain situations, humans have
a tendency to over-rely on automated decision-making,
called “automation bias”, which can result in a potential
failure to recognize errors in the black box (Goddard
et al., 2012). As an example, medical doctors ignored
their own diagnoses, even when they were correct,
because their diagnosis was not recommended by the
AI system (Friedman et al., 1999; Goddard et al., 2011).
Furthermore, automation bias can foster the process of
“deskilling”, either because of the attrition of existing
skills or due to the lack of skill development in general
(Amold & Sutton, 1998; Sutton et al., 2018). Such pro-

Horse-picture from Pascal VOC data set

   

 

Source tag
present

{

Classified
as horse

No source
tag present

|

Not classified
as horse

blems highlight the overall risk of inappropriate trust of
humans toward AI (Herse et al., 2018).

Not only humans can have a bias but also the AI
system itself. For instance, such systems can intentionally
or unintentionally be biased toward wrongful output.
Caliskan et al. (2017) point out, how text and web corpora
in training data can contain human bias, leading to
a machine learning model that is biased against race or
gender, consequently establishing Al-based discrimina-
tion, racism, or sexism. Bias in the “real” world, and
consequently in historical data, may therefore lead to
statistical bias, which again can perpetuate bias in the
real world (Parikh et al., 2019). For example, as shown
in a recent review, Apple’s face recognition systems failed
to distinguish Asian users, Google’s sentiment analyzer
got homophobic and anti-Semitic, a predictive policing
system disproportionately targeted minority neighbor-
hoods, and a bot designed to converse with users on
Twitter became verbally abusive (Yampolskiy, 2019, pp.
141-142). Moreover, AI may learn correlations that are
not linked to causal relations in the real world
(Lapuschkin et al., 2019). In Figure 1, such a classifier is
depicted that learned to focus on a source tag, which was
found for about 20% of images of horses in the training
data. When the source tag was removed, the classifica-
tions changed accordingly. Hence, when the same source
tag was implemented on an image of a car, the AI still
classified it as a horse.

In another case, a machine learning model used the
presence of a ruler on images for diagnosis of malignant
skin tumors (Narla et al., 2018). The reason was that
dermatologists tend to only mark lesions with a ruler
that are a cause for concern to them, hence introducing
bias to the training data set.

In addition, there is a “dark side” of AI based on
misuse (Schneider et al., 2020; Xiao et al., 2020). We
leave a digital footprint everywhere (Vidgen et al., 2017),

Artificial picture of a car

   

 

Figure 1. Explanations for Al-based classifications using Grad-CAM (Lapuschkin et al., 2019, p. 3).
through, for instance, online shopping, social media
conversations, or usage of mobile navigation apps.
While such data deluge has led to the proliferation of
data analytics and AI for economic and business poten-
tial (Mikalef et al., 2020), it may also lead to a significant
power imbalance and unwanted authority of private
businesses (Zuboff, 2015) or public institutions alike
(Brundage et al., 2018). Moreover, in so-called manip-
ulative “adversarial attacks” only few pixels of an image
need to be modified, which yet lead machine learning
models to predict completely different classes (Su et al.,
2019).

These exemplary risks highlight the need for explain-
able AI and control. In the following section, we will
now provide an overview of how explainability has been
investigated in information systems so far.

Explainability in information systems research

Investigating explainability is not completely new to the
information systems community. With the rise of sys-
tems termed knowledge-based systems, expert systems
or intelligent agents in the 1980ies and 1990ies, informa-
tion systems research started to investigate the necessity
for explanations to learn about and from the artifacts’
reasoning. For instance, scholars discussed the potential
impact of explanations on users’ improved understand-
ing about the system, consequently influencing the effec-
tiveness and efficiency of judgmental decision making,
as well as on the perception of the system’s usefulness,
ease of use, satisfaction, and trust (Dhaliwal & Benbasat,
1996; Mao & Benbasat, 2000; Ye & Johnson, 1995). It
was found that novices had a higher and different need
for explanations than experts, and that justifications of
the system’s actions or recommendations (why) are
more requested than rule-oriented explanations of how
the system reasoned (Mao & Benbasat, 2000; Ye &
Johnson, 1995).

Combining a cognitive effort perspective with cogni-
tive learning theory and Toulmin’s model of argumen-
tation, further work emphasized on a detailed
classification of explanations: Type I, trace or line of
reasoning (which explain why certain decisions were or
were not made), type II, justification or support (which
justify the reasoning process by linking it to the “deep
knowledge” from which it was derived), type III, control
or strategic (which explain the system’s control behavior
and problem-solving strategy), and type IV, terminolo-
gical (which supply definitional or terminological infor-
mation) (Gregor & Benbasat, 1999, based on
Chandrasekaran et al., 1989; Swartout & Smoliar,
1987). Explanations should be understandable for the
user and easy to obtain, e.g., automatically, if this can be

INFORMATION SYSTEMS MANAGEMENT © 55

done unobtrusively. They should also be context-specific
rather than generic (Gregor & Benbasat, 1999).

Subsequent work analyzed how natural language
reports based on variable comparisons, which explain
why a system suggests certain strategic decisions
in situations of nuclear emergencies, help to evaluate
the overall decision support system (Papamichail &
French, 2005). It was furthermore shown, that long
explanations with a conveyed strong confidence level
and higher information value lead to an increased
acceptance of interval forecasts compared to short
explanations and conveyed weak confidence level with
low information value (Génil et al., 2006). Arnold
et al. (2006) showed that users were more likely to
adhere to recommendations of the KBS when an expla-
nation facility was available, while choice patterns indi-
cated that novices used feedforward explanations more
than experts did, while experts mostly used feedback
explanations. Further studies in the area of decision
support systems indicate that tools,
enhanced explanatory facilities and provide justifica-
tions at the end of the consultation process, lead to
improved decision-process satisfaction and decision-
advice transparency, subsequently leading to empower-
ing effects like a higher sense of control and a lower
perceived power distance (Li & Gregor, 2011). The
authors also showed that personalization of explana-
tions with a focus on a cognitive fit can increase the
perceived explanation quality and hence explanation
influence as well as perceived usefulness of the system
(Li & Gregor, 2011).

Aforementioned systems, such as knowledge-based
or expert systems, are referred to as symbolic AJ, or
Good Old Fashioned AI (GOFAI), since human knowl-
edge was instructed through rules in a declarative form
(Haugeland, 1985). With the turn of the millennium
and discussions of “new-paradigm intelligent systems”
(Gregor & Yu, 2002) like artificial neural networks, it
was recognized, that the latter are typically neither
capable to inherently declare the knowledge they con-
tain, nor to explain the reasoning processes they go
through. In that context, it was argued, that explana-
tions could be obtained indirectly, e.g., through sensi-
tivity analysis (Rahman et al., 1999), which derives
conclusions from output variations caused by small
changes of a particular input (Gregor & Yu, 2002).
Besides only very few examples (e.g. Eiras-Franco
et al., 2019; Giboney et al., 2015; Martens & Provost,
2014), since then most of the publications’ on explain-
ability of AI systems, or “Explainable Artificial
Intelligence” (XAI), have been published outside of
the information systems community, mostly in com-
puter science. As one can see, the existing IS literature

which have
56 @) C. MESKEET AL.

is very valuable but with its peak in the 1990ies and
early 2000s also comparatively dated, which motivates
our call for more IS research on the explainability
of Al.

For a better understanding, in the following section
we will first discuss the term XAI and its origin, XAI
objectives, and stakeholders, as well as quality criteria of
personalized explanations.

Explainable artificial intelligence
Terminology

Symbolic AI such as MYCIN, an expert system to diag-
nose and recommend treatment for bacteria-related
infections in the 1970s (Fagan et al., 1980), was already
able to explain its reasoning for diagnostic or instruc-
tional purposes. However, to the best of our knowledge,
it took until 2002, when the term “Explainable Artificial
Intelligence” was mentioned the first time as a side-note
in a review of “Full Spectrum Command” (FSC,
Brewster, 2002), a PC-based military simulation of tac-
tical decision making. In this review of a preliminary
beta version of FSC, which was still a GOFAI knowl-
edge-based system, XAI referred to the feature that it
“can tell the student exactly what it did and why”
(Brewster, 2002, p. 8), consequently augmenting the
instructor-facilitated after-action review. Two years
later, FSC was presented by their developers in an article
at the computer science conference on Innovative
Applications of Artificial Intelligence, in which FSC
was described as an “XAI System” for small-unit tactical
behavior (Van Lent et al., 2004). In this paper, XAI
systems were officially introduced and defined as sys-
tems that “present the user with an easily understood
chain of reasoning from the user’s order, through the
system’s knowledge and inference, to the resulting beha-
vior” (Van Lent et al., 2004, p. 900).

A more current, machine learning-related and often-
cited definition of XAI reads as follows: XAI aims to
“produce explainable models, while maintaining a high
level of learning performance (prediction accuracy); and
enable human users to understand, appropriately, trust,
and effectively manage the emerging generation of arti-
ficially intelligent partners” (Gunning, 2017). However,
there is no generally accepted definition for that term. It
rather refers to “the movement, initiatives, and efforts
made in response to Al transparency and trust concerns,
more than to a formal technical concept” (Adadi &
Berrada, 2018, p. 52140).

In literature, the terms explainability and interpret-
ability are often used synonymously. One way to

describe potential differences is the following: if humans
can directly make sense of a machine’s reasoning and
actions without additional explanations, we speak of
interpretable machine learning or interpretable AI
(Guidotti et al., 2018). Interpretability may therefore be
seen as a passive characteristic of the artifact (Rudin,
2019). However, if humans need explanations as a proxy
to understand the system’s learning and reasoning pro-
cesses, for example, because an artificial neural network
is too complex, we speak of research on explainable AI
(Adadi & Berrada, 2018).

In computer science, in which most of the research
on XAT has been taking place, different instruments to
explain an Al’s inner working have been developed and
categorized (Ras et al., 2018). Some of these methods
allow to interpret a single prediction of a machine learn-
ing model, others allow to understand the whole model,
leading to the differentiation between “local” and “glo-
bal” explanations. The explanation output can be pre-
sented in the form of “feature attribution” (pointing out
how data features supported or opposed a model’s pre-
diction, see also Figure 1 back in Section 2), “examples”
(returning data instances as examples to explain the
model’s behavior), “model internals” (returning the
model’s internal representations, e.g., of the model’s
neurons) and “surrogate models” (returning an intrinsi-
cally interpretable, transparent model which approxi-
mates the target black-box model). Some XAI methods
can be used for any machine learning model (“model-
agnostic explanations”), others work only for e.g., neural
networks (“model-specific explanations”). Certain XAI
methods just work with textual input data, others only
with tabular, visual, or audio data, and again others work
with multiple inputs. For a detailed technical overview
and categorization of existing XAI methods we refer to
extensive surveys such as (Gilpin et al., 2018; Guidotti
et al., 2018; Ras et al., 2018).

Objectives and stakeholders of explainable artificial
intelligence

First, as our section on AI risks and failures highlights, it
is important to build a sufficient understanding about
the system’s behavior to detect unknown vulnerabilities
and flaws, for example, in order to avoid phenomena
related to spurious correlations. As for that, so we argue,
explainability is crucial for the human ability to evaluate
the system (see Figure 2).

Second, especially from a developer’s design perspec-
tive, understanding the inner workings of AI and con-
sequent outcomes is vital to enhance the algorithm.
Explainability can therefore support to increase the
 

Explainability to

 

Evaluate Al
Explainability to Explainability to Explainability to
Justify Al Manage Al Improve Al
~~ a

 

 

 

Explainability to
Learn from Al

 

 

 

Figure 2. Generalized objectives of explainable artificial
intelligence.

system’s accuracy and value. Hence, improvement is an
additional goal that can be achieved with the application
of XAI methods (Gilpin et al., 2018).

Third, referring back to our discussion of knowledge-
based systems, certain types of explanations provide
information on why (or based on which knowledge)
certain rules were programmed into the system, which
represented “deep knowledge” (Chandrasekaran et al.,
1989; Gregor & Benbasat, 1999). While there is no
corresponding programmed knowledge in machine
learning models, AI explanations could be used, for
instance, to discover unknown correlations with causal
relationships in data. We thus call it the goal of XAI to
learn from the algorithm’s working and results in order
to gain deep knowledge.

Fourth, AI is increasingly used in critical situations
which have potentially severe consequences for humans.
Whether legislation, such as the General Data Protection
Regulation (GDPR) in Europe, established a formal
“right for explanation” (Goodman & Flaxman, 2017) is
debatable, however, they are usually clear on the
demand for accountability and transparency in auto-
mated decision processes, which lead to potential con-
sequences that significantly affect the individual
(European Union, 2016). Hence, to justify, as Adadi
and Berrada (2018) call it, is an important goal of XAI.

Fifth, with a focus on implementation and usage, AI
adds a level of novelty and complexity that goes beyond
traditional IT and data applications, inserting new forms
of material agency into organizational processes, poten-
tially changing how work routines emerge and outcomes
from work are produced (Berente et al., 2019; Rai et al.,
2019). We hence argue that for tackling these challenges,
we need explainability to evaluate, to improve, to learn,
and to justify in order to achieve the overarching goal of
to manage AI. Figure 2 summarizes the generalized
objectives.

The generalized objectives of XAI manifest differently
for various stakeholder groups. For instance, AI Developers

INFORMATION SYSTEMS MANAGEMENT © 57

focus on improving the algorithm’s performance as well as
on debugging and verification in order to pursue
a structured engineering approach based on cause analysis
instead of trial and error (Hohman et al., 2019). As such
systems are increasingly used in critical situations, and
depending on corresponding legislative circumstances, it
may need certification. In consequence, there are AI
Regulators, who need explanations in order to being able
to test and certify the system.

In an organizational context, there are “AI Managers”
who, for example, need explanations to supervise and
control the algorithm, its usage and assure its compliance.
Those who apply a given system, called “AI Users”, are
rather interested in explainability features to understand
and compare the artifact’s reasoning with his or her own
reasoning, in order to analyze its validity and reliability,
or to determine influential factors for a specific prediction
(e.g., doctors). Eventually, so we argue, there are
Individuals affected by Al-based decisions (e.g., patients)
caused by AI users or even by autonomous ruling, who
may have an interest in explainability to evaluate the
fairness of a given Al-based decision. The following
Figure 3 provides an overview of potential stakeholder
groups and their exemplary interests in explainability
of Al.

Members between different and within the same sta-
keholder groups can have varying backgrounds regard-
ing training, experience, and demographic
characteristics. This can lead to different needs for AI
explanations as well as their perceptions as, e.g., being
useful. Thus, based on personal traits and in combina-
tion with their task-related interest in transparency,
explanations need to be personalized (Kil et al., 2019;
Schneider & Handali, 2019). Corresponding quality cri-
teria of personalized explanations will be described in
the following section.

Quality criteria of personalized explanations

There are different factors that determine the quality
of explanations, which in addition can be perceived
differently by the various XAI stakeholder groups. As
described in Section 3, explanations should, amongst
others, be understandable for the individual user,
easy to get, context-specific rather than generic,
with a conveyed strong confidence level and high
information value, and personalized to the explainee
(Gonitil et al., 2006; Gregor & Benbasat, 1999; Li &
Gregor, 2011). In the following, we provide a list of
overarching quality criteria for personalized explana-
tions based on and extended from (Schneider &
Handali, 2019).
58 @ C.MESKE ETAL.

   
   
    

  
  
  
   
   
  
  
  
  
  
 

Al Regulators

Certification of Al
systems

 

Al Developers

Debugging and
improvement of Al
systems

Figure 3. Stakeholder groups of explainable artificial intelligence.

Fidelity describes, to which extend a black-box accu-
rately matches the input-output mapping of a given
model (Guidotti et al, 2018; Ras et al, 2018).
Generalizability refers to the range of models which the
XAI technique can explain or be applied to, whereby
a high generalizability increases the usefulness of the
explanation technique (Ras et al., 2018). Explanatory
power refers to the scope of questions that can be
answered: explanations that allow to understand the
general model behavior have more explanatory power
compared to explanation of specific predictions only
(Ras et al., 2018; Ribeiro et al., 2016). Interpretability
describes to which extend an explanation is understand-
able for humans (Guidotti et al., 2018).

Comprehensibility refers to the capacity of an expla-
nation to aid a human user in performing a task, while
plausibility can be understood as a measure regarding
the acceptance of the explanatory content (Fiirnkranz
et al., 2020). Effort addresses the (ideally few) resources
needed in order to understand or interpret an explana-
tion (Schneider & Handali, 2019). Privacy should pre-
vent the risk that (meta)data, for instance, in the course
of XAI personalization, can be used to draw conclusions
about the person or its behavior (Radaelli et al., 2015).
Fairness refers to the goal that explanations should be
egalitarian, e.g., in terms of the quality presented to
different groups of explainees (Binns, 2018; Kusner
et al., 2017). Figure 4 summarizes the quality criteria
for personalized explanations.

Findings from the social sciences can help to tailor
the design of XAI more precisely to the requirements of
the various stakeholders, for example, individually
accepted indicators of trustworthiness for services with
predominant credence qualities (BGhmann et al., 2014;
Kasnakoglu, 2016; Lynch & Schuler, 1990; Matzner et al.,
2018; Wood & Schulman, 2019).

Al Managers

Assure compliance of
Al systems in the
organization

  

Al Users

Validate Al reasoning
with own reasoning

   
  

  
  
 
 
  
  
  

   
    
  
 

Individuals Affected
by Al-based
decisions
Evaluate fairness of
decisions

  
  
   
 

 
  

Generaliza-
bility

   

Quality
Criteria for
Personalized
Explanations

 
 
  

     
     
 
  

Explanatory
Power

Interpreta-
bility

  

  

Plausability

 
    

Comprehensi-
bility

 
   
    

 

Figure 4. Quality criteria for personalized explanations.

Further research opportunities

Explainability is described as being as old as the topic of AI
itself rather than being a problem that arises through AI
(Holzinger et al. 2019). In the early days of AI research, the
models often consisted of reasoning methods, which were
logical and symbolic, resulting in limited performance,
scalability, and applicability. However, such kind of AI
systems delivered a basis for explanations as they performed
some sort of logical inference on symbols that were readable
for humans. In contrast, the AI systems of today are more
complex why explainability is more challenging. Hence,
research on XAI and computer-aided verification “needs
to keep pace with applied AI research in order to close the
research gaps that could hinder operational deployment.”
(Kistan et al., 2018, p. 1). We argue that this does not only
INFORMATION SYSTEMS MANAGEMENT © 59

Table 1. Summary of potential research opportunities and contributions.

 

 

Research
stream Research question Research contribution
Behavioral How do Al explanations influence the users’ and mangers’ cognitive © Knowledge about how explainability may be an important variable in
Science perception of the Al? existing theories about human perception of the world and IS
artifacts (e.g., affordance theory, mental model theory, sensemaking,
UTAUT, and others).
How do explanations influence employees’ compliance behavior and Knowledge on how Al explanations support IT governance.
work practices?
How do explanations help to detect bias in managerial decision Knowledge on how a higher degree of Al transparency leads to a better
making? understanding of potentially undesired practices in the
organizational offline world, which found their way into the data
sets (e.g., when it comes to racial or gender bias).
Under which circumstances do explanations support or inhibit Knowledge on how different levels of expertise and personality traits
individual’s trust toward the Al? like risk aversion elicit different reactions to Al explanations.
How can explanations fulfill task-related needs of the different XAl Knowledge on when and how explanations should be presented to
stakeholders? users in order to increase task performance.
What are adequate metrics to evaluate Al explanations? Knowledge on the dimensions that are relevant for explanations to be
effective; differentiate “good” from “bad” explanations.
How do explanations influence (de)skilling of employees? Knowledge on how explanations help to maintain or increase user
qualification and self-efficacy regarding Al usage.
Design How can the technical advancements of computer science (e.g., XAl Bring together knowledge and methodical expertise of different
Science instruments) be integrated with advancements of information disciplines in order to accelerate and improve XAI research across

systems (e.g., theorizing and categorization of explanations)?
Which features in explanations support the evaluation of an Al’s
ethicality and morality?

How can the transdisciplinary design of Al explainability across
different stakeholders look like?

What are design principles on how to build explainable Al systems that

allow for a stakeholder- and domain-specific personalization?

How should mechanisms of push and pull information through
explanations look like?

How can the analysis of XAl feature usage help to improve the design

and hence quality of Al explanations?

How should explanation interfaces in the context of interactive

machine learning be designed, in order to improve the Al system

based on a users’ feedback to its reasoning?

research communities.

Derive an understanding of how an Al's state of ethicality and morality
can be evaluated and which information need to be provided via
explanations.

Conceptualization of a standardized design process for fair,
accountable and transparent Al, that take the needs of different
stakeholders into account.

Knowledge of technical possibilities to allow for a flexible adaptation of
explanations by users (based on their task-specific needs and level of
expertise).

Knowledge on when the system needs to push information on its
reasoning or emerging risks, and how the user can be enabled to
individually pull explanations (which includes different regulatory
needs for explainability of Al according to its criticality).

Knowledge on how the manual or automatic analysis of Al usage data
improve the understanding of the users’ information needs and
hence Al explanations.

Improving our understanding on the role of explanations in the
context of Human-in-the-loop (HITL) interactions between users and
Al.

 

refer to the development of new XAI methods but also
requires a socio-technical perspective. There are hence
various opportunities for further investigations on the
topic of explainability in information systems, of which
we outline examples in Table 1.

Conclusion

AI has diffused into many areas of our private and
professional life. It hence influences how we live and
work. Moreover, it is increasingly used in critical situa-
tions with potentially severe consequences for indivi-
dual human beings, businesses, and the society as
a whole. In consequence, new ethical questions arise
that challenge necessary compromises between an
open development of Al-based innovations and regu-
lations based on societal consensus (EU Commission,
2019; Jobin et al., 2019). Research on explainability, so

we argue, is an important factor to support such com-
promises. In the last 70 years, there have been several
AI “summers” (Grudin, 2019). As our brief review on
explainability in information systems highlights, there
has also been an “explainability summer” in the
1990ies and an “explainability winter” since the dawn
of the new millennium. At the moment, witnessing
another raise of attention for AI, we therefore call for
a second summer of explainability research in infor-
mation systems. In summary, it can be concluded that
XAI is a central issue for information systems research,
which opens up a multitude of interesting but also
challenging questions to investigate.

Note

1. We acknowledge that there have been recent XAI pub-
lications on IS conferences. However, in this section, we
only focus on articles in IS journals.
60 @@) C.MESKE ET AL.

Funding

This work was supported by the Einstein Stiftung Berlin
[Einstein Center Digital Future].

Notes on contributors

Christian Meske is Assistant Professor at the Department of
Information Systems, Freie Universitat Berlin, and board
member of the Einstein Center Digital Future (Berlin),
Germany. His research on digital transformation and colla-
boration has been published in journals such as Business &
Information Systems Engineering, Business Process
Management Journal, Communications of the Association
for Information Systems, Information Systems Frontiers,
Information Systems Management, Journal of Enterprise
Information Management, or Journal of the Association for
Information Science and Technology. Amongst others, he has
been recognized with the AIS Best Information Systems
Publication of the Year Award and ICIS Paper-a-Thon Award.

Enrico Bunde is a research assistant and PhD at the
Department of Information Systems at Freie Universitat
Berlin (Germany). There he is member of the research group
“Digital Transformation and Strategic Information
Management”, and focuses on explainable artificial intelli-
gence and decision support systems. His work has been pub-
lished or accepted for publication at conferences such as the
International Conference on Information Systems, Hawaii
International Conference on System Sciences, or
International Conference on Artificial Intelligence in Human-
Computer Interaction.

Johannes Schneider holds a tenure-track position as Assistant
Professor in Data Science at the Institute of Information
Systems at the University of Liechtenstein. His research has
been published within and outside the Information Systems
community, including journals such as Journal of the ACM,
the ACM Transactions on Knowledge Discovery from Data,
IEEE Transactions on Software Engineering, the journal of
Theoretical Computer Science and the International Journal
of Information Management.

Martin Gersch is a full professor of Business Administration,
Information and Organization at the School of Business &
Economics of Freie Universitat Berlin (Germany) and there
one of the founders of the Department of Information
Systems. He serves also, amongst others, as Principle
Investigator on Digital Transformation at the Einstein
Center Digital Future and as a mentor for startup teams in
the digital economy. His research has been published in jour-
nals such as Journal of Management Studies, Electronic
Markets, Business & Information Systems Engineering,
Journal of Business Process Management, Organization
Studies and interdisciplinary also e.g., in Journal as BMC
Health Services Research or Journal of the Intensive Care
Society.

ORCID

Christian Meske @ http://orcid.org/0000-0001-5637-9433

References

Abdul, A., Vermeulen, J.. Wang, D., Lim, B. Y., &
Kankanhalli, M. (2018). Trends and trajectories for explain-
able, accountable and intelligible systems. Proceedings of the
2018 CHI conference on human factors in computing systems
- CHI 18, Montréal, Canada, 1-18. https://doi.org/10.1145/
3173574.3174156

Adadi, A., & Berrada, M. (2018). Peeking inside the black-box:
A survey on explainable artificial intelligence (XAI). IEEE
Access, 6, 52138-52160. https://doi-org/10.1109/ACCESS.
2018.2870052

Alt, R. (2018). Electronic markets and current general
research. Electronic Markets, 28(2), 123-128. https://doi.
org/10.1007/s12525-018-0299-0

Arnold, V., Clark, N., Collier, P. A., Leech, S. A, &
Sutton, S. G. (2006). The differential use and effect of
knowledge-based system explanations in novice and expert
judgment decisions. MIS Quarterly: Management
Information Systems, 30(1), 79. https://doi.org/10.2307/
25148718

Arnold, V., & Sutton, S. G. (1998). The theory of technology
dominance: Understanding the impact of intelligent deci-
sion maker’s judgments. Advances in Accounting
Behavioral Research, 1(3), 175-194.

Berente, N., Gu, B., Recker, J., & Santhanam, R. (2019). Call
for papers MISQ special issue on managing AI. MIS
Quarterly, 1-5. https://misq.org/skin/frontend/default/
misq/pdf/CurrentCalls/Managing AI pdf

Binns, R. (2018). Fairness in machine learning: Lessons from
political philosophy. Maschine Learning Research, 81, 1-11.
https://arxiv.org/abs/1712.03586

Blair, A., & Saffidine, A. (2019). AI surpasses humans at
six-player poker. Science, 365(6456), 864-865. https://doi.
org/10.1126/science.aay7774

Béhmann, T., Leimeister, J. M., & Méslein, K. (2014). Service-
systems-engineering. WIRTSCHAFTSINFORMATIK, 56
(2), 83-90. https://doi.org/10.1007/s11576-014-0406-6

Brewster, F. W., II. (2002). Using tactical decision exercises to
study tactics. Military Review, 82(6), 3-9. https://www.seman
ticscholar.org/paper/Using-Tactical-Decision-Exercises
-to-Study-Tactics-Brewster/acc2892aa434e4a743e7638f
769b06be0ee0639d

Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P.,
Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B.,
Anderson, H., Roff, H., Allen, G. C., Steinhardt, J.,
Flynn, C., & Amodei, D. (2018). The malicious use of arti-
ficial intelligence: Forecasting, prevention, and mitigation.
1-101.

Bruun, E. P. G., & Duka, A. (2018). Artificial intelligence, jobs
and the future of work: Racing with the machines. Basic
Income Studies, 13(2), 1-15. https://doi.org/10.1515/bis-
2018-0018

Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics
derived automatically from language corpora contain
human-like biases. Science, 356(6334), 183-186. https://
doi.org/10.1126/science.aal4230

Chandrasekaran, B., Tanner, M. C., & Josephson, J. R. (1989).
Explaining control strategies in problem solving. [EEE
Expert, 4(1), 9-15. https://doi.org/10.1109/64.21896

European Comission, High-Level Expert Group on AI.
(2019). Ethics guidelines for trustworthy AI. Retrieved
June 5, 20209, from https://ai.bsa.org/wp-content/
uploads/2019/09/AIHLEG_EthicsGuidelinesfor
TrustworthyAI-ENpdf.pdf

Dastin, J. (2018). Amazon scraps secret Al recruiting tool that
showed bias against women. Reuters. https://www.reuters.
com/article/us-amazon-com-jobs-automation-insight/ama
zon -scraps-secretai-recruiting-tool-that-showed-bias-
against-women-idUSKCNIMK08G

Dewey, M., & Wilkens, U. (2019). The bionic radiologist:
Avoiding blurry pictures and providing greater insights.
Npj Digital Medicine, 2(1), 65. https://doi.org/10.1038/
s41746-019-0142-9

Dhaliwal, J. S., & Benbasat, I. (1996). The use and effects of
knowledge-based system explanations: Theoretical founda-
tions and a framework for empirical evaluation.
Information Systems Research, 7(3), 342-362. https://doi.
org/10.1287/isre.7.3.342

Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science
of interpretable machine learning, arXiv preprint, 1-13.
https://arxiv.org/abs/1702.08608

Eiras-Franco, C., Guijarro-Berdirias, B., Alonso-Betanzos, A.,
& Bahamonde, A. (2019). A scalable decision-tree-based
method to explain interactions in dyadic data. Decision
Support Systems, 127, 113141. https://doi.org/10.1016/j.dss.
2019.113141

Elbanna, A., Dwivedi, Y., Bunker, D., & Wastell, D. (2020).
The search for smartness in working, living and organising:
Beyond the ‘technomagic. Information Systems Frontiers, 22
(2), 275-280. https://doi-org/10.1007/s10796-020-10013-8

Fagan, L. M., Shortliffe, E. H., & Buchanan, B. G. (1980).
COMPUTER-BASED MEDICAL DECISION MAKING:
FROM MYCIN TO VM. Automedica.

Fernandez, A., Herrera, F., Cordon, O., Jose Del Jesus, M., &
Marcelloni, F. (2019). Evolutionary fuzzy systems for
explainable artificial intelligence: Why, when, what for,
and where to? IEEE Computational Intelligence Magazine,
14(1), 69-81. https://doi.org/10.1109/MCI.2018.288 1645

Friedman, C. P., Elstein, A. S., Wolf, F. M., Murphy, G. C.,
Franz, T. M., Heckerling, P. S., Fine, P. L., Miller, T. M., &
Abraham, V. (1999). Enhancement of clinicians’ diagnostic
reasoning by computer-based consultation. JAMA, 282(19),
1851-1856. https://doi-org/10.1001/jama.282.19.1851

Fiirnkranz, J., Kliegr, T., & Paulheim, H. (2020). On cog-
nitive preferences and the plausibility of rule-based
models. Machine Learning, 109, 853-898. https://doi.
org/10.1007/s10994-019-05856-5 4 109 doi:10.1007/
s10994-019-05856-5

Garlick, B. (2017). Flying smarter: AI & machine learning in
aviation autopilot systems. Stanford University.

Giboney, J. S., Brown, S. A., Lowry, P. B., & Nunamaker, J. F.
(2015). User acceptance of knowledge-based system recom-
mendations: Explanations, arguments, and fit. Decision
Support Systems, 72, 1-10. https://doi-org/10.1016/j.dss.
2015.02.005

Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &
Kagal, L. (2018). Explaining explanations: An overview of
interpretability of machine learning. 2018 IEEE 5th
International Conference on Data Science and Advanced
Analytics (DSAA), 80-89. https://arxiv.org/abs/1806.00069

Goddard, K., Roudsari, A., & Wyatt, J. C. (2011). Automation
bias - a hidden issue for clinical decision support system

INFORMATION SYSTEMS MANAGEMENT © 61

use. Studies in Health Technology and Informatics, 164,
17-22. https://doi.org/10.3233/978-1-60750-709-3-17

Goddard, K., Roudsari, A., & Wyatt, J. C. (2012). Automation
bias: A systematic review of frequency, effect mediators, and
mitigators. Journal of the American Medical Informatics
Association?: JAMIA, 19(1), 121-127. https://doi.org/10.
1136/amiajnl-2011-000089

Goniil, M. S., Onkal, D., & Lawrence, M. (2006). The effects of
structural characteristics of explanations on use of a DSS.
Decision Support Systems, 42(3), 1481-1493. https://doi.org/
10.1016/j.dss.2005. 12.003

Goodman, B., & Flaxman, S. (2017). European union regula-
tions on algorithmic decision-making and a right to
explanation. Al Magazine, 38(3), 50-57. https://doi.org/10.
1609/aimag.v38i3.2741

Gregor, S., & Yu, X. (2002). Exploring the explanatory cap-
abilities of intelligent system technologies. In V. Dimitrov &
V. Korotkich (Eds.), Fuzzy Logic (pp. 288-300). Physica-
Verlag HD.

Gregor, S., & Benbasat, I. (1999). Explanations from intelligent
systems: Theoretical foundations and implications for
practice. MIS Quarterly, 23(4), 497-530. https://doi.org/10.
2307/249487

Grigorescu, S., Trasnea, B., Cocias, T., & Macesanu, G. (2020).
A survey of deep learning techniques for autonomous
driving. Journal of Field Robotics, 37(3), 362-386. https://
doi.org/10.1002/rob.21918

Grudin, J. (2019). AI summers’ do not take jobs.
Communications of the ACM, 59(2), 8-9. https://dl.acm.
org/doi/fullHtml/10.1145/2871050

Guidotti, R. Monreale, A., Ruggieri, S., Turini, F.,
Pedreschi, D., & Giannotti, F. (2018). A survey of methods
for explaining black box models. ACM Computing Surveys
(CSUR), 51(5), 1-42. https://doi.org/10.1145/3236009

Gunning, D. (2017). Explainable artificial intelligence (XAl).
DARPA Program Update November. https://www.darpa.
mil/attachments/XAIProgramUpdate.pdf

Haugeland, J. (1985). Artificial intelligence: The very idea.
Massachusetts Institute of Technology, MIT PRESS.

Herse, S., Vitale, J., Tonkin, M., Ebrahimian, D., Ojha, S.,
Johnston, B., Judge, W., & Williams, M. A. (2018). Do you
trust me, blindly? Factors influencing trust towards a robot
recommender system. RO-MAN 2018-27th IEEE
International Symposium on Robot and Human Interactive
Communication. https://doi.org/10.1109/ROMAN.2018.
8525581

Hohman, F., Kahng, M., Pienta, R., & Chau, D. H. (2019).
Visual analytics in deep learning: An interrogative survey
for the next frontiers. [EEE Transactions on Visualization
and Computer Graphics, 25(8), 2674-2693. https://doi.org/
10.1109/TVCG.2018.2843369

Holzinger, A., Langs, G., Denk, H., Zatloukal, K., Miiller, H.
(2019). Causability and explainability of artificial intelli-
gence in medicine. WIREs Data Mining and Knowledge
Discovery, 9(4), 1-13. https://doi.org/10.1002/widm.1312

Jobin, A., Ienca, M., & Vayena, E. (2019). Artificial intelli-
gence: The global landscape of ethics guidelines. Nature
Maschine Intelligence, 1(9), 389-399. https://doi.org/10.
1038/s42256-019-0088-2

Kaplan, A., & Haenlein, M. (2019). Siri, Siri, in my hand:
Who’s the fairest in the land? On the interpretations, illus-
trations, and implications of artificial intelligence. Business
62 @) C.MESKE ET AL.

Horizons, 62(1), 15-25. https://doi.org/10.1016/j.bushor.
2018.08.004

Kasnakoglu, B. T. (2016). Antecedents and consequences of
co-creation in credence-based service contexts. The Service
Industries Journal, 36(1-2), 1-20. https://doi-org/10.1080/
02642069.2016.1138472

Kistan, T., Gardi, A., & Sabatini, R. (2018). Machine learning
and cognitive ergonomics in air traffic management: Recent
developments and considerations for certification.
Aerospace, 5(4), 103. https://doi.org/10.3390/
aerospace5040103

Kuhl, N., Lobana, J., & Meske, C. (2019). Do you comply with
Al? - Personalized explanations of learning algorithms and
their impact on employees’ compliance behavior. 40th
International Conference on Information Systems (ICIS),
1-6. https://arxiv.org/pdf/2002.08777.pdf

Kusner, M. J., Loftus, J., Russell, C., & Silva, R. (2017).
Counterfactual Fairness. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, $. Vishwanathan, &
R. Garnett (Eds.), Advances in neural information processing
systems 30 (pp. 4066-4076). Curran Associates, Inc.

Lapuschkin, S., Waldchen, S., Binder, A., Montavon, G.,
Samek, W., & Miiller, K.-R. (2019). Unmasking clever
hans predictors and assessing what machines really learn.
Nature Communications, 10(1096), 1-8. https://doi.org/10.
1038/s41467-019-08987-4

Li, M., & Gregor, S. (2011). Outcomes of effective explana-
tions: Empowering citizens through online advice. Decision
Support Systems, 52(1), 119-132. https://doi.org/10.1016/j.
dss.2011.06.001

Lynch, J., & Schuler, D. (1990). Consumer evaluation of the
quality of hospital services from an economics of informa-
tion perspective. Journal of Health Care Marketing, 10(2),
16-22. https://pubmed.ncbi.nlm.nih.gov/10105192/

Malhotra, A., Melville, N. P., & Watson, R. T. (2013). Spurring
Impactful Research on Information Systems for
Environmental Sustainability. MIS Quarterly, 37(4),
1265-1274. https://doi.org/10.1002/mrdd

Mao, J.-Y., & Benbasat, I. (2000). The use of explanations in
knowledge-based systems: Cognitive perspectives and a
process-tracing analysis. Journal of Management
Information Systems, 17(2), 153-179. https://doi.org/10.
1080/07421222.2000.11045646

Martens, D., & Provost, F. (2014). Explaining data-driven
document classifications. MIS Quarterly: Management
Information Systems, 38(1), 73-99. https://doi.org/10.
25300/MISQ/2014/38.1.04

Matzner, M., Biittgen, M., Demirkan, H., Spohrer, J., Alter, S.,
Fritzsche, A., Ng, IL C. L., Jonas, J. M., Martinez, V.,
Moslein, K. M., & Neely, A. (2018). Digital transformation in
service management. Journal of Service Management Research,
2(2), 3-21. https://doi.org/10.15358/2511-8676-2018-2-3

Mccarthy, J., Minsky, M., Rochester, N., & Shannon, C. E.
(2006). A proposal for the dartmouth summer research
project on artificial intelligence, August 31, 1955. Al
Magazine, 27(4), 12-14. https://doi.org/10.1609/aimag.
v27i4, 1904

McKinney, S. M., Sieniek, M., Godbole, V., Godwin, J.,
Antropova, N., Ashrafian, H., Back, T., Chesus, M.,
Corrado, G. C., Darzi, A., Etemadi, M., Garcia-Vicente, F.,
Gilbert, F. J., Halling-Brown, M., Hassabis, D., Jansen, S.,
Karthikesalingam, A., Kelly, C. J., King, D., & Shetty, S.

(2020). International evaluation of an AI system for breast
cancer screening. Nature, 577(7788), 89-94. https://doi.org/
10.1038/s41586-019-1799-6

Mikalef, P., Popovic, A., Lundstrém, J. E., & Conboy, K.
(2020). special issue call for papers: Dark side of analytics
and AL The European fournal of Information Systems.
https://www.journalconferencejob.com/ejis-dark-side-of-
analytics-and-ai

Miller, T. (2019). Explanation in artificial intelligence: Insights
from the social sciences. Artificial Intelligence, 267, 1-38.
https://doi.org/10.1016/j.artint.2018.07.007

Narla, A., Kuprel, B., Sarin, K., Novoa, R., & Ko, J. (2018).
Automated Classification of Skin Lesions: From Pixels to
Practice. Journal of Investigative Dermatology, 138(10),
2108-2110. https://doi-org/10.1016/j.jid.2018.06.175

Papamichail, K. N., & French, S. (2005). Design and evaluation
of an intelligent decision support system for nuclear
emergencies. Decision Support Systems, 41(1), 84-111.
https://doi.org/10.1016/j.dss.2004.04.014

Parikh, R. B., Teeple, S., & Navathe, A. S. (2019). Addressing
Bias in Artificial Intelligence in Health Care. JAMA, 322
(24), 2377-2378. https://doi.org/10.1001/jama.2019.18058

Radaelli, L., de Montioye, Y.-A., Singh, V. K., & Pentland, A. P.
(2015). Unique in the shopping mall: On the reidentifiabil-
ity of credit and card metadata. Science, 347(6221),
536-539. https://doi.org/10.1126/science. 1256297

Rahman, M., Yu, X., & Srinivasan, B. (1999). A neural
networks based approach for fast mining characteristic
rules. In Foo N,(eds) advanced topics in artificial intelli-
gence. AI 199. Lecture notes in computer science, vol
1747 (pp. 36-47). Springer. https://doi.org/10.1007/3-
540-46695-9_4

Rai, A., Constantinides, P., & Sarker, S. (2019). Editor’s com-
ments: Next-generation digital platforms: Toward human-
Al hybrids. MIS Quarterly, 43(1), 3-4. https://dl.acm.org/
doi/10.5555/3370135.3370136

Ras, G., van Gerven, M., & Haselager, P. (2018).
Explanation methods in deep learning: Users, values,
concerns and challenges. In H. J. Escalante, S. Escalera,
I. Guyon, X. Bard, Y. Giiciitirk, U. Giiclii, &
M. Gerven (Eds.), Explainable and interpretable models
in computer vision and machine learning. The Springer
series on challenges in machine learning (pp. pp. 19-36).
Cham, Schweiz.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should
I Trust You?”. Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 1135-1144. https://doi-org/10.1145/2939672.
2939778

Rudin, C. (2019). Stop explaining black box machine learning
models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence, 1(5),
206-215. https://doi.org/10.1038/s42256-019-0048-x

Schneider, J., & Handali, J. (2019). Personalized explanation in
machine learning: A conceptualization. 27th European
Conference on Information Systems (ECIS 2019), 1-17.
https://arxiv.org/pdf/1901.00770.pdf

Schneider, J., Handali, J., Vlachos, M., & Meske, C. (2020).
Deceptive AI Explanations: Creation and Detection. arxiv,
2001, 07641. https://arxiv.org/pdf/2001.07641.pdf
Searle, J. R. (1980). Minds, brains, and programs. Behavioral
and Brain Sciences, 3(3), 417-424. https://doi-org/10.1017/
$0140525.X00005756

Shirer, M., & Daquila, M. (2019). Worldwide spending on
artificial intelligence systems will be nearly $98 Billion in
2023, According to New IDC Spending Guide. International
Data Corporation (IDC).

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, L,
Huang, A. Guez, A., Hubert, T., Baker, L. Lai, M.,
Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den
Driessche, G., Graepel, T., & Hassabis, D. (2017). Mastering
the game of Go without human knowledge. Nature, 550
(7676), 354-359. https://doi.org/10.1038/nature24270

Su, J., Vargas, D. V., & Sakurai, K. (2019). One pixel attack for
fooling deep neural networks. [EEE Transactions on
Evolutionary Computation, 23(5), 828-841. https://doi.
org/10.1109/TEVC.2019.2890858

Sutton, S. G., Arnold, V., & Holt, M. (2018). How much
automation is too much? Keeping the human relevant in
knowledge work. fournal of Emerging Technologies in
Accounting, 15(2), 15-25. https://doi.org/10.2308/jeta-
52311

Swartout, W. R., & Smoliar, S. W. (1987). On making expert
systems more like experts. Expert Systems, 4(3), 196-208.
https://doi.org/10.1111/j. 1468-0394. 1987.tb00143.x

Union, E. (2016). Regulation (EU) 2016/679 of the European
Parliament and of the Council of 27 April 2016 on the
protection of natural persons with regard to the processing
of personal data and on the free movement of such data, and
repealing Directive 95/46/EC (General Da (Off. J. Eur.
Union L119; pp. 1-88).

van Lent, M., Fisher, W., & Mancuso, M. (2004). An explain-
able artificial intelligence system for small-unit tactical
behavior. Proceedings of the 16th Conference on Innovative
Applications of Artificial Intelligence, 900-907. https://www.
aaai.org/Papers/IAAI/2004/IAAI04-019. pdf

INFORMATION SYSTEMS MANAGEMENT © 63

Vidgen, R., Shaw, S., & Grant, D. B. (2017). Management
challenges in creating value from business analytics.
European Journal of Operational Research, 261(2),
626-639. https://doi.org/10.1016/j.ejor.2017.02.023

Wang, H., Li, C., Gu, B., & Min, W. (2019). Does
Al-based credit scoring improve financial inclusion?
Evidence from online payday lending. In proceedings
of the 40th international conference on information sys-
tems, Paper ID 3418, Munich, Germany, pp. 1-9.

Watson, H. (2017). Preparing for the cognitive generation of
decision support. MIS Quarterly Executive, 16(2), 153-169.
https://www.semanticscholar.org/paper/Preparing-for-the-
Cognitive-Generation-of-Decision-Watson/
766825 192ccec1419564c9882a857339cc4e9a44

Wood, S., & Schulman, K. (2019). The doctor-of-the-future is
in: patient responses to disruptive health-care innovations.
Journal of the Association for Consumer Research, 4(3),
231-243. https://doi.org/10.1086/704106

Xiao, L., Shen, X.-L., Cheng, X., Mou, J., & Zarifis, A. (2020).
Call for Papers - The Dark Sides of AI. Electronic Markets.
https://www.springer.com/journal/12525/updates/
17695144

Yampolskiy, R. V. (2019). Predicting future AI failures from
historic examples. Foresight, 21(1), 138-152. https://doi.
org/10.1108/FS-04-2018-0034

Ye, L. R., & Johnson, P. E. (1995). The impact of explanation
facilities on user acceptance of expert systems advice. MIS
Quarterly, 19(2), 157-172. https://doi.org/10.2307/249686

Zolbanin, H. M., Delen, D., Crosby, D., & Wright, D. (2019).
A predictive analytics-based decision support system for
drug courts. Information Systems Frontiers, 22, 1-20.
https://doi.org/10.1007/s10796-019-09934-w

Zuboff, S. (2015). Big other: Surveillance capitalism and the pro-
spects of an information civilization. Journal of Information
Technology, 30(1), 75-89. https://doi.org/10.1057/jit.2015.5
2101.03613v1 [cs.LG] 10 Jan 2021

arXiv

Explainable Artificial Intelligence (XAI): An
Engineering Perspective

Fatima Hussain, Rasheed Hussain, SM/EEE, and Ekram Hossain, FIEEE

Abstract—The remarkable advancements in Deep Learning
(DL) algorithms have fueled enthusiasm for using Artificial
Intelligence (AI) technologies in almost every domain; however,
the opaqueness of these algorithms put a question mark on
their applications in safety-critical systems. In this regard, the
‘explainability’ dimension is not only essential to both explain
the inner workings of black-box algorithms, but it also adds
accountability and transparency dimensions that are of prime
importance for regulators, consumers, and service providers.
eXplainable Artificial Intelligence (XAI) is the set of techniques
and methods to convert the so-called black-box AI algorithms
to white-box algorithms, where the results achieved by these
algorithms and the variables, parameters, and steps taken by
the algorithm to reach the obtained results, are transparent and
explainable. To complement the existing literature on XAI, in
this paper, we take an ‘engineering’ approach to illustrate the
concepts of XAI. We discuss the stakeholders in XAI and describe
the mathematical contours of XAI from engineering perspective.
Then we take the autonomous car as a use-case and discuss the
applications of XAI for its different components such as object
detection, perception, control, action decision, and so on. This
work is an exploratory study to identify new avenues of research
in the field of XAI.

Index Terms—Explainable Artificial Intelligence (XAI), Model
Transparency, Interpretability, Explanability, Autonomous cars

I. INTRODUCTION AND BASIC CONCEPTS
A. Background

The inception of futuristic technologies such as the Internet
of Things (IoT), autonomous driving, augmented, and virtual
reality, at least in part, owe to the advancements in com-
munication technologies. Similarly, the idea of smart ‘things’
has revolutionized the applications and services space which
directly affects our lives in a positive way. These applications
generate huge amount of high dimensional and heterogeneous
data, and efficient methods are required to extract valuable
information from such data. For instance, an autonomous
car is expected to produce 5 to 20 Terabytes of data every
day (depending on the duration of driving)'. These data may
require real-time or near-real-time processing for monitoring,
prediction, decision making, and control purposes (e.g., for
autonomous navigation). Artificial Intelligence (AI) and its

F. Hussain is with User Behaviour Analytics and Insider Threat Team, Royal
Bank of Canada, Toronto, Canada (email: fatima-hussain@rbc.com).

R. Hussain is with Networks and Blockchain Laboratory, Innopolis Uni-
versity, Innopolis, Russia (email: r.hussain @ innopolis.ru).

E. Hossain is with Department of Electrical and Computer
Engineering at University of Manitoba, Winnipeg, Canada (email:
Ekram.Hossain@umanitoba.ca). His work was supported by a Discovery
Grant from the Natural Sciences and Engineering Research Council of
Canada (NSERC).

‘https://cutt.lynh8rMyj

breeds, Machine Learning (ML), and Deep Learning (DL)
techniques are primarily used as data analytics tools in such
scenarios.

To date, remarkable research results have been achieved by
using ML and DL in futuristic technologies. For instance, ML
and DL are widely used in autonomous cars for lane and object
detection, and perception, mapping, planning, route calcula-
tion, actuation, and so on [1], [2], [3]. Furthermore, cloud
and fog computing are becoming key components for many
technologies and applications. Data-driven machine learning
will be a key ingredient in cloud and fog computing platforms.

Despite the success of ML- and DL-based solutions in
different domains, there are a number of challenges faced
by ML and DL. For instance, the overhead incurred by the
ML- and DL-based solutions in different domains such as
IoT networks, healthcare, autonomous driving, and so on, will
greatly affect their scalability. Furthermore, outsourcing the
data generated by the users in every domain, may infringe
the privacy whereas the management of such data is another
challenge.

It is worth mentioning that depending on the domain, the
use of ML and DL is extremely critical from the point of view
of decisions taken by these algorithms. In such cases, even a
slight dysfunction of an ML/DL algorithm can have catas-
trophic consequences. For instance, in case of an autonomous
car, even a minor problem in computer vision component
may lead to fatality. A similar situation may occur in other
domains such as healthcare during diagnosis, prognosis, and
surgery. The explainability, transparency, and accountability
features are generally missing in the existing ML and DL
models. For instance, if there is a situation where the ML/DL
model’s output is undesirable (e.g., an autonomous car takes an
unexpected decision), then the accountability factor becomes
of paramount importance. Who should be accountable for that
decision? Why did the ML/DL model came to that conclusion?
These questions need to be answered.

B. Basic Attributes of XAI

When considering the comprehensiveness of the AI models
[4], there are three dimensions as discussed below.

1) Explainability: It is an active feature of a learning model
through which the processes undertaken by the model
can be clearly described. The aim is to clarify the
inner working of the learning model. Note that, critical
applications need explanability not just for intellectual
curiosity, but the risk factor is weighed above all other
factors when human lives are in danger [5].
2) Interpretability: Unlike explainability, it is a passive
feature of a learning model which enables the users to
understand the model and make sense out of it.

3) Transparency: Transparency is also directly related to
understandability where a learning model is considered
transparent if it exhibits understandability on its own
and without any interface. When a learning model is
inherently understandable without any extra components
introduced to the model, the model is transparent.

From the above definitions, it is also clear that transparency
embodies the explainability and interpretability and hence
emerges as the strong dominating aspect of comprehensiveness
in a learning model. Interpretability in ML and DL models
has been a hot topic of debate in the academia and industry
[6]. According to Tjoa et al., interpretability has inherent
benefits of improving the performance of ML and DL models
in terms of causality, reliability, and usability [6]. We also
note that different literature provide different definitions for
explainability and interpretability (as discussed above) [7], [8],
[9], [4], [10]; however, Tjoa et al. used these two terms inter-
changeably. According to their definition, interpretability (and
thus explainability) is a mechanism by which the decisions
taken by the algorithms can be explained, the inner details of
the model are understandable, and the models can be explained
mathematically. In our discussion, without loss of generality,
we will follow the approach of Tjoa et al. and use the terms
interpretability and explanability interchangeably.

The explainability feature is essential for the AI models’,
and their decisions, to be transparent. On the other hand,
the recent threats and adversarial attacks on ML and DL
algorithms also warrant the need for algorithmic and functional
transparency in ML and DL mechanisms [11], [12], [13].
Without loss of generality, the transparency in AI models
will serve several-fold purposes, for instance, it will increase
the trust in AI with positive expected outcomes, enable the
decision to whether fully rely on AI or consider human-factor
for decision-making, and address the security attacks on AI-
based techniques.

Another important dimension of XAI is accountability,
which is a legal dimension. Amidst the implementation of
General Data Protection Regulation (GDPR), the legal dimen-
sion of accountability becomes more prevalent because of the
liability issues in the emerging applications that deal with
privacy-sensitive data (e.g., autonomous cars, IoT, augmented
reality, etc.). In such situations (to be compliant with GDPR),
risk assessment can be carried out to decrease the liability
issues. For instance, in case of healthcare IoT applications that
collect and aggregate data from Body Area Network (BAN)
and then the data is either locally processed in a mobile
device or sent to a cloud storage for further processing. The
results obtained from such data could be shared with different
stakeholders including doctors, insurance agencies, and so on.
The risk of sharing this data with different entities (with proper
access control mechanism) should also take into account the
liability issues in case of any mishap.

2while discussing XAI, we focus on the breeds of AL, i.e., ML and DL
models. Therefore, in this paper, although we mention ML and DL models,
in fact are the AI models.

In the light of the above discussion, a new term Ex-
plainable Artificial Intelligence (XAD was tossed that adds
the explainability, transparency, and accountability dimensions
to the existing ML and DL (and thus AI) models [8]. In
other words, XAI is an effort to convert the black-box AI
models to white-box approaches where the overall process
taken by the algorithms and models to reach a decision can
be explained and interpreted, categorically. XAI is a Defense
Advanced Research Projects Agency (DARPA) initiative, and
it enables AI systems to have the feature of a glass box in
which machines can understand the context in their operating
environment and with human in the loop of the decisions.
The other objective of XAI is to build underlying explanatory
models for characterizing real world phenomena. DARPA has
divided XAI into three categories namely, Deep Explanation,
Interpretable Models, and Model Induction. Broadly speaking,
we need two distinct systems, one which can interpret the al-
ready existing complex models, and second which can explain
the newly designed models and the decision they make [14],
[15], [4].

C. Contribution and Organization of the Paper

The existing literature on XAI takes the application-level
approach to explain the current ML and DL algorithms and
argue on transparency. On the other hand, in this paper,
we take an engineering perspective of the explainability and
transparency of the ML and DL mechanisms. More precisely,
we explain the explainability from an engineering perspective
by explaining the problem of AI models that need explanation
and focus on the mathematical perspective of explanation.
We discuss different stakeholders of XAI and their perception
and requirements of XAI. Moving forward, we discuss the
engineering and mathematical perspective of XAI and take
autonomous car as a use-case of XAI. In the autonomous car
use-case, we identify and discuss in detail, the aspects that
need and benefit from explainable AI models, and discuss the
current solutions for XAI in autonomous cars. More precisely,
from an engineering perspective, we identify the problems of
AI models used in different components of autonomous car
such as object detection, control, perception, etc. and discuss
the existing solutions towards adding explainability features
to the AI models used in autonomous car. The aim of this
paper is to spur further discussion on the need for XAI in
critical applications and look at XAI in critical applications
from engineering perspective. The autonomous car use-case is
selected because it covers the engineering domain where we
define the clear problems pertaining to different components
of autonomous.

II. XAI SYSTEMS AND TECHNIQUES

In the literature, various models are developed and explained
for the explanation of the existing ML models; however,
only explanations about, “how the prediction is made?,’ is
not enough to justify the validity of results. This white box
model is not useful due to the complexities of most of the
models, which are also quantitative and non-intuitive in nature.
Therefore, users (who are not ML experts) are unable to
understand the functionalities of the model, no matter how
transparent the model is. Hence, we require interpretable
models to produce comprehensive details of the decisions and
predictions made by the models.

In the following, we discuss these two broad categories of
XAI systems.

A. Transparent Models

These models are also known as interpretable models and
are interpretable by design, in contrast to black-box ML
models. These transparent models refer to explanation and
understandability from the perspective of algorithmic trans-
parency followed by algorithm decomposability as well as
simulation ability of a model. All of these explanation types
define various levels of explanations that a specific model is
capable of providing.

First level of transparency of any model is its algorithmic
transparency; i.e., how well a user can understand the output
obtained from a given set of inputs. More specifically, trans-
parent model is fully explorable with the help of mathematical
analysis and methods. Linear models are easy to interpret
whereas non-linear models require more sophisticated meth-
ods.

Second level of transparency is the level of decomposability
of a model. This means, how intelligent an explanation of each
part (input, various parameters, and calculation performed) of
a model is? As a result of this explanation, it is easier to
understand and explain the behavior of a model. However,
it requires all the inputs to be readily interpretable which is
not possible for every model. This is due to the reason that
cumbersome and complex features and parameters might not
fulfill these requirements, and cannot be understood without
incorporating additional tools.

Furthermore, a completely transparent model or third level
of transparency is achieved if the model can be simulated.
It is the ability of the model to be simulated by humans.
Complexity of any model is definitely having prominence in
this transparency level.

In conclusion, an interpretable model can be easily ex-
plained to humans with the help of text and visualizations.
Furthermore, decomposable model having ability to be simu-
lated means that, model is self-contained in a way that it can
be understood, analyzed, and justified as a whole (by humans
without need of additional tools).

B. Prediction Interpretation and Justification Models

Black box models that are not interpretable by design,
require post-hoc explainability to enhance their interpretability
by various human-adopted explanation methods. These ex-
palinability methods can be text explanations, visual explana-
tions, explanations by simplification, explanations by example,
and explanations by feature relevance. Broadly speaking, such
post-hoc explainability depends upon the explanation tech-
niques adopted by the interpreters (authors), types of data
on which they applied and actual technique being used. For
instance, interpreter (author) can use explanation by simpli-
fication and perform sensitivity analysis on images. Or the

interpreter can either choose to perform explanation by using
different symbols or by various examples, etc.

In [16], the authors discuss various types of black box
models and presented the a broader distinction on the basis
of transparency inferred by the users. The authors classify the
systems into three types: opaque, interpretable, and compre-
hensible systems. In opaque systems, the mappings from input
to output is invisible to the user, while in interpretable systems,
users are aware of and can mathematically analyze the input to
output mappings. Lastly, in comprehensible systems, users are
not only aware of the mapping but also specific rules behind
those mappings. In short, comprehensible systems provide a
comprehensive view of symbols (rules) of mapping along with
their specific output in a way that is completely understood
by the users.

These post-hoc explanation techniques can be applied to
infer an interpretable model from any black box model. The
techniques used to infer an explainable model from any black
box model are also know as model induction [15]. However,
it is difficult to interpret the entire model with comprehensive
explanation, unless complete description of the model is
performed. Local or global context is also an important factor
to consider, i.e., explanation in local fidelity might not be
completely true in global fidelity [17].

III. STAKEHOLDERS IN XAI AND EXPLAINABILITY
REQUIREMENTS

As discussed earlier, an intelligent system cannot be com-
pletely trusted if it does not provide enough justification for
the predictions it makes. It is more trusted by clients if the
system is explained as per caliber and explanation requirement
of various stakeholders of that system. To this end, the most
important task is to identify the stakeholders and afterwards,
provide acceptable explanation and interpretation for each type
of recipient (such as, engineers, designers, theorists etc.).

Before we dive into scientific and mathematical perspective
of XAI, in the following, we briefly discuss various stakehold-
ers and associated requirements to construct the narrative.

Once the mathematical description of ML models is avail-
able, testing (functionality) the model is possible not only
through mathematical analysis (for certain attributes) but can
also be checked for operational validity. This is done by
feeding new input data to the model and obtaining antic-
ipated correct output, thus validating the accuracy of the
model (the model under consideration). This engineering and
mathematical perspective may not be required for an ordinary
customer but is of utmost importance for a system designer
and developer. They can understand such a system in a better
way if it is more transparent and interpretable by design and
at development stage.

A. Stakeholders in XAI

In this section, we discuss different stakeholders in XAI and
shed light on their role as well as their expectations from XAI.
 

 

 

 

Engineers and Scientists

Ethicists

 

 

 

 

End Users and Consumers

 

 

 

&

* What is the overall logic of a system?

* What is the objective function and
learning algorithm of a model?

* What are various parameters and
variables and how the decision/
prediction is changed with
parameters, variables, context, etc.

* How did the learning take place and
which training data is used ?

* How the model was tested and
which test data is used?

* How model can be more robust?

* What is the impact of model
malfunction?

discrimination?

 

 

 

Fig. 1. Stakeholders in XAI

1) End-users and consumers: End users and consumers
need explanations to justify the actions of a system and decide
whether the outputs of the system are correct or not. In fact,
assurance and confidence should be given to users through
post-hoc explanation (with policy and ontology elements),
that the system does the right thing and give the unbiased
expected outcome. From the end-user perspective, explanation
of an XAI system should always reflect the psychological
processes of human understanding and essentially must follow
the following basic principles:

e User is able to understand the conceptual model and

underlying mechanisms of the system.

e Entire functionality of a system is visible to users and
enough explanations are given about the capability of the
the system.

e The visible elements of the system can be intuitively
mapped to respective functionalities.

e Users are always informed about the system’s current
state.

2) Ethicists and theorists: Ethicists are more concerned
about distinct explanations regarding transparency and fair-
ness of an AI systems, and include various stakeholders
such as computer scientists, engineers, lawyers, journalists,
economists, and politicians. The sole purpose of these stake-
holders is to go beyond the technical details and assure
fairness and unbiased behaviour in AI system, thus making
it accountable. Furthermore, assurance and confidence are
required by these ethicists through post-hoc explanations that
the system has reached ethical decisions without any biases.

Theorists, on the other hand are more interested in under-
standing the theory behind AI and further advancing it. These
stakeholders include academic or industrial researchers who
are not much interested in delivering the practical applica-
tions. These stakeholders also demand transparent relationship
between internal states of the model and the achieved output.

“Ta \

Is privacy of a user protected?

Is data privacy ensured?

Is user given right to challenge the
decsion of a Al model

Is any harm to society identofied ?

Is the model biased ? How to protect the

7,

 

2

Overall review of the model

What is the logic behind the decisions
made?

How my data is being protected and
used?

What are the parameters used for
decision making?

Is there some biases in the system?
Can I challenge the decision?

WY

 

 

 

3) Engineers and mathematicians: Mathematical models
are quantitative ways to describe both theoretical and real-
world systems. Engineers use mathematical models to describe
the behavior of any newly designed system or examine an
existing one. Therefore, once we demand or define explanation
for engineers and scientists, it is not just the data description
or high-level discussion about various decisions being made
by that system. They also expect transparency-based bindings
to the internal states of the model (i.e., traceability to any
action/state is possible), such that these explanations are not
just post-hoc rationalisation.

B. Role-Based Explainability Requirements

It is important to identify the stakeholder for explana-
tion, i.c., whom are the explanations for? Is it for advanced
mathematicians or engineers, or employees and customers?
There should be complete definition of the “Explainer” roles
to make AI models more relevant and readily interpretable.
Furthermore, a clear definition of “What” is required to carry
out the “explanation”. It is at par important to understand
the subject of explanation and to figure out whether it is the
model or the data itself. For instance, consider a system used
for identification of various cars on road. An explanation of
data (obtained from a source) may include the description that
a particular image is of a car as it is similar (in shape and
dimensions) to another image of a car (KNN application), or
the decision is made by considering various features (feature
selection) like four wheels, shape, back and front lights, etc.

In a nutshell, comprehensibility and explainability should be
considered in the context of a specific use-case or a stakeholder
[18], [19]. To put this discussion into perspective, we discuss
the use-case of autonomous car in detail in Section V. In Fig.
1, various stakeholders and corresponding questions for which
they need an answer, are listed.
 

 

Objective

 

Model equations

 

 

 

* What are we looking for?
* What we want to know?
* What we are trying to achieve?

 

 

Scope

 

=

X=x1,x7, 28, 0,20"

 

 

* Is model used for specfic application?
* Is model complex and used for
numerous application?

 

 

Model framework

Variables
What do we know?
SN ALTOS

Pele ley
+ How we can look at the model
* How we can improve the model

Operations
+ How we can look at the model?
SPW SRR eo eGo

rela tla)

Pa Maiele nsec ccrekg

eee ieee eee alee

* How we can improve the model?

 

 

xls input instances Y = output
Model description Training data
Ee
my ; y=822.0,., Xs 3 y
a F if x =high,y =1
xXx. ”" N L>y f x4 ne Wy a
Re x, = low,y =
xs xy = low,y = low .,

 

 

 

 

 

x3 =0y=0

 

xy Xa

 

 

 

Algorithmic description

if = X14%2 SCG, then y=x,+ Xs
FQ)= 1
(4) Ise
F(x) = Log (xs)
Rule # 1: 80 % of smaples have X;nad Xj in it

Rule # 2 : X, and X, are always positive

 

Feature relevance

X, has 80 % importance in Y

x1 x2 x3 x4 XS

 

 

 

 

 

 

 

 

 

 

Fig. 2. Mathematical perspective of XAI model

C. Explainability in the Context of Engineers and Scientists set of inputs.

Essentially the mathematical explanation of any ML model

requires discussion about the following pivots:

e Objective: It defines the goals and purpose of the model,
and this definition is adequate enough to define what the
ML model is trying to achieve.

e Scope: Scope of the ML model is explained clearly and
precise enough so that it can answer questions such as,
is the scope too small and have limited application, or it
is too large resulting in complex and resource-intensive
ML model, that is to be used in numerous applications.

e Model framework: Description of the system architec-
ture and entire operational scheme is given. This is the
description of variables or objectives, and also includes
operational details of the model such as governing rules,
regulations, and underlying assumptions necessary for
system operations.

e Modelling and mathematical equations: Explanation in-
cludes self-explanatory mathematical equations represent-
ing the system structure. These equations define the
relationship among various variables, constants, and con-
straints. In essence, mathematical modeling starts with
the general model equation describing inputs to the model
and potential output, followed by the detailed architecture
of various sub-networks and logic components. These
model descriptions also give insight to the intermediate
outputs, as well as combinations of these outputs leading
to final output of the model. Another important parameter
in model explanation is the algorithmic transparency, and
it is defined as the ability of the user to understand the
model and how a specific output is obtained for certain

Moreover, the detailed description of training data is
primarily important to further support the transparency
of a model and prediction decisions. In this regard,
linguistic rules are helpful to interpret training datasets
that include descriptions such as, if x2, is high then y
is high, and if x; is low and x2 is high, then y is
low (as shown in Fig. 2). This description of training
datasets make model simulate-able as well as the rule
specifications improve interpretability. Feature relevance
is another important parameter that contributes to the final
output decision and model transparency. For instance, if
5 features (x1, 22, x3, 24,75) are selected for any model
description and x1 has prime importance compared to the
rest of the four, then it will have for sure, major stake
in the prediction decision. Furthermore, one can easily
understand that slight change in x1 can drastically change
the model output.

We further elaborate these pivots in Fig. 2. Explanation of
a model starts with the objective and scope of the model,
and ‘why’ we need that model. It is also very important to
know about the governing principles of the model along with
available data and underlying assumptions (on the unknown
parameters). Moreover, the model framework including the
system equations is the heart of any mathematically-focused
XAI model. The framework captures details about variables,
possible parameters, and operational details. Similarly, model
equations provide details about the relationship between input
and output of a system. Additionally, algorithmic description
along with the details about training and test data contribute to
the essential part of XAI model. For instance, algorithmically
transparent models are explorable with mathematical analysis
and methods. Linear models are considered algorithmically
transparent because it can be easily understood how these
models will behave in particular situations. Additionally, their
error surface is known as well. On the other hand, models
with deep architectures are difficult to understand, and their
behaviour has to be approximated. Therefore, algorithmic
transparency and details of datasets can help in the simulation
and (re)testing of the model. Furthermore, feature relevance
description clarifies the inner functioning of a model because
the relevance score of each variable quantifies the affect of
any feature upon the decision making of the model. It also
quantifies the importance given to any variable for obtaining
the output of a model.

D. Explainability of Currently Used ML Models

Here, we discuss the explainability of commonly used ML
algorithms such as Decision Tree (DT), K-Nearest Neighbors
(KNN), and Bayesian models, which are considered as trans-
parent algorithms. Decision trees are hierarchical structures
and used for decision making in regression as well as clas-
sification problems. A small DT can be easily simulated and
managed by a user because the number of features along with
their meanings are easily understandable. Therefore, a user
can easily simulate DT model and can obtain the prediction
of DT, without the need of any mathematical background.
Furthermore, generic rules defining the model and constructing
the tree do not change with a change in the dataset, and
the model readability is preserved. However, an increase in
the size of DT impedes its full evaluation (simulation) by a
human, and therefore, it is considered to be a decomposable
model. On the other hand, Bayesian models are probabilistic
models and are built in the form of directed acyclic graphical
model such that the links between various sets of variables
are connected through conditional dependencies. In Bayesian
models, relationships between features and the target (con-
nections linking variables to each other) are very clear. For
instance, if a Bayesian network is used for the diagnosis
of diseases, the probabilistic relationships between diseases
and symptoms is intuitively clear, and probability of presence
of any diseases can be easily determined by inspecting the
symptoms. However, in the presence of overly complex and
cumbersome variables, statistical relationships cannot be in-
terpreted easily, and can only be analyzed using mathematical
tools. In case of KNNs, complexity of the model and detailed
description of number of variables, and similarity measures
are pretty intuitive for humans and, KNNs are easily simulate-
able. Therefore, the afore-mentioned models are considered as
transparent models. These are also decomposible as various
variables can be decomposed and analyzed easily. However,
when the similarity measure of variables is very high and too
complex to decompose, then mathematical tools are used for
explanation.

IV. RELATED WORK

Here we discuss the existing literature on the general XAI
techniques and their use in different domains including IoT.

In [20], the authors presented explainable causal models
that augment the underlying AI models. They derived their
model by analyzing 398 explanation dialogues across six
different dialogues (using grounded theory). Their proposed
model accurately defines the structure and the sequence of an
explanation dialog, and appears to support natural interaction
among human audience compared to explanations obtained
from the existing models. In the same spirit, the authors in
[21] examined the impact of virtual agents in XAI on the
trustworthiness of autonomous systems perceived by human
users. They conducted a user study based on a simple speech
recognition task and found significant evidences that the
integration of virtual agents and XAI leads to an increased
trust in the autonomous intelligent system. Their results also
showed that users trust AI system which use visual agents
more than AI system interpreting mere explanation.

In [22], the authors proposed a scenario-based design for
system development and leveraged various possible scenarios
to anticipate and predict new scenarios. The authors presented
a case study of aging-in-place monitoring, and demonstrated
the relevance of the scenario-based design to XAI design
practice. In [4], the authors discussed recent contributions
related to the explainability of different ML models and related
challenges. They also presented the concept of responsible
AI, and related guidelines for explainable and responsible AI
such as, fairness, accountability and privacy. In [23], the au-
thors identified the gaps between the current XAI algorithmic
practices and to create related explainable AI products. They
created the details of an XAI design space by developing
algorithm informed XAI question bank. This question bank
can be used as a baseline to create user-centered XAI and
incorporate important explainability features into the XAI
systems.

In [24], the authors presented a generic interpretable model
to learn interpretable convolutional filters used in the construc-
tion of interpretable CNN. These filters learn to represent the
same object portion across all the images fed to CNN, and do
not require any additional object annotations or textures (just
training data). During the learning process, the interpretable
filters are assigned automatically to each convolutional layer.
These filters represent explicit knowledge of various param-
eters in each of the CNN’s convolutional layers, such as,
encoded logic of CNN, what patterns are extracted from input
image, and how prediction is made, etc.

V. USE-CASE: AUTONOMOUS CAR

To discuss the explainability from an engineering perspec-
tive, we take the autonomous car use-case that extensively
uses artificial intelligence [1], [3]. We discuss explanatory
requirements in autonomous cars as a use-case of XAI and its
perspective of impacting human lives. In essence, we consider
the key components of autonomous car such as perception,
object detection, and actuation and discuss the necessity, role,
and techniques used to realize and incorporate XAI.
A. General Objective, and Transparency and Explainability
Requirement for the XAI Model

The general objective of an XAI model for any autonomous
vehicle is relatively straightforward, i.e., safe operation (per-
ception, planning, and actuation) of an autonomous vehicle.
Also, it should incorporate the rendering of surrounding envi-
ronment in the navigational decision making. A system should
be intelligent enough not only to predict the possible changes
in the environment but should also be adaptable to those
changes. In this regard, reinforcement learning techniques can
be used to interact and learn from the environmental changes.
To date, reinforcement learning has been used for different
functionalities in autonomous car such as navigation [25],
planning [26], and control [27]. Also, for autonomous cars, the
scope should be clearly defined such as, is vehicle designed
to operate in north American traffic or is it equally capable of
operating globally? In other words, the context is important.

It is essential to discuss the need for transparency and
explanation of the Al-related solutions in autonomous car. If
an autonomous car makes an unexpected decision (such as
wrong direction, wrong turn, sudden brakes, problems with
object identification, crashing into other objects, or fail to
apply brakes, etc.), it is essentially important to understand
why it happened, for a couple of reasons. The first reason is
to fix the problem and to improve the user experience and
trust in the autonomous car technology. The second reason is
that in case of an accident, it is critically important to perform
forensics and figure out the source of the accident. That will
be possible only if the decisions taken by the autonomous
car are transparent and explainable. From this perspective,
XAT is about the improvement, transparency, explanability, and
optimization of the decisions taken by autonomous cars.

Shen et al. [28] thoroughly investigated when an explanation
is needed during autonomous car drive and how does the
explanation changes with context? This study reveals that the
explanation purely depends on the driving scenarios and a real-
time text explanation can be provided to the passengers based
on the analysis of the data obtained from on-board camera.
The study reveals a number of important aspects of the need
for explanation in autonomous cars. For instance, explanation
is crucial for near-crash scenarios and un-expected decrease in
speed. While some scenarios call for decrease in speed such as
stop sign, while others do not. The explanation can be tightly
coupled with the passengers’ expectations. For instance, if the
speed was decreased and not expected by the passenger, then
explanation is provided to the passenger.

There is an ethical perspective of appropriate responses of
a car to certain situations, and ethicists are curious to know
the decision variables. However, automakers (engineers) need
explanations about how and what an autonomous car will do
when an accident is inevitable? Will it prioritize the protection
of the occupants or pedestrians (the trolley problem)? Will it
apply emergency breaks or crash into an object? Engineers
and scientists can track the data-set (with various driving
scenarios) by explaining how the decision was made, and thus
make it easier to debug, improve the decision procedure, test,
and so on. Similarly, explanation about route selection is also

Autonomous Vehicle

 

Sensors

 

Object detection + Obie collision
detection

Localization

| Path and trajectory planning |

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
    

Controls and navigation

GPS: Global Positioning System
IMU: Inertial Measurement Unit

 

I

|

I

|

I

I

| ]
I

| INerustera Steer angle &
I speed

|

I

|

I

I

I

I

Fig. 3. Autonomous Vehicle Decision Model

important for algorithmic understanding. More precisely, the
question of whether a good route is the shortest recommended
route, or the car should opt for a longer route but scenic,
can be taken into account. Additionally, passengers also need
explanation (in general) about how ethical decisions are made
by the car. As the occupants have to decide, if they are
comfortable traveling in a vehicle, that is designed to make
specific decisions in special circumstances.

B. Mathematical Perspective of the XAI Model in Autonomous
Cars

We dive into the mathematical perspective of an XAI
model for autonomous cars. We note that it is a complex
system consisting of many interacting agents including cars
(including software and hardware), road semantics, pedestri-
ans, and car position and controls. Interactions among these
agents can be modeled through differential equations, and
these equations are used to understand the behaviour of any
autonomous model. Following the foundational pivots defined
in Fig. 2, XAI model framework (mathematically driven) of
an autonomous car is shown in Fig. 3. In the following, we
discuss the types and levels of explanation required of different
functional entities in autonomous cars.

1) Object detection and classification: What are various
types of external and internal sensors such as LiDAR, Radars,
cameras, or Internet of Things (IoT) devices used in the
image sensing? How data fusion is performed on data ob-
tained from different sources? Details of pattern recognition
algorithms used in image classification (after images obtained
through sensors) are required. Since various types of images
(environmental) data is received through Advanced Driver
Assistance Systems (ADAS) data, reduction algorithms are
used for filtering the irrelevant images. Details of the type
of data reduction algorithm should be provided, ie., is it
Principle Component Analysis (PCA), Histograms of Oriented
Gradients (HOG), or Support Vector Machines (SVM)?

After image classification, object detection is performed
which leads to the following questions. Which Regression
algorithm is used for object detection? What are the dependent
and independent variables used in the regression algorithm?
It is worth mentioning that latent structures are exploited
to interpret the end-to-end learning and inference models in
object detection [29]. The authors in [29] use two-stage con-
volutional networks based on regions in an image and generate
rationale which serves as explanation. The authors combined
top-down grammar models with the bottom-up convoluntional
networks. They used hierarchical representation of the Region
of Interest (RoI) instead of widely used flat structure and
also enriching the Rols for interpretable object detection.
Similarly, Soares et al. [30] use 0-order fuzzy rules to provide
human understandable ’IF-THEN’ representation pertaining to
different decisions taken by autonomous cars (for instance
acceleration, lane change, etc.). The proposed classifier not
only improves the classification accuracy but also focuses on
the density of the features that contribute to the underlying
decision by leveraging massively parallel 0-order fuzzy rules.

2) Object localization, perception, and movement predic-
tion: Regression algorithms are used for motion prediction and
to create a statistical relational model of the detected object’s
current and future position. It leads to questions such as,
which regression algorithm is used for position prediction and
why? Decision Forest regression, neural network regression
or Bayesian regression? Is regression algorithm used for short
prediction or long time global prediction? Which localization
algorithms are used and what are the reasons for selecting
particular algorithms? To the best of our knowledge, the expla-
nation has not been introduced to the object localization and
movement prediction of autonomous cars. However, efforts
have been put forth in introducing interpretation to perception
in autonomous cars.

Here we discuss the existing work in this direction. In
[31], Kim et al. used visual attention networks for the in-
terpretation of perception developed by the autonomous cars.
Visual attention networks [32] play pivotal role in interpretable
learning where these networks provide visual attentions to
areas in an image that are influenced by the network and the
areas attended by the underlying networks. These networks
are inherently suitable for autonomous cars use-case where
real-time images are captured during driving. In this work,
Kim et al. applied post-processing to the output of the visual
attention network on the images from the autonomous car and
then determined the effect of the attention areas on the end-to-
end output of the network. Then the areas of image that have
causal effect on the end-to-end network learning are identified
and presented to the user. In other words, the authors detected
the visual saliency in images. Furthermore, this work also
concluded that the visual attention heat maps are essentially
important for suitable explanation and do not degrade the
efficiency of object detection and perception. Additionally,
Kim et al. also reported that causal filtering improves the
explanation by removing unnecessary features that do not
induce the output.

In the realm of perception, visual intelligence plays a pivotal
role in autonomous cars and the explainability related to visual
intelligence is important at par. In this regard, Zhang et al.
[33] claim that the existing methods of visual perceptions
have inherent disadvantages in real-world scenarios such as

extreme weather, low illumination, etc. and these might be
caused due to biasedness in the training dataset. Although
interpretability of learned representations by the deep networks
partially addresses the afore-mentioned challenge; however,
external factors related to test data are equally important.
Zhang et al. proposed an explainable AI evaluation where they
tried to include human domain knowledge to the variances in
test data that would augment the semantic concepts and the
output of the underlying algorithms used for visual perception.
They used Ridge Regression to find deterministic relationships
and then test the output of a specific algorithm.

Similarly, in perception, visual regions and their significance
as well as importance are of supreme importance. Therefore,
it is essential to focus on such regions in visual scenes while
performing perception in autonomous driving. Yang et al. [34]
proposed a framework to interpret the role of CNN in end-
to-end learning in understanding the driving scenarios. The
proposed framework determines the regions in visual scenes
that have major contributions in the perception. The authors
ranked the importance of the regions in scenes to determine
how different scenarios and the regions in scene affect the
performance of CNN in perception.

Motion planning is the next step to perception and predic-
tion in autonomous driving. The output of the perception and
prediction (that predicts the future positions and trajectories
of the objects detected and perceived) is used for motion
planning in terms of safe trajectory. However, this is not
an easy task as it may result in sub-optimal performance at
individual tasks (such as detection, perception, and prediction).
In other words, Zeng et al. [35] pointed out that optimization
in one local task does not universally translate to other
tasks. In this regard, Zeng et al. [35] proposed a new end-
to-end motion planning framework in autonomous cars that
is interpretable. The proposed scheme takes LiDAR data as
inputs and interprets the intermediate representation of the
input data into future trajectories. It also identifies the degree
of goodness of each possible position in the planned trajectory
that the autonomous car can take. This work is extended to [36]
to include interpretable perception and prediction as well as
motion planning. Furthermore, in the extended work, the same
authors implemented learning from human demonstration to
imitate the real driving scenarios.

3) Action decision: Decision is the pinnacle to the au-
tonomous car operation and as a result of perception, a
decision is taken by the autonomous car. This decision could
be the result of a learning process which warrants explanation
as we mentioned before. To this end, Xu et al. [37] proposed an
enhanced pipelined system based on action-inducing objects.
More precisely, the objects that are directly related to the
driving tasks. For example, pedestrians walking on a side-walk
are not as important as pedestrian crossing the roads. The latter
are referred to as action-inducing objects. Xu et al. proposed an
architecture where every action-inducing object is responsible
for an action (for instance, an object detected on the road might
induce the autonomous car to apply brakes) provided with an
explanation and they argued that the set of explanations is
finite in a given scenario. To achieve this goal, the authors
annotated a dataset for different commands pertaining to
driving and their respective explanations. Furthermore, the
authors solved the problem of object reasoning keeping in
mind the scene context with a goal to detect objects that induce
the decision, i.e., objects that may create hazard in driving.
And the detection includes the respective explanation about
the prediction of action taken by the autonomous car.

4) Navigation and control: Navigation and control are
important at par, to realize autonomous driving. Control algo-
rithms are responsible for the action decisions in autonomous
driving, therefore, the explanability and transparency of these
algorithms is essential. In terms of transparency and explan-
ability, some important questions should be answered. For
instance, what type of decision matrix algorithm is employed
for making specific decisions in specific scenarios? The ques-
tion whether a car needs to apply brake or take a turn, is
directly related to the level of confidence in the recognition,
classification algorithms as well as on the prediction of the
next movement of detected objects. How many decision mod-
els contribute towards this decision, and how these models
are trained? How decisions made by individual models are
combined to make the overall prediction while considering
the possibility of decreasing errors while making decisions?

Another very important description (explanation) is related
to motion dynamics of a vehicle considering elements such as
motor dynamics, brake system dynamics, and roll and pitch
motion, etc. Is six Degree of Freedom (DOF) model used
(considering all possible movements) or a simplified kinematic
model is used (that does not take into account dynamics
such as roll, pitch and Z motion)? What are various set of
equations that describe the vehicle motion in the X-Y plane?
Do these system equations describe the drive-train dynamics,
brake dynamics, wheels dynamics and the engine dynamics
clearly? Furthermore, car position at every instant has to be
updated and depends on its current velocity, acceleration, and
deceleration due to other vehicles and random events. Is such
deceleration due to extra fuel consumption or due to some road
blocker? What are the deciding factors for this deceleration?
To this end, according to the best of our knowledge, not
much work has been done in this direction. There exist only
a few works that look into the explanabitlity and transparency
in navigation. For instance, He et al. [38] proposed an
explainable DRL mechanism for navigation in autonomous
cars and autonomous drones. The proposed scheme aims at
both non-expert and expert users of the system to provide
them with explanation on the decision taken by the model.
These explanations are either visual or textual. In essence, the
proposed scheme used post-hoc explanation for the trained
navigation policy.

From the preceding discussion, we can conclude that there
are still many questions that are unanswered and warrant
further investigation in autonomous cars from the perspective
of XAL

VI. CONCLUSION AND OPEN CHALLENGES

The past few years have seen extensive interest in the field
of XAI. Due to the varying stakeholders, the same XAI can
be approached differently. In this paper,we have shed light on

the need for XAI, how it works generically, and how can we
look at XAI from an engineering perspective. Then we have
taken the autonomous car as a use-case and discussed different
components of autonomous driving from explainability and
transparency standpoint. We have reviewed the existing works
that focus on these different aspects of autonomous cars and
provide transparent, interpretable, and explainable solutions.

Despite the surge in XAI solutions in the literature, there
are still open challenges that need attention from the research
community. In the following, we discuss the open challenges
in XAI.

1) Generalization of XAI: From the preceding discus-
sion, we note that XAI, at least at the moment, is highly
environment- and domain-dependent. Generalization of XAT
might take long time to realize; however, the need for XAT
cannot be ignored, even more so with the realization of GDPR.
The real question is ‘when’ and not ‘why’. However, further
investigation is needed in this direction. More precisely, due to
the variation in stakeholders and their requirements, it will be
challenging to generalize XAI; however, one possible solution
would be to focus on domain-level explanation and work
towards local generalization from domain standpoint.

2) Adaptation of XAI: There have been noteworthy efforts
in the field of XAI covering different gaps in different do-
mains, but there is still a struggle with the applicability of
XAI. Among other reasons, the main reason is the variety
and level of explanations needed for different stakeholders.
For instance, in case of an autonomous car, the same AI
model might need different level of explanation for end-users,
developers, designers, and so on. The adaptation of XAT is, by
far, one of the biggest challenges that needs further research.
Domain adaptation might be a good starting point to make the
XAI applicable as generally as possible.

3) Security of XAI: The adversarial learning is gaining
ground and its effects on XAI are still under-explored. The
effect of adversarial machine learning is unquestionably an
important topic of research, covering both attacks on ML and
DL models, and the use of ML and DL models in adversarial
settings [39], [40]. The adversarial ML and DL models (or
the use of ML and DL in adversarial settings) in XAI will be
an interesting topic to investigate. Perturbations in input data
to learning models, bias, and fairness are the key enablers for
the security of AI models. These features will be of paramount
importance in XAI and must be investigated.

4) XAI and Responsible Al: Although Responsible AI
(RAD embodies the explainability, interpretability, and trans-
parency, these components must be put into perspective. In
other words, the aforementioned features of AI are currently
researched under the umbrella of XAI. But their real-world
applications should be investigated as RAI where the effect
of XAI on usability of AI models is subject to further
investigation. For instance, among the research community,
the rationale for XAI is still controversial. Furthermore, apart
from technical perspective, the social and legal dimensions
are also important at par, to realize the XAI in real-world
scenario, which is what RAI covers. Therefore, there are a lot
of research opportunities in this direction.
5) Performance of XAI: Although XAI adds the necessary
features to the baseline AI models, it is also very important
to assess the performance of AI models with the additional
features related to explanation. It is imperative to think that
the new features will likely incur overhead and may affect the
accuracy. To date, some works in the literature have focused on
the performance of XAI [7], [41], [42]. Further investigation
is needed in this direction. Also, the effect of XAI on the
original model (whether good or bad) also warrants further
in-depth investigation to open new avenues of research.

REFERENCES

[1] R. Hussain and 8. Zeadally, “Autonomous cars: Research results, issues,
and future challenges,” EEE Communications Surveys Tutorials, vol. 21,
no. 2, pp. 1275-1313, Secondquarter 2019.

[2] S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, “A survey
of deep learning techniques for autonomous driving,” Journal of Field
Robotics, vol. 37, no. 3, pp. 362-386, 2020. [Online]. Available:
https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21918

[3] Y. Ma, Z. Wang, H. Yang, and L. Yang, “Artificial intelligence applica-
tions in the development of autonomous vehicles: a survey,” IEEE/CAA
Journal of Automatica Sinica, vol. 7, no. 2, pp. 315-329, 2020.

[4] A. Barredo Arrieta, N. Dfaz-Rodriguez, J. Del Ser, A. Bennetot,
8. Tabik, A. Barbado, S. Garcia, S$. Gil-Lopez, D. Molina, R. Benjamins,
R. Chatila, and F. Herrera, “Explainable artificial intelligence (xai):
Concepts, taxonomies, opportunities and challenges toward responsible
ai,” Information Fusion, vol. 58, pp. 82 — 115, 2020. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S 15662535 19308103

[5] Y. Xie, X. A. Chen, and G. Gao, “Outlining the design space
of explainable intelligent systems for medical diagnosis,” in Joint
Proceedings of the ACM IUI 2019 Workshops co-located with
the 24th ACM Conference on Intelligent User Interfaces (ACM
IUI 2019), Los Angeles, USA, March 20, 2019, ser. CEUR
Workshop Proceedings, C. Trattner, D. Parra, and N. Riche,
Eds., vol. 2327. CEUR-WS.org, 2019. [Online]. Available: http:
/ceur-ws.org/Vol-2327/1U119WS-ExSS2019- 18.pdf

[6] E. Tjoa and C. Guan, “A survey on explainable artificial intelligence

(xai): Toward medical xai,” IEEE Transactions on Neural Networks and

Learning Systems, pp. 1-21, 2020.

A. Das and P. Rad, “Opportunities and challenges in explainable artificial

intelligence (xai): A survey,” 2020.

[8] A. Adadi and M. Berrada, “Peeking inside the black-box: A survey on
explainable artificial intelligence (xai),” [EEE Access, vol. 6, pp. 52 138-
52 160, 2018.

[9] A. Samih, A. Adadi, and M. Berrada, “Towards a knowledge
based explainable recommender systems,” in Proceedings of the
4th International Conference on Big Data and Internet of Things,
ser. BDIoT’19. New York, NY, USA: Association for Computing
Machinery, 2019. [Online]. Available: https://doi.org/10.1145/3372938.
3372959

[10] W. Samek and K.-R. Miiller, Towards Explainable Artificial Intelligence.
Cham: Springer International Publishing, 2019, pp. 5-22. [Online].
Available: https://doi.org/10.1007/978-3-030-28954-6_1

[11] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning
in computer vision: A survey,” [EEE Access, vol. 6, pp. 14410-14430,
2018.

[12] X. Ma, Y. Niu, L. Gu, Y. Wang, Y. Zhao, J. Bailey, and F Lu,
“Understanding adversarial attacks on deep learning based medical
image analysis systems,” Pattern Recognition, vol. 110, p. 107332,
2021. [Online]. Available: http://www.sciencedirect.com/science/article/
pii/S003 1320320301357

[13] K. Ren, T. Zheng, Z. Qin, and X. Liu, “‘Adversarial attacks and
defenses in deep learning,” Engineering, vol. 6, no. 3, pp. 346 — 360,
2020. [Online]. Available: http://www.sciencedirect.com/science/article/
pii/S209580991930503X

[14] O. Biran and C. Cotton, “Explanation and justification in machine
learning: A survey,” in International Joint Conferences on Artificial
Intelligence, August 2017.

[15] H. Hagras, “Toward human-understandable, explainable ai,” Computer,
vol. 51, no. 9, pp. 28-36, Sep. 2018.

[16] D. Doran, 8. Schulz, and T. R. Besold, “What does explainable ai really
mean? a new conceptualization of perspectives,” arXiv:1710.00794,
2017.

[7

117)

[18]

[19]

[20]

[21]

[22]

[23]
[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

M. Ribeiro, S. Singh, and C. Guestrin, “Why should i trust you?’:
Explaining the predictions of any classifier,” ACM SIGKDD Int’! Conf.
Knowledge Discovery and Data Mining (KDD 16), 2016.

R. et al., “Interpretable to whom? a role-based model for analyzing in-
terpretable machine learning systems,” https://arxiv.org/abs/1806.07552,
2018.

A. D. Preece, D. Harborne, D. Braines, R. Tomsett, and S. Chakraborty,
“Stakeholders in explainable AI,” CoRR, vol. abs/1810.00184, 2018.
[Online]. Available: http://arxiv.org/abs/1810.00184

P. et al., “A grounded interaction protocol for explainable artificial
intelligence,” Attps:/arxiv.org/pdf/1903.02409.pdf, pp. 1-9, 2019.

K. W. et al, “’do you trust me?”: Increasing user-trust by integrating
virtual agents in explainable ai interaction design,” in ACM Digital
Library, July 2019, pp. 1-3.

C. T. Wolf, “Explainability scenarios: Towards scenario-based xai
design,” in Proceedings of the 24th International Conference on
Intelligent User Interfaces, ser. 1U1’19. New York, NY, USA: ACM,
2019, pp. 252-257. [Online]. Available: http://doi.acm.org/10.1145/
3301275.3302317

L. et al., “Questioning the ai: Informing design practices for explainable
ai user experiences,” CHI, ACM, 2020.

Q. et al., “Interpretable cnns for object classification,” EEE Transaction
on Pattern Analysis and Machine Intelligence, 2020.

D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura,
“Navigating occluded intersections with autonomous vehicles using deep
reinforcement learning,” in 20/8 IEEE International Conference on
Robotics and Automation (ICRA), May 2018, pp. 2034-2039.

C. You, J. Lu, D. Filev, and P. Tsiotras, “Advanced planning for
autonomous vehicles using reinforcement learning and deep inverse
reinforcement learning,” Robotics and Autonomous Systems, vol. 114,
pp. 1 — 18, 2019. [Online]. Available: http://www.sciencedirect.com/
science/article/pii/S0921 889018302021

W. Xia, H. Li, and B. Li, “A control strategy of autonomous vehicles
based on deep reinforcement learning,” in 2016 9th International Sym-
posium on Computational Intelligence and Design (ISCID), vol. 2, 2016,
pp. 198-201.

Y. Shen, S. Jiang, Y. Chen, E. Yang, X. Jin, Y. Fan, and K. D. Campbell,
“To explain or not to explain: A study on the necessity of explanations
for autonomous vehicles,” 2020.

T. Wu and X. Song, “Towards interpretable object detection by unfold-
ing latent structures,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), October 2019.

E. Soares, P. Angelov, D. Filev, B. Costa, M. Castro, and S. Nageshrao,
“Explainable density-based approach for self-driving actions classifica-
tion,” in 2019 18th IEEE International Conference On Machine Learning
And Applications (ICMLA), 2019, pp. 469-474.

J. Kim and J. Canny, “Interpretable learning for self-driving cars by
visualizing causal attention,” in Proceedings of the IEEE International
Conference on Computer Vision (ICCV), Oct 2017.

K. Xu, J. L. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinovy, R. S.
Zemel, and Y. Bengio, “Show, attend and tell: Neural image caption gen-
eration with visual attention,” in Proceedings of the 32nd International
Conference on International Conference on Machine Learning - Volume
37, ser. ICML’15. JMLR.org, 2015, p. 2048-2057.

C. Zhang, B. Shang, P. Wei, L. Li, Y. Liu, and N. Zheng, “Building
explainable ai evaluation for autonomous perception,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) Workshops, June 2019.

S. Yang, W. Wang, C. Liu, and W. Deng, “Scene understanding in deep
learning-based end-to-end controllers for autonomous vehicles,” [EEE
Transactions on Systems, Man, and Cybernetics: Systems, vol. 49, no. 1,
pp. 53-63, 2019.

W. Zeng, W. Luo, S. Suo, A. Sadat, B. Yang, $8. Casas, and R. Urtasun,
“End-to-end interpretable neural motion planner,” in 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2019,
pp. 8652-8661.

A. Sadat, S. Casas, M. Ren, X. Wu, P. Dhawan, and R. Urtasun,
“Perceive, predict, and plan: Safe motion planning through interpretable
semantic representations,” 2020.

Y. Xu, X. Yang, L. Gong, H. Lin, T. Wu, Y. Li, and N. Vasconcelos,
“Explainable object-induced action decision for autonomous vehicles,”
in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2020, pp. 9520-9529.

L. He, A. Nabil, and B. Song, “Explainable deep reinforcement learning
for uav autonomous navigation,” 2020.
[39]

[40]

[41]

[42]

N. Papernot, P. McDaniel, A. Sinha, and M. P. Wellman, “Sok: Security
and privacy in machine learning,” in 20/8 IEEE European Symposium
on Security and Privacy (EuroS P), 2018, pp. 399-414.

X. Wang, J. Li, X. Kuang, Y. an Tan, and J. Li, “The security of machine
learning in an adversarial setting: A survey,” Journal of Parallel and
Distributed Computing, vol. 130, pp. 12 — 23, 2019. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S074373 1518309183

J. Tritscher, M. Ring, D. Schlr, L. Hettinger, and A. Hotho, “Eval-
uation of post-hoc xai approaches through synthetic tabular data,” in
Foundations of Intelligent Systems, D. Helic, G. Leitner, M. Stettinger,
A. Felfernig, and Z. W. Ras, Eds. Cham: Springer International
Publishing, 2020, pp. 422-430.

Y.-S. Lin, W.-C. Lee, and Z. B. Celik, “What do you see? evaluation
of explainable artificial intelligence (xai) interpretability through neural
backdoors,” 2020.
Explainable Artificial | ntelligence ( XAI )

AA

= ARTIFICIAL ee

 

 

 

_= 7 FY18 FY19 FY20 FY21

David Gunning
DARPA/120
Program Update November 2017

 

Approved for public release: distribution unlimited.
Need for Explainable Al

 

 

 

http://listverse. coms
© 2007-201,7.Listverse Ltd

 

 

 

 

 

http://explainthatstuff.com

 

¢ We are entering a new
age of Al applications

¢ Machine learning is the . Sn

core technology P ~ Security *

 

 

 

 

¢ Machine learning models

 

are opaque, non-intuitive,
and difficult for people to
understand

Mttp://blog:soliant.com/
©2004-2017 Sollanishealth

Medicine

 

 

Military

 

 

EXPLAINABLE ARTIFICIAL INTELLIGENCE

   

 

¢ Why did you do that?
¢ Why not something else?
¢ When do you succeed?

¢ When do you fail?

¢ When can | trust you?

¢ How do | correct an error?

¢ The current generation of Al systems offer tremendous benefits, but their effectiveness will be limited by
the machine’s inability to explain its decisions and actions to users

¢ Explainable Al will be essential if users are to understand, appropriately trust, and effectively manage

this incoming generation of artificially intelligent partners

Approved for public release: distribution unlimited.
XAI In the News AAD

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Ehe New York Eimes Magazine

Can A.l. Be

. Taught to
Explain Itself?
Cliff Kuang
November 21, 2017

MIT THE WALL STREET IOURNL. | ngide DARPA’s Push to
Technology Make Artificial Intelligence

Review Explain Itself
The Dark Secret at Sara Castellanos and Steven

Norton
the Heart of Al
Will Knight August 10, 2017

April 11, 2017
The& Register’ f
Work

FI You better explain
Richard Waters

yourself, mister:
July 11, 2017 INANCIA

   

Intelligent Machines
Are Asked to Explain
How Their Minds

e es
ExecutiveBiz
Charles River Analytics-Led
Team Gets DARPA Contract to §
Support Artificial Intelligence
Program (i
Ramona Adams ®

DARPA's mission to __
make an accountable Al
Dan Robinson

September 29, 2017

      

- Team investigates artificial i June 13, 2017
KSC intelligence, machine learning | Itary
Elon Musk and Mark ~~. in DARPA project FMBENDED SYSTEMS EASTGOMPAN
; Lisa Daigle
Zuckerberg Are Arguing June 14, oe Why The Military Nt

 
  

About Al But They're Corporate America Want Jp
Both Missing the Point To Make Al Explain x=

=
Artur Kiulian = in | Ghosts in the Machine ltself
Christina Couch
July 28, 2017 INOVA\) _— sit u Steven Melendez “™

October 25, 2017
June 22, 2017

Zi COMPUTERWORLD— screxrtiic

How Al detectives are cracking open

  

DARPA’s XAl seeks Oracle quietly "ec \ AMERICAN. the black box of deep learning
explanations from Ex Iainable AD q Demystifying the 5 Paul voosen
autonomous systems P George Nott s Black Box That Is Al Science duly 6, 20

Geotf Fein
November 16, 2017

Ariel Bleicher &
August 9, 2017

RVAAAS

May 5, 2017

  

Approved for public release: distribution unlimited. 3
Deep Learning Neural Networks
Architecture and How They Work EXPLAINABLE ARTIFICIAL INTELLIGENCE

Wan

  

 

 

Deep Learning Neural Network

@ Input Layer

 

@ Hidden Layer

 

 

 

   

Input
(unlabeled image)
Low-level
features to Neurons . , . =
high-level respond to © @® _ ® ») 1s Layer
features

simple shapes

 

Neurons respond
to more complex 2
structures

Neurons canna SS
to highly complex.) (5) @ nth Layer

abstract concepts \
Automatic algorithm
(feature extraction and classification) Cx
i

https://www.xenonstack.com/ . http://fortune.com/
XenonStack © 10% WOLF 90% DOG © 2018 Time Inc.

@ Output Layer

 

 

 

 

Approved for public release: distribution unlimited. 4
What Are We Trying To Do?

 

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Today

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

ZEEE? -o ¢ Why did you do that?
SEeaSee * Why not something else?
aeeeee Learning |, This is a cat * When do you succeed?
(SAN a Process (p = 93) . nen do vou fai? ,
HBsHaeer . en can | trust you”
EDRs * How do | correct an error?
Training Learned Output User with
Data Function a Task
¢ | understand why
se This is a cat: p * | understand why not
New i 6 { fhe «| *Ithas fur, whiskers, * | know when you'll succeed
Learning P and claws. * | know when you'll fail
bi F {: th elt has this feature: y
Process ARR i. i - * | know when to trust you
LLL : ¢ | know why you erred
Training Explainable Explanation User with
Data Model Interface a Task

 

Approved for public release: distribution unlimited.
 

 

 

Challenge Problems

 

 

 

 

 

Learna Explain Use the
model decisions explanation
Data
Analytics leur | inrace
Classification

Learning Task

Multimedia Data

 

 

 

 

Classifies items of
interest in large data set

Explains why/why not
for recommended items

Analyst decides which
items to report, pursue

 

Autonomy

Reinforcement
Learning Task

el OArdiPKatorg
ArduPilot & SITL Simulation

 

 

Explainable
Model

Explanation
Interface

 

 

 

 

 

 

 

 

Learns decision policies
for simulated missions

Explains behavior in an
after-action review

Operator decides which
future tasks to delegate

 

 

Approved for public release: distribution unlimited.

EXPLAINABLE ARTIFICIAL INTELLIGENCE

An analyst is
looking for items of
interest in massive
multimedia data
sets

An operator is
directing
autonomous
systems to
accomplish a
series of missions
Wty Goal: Performance and Explainability

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

¢ XAI will create a suite of machine learning techniques that

¢ Produce more explainable models, while maintaining a high level
of learning performance (e.g., prediction accuracy)

¢ Enable human users to understand, appropriately trust, and
effectively manage the emerging generation of artificially intelligent
partners

Performance vs. Explainability
A

S

c oO @

cS ° @ Tomorrow
O @

© Today

® @

oO. O

oD

< f

Cc

Soe

o

o

— >

 

 

Explainability (notional)

Approved for public release: distribution unlimited. 7
aN Measuring Explanation Effectiveness

 

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Measure of Explanation

Effectiveness

 

User Satisfaction

 

 

Explanation Framework

¢ Clarity of the explanation (user rating)
Task ¢ Utility of the explanation (user rating)
Mental Model

¢ Understanding individual decisions
¢ Understanding the overall model
* Strength/weakness assessment

 

 

Recommendation,
Decision or
Action

 
  
  

 

 

 

 

 

 

 

 

expeainable | Exclanation Decision - ‘What will it do’ prediction
The user * ‘How do | intervene’ prediction

. makes a
XAI System Explanation decision Task Performance
The system takes The system provides based on the * Does the explanation improve the
input from the current = an explanation to the explanation user’s decision, task performance?
task and makes a user that justifies its * Artificial decision tasks introduced to
recommendation, recommendation, diagnose the user’s understanding
decision, or action decision, or action

 

 

 

 

Trust Assessment

 

¢ Appropriate future use and trust

 

Correctability (Extra Credit)

 

¢ Identifying errors
* Correcting errors
* Continuous training

 

 

 

Approved for public release: distribution unlimited.
Yt Performance vs. Explainability

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

  
  
  

  
   
  

   

Learning Techniques (today) Explainability
(notional)
A
Graphical
Models == te
rf Ensemble 20

   
   
 
    

Bayesian
Belief Nets

\

Statistical
Models

 

 

Explainability

Approved for public release: distribution unlimited. 9
Wt Performance vs. Explainability

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

New
Approach

Learning Techniques (today)

  
  
   
 
  

Graphical
Models

Create a suite of
machine learning
techniques that
produce more
explainable models,
while maintaining a
high level of
learning
performance

 

; Ensemble
Bayesian \

Belief Nets

  
   

Explainability
(notional)
4
r@® @ ®@
ES ePpmme Ee ig
r7O O

 

 

 

 

 

sheoagke
bhkk Prep

eo Ee

 

 

 

 

 

 

 

 

Deep Explanation

Modified deep learning
techniques to learn
explainable features

 

 

interpretable Models

Techniques to learn more
structured, interpretable,
causal models

 

 

 

 

    

Model
pes rN

Experiment

 

 

 

 

 

 

 

 

 

Model Induction

Techniques to infer an
explainable model from any
model as a black box

 

 

Approved for public release: distribution unlimited.

10
 

XAI Concept and Technical Approaches DUAL

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

cae aa 4

Fehon cae SLI

 

   

 

New
Learning
Process

  
 
 

 

 

 

Training Data
UC Berkeley

Charles River Analytics
UCLA

Oregon State
PARC

CMU

SRI International
Raytheon BBN
UT Dallas

Texas A&M

Rutgers

Deep Learning

 

Reflexive and Rational

 

Explanation

Fs or Hist

 

Explanation User
Quality Performance
User Satisfaction
User's Mental Better
Model Performance
User
Comprehension
Appropriate r
L_y Trust [PH rvercriat Use]

   
  
    
 

 

 

  

 

 

 

Causal Modeling

Narrative Generation

 

Pattern Theory+

3-Level Explanation

 

Adaptive Programs

Acceptance Testing

 

Cognitive Modeling

Interactive Training

 

Explainable RL (XRL)

XRL Interaction

 

Deep Learning

Show and Tell Explanations

 

Deep Learning

Argumentation and Pedagogy

 

Probabilistic Logic

Decision Diagrams

 

Mimic Learning

Interactive Visualization

 

 

Model Induction

 

Bayesian Teaching

 

 

Approved for public release: distribution unlimited.

IHMC
Psychological Model of
Explanation

11

 
Approaches to Deep Explanation DUAL
(Berkeley, SRI, Raytheon BBN, OSU, CRA, PARC) EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

 

 

 

Attention Mechanisms Modular Networks
(Top-down Caption Saliency > (Neural module networks >)
[Ramanishka et al. CVPR17] [Andreas et al.CVPR16,EMNLP16] [Hu et al. CVPR17]

Caption: A man in a jacket is standing at the slot machine
Q: Can you park here?

man jacket
NO | Prediction
, Neural o
oo module [| at
network », Attention visualization
standing slot machine Me
i) a Decision path

Feature Identification Learn to Explain

  
 

  

 

 

 

 

 

 

 

 
  

) (— Downy Woodpecker Definition: >

This bird has a white breast, black
wings, and a red spot on its head.

| RNN

Generator

 

 

 

    

Image Explanation:

This is a Downy Woodpecker because it
is a black and white bird with a red

Tagger | Seer) \ spot on its crown. yy,

Approved for public release: distribution unlimited. 12

Target network

 

 

 

 

 

 

 

 

 

 

rr
Cap Network Dissection Quantifying Interpretability of DUAL

Dee p Re p r ese n t at O n S ( M | T) EXPLAINABLE ARTIFICIAL INTELLIGENCE

Buildings Furniture
18) billard table

    

   
 

56) building
a Audit trail: for a particular output unit, the
drawing shows the most strongly activated

120) arcade 155) bookcase path
ae ddan, i p 7TYr
Pr ys LITT ” TT nue2cee waquaqquesser SRS eS Pease eee

 

      
 

     
  

   

8) bridge 116) bed LA
| | ge
123), building 38) cabinet

 

 

ee Pd
i |
Indoor objects Interpretation of

 

182) food several units in
pool5 of AlexNet
trained for place
recognition

 

 

c screen

53) staircase

 

 

Approved for public release: distribution unlimited. 13
1 Causal Model Induction (CRA)

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

 

 

     
     

ML Technique

oe Explanatory Concepts
Parameterization

Causal Model Template

Test
Data

 

TRAINING

 

 

Causal Model Induction: Experiment with the learned model (as a grey box) to
learn an explainable, causal, probabilistic programming model

Approved for public release: distribution unlimited. 14
Explanation by Selection of Teaching Examples
( Ru t Q e rs) EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

 

 

 

 

 

 

TRAINING DATA (2 2] a
@AGg2aa| ||o Ale" ,| [ae

brow mouth This face is Angry
lowered nostrils raised
a flared =
aed \@ é because it is similar to these

|@| examples
@ |aBe} “on, is
lips cheekbones 4
thinned/ raised ¢ Pe

pushed EYPLAINABLE

out

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

CLASSI FI CATI ON MODEL and dissimilar to these
c 3) (any
s = 2s

 

 

 

BAYESI AN TEACHING for optimal selection of examples for machine explanation

Approved for public release: distribution unlimited. 15
 

Common Ground Learning and
Explanation (COGLE)
An interactive sensemaking system to explain
the learned performance capabilities of a UAS
flying in an ArduPilot simulation testbed

Wey Autonomy (PARC, OSU)

 

Pp ~ {
: ay

EXPLANATION LAYER

 

  

7 NK oe 5 -
& Re ie \ ‘Explanations for mission
< mance Rre LC I [| performance and for assessing
Interactions b ; .
) ge / skills, risks & coverage.
COGNITIVE LAYER
Cast learned abstractions,
policies & clusters into
/ oe explainable form.
COGLE LEARNING LAYER
| . ZN. Learn policies from the
Common Ground Builder
i . x 7 sensed world.
* Explain ee te
* Train TEST BED LAYER
° Evaluate!
Series 1. Primitives: Navigating with Constraints and Lookahead ........:ssscsese 7

Lesson 1.1: Taking off.
Lesson 1.2: Taking off and Landing.
Lesson 1.3: Reconnaissance Over a Point (3 Months) ......ss:sesn
Lesson 1.4: Looking Ahead to Avoid Crashing into Mountains...
Lesson 1.5: Choosing a Safe Descent Approach fo for Landing
Lesson 1.6: Provisioning a Hiker (6 months) ....

 

 

 

 
 
   
   
 

Series 2. Behaviors: Managing Competing Goals and Foraging...
Lesson 2.1: Provisioning a Hiker in a Box Canyon (opt)...
Lesson 2.2: Taking an Inventory of a Region and Refueling (opt).
Lesson 2.3: Foraging Around a Point for a Hiker (opt)..... asa
Lesson 2.4: Foraging Around a Point with an Interfering Obsta

Series 3. Missions: Harder Missions and Heavy Testing.....
Lesson 3.1: Double Hiker Jeopardy (9 MONtHS) .....sscsscsseseeeen
Lesson 3.2: Bear on the Runway
Lesson 3.3: Auto-Generated Missions with Testing (12 MOMths) ...scssssssssseneeneed 2

 

 

Robotics Curriculum

Deep Adaptive Program

KUAD

EXPLAINABLE ARTIFICIAL INTELLIGENCE

Explanation-I nformed Acceptance Testing
of Deep Adaptive Programs (xACT)
Tools for explaining deep adaptive programs
and discovering best principles for
designing explanation user interfaces

xFSM

 

 

 

Decision Net

= Explanation
Learner

 

 

AdaptiveChoice
strategyChoice(); Saliency
vo Visualizer

 
       
   

Interactive
Naming

 

 

Interface

    
 

Annotation Aware i Visual
Reinforcement Seg Words
Learning :

Game Engine

Approved for public release: distribution unlimited. 16
GD Four Modes of Explanation (Raytheon BBN) UAL

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Analytic (didactic) statements Visualizations
in natural language that describe that directly highlight portions of the
the elements and context that raw data that support a choice and
support a choice allow viewers to form their own

perceptual understanding

Explanation
Modes

 

 

Cases Rejections of alternative choices

that invoke specific examples or (or “common misconceptions” in

stories that support the choice pedagogy) that argue against

less preferred answers based on
analytics, cases, and data

‘ei : J

 

 

 
Way XA Program Structure

 

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Challenge
Problem

Multimedia Data

Autonomy
ArduPilot &
SITL Simulation

TA1:
Explainable
Learners

Teams that provide
prototype systems
with both components
¢ Explainable
Model
¢ Explanation
Interface

2 fics bs
fltk fie

Abb Be bh

 

Experiment

 

Deep
Learning
Teams

Interpretable
Model
Teams

Model
Induction
Teams

Evaluator
Naval Research Laboratory

TA1: Explainable Learners

¢ Multiple TA1 teams will develop prototype explainable learning systems that
include both an explainable model and an explanation interface

TA2: Psychological Model of Explanation

TA2:
Psychological
Model of
Explanation

* Psychological
Theory of
Explanation

* Computational
Model Consulting

Evaluation
Framework

Learning
Performance

Explanation
Effectiveness

Explanation
Measures

¢ User Satisfaction
¢« Mental Model

¢ Task Performance
¢ Trust Assessment
* Correctability

 

¢ At least one TA2 team will summarize current psychological theories of explanation
and develop a computational model of explanation from those theories

Approved for public release: distribution unlimited.

18
GoD Challenge Problem Candidates DUAL

EXPLAINABLE ARTIFICIAL INTELLIGENCE

Analytics Autonomy

Visual Question Answering

Strategy Games

L

STE Lae e182 =e alae ks

 

Activity Recognition Vehicle Control

 

 

Jed SECON Tider Ay
ee || ae 3

 

S meee lil
tit WME PE
laa |i

Household activites § + co =< neon caning = 5 Cleaning windows

hada aaa NC aT Lae

ass eS a 7 7
Yue
Approved for public release: distribution unlimited.

  

 

 

 

 

 

   

 

 

 

19
 

A

i Psychological Model of Explanation (IHMC) )xQ/A\U

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

. XAl
Model of the Explanation
UAL Process and Possible Metrics
XAl
EXPLAINABLE ARTIFICIAL INTELLIGENCE

System

 

 

 

   

 

 

      

 

 

 

 

 

 

 

    

  

         

 

 

receives . revises User’s Mental | enables Better
User > ~T
Model Performance
may initially fi “ by is assessed by is assessed by
‘odes Test of Test of Test of
Criteria ar, | Comprehension Performance
can —l— involves
Trust or | gives way to (mma) enables | Appropriate

     

Mistrust

     

 

 

| Tust J st Use
 

DARPA Schedule and Milestones

KUAD

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Evaluator

TA1

TA2

Meetings

 

 

   

 
    

 

 

 

 

   

 

 

 

  
 
    

 
     

 

 

 

 

 

 

 

 

   

   

   

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

2017 2018 2019 2020 2021
APR | MAY] JUN | JUL | AUG] SEP | OCT | NOV] DEC] JAN | FEB | MAR} APR | MAY] JUN | JUL | AUG] SEP | OCT | NOV] DEC] JAN | FEB | MAR] APR | MAY] JUN | JUL | AUG] SEP | OCT | NOV] DEC] JAN | FEB | MAR] APR | MAY] JUN | JUL | AUG] SEP | OCT | NOV] DEC | JAN | FEB | MAR] APR | MAY
PHASE 1: Technology Demonstrations PHASE 2: Comparative Evaluations
. . Prep for Analyze Analyze ‘acl Analyze Results &
Define Evaluation Framework Prep for Eval 2 Prep for Eval 3 .
Eval1 Results Results Accept Toolkits
Refine & Test Explainable Refine & Test Explainable
Develop & Demonstrate Explainable Models a a Deliver Software
. Learners Learners .
(against proposed problems} . . Toolkits
(against common problems) (against common problems)
LT TTT ?ET tT te ttt tt LTT ttt tt LTT tT TTT [ [Tt |
. . . . Deliver
Summarize Current Psychological Develop Computational Model of Refine & Test Computational
Theories of Explanation Explanation Computational Model .
Model
KickOff Progress Report Tech Demos Eval 1 Results Eval 2 Results Final
May 9-11 Nov 6-8 May 7-9

¢ Technical Area 1 (Explainable Learners) Milestones

Demonstrate the explainable learners against problems proposed by the developers (Phase 1)
Demonstrate the explainable learners against common problems (Phase 2)
Deliver software libraries and toolkits (at the end of Phase 2)

Technical Area 2 (Psychology of Explanation) Milestones

Deliver an interim report on psychological theories (after 6 months during Phase 1)
Deliver a final report on psychological theories (after 12 months, during Phase 1)
Deliver a computational model of explanation (after 24 months, during Phase 2)
Deliver the computational model software (at the end of Phase 2)

Approved for public release: distribution unlimited. 21
EXPLAINABLE ARTIFICIAL INTELLIGENCE

DARPA XAI Developers (TA1)

 

 

Explainable Model Explanation Interface

UC Berkeley Deep Learning Reflexive and Rational
3 Charles River Causal Modeling Narrative Generation
UCLA Pattern Theory+ 3-level Explanation
= Oregon State Adaptive Programs Acceptance Testing
PARC Cognitive Modeling Interactive Training
2 CMU Explainable RL (XRL) XRL Interaction
SRI International Deep Learning Show and Tell Explanation
6 Raytheon BBN Deep Learning Argumentation and Pedagogy
= UT Dallas Probabilistic Logic Decision Diagrams
< Texas A&M Mimic Learning Interactive Visualization
Rutgers Model Induction Bayesian Teaching

Approved for public release: distribution unlimited. 22
Hata Berkeley/BU/U. Amsterdam/Kitware

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Deeply Explainable Artificial | ntelligence

Explainable Model Explanation Interface Challenge Problem

Deep Learning Reflexive & Rational Autonomy

* Explain /mpiicit (latent) * Reflexive explanations ¢ ArduPilot and OpenAl
nodes by training (that arise directly Gym Simulations
additional DL models from the model)

¢ Explain exp/icit nodes ¢ Rational explanations Data Analytics
thru Neural Module (that come from | © Visual QA and
Networks (NMNs) Nalisle) about user’s Multimedia Event QA

elie

 

PI: Trevor Darrell (Berkeley)

Pieter Abbeel (Berkeley) ¢ Dan Klein (Berkeley) ¢ Anthony Hoogs (Kitware)
Tom Griffiths (Berkeley) ¢ John Canny (Berkeley)

Kate Saenko (BU) ¢ Anca Dragan (Berkeley)

Zeynep Akata (U.

Amsterdam)

Approved for public release: distribution unlimited. 23
EXPLAINABLE ARTIFICIAL INTELLIGENCE

3% CRA/U. Mass/Brown

 

CAMEL: Causal Models to Explain Learning

Explainable Model Explanation Interface Challenge Problem

Model | nduction Narrative Generation Autonomy

Causal Models ¢ Interactive visualization ¢ Minecraft, Starcraft
¢ Experiment with the based on the generation ;
learned model (as a of temporal, spatial Data Analytics

grey box) to learn an narratives from the ¢ Pedestrian Detection
explainable, causal, causal, probabilistic (INRIA), Activity
probabilistic models Recognition

programming model (ActivityNet)

 

PI: Brian Ruttenberg (CRA)

Avi Pfeffer (CRA) James Niehaus (CRA)

David Jensen (U. Mass) ¢ Emilie Roth (Roth Cognitive Engineering)
¢ Michael Littman (Brown) ¢ Joe Gorman(CRA)

¢ James Tittle (CRA)

Approved for public release: distribution unlimited. 24
Haya UCLA/OSU/ Michigan State

Learning and Communicating Explainable Representations for
Analytics and Autonomy

Explainable Model Explanation Interface Challenge Problem

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Pattern Theory+ 3-Level Explanation Autonomy

° Integrated ¢ Integrate 3 levels of * Humanoid robot
representation across explanation: behavior and VR

an entropy spectrum: * Concept compositions simulation platform
* Deep Neural Nets * Causal and Data Analytics
¢ Stochastic And-Or- counterfactual ¢ Understanding

Graphs (AOG) reasoning complex multimedia
¢ Predicate Calculus ¢ Utility explanations events

 

¢ PI: Song-Chun Zhu (UCLA)

¢ Ying Nian Wu (UCLA) ¢ Joyce Chai (Michigan State)
¢ Sinisa Todorovic (OSU)

Approved for public release: distribution unlimited. 25
OSU

xACT: Explanation-| nformed Acceptance Testing of Deep

Explainable Model

Adaptive Programs

* Explainable Deep
Adaptive Programs
(xDAPs) — a new
combination of
Adaptive Programs,
Deep Learning and
explainability

¢ PI: Alan Fern (OSU)

Tom Dietterich (OSU)
Fuxin Li (OSU)

Prasad Tadepalli (OSU)
Weng-Keen Wong (OSU)

Adaptive Programs
Explanation Interface

Acceptance Testing

* Provides a visual & NL
explanation interface
for acceptance testing
by test pilots based on
Information Foraging
Theory

¢ Margaret Burnett (OSU)
¢ Martin Erwig (OSU)
¢ Liang Huang (OSU)

Approved for public release: distribution unlimited.

EXPLAINABLE ARTIFICIAL INTELLIGENCE

Challenge Problem

Autonomy

¢ Real-Time Strategy
Games based on
custom designed game
engine designed to
support explanation

¢ Possible use of
Starcraft

 

26
EXPLAINABLE ARTIFICIAL INTELLIGENCE

17.)  PARC/CMU/U. Edinburgh/U. Mich./West Point

 

COGLE: Common Ground Learning and Explanation

Explainable Model Explanation Interface Challenge Problem

Cognitive Model Interactive Training Autonomy

¢ 3-layer architecture: * Interactive visualization * ArduPilot simulation
¢ Learning Layer of states, actions, environment
(DNNs) policies & values * Value of Explanation
* Cognitive Layer ¢ Includes a module for (VoE) framework for
(ACT-R Cog. Model) test pilots to refine and measuring explanation
* Explanation Layer train the system effectiveness
(HCI)

 

* PI: Mark Stefik (PARC)

¢ Honglak Lee (U. Mich.) * Christian Lebiere (CMU) ¢ Michael Youngblood
¢ Subramanian ¢ John Anderson (CMU) (PARC)
Ramamoorthy (U. ¢ Robert Thomson (USMA)
Edinburgh)

Approved for public release: distribution unlimited. 27
Mi) CMU/Stanford Ne

XRL: Explainable Reinforcement Learning for Al Autonomy

Explainable Model Explanation Interface Challenge Problem

XRL Models XRL Interaction Autonomy

Create a new scientific * Interactive Open Al Gym
discipline for Explainable explanations of Autonomy in the

Reinforcement Learning dynamic systems electrical grid
with work on new ¢ Human-machine Mobile service robots
algorithms and interaction to improve Self-improving
representations performance educational software

 

¢ PI: Geoff Gordon (CMU)

¢ Zico Kolter (CMU) ¢ Manuela Veloso (CMU)
¢ Pradeep Ravikumar (CMU) ¢ Emma Brunskill (Stanford)

Approved for public release: distribution unlimited. 28
Wty SRI/U. Toronto/UCSD/U. Guelph

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

DARE: Deep Attention-based Representations for Explanation

Explainable Model Explanation Interface Challenge Problem

Deep Learning Show-and-Tell Data Analytics

¢ Multiple deep learning Explanations Visual Question
techniques: ¢ DNN visualization Answering (VQA) using
¢ Attention-based * Query evidence that Visual Gnome, Flickr30
mechanisms explains DNN decisions MovieQA
* Compositional NMNs ¢ Generate natural
° GANs language justifications

 

PIs: Giedrius Burachas (SRI), Mohamed Amer (SRI)

Shalini Ghosh (SRI) Richard R. Zemel (U. Toronto) ¢ Jurgen Schulze (UCSD)
Avi Ziskind (SRI) Sanja Fidler (U. Toronto)
Michael Wessel (SRI) °¢ David Duvenaud (U. Toronto)

¢ Graham Taylor (U. Guelph)

Approved for public release: distribution unlimited. 29
WEN) Raytheon BBN/GA Tech /UT Austin/MIT

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

EQUAS: Explainable QUestion Answering System

Explainable Model Explanation Interface Challenge Problem

Deep Learning Argumentation Data Analytics

¢ Semantic labelling of Theory ¢ Visual Question
DNN neurons ¢ Comprehensive Answering (VQA),

¢ DNN audit trail strategy based on beginning with images
construction argumentation theory and progressing to

° Gradient-weighted ¢ NL generation video
Class Activation ¢ DNN visualization

Mapping

 

¢ PI: William Ferguson (Raytheon BBN)

¢ Antonio Torralba (MIT) ¢ Devi Parikh (GA Tech)
¢ Ray Mooney (UT Austin) ¢ Dhruv Batra (GA Tech)

Approved for public release: distribution unlimited. 30
NY UT Dallas/UCLA/Texas A&M/IIT-Delhi

Tractable Probabilistic Logic Models: A New, Deep Explainable
Representation

Explainable Model Explanation Interface Challenge Problem

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Probabilistic Logic Probabilistic Data Analytics
Decision Diagrams

¢ Tractable Probabilistic ¢ Infer activities in
Logic Models (TPLMs) * Enables users to multimodal data (video
— an important class of explore and correct and text)
(non-deep learning) the underlying model Using the Wetlab
interpretable models as well as add (biology) and TACoS
background knowledge (cooking) datasets

 

¢ PI: Vibhav Gogate (UT Dallas)

¢ Adnan Darwiche (UCLA) ¢ Eric Ragan (Texas A&M)
¢ Guy Van Den Broeck (UCLA) »¢ Parag Singla (II T-Delhi)
¢ Nicholas Ruozzi (UT Dallas)

Approved for public release: distribution unlimited. 31
DARPA Texas A&M/Wash. State

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Transforming Deep Learning to Harness the I nterpretability of

Shallow Models: An Interactive End-to-End System

Explainable Model Explanation Interface

Mimic Learning Interactive

Develop a mimic Visualization

learning framework * Interactive visualization

that combines deep over multiple views,
learning models for using heat maps & topic

prediction and shallow modeling clusters to
models for show predictive

explanations features

¢ PI: Xia Hu (Texas A&M)

¢ Shuiwang Ji (Wash. State) ¢ Eric Ragan (Texas A&M)

Approved for public release: distribution unlimited.

Challenge Problem

Data Analytics

¢ Multiple tasks using
data from Twitter,
Facebook, ImageNet,
UCI, NIST and Kaggle

¢ Metrics for explanation
effectiveness

 

32
Mate Rutgers AD

Model Explanation by Optimal Selection of Teaching Examples

 

Explainable Model Explanation Interface Challenge Problem

Model I nduction Bayesian Teaching Data Analytics

¢ Select the optimal ¢ Example-based Movie descriptions
training examples to explanation of: Image processing
explain model ¢ the full model Caption data
decisions based on * user-selected sub- Movie events
Bayesian Teaching structure Human motion events
user submitted
examples

 

¢ PI: Patrick Shafto (Rutgers)

¢ Scott Cheng-Hsin Yang (Rutgers)

Approved for public release: distribution unlimited. 33
asta | HMC/MacroCognition/ Michigan Tech

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

Naturalistic Decision Making Foundations of Explainable Al

Literature Review

Naturalistic Theory

¢ Extensive review of
relevant psychological
theories

¢ Extend the theory of
Naturalistic Decision
Making to cover
explanation

Computational Model

Bayesian Framework

¢ Represent reductionist
mental models that
humans develop as
part of the explanatory
process

* Including mental
simulation

¢ PI: Robert R. Hoffman (IHMC)

° Gary Klein
(MacroCognition)

¢ Shane T. Mueller (Michigan
Tech)

¢ William J. Clancey (IHMC)
¢ COL Timothy M. Cullen
(SAASS)

Approved for public release: distribution unlimited.

Model Validation

Experiments

* Conduct interactive
assessment and formal
human experiments

¢ Validate the model

¢ Develop metrics of
explanation
effectiveness

¢ Jordan Litman (IHMC
Psychometrician)

¢ Simon Attfield (Middlesex
University-London)

¢ Peter Pirolli (IHMC)

 

34
1) Naval Research Laboratory

EXPLAINABLE ARTIFICIAL INTELLIGENCE

 

XAI Evaluation

 

 

 

 

Challenge Problems Evaluation Framework Measurement
* Evaluation protocols ®
Analytics * Training environment o|° -
¢ Training data E ° Teena
¢ Simulation 5 | Today -
oO °
environment Oo i
Testing environment =
Autonomy * Subjects ®
* Web infrastructure 7 >
* Baseline systems Explanation Effectiveness

 

 

 

 

PI: David Aha (NRL)

Justin Karneeb (Knexus) ° Mike Pazzani (UC Riverside)
Matt Molineaux (Knexus)
Leslie Smith (NRL)

Approved for public release: distribution unlimited. 35
 

www.darpa.mil

Approved for public release: distribution unlimited.

36
2104.00950v1 [cs.LG] 2 Apr 2021

arXiv

UNDER REVIEW

Explainable Artificial Intelligence (XAI) on Time
Series Data: A Survey

Thomas Rojat, Raphaél Puget, David Filliat, Javier Del Ser, Rodolphe Gelin, and Natalia Diaz-Rodriguez

Abstract—Most of state of the art methods applied on time
series consist of deep learning methods that are too complex to
be interpreted. This lack of interpretability is a major drawback,
as several applications in the real world are critical tasks,
such as the medical field or the autonomous driving field. The
explainability of models applied on time series has not gather
much attention compared to the computer vision or the natural
language processing fields. In this paper, we present an overview
of existing explainable AI (XAI) methods applied on time series
and illustrate the type of explanations they produce. We also
provide a reflection on the impact of these explanation methods
to provide confidence and trust in the AI systems.

Index Terms—Explainable Artificial Intelligence, Deep learn-
ing, Time Series, Convolutional Neural Networks, Recurrent
Neural Networks

I. INTRODUCTION

Time series are ubiquitous in nature, as they can represent
any variable that varies over time. In industry, too, there
are areas such as the medical field where temporal data is
of particular importance. Thus, machine learning applied to
temporal data is of particular importance as it has many
potential applications in various fields. Several tasks can be
performed by the machine learning methods applied to time
series, the main ones being time series classification, time
series forecasting, and time series clustering. To perform these
tasks, deep learning (DL) methods are since several years
state of the art models for problems with time series as
input data. Recurrent methods are adapted to work with time
series thanks to their memory state and their ability to learn
relations through time. Convolutional neural networks (CNNs)
with temporal convolutional layers are able to build temporal
relationships as well, and generate high level features from
raw data. The introduction of those methods to work on time
series enable to increase the accuracy of the models and avoid
the heavy data pre-processing of former methods that could
not directly take as input raw data. However, one of the major
drawback of these methods is the lack of interpretability due

T. Rojat is with U2IS, ENSTA Paris, Institut Polytechnique Paris and Inria
Flowers and Renault, France, France. Email: thomas.rojat@renault.com.

R. Puget is with Renault, France, France. email:
raphael. puget @renault.com.

D. Filliat is with U21S, ENSTA Paris, Institut Polytechnique Paris and Inria
Flowers, France. Email: david.filliat@ensta-paris. fr.

J. Del Ser is with TECNALIA, Basque Research and Technology Alliance
(BRTA), 48160 Derio, Spain and the University of the Basque Country
(UPV/EHU), 48013 Bilbao, Spain- Email: javier.delser@tecnalia.com.

R. Gelin is with Renault France, France.
rodolphe.gelin@renault.com.

N. Diaz-Rodriguez is with U2IS, ENSTA Paris, Institut Polytechnique Paris,
Inria Flowers Team, 828 boulevard des Maréchaux, 91120 Palaiseau, France.
Email: natalia.diaz@ ensta-paris.fr

Email:

to their high complexity. EXplainable Artificial Intelligence
(XAD) is therefore a key concern for time series as most state of
the art methods are not interpretable. This is a major drawback,
as several applications of time series in the real world, such
as the medical field or the autonomous driving field, are of
critical importance and therefore require interpretability.

Although a lot of work has been done on explainability in
the computer vision and natural language processing (NLP)
fields, there is still a lot of work to be done to explain
methods applied on time series. This might be caused by the
unintuitive nature of time series [|], that we can not understand
at first sight. Indeed, when a human being looks at a photo,
or reads a text, he intuitively and instinctively understands
the underlying information contained in the data. Although
temporal data is ubiquitous in nature, through all forms of
sound for example, humans are not used to representing this
temporal data in the form of a signal that varies as a function of
time. This may have an impact on the EXplainable Artificial
Intelligence (XAJI) applied to time series, especially for the
evaluation of explanations, where qualitative assessments may
have less potential than for the domains of computer vision or
natural language explanation.We need some expert knowledge
or additional methods to leverage the underlying information
present in the data. This problem might explain why, as for
the computer vision field, a lot of existing methods focus on
highlighting the parts of the input responsible for a prediction
[2] [].

Motivated by the rationale above, the aim of this overview
is to critically examine the current state of the art related to the
explainability of models learned from time series data. Specif-
ically, the contributions of this survey can be summarized as
follows:

« An overview of the XAI methods applied on time series
through their methodology, scope, and targets.

« An overview of explainable methods that can be used to
increase the confidence, stability and robustness of models.

e An overview of the different approaches to qualitative and
quantitative evaluate explanations provided by XAI methods
applied to time series.

e A discussion on the state of the art of explainability methods
applied on time series, its limitations and potential lines of
research.

The rest of the paper is organized as follows: First we
present some important definitions of the XAI field, and
the concepts of confidence, robustness and trustworthiness in
Section I]. Then, we present an overview of XAI methods
applied on time series through their methodology in II, scope
UNDER REVIEW

in Section [V and targets in Section V. Then, we tackle
the evaluation of the explanations in the Section V1. Finally,
Section \ II discusses the main outcomes of the paper. Figure
| illustrates schematically the overall structure followed in our
survey.

 
 
 
 
   
     
  

XAI METHODS APPLIED
TO TIME SERIES

EVALUATION OF THE
EXPLANATION
{SECTION 6)

Qualitative evaluation

Section
6 (Quantitative evaluation

4

INTRODUCTION Sections
(SECTION

2and 5
= Context
= Motivation

Contributions
XAI TERMINOLOGY

(section 2)
Trustworthiness

+ Robustness/Staaility
Confidence

PURPOSES AND

 

(section 4)

INTENDED AUDIENCE
ECTION 5)

Global explanations

= Trustworthiness through = Local explanations

explanations for every audience
Confidence

Fig. 1: Questions answered throughout the survey and their
connection to the different sections in which it is structured.

Il. XAI TERMINOLOGY AND DEFINITIONS

The explainability methods always have one or more ob-
jectives to achieve through the explanations that generate.
These goals will have their importance in the choice of the
methodology, the scope of the explanations, and the targets.
The following is a list of potential purposes of explainability
methods that are necessary to know for understanding the rest
of the paper, which we relate to each other in the knowledge
graph depicted in Figure 2:

e Explainability: An active characteristic of a model, denot-
ing any action or procedure taken by a model with the intent
of classifying or detailing its internal functions” [4], [5].
Given an audience, an explainable Artificial Intelligence is
one that produces details or reasons to make its functioning
clear or easy to understand” [5].
Interpretability: The passive characteristic of a model
referring to the level at which a given model makes sense for
a human observer [4]. An interpretable system is a system
where a user cannot only see, but also study how inputs are
mathematically mapped to outputs [6].
e Trustworthiness: The "confidence of whether a model will
act as intended when facing a given problem” [4]. Trust
can be achieved when the model can provide "detailed
explanations” of its decisions [6]. A person may be more
confident using a model if he understands it [7].
Interactivity: The interactivity with the user is one of the
goals targeted by an explainable machine learning model?”
[5]. This is specially important in fields where “users are of
great importance”.
Stability: A model is stable if it is not misled by small
perturbations that might occur in the real world, such as
noises due to the source of data itself (e.g. thermal noise in
a sensor).
e Robustness: A model is considered robust if it is able
to withstand disturbances that may have been intentionally
created by humans.

e Reproducibility: A model is reproducible if it repeatedly
obtains similar results when run several times on the same
dataset.

« Confidence: The confidence is the probability of an event
coming true. The goal is to quantify the trust in the decision
[8]. It is defined in [9] "measure of risk as to how sure
users are that they received the correct suggestions by the
Al-based model”. A model with a high confident score on its
predictions should be reproducible as it should get similar
predictions when it is repeatedly run on the same dataset.

     
  

Stabili
Needed by v

     
   
    
   
 
  
   

Robustness Reproducibility
s

©

increases vorifios

Trustworthiness

contributes to

Interactivity
71
go

enriches us

Fig. 2: Knowledge graph relating all the purposes of explain-
ability methods for time series.

The whole purpose of explainability is to explain models
that are too abstract to be interpretable by themselves. The
need for explainability arises in certain practical scenarios
when the task to perform is both too complex to be solved
by a simple interpretable model, and too critical to be solved
by a model that we cannot understand, and therefore cannot
trust.

Providing trustworthiness by explaining the inner behavior
of these complex models can be one way to overcome these
limitations. Most of the methods covered in this survey aim to
provide trustworthiness in the model they explain. The inter-
activity with the final user is often neglected. Many methods
provide insights on model behaviors without considering how
a final user would receive the information provided by their
methods.

The explainability brought by most of the methods pre-
sented in this survey can not assert the stability, robustness,
and confidence in machine learning (ML) models applied on
time series. This is why, to the best of our knowledge, there
still are needs for developing metrics that will guarantee the
right behavior of a model. Indeed there is little interest in
providing an explanation if a small input noise can radically
change the model behavior, making this explanation valid only
for a very specific example.

As shown in Figure 3, in the remainder of this section
we elaborate on how XAI methods for time series analysis
UNDER REVIEW

can contribute to the stability, robustness and confidence of
systems with this particular kind of data at their core, delving
into the contributions reported to date where such purposes
have been targeted.

   

Cis

a>

BP ola)
(SECTION 2.B)

~ Detailed explanations of a decision
- Understanding of a model's inner
mechanisms

- Measure of risk as to how
sure users are that they
received the correct
suggestions by a model

CONFIDENCE
C(O sar)

- Model resistance against
perturbations that may be
created by humans

Fae eh Ss)
(sa lel P24)

- Model resistance against
disturbances that may occur
and that are not intentionally
created by humans

Eas} Bias

Xo)

(SECTION 2.A.1)

Fig. 3: Rationale connecting the contents of Section I.

A. Stability, robustness and confidence of systems

It should be necessary for the certification of Artificial
Intelligence (AI) systems to carefully assess the risks of the
impacts of the Artificial Intelligence system on its environment
[10]. Let us take the example of an automated car. The risks
involved are the road users that can be injured or killed if
the automated system takes one wrong decision. The task to
perform is critical and the risks associated are high.

For a classification task, the metric always used to certify
the quality of an AI system is the accuracy. It gives us the
percentage of the examples that have been correctly classified,
which is a satisfying insight to assess the quality of an AI
system in most cases. However, some aspects are not covered
by the accuracy. For instance, it cannot certify that the model
output does not change when a small perturbation is applied on
the input or the model, or when a noise is added in the input.
Therefore, a high accuracy does not ensure the good behavior
of the system, because such situations might occur in the real
world. When the task to perform is critical, additional metrics
are needed because we need to be sure that the AI system
behaves well in every situation.

The study of the stability, robustness, and confidence of AI
system might be a way to tackle this limitations.

1) Stability: As stated above, a model is stable if its output
does not change when a small perturbation is applied on the
input or on the model. These perturbations can occur in the
real world.

Let us take again the example of an automated car. Let us
imagine that the car arrives in front of a stop sign. This stop
sign is unusual because it has a white sticker sticked on its red
part [11]. Eykholt et al. [11] show in their experiments that
all the model deployed in the autonomous car missclassified
these modified stop signs. However, it is crucial that the model

identifies it correctly, because otherwise, it could lead to an
accident. When facing this perturbation in the input sample,
a stable model should be able to correctly classify the sample
as a stop sign. If not, it should at least be able to warn the
system that it is not certain of this prediction, as the situation
encountered is unusual.

2) Robustness: A robust model is defined as a model that
can withstand adversarial attacks. Morgulis et al. [12] conduct
an experiment in which they perform adversarial attacks on
traffic sign images by adding some imperceptible noise into
the image such that human eyes cannot differentiate the real
image and the modified one. They show that with the right
amount of noise, the model deployed in the autonomous car
is not able to identify correctly the objects in the modified
images. The models that are fooled by these attacks cannot be
labeled as robust. This might be a major security concern as
some hackers can endanger the drivers life on the road simply
by conducting these attacks on the input samples representing
every type of traffic signs.

Another type of intentional perturbation of the inputs in
order to modify the output of a given model are counter-
factuals. Counterfactuals are defined in [13] as “the smallest
change to the feature values that changes the prediction
to a predefined output”. While perturbations generated by
adversarial attacks are undetectable by humans, counterfactual
perturbations are plausible and realistic because the modified
samples are contained in the underlying distribution of data
that can be encountered in the real world.

3) Confidence: When an AI system encounters attacks and
perturbations similar as those describe in Sections [/-Al and
I|-A2, we cannot have the guarantee that the model will remain
stable and robust. There always might be a perturbation, or
a noise that can mislead the system, and lead to a potential
accident in the case of the automated car.

Therefore, training the system to handle noises and pertur-
bations might never be enough to guarantee the robustness
and the stability of a system. However, what a model can
potentially do is assess how unusual is a prediction vector
compared to other points in the validation dataset [14]. It could
identify examples that are far from the input data distribution,
and thus identify samples that the model cannot classify with
confidence. Thus, to every prediction could be associated a
score relating how confident is the model in its decision. We
could define a threshold, so that if the model confidence score
is below this threshold, the model does not know and is not
able to take a decision.

Generally, uncertainty can be classified into two categories,
aleatoric uncertainty and epistemic uncertainty. Aleatoric un-
certainty is caused by the hazards that can occur when
performing the same experiment several times and that can
therefore change its result. Epistemic uncertainty is of different
nature, it is due to limited data and knowledge. It occurs
when a model encounters an example far from the distribution
of input data, or when a model has difficulty extrapolating
between the different examples in the learning base and thus
generalizing.

We will focus on epistemic uncertainty in this paper, and
we will see in Section V-B how the explainability provided
UNDER REVIEW

by XAI methods can be used to increase the robustness and
the stability of a model by reducing epistemic uncertainty, and
thus provide confidence in its outcomes.

B. User Trust

Even if the explainability methods can bring confidence,
explanations naturally bring trustworthiness thanks to the
information they provide to better understand the model and
its predictions. While confidence and robustness may be
approached with technical considerations, the trust of a user
can also be built by other means than objective metrics. The
trust of a user in an AI system can be both defined subjectively
and objectively [7]. Interactions between the user and the
AI system might be a key point to build trust between the
automated system and the user [15]. Without trust, users will
not rely in automated systems, especially to conduct critical
tasks. In automated driving for instance, AI systems suffer
from the lack of trust of the users in the vehicle’s autonomy
[16]. Such systems might suffer from a lack of feedback
[17] explaining for example why a specific action has been
conducted by the automated system. These interactions are
specifically important in semi-automated systems [17].

The interactions are particularly important when the AT is
in opposition to a user or a business stakeholder [18]. The user
might try to find why he disagrees with the AI prediction. He
might therefore either find an example of failure of the system,
or learn from its outcomes. This might not be achieved without
proper feedback and interaction between the AI system and
the user [| 5]. Without it, it is rather unlikely that the driver
handles the uncertainty and risk associated with giving driving
control to the vehicle’s autonomy [1°].

One important concern is how to provide efficient interac-
tion and feedback to the user [| 7]. Providing the explanation
before rather than after the action is likely to lead to greater
trust in the autonomous system [15]. Explaining why and how
an action will be conducted might be appropriate to explain to
users the action that is going to be conducted by the system
[17]. In semi-automated driving systems, providing drivers
with an option to decide if the automated car will perform the
action or not should lead to more trust beyond just providing
an explanation [15]. The feedback could also be from the
human to the system, by giving the possibility to edit the
system [20] whenever the human identifies a failure. This
would be an example of collaborative exploration that leads
to improvement of both the user and the system.

All the studies around the interactions between AI and
humans should be human-centered [17]. A stated in [21],
"we must design our technologies for the way people actually
behave, not the way we would like them to behave”. These
involve the related notions of inclusion and accessibility [22],
[23], related to the FAT and FATE AI (Fairness, Accessibility,
Transparency, Ethics). The human responses to feedback and
interaction with the system should be analyzed to progressively
design interaction systems that lead to learn continuously [24]
and increase trust and acceptance of the system.

Most of the explainable methods applied on time series
produce explanations for the developers. Those methods focus

on the technical aspects of their models without considering
the human dimension of the explainability. More generally, for
other types of users, ML is not the only field to consider when
designing an explainable automated system. The psychology
of the driver, for instance, is something to consider to be sure
that the explanations provided will be useful for the user.

III. XAI TECHNIQUES FOR TIME SERIES

We will now present explainable methods that increase
the trust in the ML model by explaining the prediction, or
explaining what the model has learnt. All the methods we will
present are applied on time series. First, we study post-hoc
methods explaining convolutional neural networks (CNNs).
Post-hoc methods approximate the behavior of a model by
extracting relationships between feature values and predictions
[25]. Post-hoc methods can be model-agnostic, usable on
every type of models, or model-specific, only usable on one
type of model. They are opposed to Ante-hoc methods that
incorporates explainability into the structure of the model, that
is thus already explainable at the end of the training phase.

The post-hoc methods that we will present are all specific
to convolutional neural network (Table !) (Sections [[I-A| and
{{]-A2). They are all originally used in the computer vision
field, and can be separated in two sections, back-propagation
based methods and perturbation-based methods. Then, we
will present some Ante-Hoc explainability methods specific to
recurrent neural networks (RNNs), that might also be applied
on the natural language processing (NLP) field, in Section
Ill-B. Finally, we will present some explainable data mining
methods applied on time series in Section I//-C and methods
that provide explainability through representative examples in
Section [[/-D.

A. XAI for Convolutional Neural Networks

We identify two types of methods to explain convolutional
neural networks applied on time series, backpropagation-based
methods and perturbation-based methods (Table 1).

1) Backpropagation-based methods: _ Backpropagation
methods provide explanations by doing a single forward and
backward pass in the network. Most of the backpropagation-
based explanation methods originally used to explain deep
learning methods applied on images can also be used on DL
methods applied on time series.

Wang et al. [26], Fawaz et al. [27], and Oviedo et al. [28] use
the class activation mapping (CAM) [29], a post-hoc method
to provide explanations that highlights the regions in the input
data that have the most influence on CNNs output classification
prediction [29]. CAM can highlight sub-sequences in the input
time series that are maximally representative of a class. It relies
on the presence of global average pooling layers at the end
of the convolutional layers. The global pooling layer takes
N channels and returns its spatial average values. Channels
with higher activations have higher signals. Then, a weight is
assigned per filter by using a dense linear layer with a softmax
activation layer. Assuming there are n classes, n heatmaps
are then created by computing the weighted sum of N filters
for every class. Finally, by up-sampling the class activation
UNDER REVIEW

maps to the size of the input time series, we can identify
the sub-sequences most relevant to any particular class. The
class activation mapping method has the disadvantage that the
model explained needs to have a specific architecture. A global
average pooling layer needs to be added after the convolutional
layers. Wang et al. [26] conduct their experiments on the UCR
time series repository datasets [30] with a little more than
80 datasets, which ares commonly used to evaluate models
applied on time series. Fawaz et al. [27] perform surgical skills
evaluation using the JIGSAWS dataset [31], and Oviedo et al.
[28] carry out time series classification from X-ray diffraction
datasets.

On the other hand, Strodthoff et al. [$2], Siddiqui et al. [1]
and Cho et al. [33] use the ’Gradient*Input’? method which
computes the partial derivative of the current layer with respect
to the input and multiplies it by the input itself. Therefore,
they compute the neurons and filters activation with respect
to one specific instance. The input subsequences processed by
the most activated filters have the highest contribution to the
prediction (Fig. 4) ©.

Classification

5
$e 7
VATE

CPHAP

 

Clusters from Layer 3
ne

Clusters from-Layer 2.

   
  

 

 

| Ww wins"

N\
& - ers from Layer |
& le | ay \ My
7 = i A WA AS

 

 

Input Time Series.

Fig. 4: Usage of ’Gradient*Input’ to identify the contribution
of the input raw data when performing time series classi-
fication. It extracts the highly activated nodes in a channel
and visualize the input sub-sequences that contribute to the
highly activated nodes. Then, each extracted sub-sequence is
assigned to a cluster of similar patterns. Figure reproduced
with authorization from Cho et al. [53].

The ’Gradient*Input’ approach can be used both for classi-
fication and regression tasks [|] as it only needs the neurons
activations to produce its explanations. For the experiments,
Strodthoff et al. [52] detect myocardial infarction using an
ECG dataset, the PTB diagnostic ECG dataset [34], [55]. Sid-
diqui et al. [1] create a dummy dataset with threee features, the
pressure, the temperature, and the torque to perform time series
classification. Finally, Cho et al. [53] interpret deep temporal
representations using two open source time series datasets:
UWaveGestureLibraryAll [56], a set of eight simple gestures
generated from accelerometers, and Smartphone Dataset for
human Activity Recognition [37], a smartphone sensor dataset
recording human perform eight different activities.

8Por the sake of exemplifying the output of the reviewed techniques while
acknowledging third party’s work, we include original figures after permission
being granted by their corresponding authors.

2) Perturbation-based methods: Perturbation-based meth-
ods directly compute the contribution of the input features
by removing, masking, or altering them, running a forward
pass on the new input, and measuring the difference with the
original input [58]. The higher the difference, the higher the
contribution of the input subsequence that has been altered. In
theory, perturbation-based methods can be used as long as it
is possible to compute distance values between the different
outputs of the model. Thus, perturbation-based methods can
be used for both classification and regression tasks [39].

ConvTimeNet [40] uses the occlusion _ sensitivity
method [41] which occludes parts of the time series and
computes the difference in the probability y for the predicted
class (Fig. 5). They perform time series classification using
85 datasets taken from UCR [30] TSC Archive Benchmark
belonging from seven diverse categories: Image outline,
Sensor Readings, Motion Capture, Spectrographs, ECG,
Electric Devices and Simulated Data.

Tonekaboni et al. [59] define the importance of each obser-
vation as the change in the model output caused by replacing
the observation with a generated one. They carry out mortality
prediction with the Intensive Care Unit (ICU) Time Series
dataset from MIMIC [42].

 

B

Input (x)
°
co
Weights

 

(a)

    
  

4) —— Filter1
—— Filter 2

Activations
N

by

 

-0.50

—0.75

 

 

-1.00

 

Occlusion Sensitivity

Oo 25 50 75 100 125
Time
(c)

Fig. 5: (a) Sample time series with top-2 relevant filters
from Two Patterns dataset, (b) their activation maps, and (c)
occlusion sensitivity plot. The goal is to use the occlusion
sensitivity method [41] to compute the raw input contribution.
Figure reproduced with authorization from Kashiparekh et
al. [40].

B. XAI techniques for Recurrent neural networks

The convolutional neural networks are not the only deep
learning methods that can perform time series classification.
The recurrent neural networks, which are perfectly adapted to
sequential data types, are also used to accomplish this kind of
task.

A first way to explain a recurrent model is to use attention
mechanisms (Table |). Attention mechanisms assign values
UNDER REVIEW

corresponding to the importance of the different parts of
the time series according to the model (Fig. 6). It helps to
overcome the fact that RNNs can’t encode the information
from too long input sequences. Attention mechanisms can
be used for time series classification [43] or time series
forecasting [44].

Choi et al. [45] combine a CNN as a feature extractor
and a Long Short Term Memory (LSTM) model to learn the
temporal dependencies. Then, the hidden states and output
states of the Long Short Term Memory (LSTM) are used as
input of a feedforward neural network layer which performs
classification. The weights of this feedforward layer are the
attention weights that indicate the importance of the different
timesteps of the time series [43], [44]. Choi et al. [45] use the
same feedforward layer to compute temporal attention, but
they stack another neural network layer that takes as input the
output of the temporal attention layer and the entire memory
state of the LSTM to compute variable attention. Vinayavekhin
et al. [46] compute more focused attention than the previous
methods by using the whole input sequence to calculate an
attention value for each timestep. Ge et al. [47] compute
variable attention directly from the weights of the LSTM.
To conduct their experiments, Schockaert et al. [44] generate
an artificial dataset to perform time series forecasting for the
temperature of the hot metal produced by a blast furnace.
Vinayavekhin et al. [46] carry out time series classification
using a public dataset for 3D human motion [48]. Finally,
Ge et al. [47] perform mortality prediction based on an ICU
dataset.

On the other hand, attention mechanisms are also at the
heart of transformers [49], which can detect globally impor-
tant variables for the prediction problem, persistent temporal
patterns, and significant events that lead to significant changes
in temporal dynamics [50]. Lim et al. [50] carry out time
series forecasting on several real world datasets like the UCI
Electricity Load Diagrams Dataset and the UCI PEM-SF
Traffic Dataset.

To summarize, attention mechanisms are Ante-Hoc explain-
ability methods (Table [) because they are embedded in the
structure of recurrent networks and the explicability they offer
is available directly at the end of the learning phase. This is
in opposition to the methods that explain convolutional neural
networks (Section [/1-A) which are specific post-hoc methods,
as their explainability mechanisms are not incorporated in
the structure of convolution networks, but are nevertheless
only usable to explain convolution networks. However, there
is also the possibility to explain recurrent models by using
a model-agnostic explanation method. Kim et al. [51] use
the SHapley Additive exPlanations (SHAP) algorithm [52], a
common model-agnostic feature attribution method, to explain
the output of a recurrent model.

C. Data mining based XAI models

As mentioned earlier, the deep learning explainability meth-
ods presented in the previous sections (Sections [I/-A and
III-B) are not specific to the domain of time series data and
can be applied to other fields. On the other hand, there are

explainability methods only applicable to the time series. This
is the case of the data mining methods that we will introduce
in this section. Several methods use data mining approaches to
perform interpretable time series classification. Some of these
methods are extensions of two data mining methods applied on
time series: Symbolic Aggregate Approximation (SAX) [53]

(Table |) and Fuzzy Logic.

(b) time

high attention

Variables
Variables

 

 

Variables
Variables

low attention

(c) time ( d) time

Fig. 6: (a) Global temporal attention, (b) Global spa-
tio/temporal attention, (c) Local temporal attention, (d) Lo-
cal spatio/temporal attention. "High attention’ means a high
contribution to the output while ’low attention’ means a low
contribution to the output. Attention mechanisms assign values
corresponding to the importance of the different parts of the
time series according to the mode. Figure reproduced with
authorization from Schockaert et al. [44]

Symbolic Aggregate approXimation (SAX) [54] transforms
the input time series into strings. The algorithm consists of
two steps. First, it transforms the time series into piece-wise
aggregate approximation (PAA) representation [55], and then
converts this representation into strings. To transform the input
data into piece-wise aggregate approximation (PAA) repre-
sentations, the time series are split into equal-sized segments
which are computed by averaging the values of these segments
. Then, symbols are assigned to each segment. Assuming that
the underlying input data distribution is Gaussian, each symbol
is assigned to a segment by equiprobability with equal sized
areas under the Gaussian curve. The input time series are then
transformed into a sequence of symbols. This method is a well
known way to detect recurrent patterns that occur in data.

Senin and Malinchik [56], and Le Nguyen et al. [57]
extend SAX to perform time series classification. They build
interpretable high level features from raw data thanks to SAX,
and select the best features according to each representations,
providing both performance and interpretability. Compared to
deep learning approaches, these approaches can be applied
on variable-length time series, and are easier to interpret.
To conduct their experiments, Senin and Malinchik [56], Le
Nguyen et al. [58], and Le Nguyen et al. [57] perform time
series classification using datasets from the UCR Time Series
Classification Archive [30].

Fuzzy logic [59], fuzzy sets [00] and computing with
words [61] approaches are other methods used as drivers
of explainability [62]. They aim at providing approximate
UNDER REVIEW

reasoning and model outputs that are closer to natural language
or use linguistic terms. In a way, their process is designed to
be like human thinking [63]. While in crisp rules, only ’True’
or ’False’ are accepted as outputs, in fuzzy logic outputs can
be associated to any value between 0 and 1, giving a degree of
possibility. It can be used to perform time series forecasting
[64], [05] or detect hidden temporal patterns [66]. It can also
be associated with neural networks to perform time series
prediction [67] and time series modeling [63].

El-Sappagh et al. [08] and Wang et al. [69] combine the
representational capacity of data-driven approaches and the
intepretability of fuzzy-based approaches. To do that, El-
Sappagh et al. [68] develop a fuzzy rule-based system (FRBS)
to perform diabetes prediction from numerical time series
data and static textual features. Wang et al. [69] propose a
fuzzy cognitive map (FCM), a system with several components
that can be connected to each other, and is interpretable
because the interactions between components are weighted,
to perform multivariate time series forecasting. They conduct
their experiments on four real-world multivariate time series
dataset.

Paiva and Dourado [70] also develop a neuro-fuzzy model
to perform time series forecasting with the goal of solving
complex tasks while keeping interpretability. That is why they
use a linguistic model with fuzzy sets, instead of a Takagi-
Sugano model with a first order fit function that is hard to
interpret. They perform experiments on the Mackey-Glass time
series [71] and Box-Jenkis gas furnace datasets [72].

D. Explaining models through representative examples

Other types of methods, such as some methods that generate
explanations by example, can be specific to the time series
field. A type of explanations by example consists in giving
the closest example in the training dataset that explains, as
prototype, what the typical behaviour of a similar sample
would look like. In this area we can find embeddings-based
models that look at the k-Nearest Neighbours (kKNNs) in the
embedding space to a given data point [73], [74], [75].

An example of methods that produce explanations by exam-
ple specific to time series are Shapelets, which are time series
subsequences that are maximally representative of a class [76]
(Fig. 7).

Shapelets were first introduced by Ye et al. [77] in time se-
ries classification to overcome the limitations of state of the art
time series classifiers. Shapelets are more interpretable, faster,
and more accurate than k-Nearest Neighbours (kNN) [7%]
which is a traditional approach to perform time series clas-
sification [79]. They are computed by finding subsequences
and associated thresholds that maximize the information gain
when splitting the set of all subsequences into two classes
following their distance to the candidate shapelet.

One limitation of shapelets is that there is a choice to make
between efficient training and interpretability [80]. Wang et
al. [1] and Kidger et al. [80] develop a regularization term
that constrains the model to learn more interpretable shapelets.
Another limitation is the computational time. Fang et al. [82]
use PAA [55] to discover candidate shapelets and thus reduce

the computational time. Finally, Li et al. [85] develop a new
way to find shapelets and perform time series classification
which strongly reduces the computational time.

 

 

 

shapelet1 shapelet1
4 @ 12 16 20 24 4 8 12 16 20 24

(a) Instance 1 of Class 1 (b) Instance 2 of Class 1

1
1
° NL 0

-1

 

 

w

 

 

 

 

 

 

shapelet1 shapelet1
4 8 12 16 20 24 a 8 12 16 20 24

(c) Instance 1 of Class 2 (d) Instance 2 of Class 2

Fig. 7: Example of shapelets, explaining maximally repre-
sentative subsequences of a class. Figure reproduced with
authorization from Li et al. [83].

To conduct their experiments, Wang et al. [81], Fang et al.
[82] and Li et al. [83] carry out time series classification using
datasets from the UCR repository Time Series Datasets [30],
while Kidger et al. [S80] perform time series calssification using
datasets from the UEA Time Series Archive [84].

IV. EXPLANATIONS SCALE

The methods presented in the Section II! can provide local
explanations or global explanations. The explanations are
qualified as local when they are valid for a specific sample,
and as global when they are valid for a set of samples or for
the entire dataset.

A. Local explanations

Methods will tend to have local explanations when they
make predictions sample by sample, and when the knowledge
is not shared from one prediction to another. Thus, explainabil-
ity methods specific to convolution networks naturally produce
local explanations.

Backpropagation-based methods (see Table |) rely on the
activation of the neurons corresponding to a single prediction.
Class Activation Mapping (CAM) [29], for instance, relies on
the average of the channel activations of the last convolutional
layer. The activation of the neurons change for every prediction
and are therefore local parameters.

Perturbation-based methods such as ConvTimeNéet [40] (Ta-
ble 1) alter a sub-sequence from the input time series and look
at the difference in prediction with the original sub-sequence.
The computed relevances correspond to the sub-sequence that
has been altered.

Although, like convolutional neural networks, recurrent neu-
ral networks (Section II/-B) make their predictions sample
by sample, they have a memory state that retains the knowl-
edge built during previous representations. Unlike convolution
neural networks, their latent representations are designed to
UNDER REVIEW

handle one or several samples depending if the internal states
are reset after each prediction. Therefore, the choice of this
parameter will influence the scope of explanations of recurrent
neural networks. The explanations are local if the internal
states represent one instance, or global if the internal states
represent several instances.

B. Global explanations

Other methods like Shapelets (Section [1/-D) or SAX (Sec-
tion [II-C) do not process the data sample by sample. For
instance, the research of the candidate shapelet is not limited
and can be carried on over the entire dataset. The scope of the
explanations is defined by the size of the time series given as

ig

1
Ir

 

 

 

  

 

ff

 

 

 

Fig. 8: (a) 1-D Convolutional Filters, (b) 1-D Convolutional
Filters with importance and saliency, and (c) 1-D Convolu-
tional Filters with importance, saliency, and clusters. The goal
is to compute input saliency and to cluster the filters. Figure
reproduced with authorization from Siddiqui et al. [1].

Finally, some papers extend the methods generating local
explanations to produce global explanations. For instance,
Oviedo et al. [28] generalize CAM to all training samples
within a class. The average CAM allows to visualize the
main discriminative features per class. Some methods provide
global explanations through the usage of clustering. The
backpropagation-based method Tsviz [|] highlights the impor-
tant regions of the input data and computes the importance of
filters for a given prediction. They also build global insights
by clustering filters according to their activation pattern, as
filters with similar activation patterns are essentially capturing
the same concepts (Fig. 8). Cho et al. [53] do not compute
clusters with filters but with input sub-sequences. Each cluster
is composed of a list of temporal sequences that activate the
same nodes. They assign a general time series profile to these
clusters with some uncertainty. This might be one of the most
complete approach to explain a convolutional neural network
with time series as input data. To the best of our knowledge, it
is one of the only explainable methods applied on time series
that explain the latent representations of convolutional neural
networks.

V. PURPOSE AND INTENDED AUDIENCE OF EXPLANATIONS

The scope of explanations has an impact on the purpose of
explainability methods. Global explanations can potentially be
interesting to give trustworthiness and confidence, but local ex-
planations may be more interesting for giving trustworthiness
because their purpose is to explain the reasons for a prediction.

The scope of the explanations will also have an impact
on the potential targets of explainability methods. Generally

speaking, a target expert in machine learning, or a target with
responsibility in the event of system failure, will rather aim at
global explanations that will seek to explain the behaviour of
the model as a whole. On the other hand, consumers of the
model will rather look for local explanations that will explain
the specific predictions that interest them.

First, we will present some applications of XAI methods
providing trustworthiness for different types of targets. Then,
we will show that some XAI methods are also able to increase
the confidence in the model.

A. Providing trustworthiness through explanations for every
audience

In this section, we will present some applications of XAI
methods applied on time series providing trustworthiness for
three different type of targets: the developer, the decision
maker, and the user. Some of the most interesting applications
are in the medical field [27]. Some other methods have
interesting insights for model debugging [28], [52].

1) Explanations for developers: The developer is the one
that makes the model. Most reviewed methods [26], [40], [95],
[39], [1], [33], [87] are not applied to any particular domain,
but rather focus on an algorithm or a family of algorithms.
As a consequence, the targets of these methods are mostly
developers, as the insights provided by these methods are quite
technical and hard to interpret for a non expert.

However, Oviedo et al. [28] conduct their experiments on
a specific domain but might be used by developers. The goal
is to perform classification from a small x-ray diffraction time
series dataset. They use explanations provided by CAM to
identify the causes of correct and incorrect classifications. This
outcome might only be interesting for developers .

Generally, developers seek for technical insights that can
explain the whole model rather than explanations of a pre-
diction. That is why developers are generally more interested
in global explanations (Section |V-5) than local explanations
(Section [V-A).

2) Explanations for end-users: A user is a person that
consumes the output of the model. Let us take the example
of the system that evaluates the surgical skills of young
surgeons [27]. The users here are the young surgeons that
improve themselves by looking at discriminative behaviors
specific to skill level. By identifying the gestures that made
the model classify them as novices, they can identify their
weaknesses and see how to improve themselves without the
human intervention of an expert surgeon.

Still in the medical field, clinicians can use the method
that perform myocardial infarction detection [52]. The inter-
pretability is provided here to build confidence and trust in the
model outcomes. Indeed, in a such critical fields, users cannot
rely on the model if they do not know why the model takes the
decisions. Thanks to the explanations provided, clinicians can
check whether the model considers relevant certain patterns
when making a decision.

Generally, users seek for the explanations of a prediction or
a group of predictions that can affect them [27], [52]. Most
of the time, they cannot take advantage of explanations of the
UNDER REVIEW

 

Model Ante-hoc/

Post-hoc

Methodology

 

 

 

 

Model Specific/
Model Agnostic

Scope Target

audience

Explanation
evaluation

 

 

 

 

 

et
, Fawaz et
7]

al. D8]
[85]

et al. [0]
et

9]

>
al. [32]
i et
, et
, et ttention
, ttention
al. [43]
, ttention
al. [44]
et ttention
1 ttention
[87]
-VsM, et

Nguyen et al. [57]
Z , et
[81]

et
et

, Tan et
[88]

ttention

- , ttention
al. [45]
et ttention
unir et
[90]

et ttention

y

[46]

, Ge et
et

ttention

Bi
-CAn, et
b2]
, Augustin et
Be]
et
a et
[o8]
et

(9)

 

Cc
Cc

TABLE I: Summary of XAI methods applied to time series. Abbreviations:
SAX: Symbolic Aggregate Approximation; DM: Decision Maker;

whole model that may be hard to interpret and not necessarily
fit to their situation. That is why users are generally more
interested in local explanations (Section [\V-A) than global
explanations (Section [V-B).

3) Explanations for decision makers: The decision maker
is characterized by a non-expertise, added to a liability in
case of a problem with the AI system. For the method that
evaluates the surgical skills of young surgeons [27], and for
the method that performs myocardial infarction detection [32],
the decision makers are the people in charge of the clinic.
They are not experts in machine learning, but need guarantees
that the system will work properly. They may not need an
explanation of how the system works, but perhaps a measure
of confidence (Section |!-A) in its decisions. In this way, they

might know whether it is reasonable or too risky to use the
system.

B. Confidence

The first purpose of explainable methods is to get insights
on how the models work, how predictions are made. However,
understanding how the model works is not enough. Particularly
for critical tasks, we also need to be confident in the decisions
taken by the model. We introduced in the Section I/-A the
notion of confidence, and the potential link it can have with
XAL. It is interesting to think that explainable methods might
be useful to help models to be more robust and stable when
facing samples out of the input distribution or adversarial
attacks. We will present in this section some examples of XAI
UNDER REVIEW

methods providing epistemic confidence in a model outcomes.
Most of these methods are applied on time series data.

Hartl et al. [96] and Siddiqui et al. [1] use the raw features
contribution to study adversarial attacks. Tsviz [|] introduce a
perturbation on the most salient part of the input subsequence,
leading to a huge drop in classification, which confirms the
sensitivity of the model to noises. Hartl et al. [96] develop
a method named feature sensitivity that can quantify the
potential of a feature to cause missclassification. Surprisingly,
they found that the most salient features are the same than the
features with highest potential to cause missclassification, and
thus easily targetable by adversarial attacks. Therefore, Hartl
et al. [96] propose a defense method in which they leave out
the most manipulable features which does not lead to a huge
drop of the accuracy.

Gee et al. [94] introduce a diversity penalty to learn more
diverse prototypes, which helps focusing on areas of the latent
space where class separation is the most difficult. This helps
the model to be more stable when classifying samples far from
the input distribution.

Cho et al. [53] compare different attribution methods by
perturbing the less salient input parts. The idea is that the
neurons activations are more stable when we perturb the
less salient input parts for the corresponding prediction. It is
interesting to note that this is the opposite approach of Tsviz
[1] which applies perturbations on the most salient parts of
the input. However, both approaches [33], [1] are based on
the same idea. Indeed, when perturbations are applied on the
most salient parts of the input, we expect strong disturbances
in the latent representations of the models [|]. Thus, when
perturbations are applied on the less salient parts of the input,
we expect small disturbances in the latent representations of
the models [33].

The overlapping interpretable Sax words (Section [II-C )
[56] have individually very small contributions to the final
prediction, which makes the methods less sensible to noises.

Ates et al. [97] combine two adversarial training procedures,
Adversarial Training (AT) [98] and Adversarial confidence
enhanced training (ACET) [99]. Adversarial Training (AT) is
better at making the model robust against adversarial attacks,
while Adversarial confidence enhanced training (ACET) is
better at tackling out of distribution samples. Ates et al. [97]
develop RATIO, a method that generates counterfactual [100]
visual explanations. These counterfactual explanations visually
show the changes in the input sample to reach the targeted AT
and ACET confidence score.

Although this is only the beginning, these are to the best
of our knowledge the first attempts to provide epistemic
confidence in the models using the insights provided by XAI
methods.

VI. EVALUATING EXPLANATIONS

After presenting the scope and the targets of explainability
methods, we now tackle the evaluation of these XAI methods.
There is not a metric globally recognized that can assess
the quality of explanations (Table [!). This might be due
to the different nature of the explanations generated and

the different input data types. However, some quantitative
evaluation approaches exist to objectively assess the quality of
the explanations generated in several fields, including the time
series domain. Qualitative evaluations can also be made by
experts to assess the relevancy of the explanations generated.

A. Qualitative evaluations

While some methods [58], [27], [28], [43], [44], [85] do not
perform any evaluation of the generated explanations, these
methods could be evaluated using domain expert assessments.
Global explanations provided by, for instance, average CAM
in Oviedo et al. [28] could be assessed by experts by just
analysing the global attribution maps computed by averaging
every class activation map representing each class, and not
all the local attribution maps one by one. Methods providing
explanations for specific users [27], [85] can also easily
benefit from experts feedback. For example, the surgeons
experimenting the model that performs evaluation of surgical
skills [27] can give some feedback, whether they found the
explanations provided relevant or not.

The backpropagation-based approach Tsviz [1] assesses the
quality of generated explanations by analyzing its explicitness.
The explicitness is provided by the clustering of hidden
representations and showing the influence of these hidden
representations on the output.

However, unlike in computer vision, qualitative evaluations
might have a limited potential in the time series field [101].
Indeed, the unintuitive nature of time series (see Section [)
make it difficult even for domain experts to qualitatively assess
the quality of the explanations generated. Therefore, accord-
ing to Arnout et al. [101], we should prioritize quantitative
evaluations for the time series field.

B. Quantitative evaluations

Arnout et al. [101] presents a way to evaluate explanations
providing the most contributing regions of the input time series
to the model. They propose to conduct perturbation on the
data, with the idea that if relevant features get changed, the
performance of an accurate model should decrease massively.
Tonekaboni et al. [59] takes another approach by comparing
its explanations with several state of the art feature attribution
methods, sensitivity analysis [102] [103], feature occlusion
[104], augmented feature occlusion [104], and Local Inter-
pretable Model-agnostic Explanations (LIME) [2]. Cho et
al. [55] also assess their explanations by providing some
comparisons with another attribution method, Layer-wise Rel-
evance Propagation (LRP) [105], thanks to some perturbation
analysis: they gradually alter subsequences that have been
identified by the attribution methods as important for a given
prediction. They compare both methods through the absolute
difference of activations of neurons depending on the per-
turbation magnitude of their identified explanations (Fig. 9).
The perturbation-based method proposed by Tonekaboni et
al. [59] performs sanity checks by doing data and model
randomization: they evaluate the faithfulness in Tsviz [|] by
removing the filters that have the highest importance and check
if the prediction changes.
UNDER REVIEW

Channel 55 Zero | Channel 55

   

— Random
— uP

  

Fig. 9: This graph illustrates the results of perturbations on the
input while preserving the regions selected by each method.
We apply three kinds of perturbations: Gaussian perturbation,
Inverse perturbation and zero perturbation. The x-axis means
the ratio of perturbed regions except preserved regions. The
y-axis means the sum of changes of activations in the channel
55. Figure reproduced with authorization from Cho et al. [33].

Finally, Arnout et al. [101] propose two new quantitative
evaluation methods to overcome the limitations of the per-
turbation approach, which has a lack of evaluation of trends
or patterns in the time series [101]. They propose two new
sequence evaluation approaches, Swap Time Points and Mean
Time Points, that take the inter-dependency of points into
account. Swap Time Points invert the order of the points in
the most salient sub-sequences, and compare the results with
another sequence where the values of the corresponding sub-
sequences have been put to 0. Mean Time Points has a similar
approach, but instead of swapping the time points, it assigns
the mean of all values of the salient sub-sequence to all points
in the corresponding sub-sequence. These two approaches are
complementary with the perturbation approach in the sense
that they offer a new way to evaluate time series salient
explanations, as they can evaluate trends or patterns of the
time series.

To summarize, we can reflect on the fact that depending on
what we want to accomplish with the explanations, qualitative
or quantitative evaluations may be more or less appropriate.
Qualitative assessments may be better suited to explanations
that target users or decision makers [27], while quantitative
assessments may be better suited to explanations that attempt
to discern new predictive knowledge in the data [53]. Then, at
this level, a question comes to mind. Are these explanations
that teach experts how the models work sufficient to develop
the confidence to put the models into practice to accomplish
critical tasks? We will attempt to answer this question in the
next section (Section VII).

VII. DISCUSSION

Some of the methods presented in this survey are originally
applied to other areas than time series. The back-propagation
based (Section [//-Al) and perturbation-based approaches
(Section [/1-A2) were first designed for the computer vision
field and then applied to the time series field. To the best of
our knowledge, there is a lack of explainable methods applied
on CNNs specifically designed for time series tasks. There
must be specificities in time series data that can be exploited
to design explainable approaches specific to CNNs that are
uniquely adapted to the time series field.

Most methods presented in this survey indicate which
specific regions of the input data get attention from the model
while classification is performed. They do not provide any
confidence in the model, neither mitigates its vulnerabilities.
However, it can be one way to provide some explanations and
increase the trust in the system. Indeed, a user or a developer
can rely more on the system if he knows that the model gives
its attention to the relevant parts of the input for a specific
prediction. However, the trust brought by these methods can
be questioned by the unintuitive aspect of time series. For
instance, saliency maps on images are directly interpretable as
we usually understand the content. It is different for time series
because expert knowledge might be needed to understand its
outcomes. Some data mining methods might be useful to
automatically extract the underlying content of time series.

As highlighted in this survey, it is possible to use the
insights provided by some explainable methods to increase
the epistemic confidence in the model. Obviously, the purpose
of XAI is to get some information and understanding of
the model, and therefore to provide trust. However, the XAT
field has more potential than just facilitating trustworthiness.
Explainability has the potential to lead to new metrics and
training practices ensuring the confidence and robustness of
the most complex and abstract models, thanks to the insights
explainable methods can provide.

We are also far from getting an end-to-end XAI system,
as methods only focus on the technical parts. XAI techniques
do not consider interactions with the user or developer that
are necessary for the AI system to be trusted and used. There
is a lack of objective tools to demonstrate the robustness of
AI systems. This is why interactive systems providing expla-
nations and feedback might be a leading way to empirically
and subjectively show the user and decision maker that the AI
system can be trusted.

ACKNOWLEDGEMENTS

We acknowledge the authors of [26], [44], [40], [1] and [83]
for letting us use their original figures for illustrative purposes.
T. Rojat would also like to thank ANRT and Renault for the
funding support. J. Del Ser would like to thank the Basque
Government for its funding support through the EMAITEK
and ELKARTEK programs (3KIA project, KK-2020/00049),
as well as the consolidated research group MATHMODE (ref.
T1294-19).

REFERENCES

[1] S. A. Siddiqui, D. Mercier, M. Munir, A. Dengel, and S. Ahmed,
“Tsviz: Demystification of deep learning models for time-series anal-
ysis,” IEEE Access, vol. 7, pp. 67 027-67 040, 2019.

[2] M. T. Ribeiro, S. Singh, and C. Guestrin, “”’ why should i trust you?”
explaining the predictions of any classifier,” in Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and
data mining, 2016, pp. 1135-1144.

[3] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh,
and D. Batra, “Grad-cam: Visual explanations from deep networks
via gradient-based localization,” International Journal of Computer
Vision, vol. 128, no. 2, p. 336-359, Oct 2019. [Online]. Available:
http://dx.doi.org/10.1007/s 1 1263-019-01228-7

[4] E. Tjoa and C. Guan, “A survey on explainable artificial intelligence
(xai): Toward medical xai,” IEEE Transactions on Neural Networks
and Learning Systems, p. 1-21, 2020. [Online]. Available: http:
//dx.doi.org/10.1109/TNNLS.2020.3027314
UNDER REVIEW

[5]

[6

7]

[8

19]

[10]

Qi

[12]

[13]
[14]

[15]

[16]

117)

[18]

[19]

[20]

 

[Model

| Quantitative/Qualitative Evaluation

| Evaluation Approach

 

et tal

, et
, Fawaz et
0. et
et
et
et
et
1g OW.
et al. [46]

Mtex-cnn,
, et

 

on

assessment

assessment

assessment

on

on

assessment

on
assessment

TABLE II: Summary of evaluation approaches for XAI methods applied to time series.

A. B. Arrieta, N. Diaz-Rodrfguez, J. Del Ser, A. Bennetot, S. Tabik,
A. Barbado, 8. Garcia, S. Gil-Lépez, D. Molina, R. Benjamins ef ail.,
“Explainable artificial intelligence (xai): Concepts, taxonomies, oppor-
tunities and challenges toward responsible ai,” Information Fusion,
vol. 58, pp. 82-115, 2020.

D. Doran, S. Schulz, and T. R. Besold, “What does explainable ai really
mean? a new conceptualization of perspectives,” 2017.

Z. C. Lipton, “The mythos of model interpretability: In machine
learning, the concept of interpretability is both important and slippery.”
Queue, vol. 16, no. 3, pp. 31-57, 2018.

B. Kailkhura, B. Gallagher, S. Kim, A. Hiszpanski, and T. Y.-J. Han,
“Reliable and explainable machine-learning methods for accelerated
material discovery,” npj Computational Materials, vol. 5, no. 1, pp.
1-9, 2019.

J. Wamer, L.-V. Herm, K. Heinrich, C. Janiesch, and P. Zschech.
(2020) White, grey, black: Effects of xai augmentation on the
confidence in ai-based decision support systems. [Online]. Available:
https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/14/

R. Hamon, H. Junklewitz, and I. Sanchez, “Robustness and explain-
ability of artificial intelligence,” Publications Office of the European
Union, 2020.

K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust physical-world attacks
on deep learning visual classification,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
1625-1634.

N. Morgulis, A. Kreines, S. Mendelowitz, and Y. Weisglass,
“Fooling a real car with adversarial traffic signs,” arXiv preprint
arXiv: 1907.00374, 2019.

C. Molnar, Interpretable machine learning. Lulu. com, 2020.

K. Xu, D. H. Park, C. Yi, and C. Sutton, “Interpreting deep
classifier by visual distillation of dark knowledge,” arXiv preprint
arXiv: 1803.04042, 2018.

J. Haspiel, N. Du, J. Meyerson, L. P. Robert Jr, D. Tilbury, X. J.
Yang, and A. K. Pradhan, “Explanations and expectations: Trust
building in automated vehicles,” in Companion of the 2018 ACM/IEEE
International Conference on Human-Robot Interaction, 2018, pp. 119-
120.

L. Petersen, D. Tilbury, X. J. Yang, and L. Robert. (2017) Effects of
augmented situational awareness on driver trust in semi-autonomous
vehicle operation. [Online]. Available: ttps://deepblue.lib.umich.edu/
handle/2027.42/137707

J. Koo, J. Kwac, W. Ju, M. Steinert, L. Leifer, and C. Nass, “Why did
my car just do that? explaining semi-autonomous driving actions to
improve driver understanding, trust, and performance,” International
Journal on Interactive Design and Manufacturing (IJIDeM), vol. 9,
no. 4, pp. 269-275, 2015.

J. Koo, D. Shin, M. Steinert, and L. Leifer, “Understanding driver
responses to voice alerts of autonomous car operations,” International
journal of vehicle design, vol. 70, no. 4, pp. 377-392, 2016.

L. P. Robert, A. R. Denis, and Y.-T. C. Hung, “Individual swift trust
and knowledge-based trust in face-to-face and virtual team members,”
Journal of Management Information Systems, vol. 26, no. 2, pp. 241-
279, 2009.

A. Chander, R. Srinivasan, S. Chelian, J. Wang, and K. Uchino,
“Working with beliefs: Ai transparency in the enterprise.” in /UI
Workshops, 2018.

[21]
[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

D. A. Norman, Living with complexity. MIT press, 2016.

N. Diaz-Rodriguez and G. Pisoni, “Accessible cultural heritage through
explainable artificial intelligence,” in Adjunct Publication of the 28th
ACM Conference on User Modeling, Adaptation and Personalization,
2020, pp. 317-324.

G. Pisoni, N. Diaz-Rodriguez, H. Gijlers, and L. Tonolli, “Human-
centred artificial intelligence for designing accessible cultural heritage,”
Applied Sciences, vol. 11, no. 2, p. 870, 2021.

T. Lesort, V. Lomonaco, A. Stoian, D. Maltoni, D. Filliat, and N. Diaz-
Rodriguez, “Continual learning for robotics: Definition, framework,
learning strategies, opportunities and challenges,” Information fusion,
vol. 58, pp. 52-68, 2020.

M. Moradi and M. Samwald, “Post-hoc explanation of black-
box classifiers using confident itemsets,” Expert Systems with
Applications, vol. 165, p. 113941, Mar 2021. [Online]. Available:
http://dx.doi.org/10.1016/|.eswa.2020.113941

Z. Wang, W. Yan, and T. Oates, “Time series classification from scratch
with deep neural networks: A strong baseline,” in 2017 International
joint conference on neural networks (IICNN). YEEE, 2017, pp. 1578-
1585.

H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P-A.
Muller, “Accurate and interpretable evaluation of surgical skills
from kinematic data using fully convolutional neural networks,”
International Journal of Computer Assisted Radiology and Surgery,
vol. 14, no. 9, p. 1611-1617, Jul 2019. [Online]. Available:
http://dx.doi.org/10,1007/s 1 1548-019-02039-4

F. Oviedo, Z. Ren, S. Sun, C. Settens, Z. Liu, N. T. P. Hartono,
S. Ramasamy, B. L. DeCost, 8. L Tian, G. Romano et ail., “Fast and
interpretable classification of small x-ray diffraction datasets using data
augmentation and deep neural networks,” npj Computational Materials,
vol. 5, no. 1, pp. 1-9, 2019.

B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
deep features for discriminative localization,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2016,
pp. 2921-2929.

Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and
G. Batista, “The ucr time series classification archive,” July 2015, www.
cs.ucr.edu/~eamonn/time_series_data/.

Y. Gao, 8. S. Vedula, C. E. Reiley, N. Ahmidi, B. Varadarajan, H. C.
Lin, L. Tao, L. Zappella, B. Béjar, D. D. Yuh ef al., “Jhu-isi gesture
and skill assessment working set (jigsaws): A surgical activity dataset
for human motion modeling,” in MICCAI workshop: M2cai, vol. 3,
2014, p. 3.

N. Strodthoff and C. Strodthoff, “Detecting and interpreting myocardial
infarction using fully convolutional neural networks,” Physiological
Measurement, vol. 40, no. 1, p. 015001, Jan 2019. [Online]. Available:
http://dx.doi.org/10.1088/1361-6579/aaf34d

S. Cho, G. Lee, and J. Choi, “Interpretation of deep temporal repre-
sentations by selective visualization of internally activated units,” arXiv
preprint arXiv: 2004. 12538, 2020.

R. Bousseljot, D. Kreiseler, and A. Schnabel, “Nutzung der ekg-
signaldatenbank cardiodat der ptb iiber das internet,” Biomedical Engi-
neering/Biomedizinische Technik, vol. 40, no. sl, pp. 317-318, 1995.
A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C.
Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E.
Stanley, “Physiobank, physiotoolkit, and physionet: components of a
UNDER REVIEW

[36]

[37]
[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

new research resource for complex physiologic signals,” circulation,
vol. 101, no. 23, pp. e215-e220, 2000.

J. Liu, L. Zhong, J. Wickramasuriya, and V. Vasudevan, “‘uwave:
Accelerometer-based personalized gesture recognition and its applica-
tions,” Pervasive and Mobile Computing, vol. 5, no. 6, pp. 657-675,
2009.

A. Asuncion and D. Newman, “Uci machine learning repository,” 2007.
M. Ancona, E. Ceolini, C. Oztireli, and M. Gross, “Towards better
understanding of gradient-based attribution methods for deep neural
networks,” arXiv preprint arXiv: 1711.06104, 2017.

S. Tonekaboni, S. Joshi, D. Duvenaud, and A. Goldenberg. (2019)
Explaining time series by counterfactuals. [Online]. Available:
https://openreview.net/pdf?id=HygDF lrY DB

K. Kashiparekh, J. Narwariya, P. Malhotra, L. Vig, and G. Shroff,
“Convtimenet: A pre-trained deep convolutional neural network for
time series classification,” in 2019 International Joint Conference on
Neural Networks (IJCNN), 2019, pp. 1-8.

M. D. Zeiler and R. Fergus, “Visualizing and understanding con-
volutional networks,” in European conference on computer vision.
Springer, 2014, pp. 818-833.

A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-Wei, M. Feng,
M. Ghassemi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark,
“Mimic-iii, a freely accessible critical care database,” Scientific data,
vol. 3, no. 1, pp. 1-9, 2016.

F. Karim, S. Majumdar, H. Darabi, and S. Chen, “Lstm fully convo-
lutional networks for time series classification,’ JEEE Access, vol. 6,
pp. 1662-1669, 2018.

C. Schockaert, R. Leperlier, and A. Moawad, “Attention mechanism
for multivariate time series recurrent model interpretability applied to
the ironmaking industry,” arXiv preprint arXtv:2007.12617, 2020.

K. S. Choi, S. H. Choi, and B. Jeong, “Prediction of IDH genotype in
gliomas with dynamic susceptibility contrast perfusion MR imaging
using an explainable recurrent neural network,” Neuro-Oncology,
vol. 21, no. 9, pp. 1197-1209, 06 2019. [Online]. Available:
https://doi.org/10.1093/neuonc/noz095

P. Vinayavekhin, S. Chaudhury, A. Munawar, D. J. Agravante,
G. De Magistris, D. Kimura, and R. Tachibana, “Focusing on what
is relevant: Time-series learning and understanding using attention,” in
2018 24th International Conference on Pattern Recognition (ICPR).
IEEE, 2018, pp. 2624-2629.

W. Ge, J.-W. Huh, Y. R. Park, J.-H. Lee, Y.-H. Kim, and A. Turchin,
“An interpretable icu mortality prediction model based on logistic
regression and recurrent neural networks with lstm units.” in AMIA
Annual Symposium Proceedings, vol. 2018. American Medical
Informatics Association, 2018, p. 460.

C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3. 6m:
Large scale datasets and predictive methods for 3d human sensing
in natural environments,” [EEE transactions on pattern analysis and
machine intelligence, vol. 36, no. 7, pp. 1325-1339, 2013.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems, vol. 30, pp. 5998—
6008, 2017.

B. Lim, S. O. Arik, N. Loeff, and T. Pfister, “Temporal fusion
transformers for interpretable multi-horizon time series forecasting,”
arXiv preprint arXiv:1912.09363, 2019.

J.-Y. Kim and S.-B. Cho, “Electric energy consumption prediction by
deep learning with state explainable autoencoder,” Energies, vol. 12,
p. 739, 02 2019.

S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting
model predictions,” in Advances in Neural Information Processing
Systems, 2017, pp. 4765-4774.

J. Lin, E. Keogh, L. Wei, and S. Lonardi, “Experiencing sax: A novel
symbolic representation of time series,” Data Min. Knowl. Discov.,
vol. 15, pp. 107-144, 08 2007.

J. Lin, E. Keogh, S$. Lonardi, and B. Chiu, “A symbolic representation
of time series, with implications for streaming algorithms,” 01 2003,
pp. 2-11.

E. Keogh, K. Chakrabarti, M. Pazzani, and S. Mehrotra, “Dimension-
ality reduction for fast similarity search in large time series databases,”
Knowledge and Information Systems, vol. 3, 01 2002.

P. Senin and S. Malinchik, “Sax-vsm: Interpretable time series clas-
sification using sax and vector space model,” in 20/3 IEEE 13th
International Conference on Data Mining, 2013, pp. 1175-1180.

T. L. Nguyen, S. Gsponer, I. Ilie, and G. Ifrim, “Interpretable
time series classification using all-subsequence learning and sym-

[58]

[59]
[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

(71)
[72]

[73]

[74]

[75]

[76]

[77]

[78]

[79]

[80]

[81]

bolic representations in time and frequency domains,” arXiv preprint
arXiv: 1808.04022, 2018.

T. Le Nguyen, S. Gsponer, I Iie, M. O’Reilly, and G. Ifrim, “In-
terpretable time series classification using linear models and multi-
resolution multi-domain symbolic representations,” Data mining and
knowledge discovery, vol. 33, no. 4, pp. 1183-1222, 2019.

L. A. Zadeh, “Fuzzy logic,” Computer, vol. 21, no. 4, pp. 83-93, 1988.
F. Herrera, E. Herrera-Viedma, and L. Martinez, “A fusion approach
for managing multi-granularity linguistic term sets in decision making,”
Fuzzy sets and systems, vol. 114, no. 1, pp. 43-58, 2000.

F. Herrera, S. Alonso, F. Chiclana, and E. Herrera- Viedma, “Computing
with words in decision making: foundations, trends and prospects,”
Fuzzy optimization and decision making, vol. 8, no. 4, pp. 337-364,
2009.

C. Mencar and J. M. Alonso, “Paving the way to explainable artificial
intelligence with fuzzy modeling,” in International Workshop on Fuzzy
Logic and Applications. Springer, 2018, pp. 215-227.

P.C. Nayak, K. Sudheer, D. Rangan, and K. Ramasastri, “A neuro fuzzy
computing technique for modeling hydrological time series,” Journal
of Hydrology, vol. 291, pp. 52-66, 05 2004.

S.-M. Chen, C.-C. Hsu ef al., “A new method to forecast enrollments
using fuzzy time series,” International Journal of Applied Science and
Engineering, vol. 2, no. 3, pp. 234-244, 2004.

S.-M. Chen and J.-R. Hwang, “Temperature prediction using fuzzy time
series,” [EEE Transactions on Systems, Man, and Cybernetics, Part B
(Cybernetics), vol. 30, no. 2, pp. 263-275, 2000.

I. Aydin, M. Karakose, and E. Akin, “The prediction algorithm based
on fuzzy logic using time series data mining method,” World Academy
of Science, Engineering and Technology, vol. 51, no. 27, pp. 91-98,
2009.

N. K. Kasabov and Qun Song, “Denfis: dynamic evolving neural-fuzzy
inference system and its application for time-series prediction,” IEEE
Transactions on Fuzzy Systems, vol. 10, no. 2, pp. 144-154, 2002.

S. El-Sappagh, J. M. Alonso, F. Ali, A. Ali, J.-H. Jang, and K.-S.
Kwak, “‘An ontology-based interpretable fuzzy decision support system
for diabetes diagnosis,” IEEE Access, vol. 6, pp. 37 371-37 394, 2018.
J. Wang, X. Wang, C. Li, J. Wu et al., “Deep fuzzy cognitive maps for
interpretable multivariate time series prediction,” /EEE Transactions on
Fuzzy Systems, 2020.

R. P. Paiva and A. Dourado, “Interpretability and learning in neuro-
fuzzy systems,” Fuzzy sets and systems, vol. 147, no. 1, pp. 17-38,
2004.

M. C. Mackey and L. Glass, “Oscillation and chaos in physiological
control systems,” Science, vol. 197, no. 4300, pp. 287-289, 1977.

G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series
analysis: forecasting and control. John Wiley & Sons, 2015.

Y.-H. Lee, C.-P. Wei, T.-H. Cheng, and C.-T. Yang, “Nearest-neighbor-
based approach to time-series classification,” Decision Support Systems,
vol. 53, no. 1, pp. 207-217, 2012.

Z. Geler, V. Kurbalija, M. Ivanovic, and M. Radovanovié, “Weighted
knn and constrained elastic distances for time-series classification,”
Expert Systems with Applications, vol. 162, p. 113829, 2020.

S. Xu, Q. Luo, H. Li, and L. Zhang, “Time series classification based on
attributes weighted sample reducing knn,” in 2009 Second International
Symposium on Electronic Commerce and Security, vol. 2. IEEE, 2009,
pp. 194-199.

L. Ye and E. Keogh, “Time series shapelets: a new primitive for
data mining,” in Proceedings of the 1Sth ACM SIGKDD international
conference on Knowledge discovery and data mining, 2009, pp. 947—
956.

——., “Time series shapelets: A novel technique that allows accurate,
interpretable and fast classification,” Data Min. Knowl. Discov., vol. 22,
pp. 149-182, 01 2011.

T. Cover and P. Hart, “Nearest neighbor pattern classification,” [EEE
Transactions on Information Theory, vol. 13, no. 1, pp. 21-27, 1967.
Y.-H. Lee, C.-P. Wei, T.-H. Cheng, and C.-T. Yang, “Nearest-neighbor-
based approach to time-series classification,” Decision Support
Systems, vol. 53, no. 1, pp. 207 — 217, 2012. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S01679236 12000097
P. Kidger, J. Morrill, and T. Lyons, “Generalised interpretable shapelets
for irregular time series,” arXiv preprint arXiv:2005.13948, 2020.

Y. Wang, R. Emonet, E. Fromont, S. Malinowski, E. Menager,
L. Mosser, and R. Tavenard, “Learning interpretable shapelets for time
series classification through adversarial regularization,” arXiv preprint
arXiv: 1906.00917, 2019.
UNDER REVIEW

[82]

[83]

[84]

[85]

[86]

[87]

[88]

[89]

[90]

[91]

[92]

[93]

[94]

[95]

[96]

[97]

[98]

[99]

[100]

[101]

Z. Fang, P. Wang, and W. Wang, “Efficient learning interpretable
shapelets for accurate time series classification,” in 2018 IEEE 34th
International Conference on Data Engineering (ICDE), 2018, pp. 497—
508.

G. Li, B. K. K. Choi, J. Xu, S. S. Bhowmick, K. Chun, and G. L.
Wong, “Efficient shapelet discovery for time series classification,” IEEE
Transactions on Knowledge and Data Engineering, pp. 1-1, 2020.

A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom,
P. Southam, and E. Keogh, “The uea multivariate time series classifi-
cation archive, 2018,” arXiv preprint arXiv:1811.00075, 2018.

A. Wolanin, G. Mateo-Garcia, G. Camps-Valls, L. Gémez-Chova,
M. Meroni, G. Duveiller, L. You, and L. Guanter, “Estimating and
understanding crop yields with explainable deep learning in the indian
wheat belt,” Environmental Research Letters, vol. 15, 01 2020.

Y. Hao and H. Cao, “A new attention mechanism to classify
multivariate time series,” in Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intelligence, LICAI-20,
C. Bessiere, Ed. International Joint Conferences on Artificial
Intelligence Organization, 7 2020, pp. 1999-2005, main track.
[Online]. Available: https://doi.org/10.24963/ijcai.2020/277

S. A. Siddiqui, D. Mercier, A. Dengel, and S. Ahmed, “Tsinsight:
A local-global attribution framework for interpretability in time-series
data,” arXiv preprint arXiv:2004.02958, 2020.

Q. Tan, M. Ye, A. J. Ma, B. Yang, T. C. F. Yip, G. L. H. Wong,
and P. C. Yuen, “Explainable uncertainty-aware convolutional recurrent
neural network for irregular medical time series,” IEEE Transactions
on Neural Networks and Learning Systems, pp. 1-15, 2020.

P. Gao, X. Yang, R. Zhang, and K. Huang, “Explainable tensorized
neural ordinary differential equations forarbitrary-step time series pre-
diction,” arXiv preprint arXiv:2011.13174, 2020.

M. Munir, S. A. Siddiqui, F. Kiisters, D. Mercier, A. Dengel,
and S. Ahmed, “Tsxplain: Demystification of dnn decisions for
time-series using natural language and statistical features,” Lecture
Notes in Computer Science, p. 426-439, 2019. [Online]. Available:
http://dx.doi.org/10.1007/978-3-030-30493-5_43

Q. Pan, W. Hu, and J. Zhu, “Series saliency: Temporal interpretation for
multivariate time series forecasting,” arXiv preprint arXiv:2012.09324,
2020.

R. Assaf, L Giurgiu, F. Bagehorn, and A. Schumann, “Mtex-cnn:
Multivariate time series explanations for predictions with convolutional
neural networks,” in 2019 IEEE International Conference on Data
Mining (ICDM), 2019, pp. 952-957.

M. Augustin, A. Meinke, and M. Hein, “Adversarial robustness on in-
and out-distribution improves explainability,” in European Conference
on Computer Vision. Springer, 2020, pp. 228-245.

A. H. Gee, D. Garcia-Olano, J. Ghosh, and D. Paydarfar, “Explaining
deep classification of time-series data with learned prototypes,” arXiv
preprint arXiv:1904.08935, 2019.

A. Kacem, Z. Hammal, M. Daoudi, and J. Cohn, “Detecting depres-
sion severity by interpretable representations of motion dynamics,” in
2018 13th IEEE International Conference on Automatic Face Gesture
Recognition (FG 2018), 2018, pp. 739-745.

A. Hartl, M. Bachl, J. Fabini, and T. Zseby, “Explainability and
adversarial robustness for mns,” 2020 IEEE Sixth International
Conference on Big Data Computing Service and Applications
(BigDataService), Aug 2020. [Online]. Available: http://dx.doi.org/10.
1109/BigDataService49289.2020.00030

E. Ates, B. Aksar, V. J. Leung, and A. K. Coskun, “Counterfactual
explanations for machine learning on multivariate time series data,”
arXiv preprint arXiv:2008.10781, 2020.

A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” arXiv preprint
arXiv: 1706.06083, 2017.

M. Hein, M. Andriushchenko, and J. Bitterwolf, “Why relu networks
yield high-confidence predictions far away from the training data
and how to mitigate the problem,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
41-50.

L Stepin, J. Alonso, A. Catala, and M. Pereira-Farina, “A survey
of contrastive and counterfactual explanation generation methods for
explainable artificial intelligence,” IEEE Access, vol. 9, pp. 11974—
12001, 01 2021.

H. Armout, M. El-Assady, D. Oelke, and D. A. Keim, “Towards a
rigorous evaluation of xai methods on time series,” in 20/9 IEEE/CVF
International Conference on Computer Vision Workshop (ICCVW).
IEEE, 2019, pp. 4197-4201.

[102]

[103]

[104]

[105]

S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Miiller, and
W. Samek, “On pixel-wise explanations for non-linear classifier deci-
sions by layer-wise relevance propagation,” PloS one, vol. 10, no. 7,
p. e0130140, 2015.

Y. Yang, V. Tresp, M. Wunderle, and P. A. Fasching, “Explaining
therapy predictions with layer-wise relevance propagation in neural
networks,” in 20/8 IEEE International Conference on Healthcare
Informatics (ICHI), 2018, pp. 152-162.

H. Suresh, N. Hunt, A. Johnson, L. A. Celi, P. Szolovits, and M. Ghas-
semi, “Clinical intervention prediction and understanding using deep
networks,” arXiv preprint arXiv:1705.08498, 2017.

A. Binder, G. Montavon, S. Lapuschkin, K.-R. Miiller, and W. Samek,
“Layer-wise relevance propagation for neural networks with local
renormalization layers,” in International Conference on Artificial Neu-
ral Networks. Springer, 2016, pp. 63-71.
1708.08296v1 [cs.AI] 28 Aug 2017

arXiv

EXPLAINABLE ARTIFICIAL INTELLIGENCE: UNDERSTANDING,
VISUALIZING AND INTERPRETING DEEP LEARNING MODELS

Wojciech Samek', Thomas Wiegand”, Klaus-Robert Miiller?**4

‘Dept. of Video Coding & Analytics, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany
2Dept. of Computer Science,Technische Universitat Berlin, 10587 Berlin, Germany
3Dept. of Brain & Cognitive Engineering, Korea University, Seoul 136-713, South Korea
“Max Planck Institute for Informatics, Saarbriicken 66123, Germany

ABSTRACT

With the availability of large databases and recent improve-
ments in deep learning methodology, the performance of AI
systems is reaching or even exceeding the human level on an
increasing number of complex tasks. Impressive examples
of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or
strategic game playing. However, because of their nested
non-linear structure, these highly successful machine learn-
ing and artificial intelligence models are usually applied in
a black box manner, i.e., no information is provided about
what exactly makes them arrive at their predictions. Since
this lack of transparency can be a major drawback, e.g., in
medical applications, the development of methods for visual-
izing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summa-
rizes recent developments in this field and makes a plea for
more interpretability in artificial intelligence. Furthermore,
it presents two approaches to explaining predictions of deep
learning models, one method which computes the sensitiv-
ity of the prediction with respect to changes in the input and
one approach which meaningfully decomposes the decision
in terms of the input variables. These methods are evaluated
on three classification tasks.

Index Terms— Artificial intelligence, deep neural networks,
black box models, interpretability, sensitivity analysis, layer-
wise relevance propagation

1. INTRODUCTION

The field of machine learning and artificial intelligence has
progressed over the last decades. A driving force for this
development were earlier improvements in support vector
machines and more recent improvements in deep learning
methodology [22]. Also the availability of large databases
such as ImageNet [9] or Sports1M [17], the speed-up gains
obtained with powerful GPU cards and the high flexibility of
software frameworks such as Caffe [15] or TensorFlow [1]

This work was supported by the German Ministry for Education and Re-
search as Berlin Big Data Center BBDC (011S14013A). We thank Grégore
Montavon for his valuable comments on the paper.

were crucial factors to success. Today’s machine learning-
based AI systems excel in a number of complex tasks ranging
from the detection of objects in images [14] and the under-
standing of natural languages [8] to the processing of speech
signals [10]. On top of that, recent AI! systems can even out-
play professional human players in difficult strategic games
such as Go [34] and Texas hold’em poker [28]. These im-
mense successes of AI systems, especially deep learning
models, show the revolutionary character of this technology,
which will have a large impact beyond the academic world
and will also give rise to disruptive changes in industries and
societies.

However, although these models reach impressive predic-
tion accuracies, their nested non-linear structure makes them
highly non-transparent, i.e., it is not clear what information
in the input data makes them actually arrive at their decisions.
Therefore these models are typically regarded as black boxes.
The 37th move in the second game of the historic Go match
between Lee Sedol, a top Go player, and AlphaGo, an artifi-
cial intelligence system built by DeepMind, demonstrates the
non-transparency of the AI system. AlphaGo played a move
which was totally unexpected and which was commented on
by a Go expert in the following way:

“It’s not a human move. I’ve never seen a
human play this move.” (Fan Hui, 2016).

Although during the match it was unclear why the system
played this move, it was the deciding move for AlphaGo to
win the game. In this case the black box character of the
AlphaGo did not matter, but in many applications the impos-
sibility of understanding and validating the decision process
of an AI system is a clear drawback. For instance, in medical
diagnosis it would be irresponsible to trust predictions of a
black box system by default. Instead every far reaching de-
cision should be made accessible for appropriate validation
by a human expert. Also in self-driving cars, where a sin-
gle incorrect prediction can be very costly, the reliance of the
model on the right features must be guaranteed. The use of
explainable and human interpretable AI models is a prereq-
uisite for providing such a guarantee. More discussion on the

'The terms artificial intelligence and machine learning are used synony-
mously.
necessity of explainable AI can be found in Section 2.

Not surprisingly, the development of techniques for “open-
ing” black box models has recently received a lot of attention
in the community [6, 35, 39, 5, 33, 25, 23, 30, 40, 11, 27].
This includes the development of methods which help to bet-
ter understand what the model has learned (i.e., it’s represen-
tation) [12, 24, 29] as well as techniques for explaining indi-
vidual predictions [19, 35, 39, 5, 26]. A tutorial on methods
from these two categories can be found in [27]. Note that ex-
plainability is also important for support vector machines and
other advanced machine learning techniques beyond neural
networks [20].

The main goal of this paper is to foster awareness for the
necessity of explainability in machine learning and artificial
intelligence. This is done in Section 2. After that in Section 3
we present two recent techniques, namely sensitivity analysis
(SA) [6, 35] and layer-wise relevance propagation (LRP) [5],
for explaining the individual predictions of an AI model in
terms of input variables. The question of how to objectively
evaluate the quality of explanations is addressed in Section 4
and results from image, text and video classification experi-
ments are presented in Section 5. The paper concludes with
an outlook on future work in Section 6.

2. WHY DO WE NEED EXPLAINABLE AI ?

The ability to explain the rationale behind one’s decisions to
other people is an important aspect of human intelligence.
It is not only important in social interactions, e.g., a person
who never reveals one’s intentions and thoughts will be most
probably regarded as a “‘strange fellow”, but it is also crucial
in educational context, where students aim to comprehend
the reasoning of their teachers. Furthermore, the explanation
of one’s decisions is often a prerequisite for establishing a
trust relationship between people, e.g., when a medical doc-
tor explains the therapy decision to his patient.

Although these social aspects may be of less importance for
technical AI systems, there are many arguments in favor of
explainability in artificial intelligence. Here are the most im-
portant ones:

e Verification of the system: As mentioned before in
many applications one must not trust a black box sys-
tem by default. For instance, in health care the use
of models which can be interpreted and verified by
medical experts is an absolute necessity. The authors
of [7] show an example from this domain, where an
AI system which was trained to predict the pneumo-
nia risk of a person arrives at totally wrong conclu-
sions. The application of this model in a black box
manner would not reduce but rather increase the num-
ber of pneumonia-related deaths. In short, the model
learns that asthmatic patients with heart problems have
a much lower risk of dying of pneumonia than healthy
persons. A medical doctor would immediately rec-
ognize that this can not be true as asthma and hearth

problems are factors which negatively affect the prog-
nosis for recovery. However, the AI model does not
know anything about asthma or pneumonia, it just in-
fers from data. In this example, the data were system-
atically biased, because in contrast to healthy persons
the majority of asthma and heart patients were under
strict medical supervision. Because of that supervi-
sion and the increased sensitivity of these patients, this
group has a significant lower risk of dying of pneu-
monia. However, this correlation does not have causal
character and therefore should not be taken as basis for
the decision on pneumonia therapy.

Improvement of the system: The first step towards
improving an AI system is to understand it’s weak-
nesses. Obviously, it’s more difficult to perform such
weakness analysis on black box models than on mod-
els which are interpretable. Also detecting biases in
the model or the dataset (as in the pneumonia exam-
ple) is easier if one understands what the model is do-
ing and why it arrives at it’s predictions. Furthermore,
model interpretability can be helpful when comparing
different models or architectures. For instance, the au-
thors of [20, 2, 3] observed that models may have the
same classification performance, but largely differ in
terms of what features they use as the basis for their
decisions. These works demonstrate that the identi-
fication of the most “appropriate” model requires ex-
plainability. One can even claim that the better we
understand what our models are doing (and why they
sometimes fail), the easier it becomes to improve them.

Learning from the system: Because today’s AI sys-
tems are trained with Millions of examples, they may
observe patterns in the data which are not accessible
to humans, who are only capable of learning with a
limited number of examples. When using explainable
AI systems we can try to extract this distilled knowl-
edge from the AI system in order to acquire new in-
sights. One example of such knowledge transfer from
AI system to human was mentioned by Fan Hui in
the quote above. The AI system identifies new strate-
gies to play Go, which certainly now have also been
adapted by professional human players. Another do-
main where information extraction from the model can
be crucial are the sciences. To put it simple, physi-
cists, chemists and biologists are rather interested in
identifying the hidden laws of nature than just predict-
ing some quantity with black box models. Thus, only
models which are explainable are useful in this domain
(c-f., [37, 32]).

Compliance to legislation: AI systems are affecting
more and more areas of our daily life. With that also le-
gal aspects, e.g., the assignment of responsibility when
the systems makes a wrong decision, have recently re-
ceived increased attention. Since it may be impossible
to find satisfactory answers for these legal questions
when relying on black box models, future AI systems
will necessarily have to become more explainable. An-
other example where regulations may become a driv-
ing force for more explainability in artificial intelli-
gence are individual rights. Persons immediately af-
fected by decisions of an AI system (e.g., persons re-
jected for loan by the bank) may want to know why the
systems has decided in this way. Only explainable AI
systems will provide this information. These concerns
brought the European Union to adapt new regulations
which implement a “right to explanation” whereby a
user can ask for an explanation of an algorithmic deci-
sion that was made about her or him [13].

These examples demonstrate that explainability is not only
of important and topical academic interest, but it will play a
pivotal role in future AI systems.

3. METHODS FOR VISUALIZING, INTERPRETING
AND EXPLAINING DEEP LEARNING MODELS

This section introduces two popular techniques for explain-
ing predictions of deep learning models. The process of ex-
planation is summarized in Fig. 1. First, the system correctly
classifies the input image as “rooster”. Then, an explanation
method is applied to explain the prediction in terms of input
variables. The result of this explanation process is a heatmap
visualizing the importance of each pixel for the prediction. In
this example the rooster’s red comb and wattle are the basis
for the AI system’s decision.

3.1. Sensitivity Analysis

The first method is known as sensitivity analysis (SA) [6, 35]
and explains a prediction based on the model’s locally evalu-
ated gradient (partial derivative). Mathematically, sensitivity
analysis quantifies the importance of each input variable i
(e.g., image pixel) as

Ri = | F100)

This measure assumes that the most relevant input features
are those to which the output is most sensitive. In contrast
to the approach presented in the next subsection, sensitivity
analysis does not explain the function value f(x) itself, but
rather a variation of it. The following example illustrates
why measuring the sensitivity of the function may be subop-
timal for explaining predictions of AI systems.

A heatmap computed with sensitivity analysis indicates
which pixels need to changed to make the image look (from
the AI system’s perspective) more / less like the predicted
class. For instance, in the example shown in Fig. 1 these
pixels would be the yellow flowers which occlude part of the
rooster. Changing these pixels in a specific way would recon-
struct the occluded parts of the rooster, which most probably
would also increase the classification score, because more
of the rooster would be visible in the image. Note that such
heatmap would not indicate which pixels are actually pivotal

for the prediction “rooster”. The presence of yellow flowers
is certainly not indicative of the presence of a rooster in the
image. Because of this property SA does not perform well in
the quantitative evaluation experiments presented in Section
5. More discussion on the drawbacks of sensitivity analysis
can be found in [27].

3.2. Layer-Wise Relevance Propagation

In the following we provide a general framework for de-
composing predictions of modern AI systems, e.g., feed-
forwards neural networks and bag-of-words models [5],
long-short term memory (LSTM) networks [4] and Fisher
Vector classifiers [20], in terms of input variables. In con-
trast to sensitivity analysis, this method explains predictions
relative to the state of maximum uncertainty, ie., it iden-
tifies pixels which are pivotal for the prediction “rooster”.
Recent work [26] also shows close relations to Taylor de-
composition, which is a general function analysis tool in
mathematics.

A recent technique called Layer-wise relevance propagation
(LRP) [5] explains the classifier’s decisions by decompo-
sition. Mathematically, it redistributes the prediction f(x)
backwards using local redistribution rules until it assigns a
relevance score #, to each input variable (e.g., image pixel).
The key property of this redistribution process is referred to
as relevance conservation and can be summarized as

SOR =...=SOR =) R=... =f(x) W
a j k

This property says that at every step of the redistribution pro-
cess (e.g., at every layer of a deep neural network), the total
amount of relevance (i.e., the prediction f(x)) is conserved.
No relevance is artificially added or removed during redistri-
bution. The relevance scores R,; of each input variable de-
termines how much this variable has contributed to the pre-
diction. Thus, in contrast to sensitivity analysis, LRP truly
decomposes the function value f(x).

In the following we describe the LRP redistribution process
for feed-forward neural networks, redistribution procedures
have also been proposed for other popular models [5, 4, 20].

Let x; be the neuron activations at layer /, R;, be the rele-
vance scores associated to the neurons at layer / + 1 and w;;,
be the weight connecting neuron 7 to neuron &. The simple
LRP rule redistributes relevance from layer /+ 1 to layer / in
the following way:

y= Lanne re “itt _Ry 2)

UpWik +E

where a small stabilization term € is added to prevent division
by zero. Intuitively, this rule redistributes relevance propor-
tionally from layer / + 1 to each neuron in layer / based on
two criteria, namely (i) the neuron activation x;, i.e., more
activated neurons receive a larger share of relevance, and (ii)
the strength of the connection wx, i.e., more relevance flows
through more prominent connections. Note that relevance
conservation holds for ¢ = 0.
classify image

Black Box

PU ta)

input @

 

\os
oa\o9

 

 

 

 

heatmap

Al system's decision is
based on these pixels

Why explainability ?

Verify predictions

‘Identify flaws and biases
: Learn about the problem
' | Ensure compliance to legislation

we oxplain prediction prediction

 

—-» | Rooster

 

 

 

prediction f(a)

Explanation methods

t .

1

i Dik, = f(a)
! (how much does each pixel

' contribute to prediction)

 

 

 

 

£f(a)||

(how much do changes in each
pixel affect the prediction)

Fig. 1. Explaining predictions of an AI system. The input image is correctly classified as “rooster”. In order to understand
why the system has arrived at this decision, explanation methods such as SA or LRP are applied. The result of this explanation
is an image, the heatmap, which visualizes the importance of each pixel for the prediction. In this example the rooster’s red
comb and wattle are the basis for the AI system’s decision. With the heatmap one can verify that the AI system works as

intended.

The “alpha-beta” rule is an alternative redistributes rule in-

troduced in [5]:
(xjwje)t (wjwjr)

where ()* and ()~ denote the positive and negative parts,
respectively. The conservation of relevance is enforced by
an additional constraint a — G@ = 1. For the special case
qa = 1, the authors of [26] showed that this redistribution rule
coincides with a “deep Taylor decomposition” of the neural
network function when the neural network is composed of
ReLU neurons.

)Re @)

3.3. Software

The LRP toolbox [21] provides a python and matlab imple-
mentation of the method as well as an integration into pop-
ular frameworks such as Caffe and TensorFlow. With this
toolbox one can directly applied LRP to other peoples’ mod-
els. The toolbox code, online demonstrators and further in-
formation can be found on www.explain-ai.org.

4. EVALUATING THE QUALITY
OF EXPLANATIONS

In order to compare heatmaps produced by different expla-
nation methods, e.g., SA and LRP, one needs an objective
measure of the quality of explanations. The authors of [31]

proposed such a quality measure based on perturbation anal-
ysis. The method is based on the following three ideas:

e The perturbation of input variables which are highly
important for the prediction leads to a steeper decline
of the prediction score than the perturbation of input
dimensions which are of lesser importance.

e Explanation methods such as SA and LRP provide a
score for every input variable. Thus, the input vari-
ables can be sorted according to this relevance score.

One can iteratively perturb input variables (starting
from the most relevant ones) and track the predic-
tion score after every perturbation step. The average
decline of the prediction score (or the decline of the
prediction accuracy) can be used as an objective mea-
sure of explanation quality, because a large decline
indicates that the explanation method was successful
in identifying the truly relevant input variables.

In the following evaluation we use model-independent per-

turbations (e.g., replacing the input values by random sample
from uniform distribution) in order to avoid biases.

5. EXPERIMENTAL EVALUATION

This section evaluates SA and LRP on three different prob-
lems, namely the annotation of images, the classification
of text documents and the recognition of human actions in
videos.

5.1. Image Classification

In the first experiment we use the GoogleNet model [38], a
state-of-the art deep neural network, to classify general ob-
jects from the ILSVRC2012 [9] dataset.

Fig. 2 (A) shows two images from this dataset, which have
been correctly classified as “volcano” and “coffee cup”, re-
spectively. The heatmaps visualize the explanations obtained
with SA and LRP. The LRP heatmap of the coffee cup im-
age shows that the model has identified the ellipsoidal shape
of the cup to be a relevant feature for this image category.
In the other example, the particular shape of the mountain
is regarded as evidence for the presence of a volcano in the
image. The SA heatmaps are much noisier than the ones
computed with LRP and large values A; are assigned to re-
gions consisting of pure background, e.g., the sky, although
these pixels are not really indicative for image category “vol-
cano”’. In contrast to LRP, SA does not indicate how much
every pixel contributes to the prediction, but it rather mea-
sures the sensitivity of the classifier to changes in the input.
Therefore, LRP produces subjectively better explanations of
the model’s predictions than SA.

The lower part of Fig. 2 (A) displays the results of the per-
turbation analysis introduced in Section 4. The y-axis shows
the relative decrease of the prediction score average over the
first 5040 images of the ILSVRC2012 dataset, i.e., a value of
0.8 means that the original scores decreased on average by
20%. At every perturbation step a 9x9 patch of the image
(selected according to SA or LRP scores) is replaced by ran-
dom values sampled from an uniform distribution. Since the
prediction score decrease is much faster when perturbing the
images using LRP heatmaps than when using SA heatmaps,
LRP also objectively provides better explanations than SA.

More discussion on this image classification experiment can
be found in [31].

5.2. Text Document Classification

In this experiment a word-embedding based convolutional
neural network was trained to classify text documents from
the 20Newsgroup dataset’.

Fig. 2 (B) shows SA and LRP heatmaps (e.g., a relevance
score R, is assigned to every word) overlayed on top of a
document, which was classified as topic “sci.med”, i.e., the
text is assumed to be about a medical topic. Both expla-
nation methods, SA and LRP, indicate that words such as
“sickness”, “body” or “discomfort” are the basis for this
classification decision. In contrast to sensitivity analysis,
LRP distinguishes between positive (red) and negative (blue)
words, i.e., words which support the classification decision
“sci.med” and words which are in contradiction, i.e., speak
for another category (e.g.,“sci.space”). Obviously, words

*http://qwone.com/~ jason/20Newsgroups

07 46,

such as “ride”, “astronaut” and “Shuttle” strongly speak for
the topic space, but not necessarily for the topic medicine.
With the LRP heatmap we can see that although the classifier
decides for the correct “sci.med” class, there is evidence in
the text which contradicts this decision. The SA method
does not distinguish between positive and negative evidence.

The lower part of the figure shows the result of the quantita-
tive evaluation. The y-axis displays the relative decrease of
the prediction accuracy over 4154 documents of the 20News-
group dataset. At every perturbation step the most important
words (according to SA or LRP score) are deleted by setting
the corresponding input values to 0. Also this result confirms
quantitatively that LRP provides more informative heatmaps
than SA, because these heatmaps lead to a larger decrease in
classification accuracy compared to SA heatmaps.

More discussion on this text document classification experi-
ment can be found in [3].

5.3. Human Action Recognition in Videos

The last examples demonstrates the explanation of a Fisher
Vector / SVM classifier [16], which was trained for predict-
ing human actions from compressed videos. In order to re-
duce computational costs, the classifier was trained on block-
wise motion vectors (not individual pixels). The evaluation
is performed on the HMDBS51 dataset [18].

Fig. 2 (C) shows LRP heatmaps overlayed onto five exemplar
frames of a video sample. The video was correctly classified
as showing the action “sit-up”. One can see that the model
mainly focuses on the blocks surrounding the upper body of
the person. This makes perfectly sense, as this part of the
video frame shows motion which is indicative of the action
“sit-up”, namely upward and downward movements of the
body.

The curve at the bottom of Fig. 2 (C) displays the distribution
of relevance over (four consecutive) frames. One can see that
the relevance scores are larger for frames in which the person
is performing an upwards and downwards movement. Thus,
LRP heatmaps not only visualizes the relevant locations of
the action within a video frame (i.e., where relevant action
happens), but it also identifies the most relevant time points
within a video sequence (i.e., when relevant action happens).

More discussion on this experiment can be found in [36].

6. CONCLUSION

This paper approached the problem of explainability in arti-
ficial intelligence. It was discussed why black box models
are not acceptable for certain applications, e.g., in the medi-
cal domain where wrong decisions of the system can be very
harmful. Furthermore, explainability was presented as pre-
requisite for solving legal questions which are arising with
the increased usage of AI systems, e.g., how to assign re-
sponsibility in case of system failure. Since the “right to ex-
planation” has become part of the European law, it can be
(A) Image classification
Explaining predictions: "Volcano", "Coffe Cup”

SA LRP

SA

 

Quantitave comparison of SA and LRP

1.0\«___ Original prediction score
(no perturbations)

  
    

Normalized
prediction score

LRP

 

10 20 30

Number of perturbations

40 50

LRP

(B) Text document classification

Explaining prediction: "sci.med"

 

It is the body's reaction to a strange environment. It appears to be induced
partly to physical and part to mental distress. Some people are
more prone to it than others, like some people are more prone to get sick

on a roller coaster ride than others. The mental part is usually induced by

a lack of clear indication of which way is up or down, ie: the Shuttle is
normally oriented with its cargo bay pointed towards Earth, so the Earth

(or ground) is "above" the head of the astronauts. About 50% of the astronauts
experience some form of motion Sickness, and NASA has done numerous tests in
space to try to see how to keep the number of occurances down.

 

 

It is the Body's reaction to a strange environment. It appears to be induced
partly to physical and part to mental distress. Some people are
more prone to it than others, like some people are more prone to get sick

on a roller coaster than others. The mental part is usually induced by

a lack of clear indication of which way is up or down, ie: the Shuttle is
normally oriented with its cargo bay pointed towards Earth, so the Earth

(or ground) is “above” the head of the astronauts. About 50% of the astronauts
experience some form of motion sickness, and NASA has done numerous tests in
Space to try to see how to keep the number of occurances down.

 

 

 

Quantitave comparison of SA and LRP

1.0 a

Original accuracy
(no deletions)

> 08

o

g

s

3 06

xt

°
BR

 

 

10

20 30
Number of word deletions

40 50

(C) Human action recognition in videos

 

LRP relevances
per frame

Explaining prediction:
" a 5

“sit-up"

   

 

60

Video frame

Fig. 2. Explaining predictions of AI systems. (A) shows the application of explainable methods to image classification. The
SA heatmaps are noisy and difficult to interpret, whereas LRP heatmaps match human intuition. (B) shows the application
of explainable methods to text document classification. The SA and LRP heatmaps identify words such as “discomfort”,
“body” and “sickness” as the relevant ones for explaining the prediction “sci.med”. In contrast to sensitivity analysis, LRP
distinguishes between positive (red) and negative (blue) relevances. (C) shows explanations for a human action recognition
classifier based on motion vector features. The LRP heatmaps of a video which was classified as “sit-up” show increased

relevance on frames in which the person is performing an upwards and downwards movement.

expected that it will also greatly foster explainability in AI
systems.

Besides being a gateway between AI and society, explain-
ability is also a powerful tool for detecting flaws in the model
and biases in the data, for verifying predictions, for improv-
ing models, and finally for gaining new insights into the
problem at hand (e.g., in the sciences).

In future work we will investigate the theoretical founda-
tions of explainability, in particular the connection between
post-hoc explainability, i.e., a trained model is given and the
goal is to explain it’s predictions, and explainability which
is incorporated directly into the structure of the model. Fur-
thermore, we will study new ways to better understand the
learned representation, especially the relation between gener-
alizability, compactness and explainability. Finally, we will

apply explaining methods such as LRP to new domains, e.g.,
communications, and search for applications of these meth-
ods beyond the ones described in this paper.

7. REFERENCES

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv
preprint arXiv: 1603.04467, 2016.

[2] L. Arras, F Horn, G. Montavon, K.-R. Miiller, and
W. Samek. Explaining predictions of non-linear classi-
fiers in nlp. In Proceedings of the 1st Workshop on Rep-
resentation Learning for NLP, pages 1-7. ACL, 2016.
[3]

[4

a

[5

a

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

L. Arras, F Horn, G. Montavon, K.-R. Miiller, and
W. Samek. ”What is relevant in a text document?”: An
interpretable machine learning approach. PLoS ONE,
12(8):e0181142, 2017.

L. Arras, G. Montavon, K.-R. Miiller, and W. Samek.
Explaining recurrent neural network predictions in sen-
timent analysis. In Proceedings of the EMNLP’17
Workshop on Computational Approaches to Subjectiv-
ity, Sentiment & Social Media Analysis (WASSA), pages
1-10, 2017.

S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.
Miiller, and W. Samek. On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance
propagation. PLoS ONE, 10(7):e0130140, 2015.

D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-
abe, K. Hansen, and K.-R. Miiller. How to explain in-
dividual classification decisions. Journal of Machine
Learning Research, 11:1803-1831, 2010.

R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and
N. Elhadad. Intelligible models for healthcare: Predict-
ing pneumonia risk and hospital 30-day readmission.
In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pages 1721-1730, 2015.

K. Cho, B. Van Merriénboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Bengio. Learn-
ing phrase representations using RNN encoder-decoder
for statistical machine translation. arXiv preprint
arXiv: 1406.1078, 2014.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
pages 248-255, 2009.

L. Deng, G. Hinton, and B. Kingsbury. New types
of deep neural network learning for speech recognition
and related applications: An overview. In IEEE Inter-

national Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 8599-8603, 2013.

F. Doshi-Velez and B. Kim. Towards a rigorous sci-
ence of interpretable machine learning. arXiv preprint
arXiv: 1702.08608, 2017.

D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Vi-
sualizing higher-layer features of a deep network. Tech-
nical Report 1341, University of Montreal, 2009.

B. Goodman and S. Flaxman. European union regu-
lations on algorithmic decision-making and a "right to
explanation”. arXiv preprint arXiv: 1606.08813, 2016.

K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-
ual learning for image recognition. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770-778, 2016.

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe:
Convolutional architecture for fast feature embedding.
In Proceedings of the 22nd ACM international confer-
ence on Multimedia, pages 675-678, 2014.

V. Kantorov and I. Laptev. Efficient feature extrac-
tion, encoding and classification for action recognition.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 2593-
2600, 2014.

A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Suk-
thankar, and L. Fei-Fei. Large-scale video classification
with convolutional neural networks. In Proceedings of

the IEEE conference on Computer Vision and Pattern
Recognition (CVPR), pages 1725-1732, 2014.

H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and
T. Serre. Hmdb: a large video database for human mo-
tion recognition. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision (ICCV), pages
2556-2563. IEEE, 2011.

W. Landecker, M. D. Thomure, L. M. A. Bettencourt,
M. Mitchell, G. T. Kenyon, and S. P. Brumby. Interpret-
ing individual classifications of hierarchical networks.
In Proceedings of the LEEE Symposium on Computa-
tional Intelligence and Data Mining (CIDM), pages 32-
38, 2013.

S. Lapuschkin, A. Binder, G. Montavon, K.-R. Miiller,
and W. Samek. Analyzing classifiers: Fisher vectors
and deep neural networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2912-2920, 2016.

S. Lapuschkin, A. Binder, G. Montavon, K.-R. Miiller,
and W. Samek. The layer-wise relevance propagation
toolbox for artificial neural networks. Journal of Ma-
chine Learning Research, 17(114):1-5, 2016.

Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Miiller.
Efficient backprop. In Neural networks: Tricks of the
trade, pages 9-48. Springer, 2012.

Z. C. Lipton. The mythos of model interpretability.
arXiv preprint arXiv: 1606.03490, 2016.

A. Mahendran and A. Vedaldi. Understanding deep im-
age representations by inverting them. In Proceedings
of the IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 5188-5196, 2015.

A. Mahendran and A. Vedaldi. Visualizing deep convo-
lutional neural networks using natural pre-images. [n-
ternational Journal of Computer Vision, 120(3):233-
255, 2016.

G. Montavon, S. Bach, A. Binder, W. Samek, and K.-
R. Miiller. Explaining nonlinear classification decisions
with deep taylor decomposition. Pattern Recognition,
65:211-222, 2017.
[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

G. Montavon, W. Samek, and K.-R. Miiller. Meth-
ods for interpreting and understanding deep neural net-
works. arXiv preprint arXiv:1706.07979, 2017.

M. Moravéik, M. Schmid, N. Burch, V. Lisy, D. Mor-
rill, N. Bard, et al. Deepstack: Expert-level artifi-

cial intelligence in heads-up no-limit poker. Science,
356(6337):508-513, 2017.

A. Nguyen, J. Yosinski, and J. Clune. Multifaceted
feature visualization: Uncovering the different types
of features learned by each neuron in deep neural net-
works. arXiv preprint arXiv: 1602.03616, 2016.

M. T. Ribeiro, S. Singh, and C. Guestrin. Why should
i trust you?: Explaining the predictions of any classi-
fier. In Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pages 1135-1144. ACM, 2016.

W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and
K.-R. Miiller. Evaluating the visualization of what a
deep neural network has learned. [EEE Transactions
on Neural Networks and Learning Systems, 2017. in
press.

K. T. Schiitt, F Arbabzadah, S. Chmiela, K. R. Miiller,
and A. Tkatchenko. Quantum-chemical insights from

deep tensor neural networks. Nature communications,
8:13890, 2017.

A. Shrikumar, P. Greenside, A. Shcherbina, and
A. Kundaje. Not just a black box: Learning im-
portant features through propagating activation differ-
ences. arXiv preprint arXiv: 1605.01713, 2016.

D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,
G. Van Den Driessche, et al. Mastering the game of

go with deep neural networks and tree search. Nature,
529(7587):484489, 2016.

K. Simonyan, A. Vedaldi, and A. Zisserman. Deep in-
side convolutional networks: Visualising image clas-
sification models and saliency maps. arXiv preprint
arXiv: 1312.6034, 2013.

V. Srinivasan, S. Lapuschkin, C. Hellge, K.-R. Miiller,
and W. Samek. Interpretable human action recognition
in compressed domain. In Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP), pages 1692-1696, 2017.

I. Sturm, S. Lapuschkin, W. Samek, and K.-R. Miiller.
Interpretable deep neural networks for single-trial eeg
classification. Journal of Neuroscience Methods,
274:141-145, 2016.

C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
novich. Going deeper with convolutions. In Proceed-
ings of the LEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 1-9, 2015.

[39] M. D. Zeiler and R. Fergus. Visualizing and under-

standing convolutional networks. In European Confer-
ence Computer Vision - ECCV 2014, pages 818-833,
2014.

[40] L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling.

Visualizing deep neural network decisions: Prediction
difference analysis. arXiv preprint arXiv: 1702.04595,
2017.
IEEE Access:

Multidiscipinary Rapid Review £ Open Access Journal

 

Received December 16, 2020, accepted January 3, 2021, date of publication January 13, 2021, date of current version January 22, 2021.

Digital Object Identifier 10.1109/ACCESS.2021.3051315

A Survey of Contrastive and Counterfactual
Explanation Generation Methods for Explainable

Artificial Intelligence

ILIA STEPIN™’, JOSE M. ALONSO™", (Member, IEEE), ALEIANDRO CATALA™!,

AND MARTIN PEREIRA-FARINA”2

1 Centro Singular de Investigacién en Tecnoloxfas Intelixentes (CITIUS), Universidade de Santiago de Compostela, 15782 Santiago de Compostela, Spain
Departamento de Filosofia e Antropoloxia, Universidade de Santiago de Compostela, 15705 Santiago de Compostela, Spain

Corresponding author: Ilia Stepin (ilia.stepin @usc.es)}

This work was supported in part by the Spanish Ministry of Science, Innovation and Universities under Grant RTI2018-099646-B-100 and
Grant RED2018-102641-T, in part by the Galician Ministry of Education, University and Professional Training under Grant ED431F
2018/02, Grant ED431C 2018/29, Grant ED431G/08, and Grant ED431G2019/04; and in part by the European Regional Development

Fund (ERDF/FEDER Program).

ABSTRACT A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions.
To disclose the reasoning behind such algorithms, their output can be explained by means of so-called
evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify
why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial
importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation
and the corresponding computational frameworks. In this work we conduct a systematic literature review
which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under
study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation.
Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation
generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected
theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and
reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and
practical approaches to contrastive and counterfactual explanation.

INDEX TERMS Computational intelligence, contrastive explanations, counterfactuals, explainable artificial

intelligence, systematic literature review.

L_ INTRODUCTION

In the last few decades, the field of Artificial Intelligence
(AD has witnessed major changes. As available computa-
tional resources have grown significantly, AI algorithms are
attracting a significant amount of attention in industry and
research [1]. While a great number of such algorithms present
strikingly accurate decisions, their decision-making appara-
tus is frequently left unclear to users of such applications.
In particular, a number of Machine Learning (ML)-based
algorithms are often perceived as “black-box” algorithms
because they are overloaded with millions of hardly inter-
pretable parameters to be optimized at the training stage. This
fact makes the algorithm’s output hard to explain. A lack of

The associate editor coordinating the review of this manuscript and
approving it for publication was Francesco Piccialli.

the ability to explain such automatic decisions undermines
users’ trust and hence decreases usability of such systems [2].
Furthermore, it prevents users from a responsible exploita-
tion of their decisions [3]. In addition, many of the existing
eXplainable AI (XAI!) methods provide summaries of auto-
matically made predictions rather than true explanations [4].
As a result, the need to motivate automatic decisions with a
clear explanation of why the algorithm outputs a particular
decision has made the XAI research field grow quickly [5].
Since the number of high-stakes AI applications found
in daily life increases, the requirements to their explana-

'XAlI stands for eXplainable Artificial Intelligence. This acronym was
made popular by the USA Defense Advanced Research Projects Agency
when launching to the research community the challenge of designing
self-explanatory AI systems (https://www.darpa.mil/program/
explainable-artificial-intelligence).

This work is licensed under a Creative Commons Attribution-NonCommercial-No Derivatives 4.0 License.

11974 For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

tory capacity increase accordingly. This also provokes the
introduction of regulations and laws concerned with expla-
nation requirements for AI-based applications. For instance,
the need for explaining reasoning mechanisms behind such
applications is now legally regulated in the European Union
by means of the General Data Protection Regulation.”
According to these legal provisions, the data subject must
be provided with “meaningful information about the logic
involved” in the automatic decision making process, which is
commonly referred to as the “‘right to explanation’’ [6]. Thus,
an AI application is expected not only to provide accurate
decisions but also to justify them in a comprehensive manner
to end-users.

The goal of approaching human-centric AI has led towards
a deeper research on the nature of explanation. However,
no agreement about a definition of explanation has been
reached despite the fact that explanation has called a signif-
icant amount of attention in, e.g., philosophy of science [7],
[8]. In its most general form, explanation is normally treated
as “‘an answer to the question of why something is the case”’
[9]. In the context of AI, it often bases on judgments about
why a certain outcome is predicted by an AI algorithm and
hypotheses about causes with respect to given effects [10].

The need of generating more human-like explanations has
attracted AI researchers’ attention to particular properties of
explanation as well as its sub-types [11]. Thus, it appears
particularly challenging to explain a given algorithm’s output
in terms of reasonable yet non-occurring alternatives given a
possibly infinite set of such options. Furthermore, this can be
enhanced with the ability of suggesting relevant changes in
the input so that the algorithm outputs a different decision.

Given a rising interest towards these types of explanation
(referred to as contrastive and counterfactual, respectively)
within the XAI community, it is of crucial importance to
review the existing theoretical accounts of contrastive and
counterfactual explanation as well as state-of-the-art compu-
tational frameworks for automatic generation thereof. Thus,
the aim of this study is to fulfill the next three objectives:
(1) to scrutinize theoretical works on the contrastive and
counterfactual accounts of explanation; (2) to summarize
state-of-the-art methods in the field of automatic explanation
generation thereof; and (3) to discuss a degree of synergy
between the revised theories and their related up-to-date
implementations.

The rest of the manuscript is organized as follows.
Section II introduces the notions of contrastive and coun-
terfactual explanation as well as their main application
areas. Section III presents the terminology used through-
out the review, poses the research questions, and describes
the methodology employed to address the given questions.
Section IV presents the main findings collected within the
present survey and the emerging taxonomy thereof. Section V
discusses peculiarities of the existing theoretical and compu-

2 https://eur-lex.europa.eu/legal-content/EN/TXT/ ?uri=CELEX:
02016R0679-20160504

VOLUME 9, 2021

tational frameworks of contrastive and counterfactual expla-
nation. Finally, we conclude in Section VI.

ll. BACKGROUND

A. CONTRASTIVE EXPLANATION

Findings on explanation accumulated in humanities and
social sciences show that it is intrinsically contrastive [11].
The property of contrastiveness presupposes that an expla-
nation answers the given why-question regarding the cause
of the event in question (““Why did P happen?’’) in terms of
hypothesized non-occurring alternatives (“Why did P happen
rather than Q?’’) [12]. Thus, supporters of the pragmatic
approach to explanation argue that it is exactly the ability
to distinguish the answer to an explanatory question from
a set of contrastive hypothesized alternatives that provides
the explainee with sufficiently comprehensive information on
the reasoning behind the question [13]. This approach is also
claimed to set a minimum criterion that an explanation must
fulfill: it must favor the probability of the observed event P
to all the hypothetical alternatives (Q), Q2, ..., Qn) [14].

Contrastive explanation is among influential topics in cog-
nitive science [15]-[17]. Thus, contrastive explanations are
claimed to be inherent to human cognition [16]. Indeed,
we are used to question those decisions that we once made,
especially if such decisions or coinciding circumstances
resulted in tragic events [18].

In addition, contrastive reasoning forms the basis of abduc-
tive inference [19], i.e., the process of inferring certain facts
that render some observation plausible [20]. In other words,
a given observation can be explained on the basis of the most
likely among a pool of competing hypotheses [21].

B. COUNTERFACTUAL EXPLANATION

Given the property of contrastiveness, it is possible to imagine
explanatory alternatives to how things would stand if a differ-
ent decision had been made at some point. They can serve to
explain potential consequences of such contrastive non-taken
alternative decisions. In this case, the mind is assumed to
construct and compare mental representations of an actually
happened event and that of some event alternative to it [22].
Cognitive scientists refer to such mental representations of
alternatives to past events as counterfactuals (‘“‘contrary-to-
fact’’) [15]. The process of ‘‘thinking about past possibili-
ties and past or present impossibilities’’ is therefore called
counterfactual thinking [23]. Alternatively, the combination
of imagining an alternative scenario in relation to the one that
actually happened and the exploration of its consequences
is referred to as counterfactual reasoning [24]. In addition,
counterfactual reasoning is claimed to be a key mechanism
for explaining adaptive behavior in a changing environment
[25], [26].

Counterfactuals describe events or states of the world that
did not occur and implicitly or explicitly contradict factual
world knowledge [27]. Formulated in natural language, coun-
terfactuals are usually presented in the form of conditional

11975
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

statements. Broadly speaking, they contain: (1) an antecedent
describing an outcome alternative to an actual event; (2)
a consequent describing (a set of) consequences, had the
antecedent been the case; and (3) a binary counterfactual
dependency relation between them. Thus, Grahne defines
a counterfactual to be a conditional statement where the
antecedent “‘can contradict the current state of affairs, or our
current knowledge thereof’’ [28]. However, despite a gen-
eral agreement on structural properties of counterfactuals,
existing interpretations of counterfactual conditionals still
compete. As such, further constraints imposed on their struc-
ture differ depending on the approach adopted. According to
Ginsberg [29], a counterfactual is a conditional statement of
the form “If P, then Q” where P is “expected to be false’’.
Aumann limits a counterfactual to be a conditional with a
false antecedent only [30]. In contrast, Spohn argues that both
the antecedent and the consequent of a counterfactual must
be false [31]. All in all, counterfactual conditional statements
are claimed to enable people to produce utterances that are
factually false yet truthful irrespective of the interpretation
adopted [32].

A line of research devoted to modeling human counterfac-
tual reasoning has been thoroughly investigated in computer
science. Thus, counterfactual reasoning in computer science
is defined as the process of evaluating conditional claims
about alternative possibilities and their consequences [33].
It is argued to be valid arising from antecedents that are
true in a hypothetical model but false in reality [34]. In this
setting, the truth of a counterfactually inferred statement is
resolved by: (1) modeling a situation where the smallest
possible change in features of the actual world (as set in the
antecedent) leads to a different (possibly, desired) state of
things (the so-called ‘‘closest’’ or “‘nearest”’ possible world);
and (2) estimating what is true in that setting [35].

Moreover, counterfactuality is among the most fundamen-
tal concepts in theories of causation [36], [37]. Indeed, coun-
terfactuals are argued to represent a causal relation between
the event happened in reality and its imaginary counterpart.
A counterfactual definition of a cause of an arbitrary event
traces back to Hume [38]. According to him, a cause is
an object (antecedent) that justifies the existence of another
object (consequent) which it is followed by: “If the first
object had not been, then the second never had existed”’.
Therefore, once a causal connection between the antecedent
and the consequent is established, a counterfactual condi-
tional can be generalized to be a conditional claim about an
alternate possibility and its consequences of the form “If X
were to occur, then Y would (or might) occur” [33]. Similarly,
Kment applies a similarity-based approach between possible
worlds to formulate a general account of counterfactuals [39]
driven by a non-epistemic interpretation of explanation (i.e.,
factors that serve as reasons for some fact to obtain are
responsible for that fact).

The conditional structure of counterfactual statements gave
rise to a probabilistic account of such statements. Thus, Pearl
extended the definition of the causal counterfactual to esti-

11976

mate the probability of the truth of the consequent caused by
the antecedent (“‘a probability statement about the truth of
y, had x been true, when it is known that y had been false
when x was false’’) [37]. This approach to counterfactuals
motivated a number of experiments on the existence of the
relation between counterfactuals and conditional probability.
In support of this assumption, Over er al. [40] showed the
existence of connection between counterfactuals and con-
ditional probability, as they experimented with probability
judgments about counterfactuals. Thus, they proposed that
the subjective probability of the counterfactual at the present
time is the same as the conditional probability P(y|x) at some
earlier time. Twenty-six subjects were asked to estimate the
probability of truth of thirty-two counterfactual conditionals
with both affirmative and negative antecedents and conse-
quents. Their findings point to a strong correlation between
the probability of the counterfactual conditional and causal
strength judgments. On a similar note, Edgington regarded
counterfactual judgments as uncertain conditional statements
and therefore evaluated them by estimating their conditional
probability given some endorsing event [41].

C. DISTINCTION BETWEEN CONTRASTIVE AND
COUNTERFACTUAL EXPLANATION

It is important to note that some researchers tend to either
collapse or intentionally distinguish contrastive reasoning
from counterfactual reasoning despite their conceptual sim-
ilarity. For instance, Lombrozo treated counterfactual and
contrastive explanations as equivalent assuming hypothe-
sized events non-occurred in reality to be ‘“‘counterfactual
cases” where a subset of these cases forms a contrastive
explanation [10]. In contrast, McGill and Klein distinguished
contrastive reasoning from its counterfactual counterpart
[42]. According to them, contrastive reasoning is concerned
with situations where different target situations are analyzed
(“What made the difference between the employee who
failed and the employees who did not fail?’”’). On the other
hand, counterfactual reasoning is claimed to deal with cases
where the antecedent is altered to account for changes in
the outcome (“Would the employee have failed had she not
been a woman?’’). Alternatively, Fang er al. [43] referred
to contrastive reasoning as a procedure operating on “but-
statements”’, as in “‘all cars are polluting, but hybrid cars are
not polluting”, which serves a principally different explana-
tion generation task in comparison with the other aforemen-
tioned approaches.

D. CONTRASTIVE AND COUNTERFACTUAL EXPLANATION
IN THE CONTEXT OF XAI

The stochastic nature of predictions made by various AI
algorithms is claimed to be among the main obstacles in
reaching a true explanation [44]. Research on automatic
contrastive and counterfactual explanation generation shows
a number of considerable observations that help overcome
this issue. Thus, empirical studies prove that incorporating
contrastiveness improves the quality of explanations offered

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

to the end-user [45]. Furthermore, contrastive explanations
can be used to personalize human-machine interaction when
a user is engaged in an explanatory dialogue with an AI
application. Thus, they can be employed with the aim of
adjusting the contents of the explanation for the algorithm’s
output in accordance with the user’s preferences [46]. Finally,
the ability to explain a decision contrastively is claimed to
lead to responsible decision-making [47].

It is important to note that contrastive explanations point
to the difference between the actual and a hypothetical deci-
sion. On the other hand, counterfactual explanations specify
necessary minimal changes in the input so that a contrastive
output is obtained. However, these terms are sometimes used
interchangeably in the context of XAI [48], [49].

Various families of techniques have been proposed to
generate contrastive and counterfactual explanations of AI
algorithm output. In the context of XAI, an explanation for an
automatic decision or prediction, treated as an observation,
can be obtained abductively by attempting the search prob-
lem over the set of the known information concerning that
observation [50]. Alternatively, counterfactual explanation is
widely addressed in the paradigm of case-based reasoning,
Le., a family of problem solving methods based on appeals
to precedent solutions. In this setting, generating the most
suitable counterfactual may be viewed as a search problem
where the most similar precedent is looked for among those
making part of the case database [14]. Furthermore, Keane
et al. argue that applying case-based reasoning techniques
for generating counterfactuals increases their explanatory
competence [51].

Counterfactual explanations are normally considered con-
trastive by nature and therefore present a source of valuable
complementary information to a given automatic predic-
tion [52]. For instance, a counterfactual explanation of an
ML-based algorithm prediction may describe “the small-
est change to the feature values that changes the predic-
tion to a predefined output” [53]. An important advantage
of counterfactual explanations over their non-counterfactual
analogs is that they are devoid of any prerequisites to the
data or model. Indeed, counterfactual explanations are data-
agnostic as they can be based on the features of the neigh-
bouring data examples extracted from the same training set
and/or on the data generated synthetically around the data
instance in question. In addition, counterfactual explanations
are, in principle, model-agnostic, as they are suitable to
explain the output of any black-box algorithm in a post-hoc
manner.

Whereas counterfactual explanation generation is con-
cerned with a number of technical challenges, it also requires
to take into account several ethical aspects. For instance,
their use is expected to be safe (revealing model’s internals
through counterfactuals may lead to model stealing) [54], fair
(discriminatory explanations should be avoided) [55], action-
able (suggested changes in the input should be feasible) [56],
and accountable (ensuring responsibility for the explanations
provided) [57].

VOLUME 9, 2021

Ill METHODOLOGY

The present survey has been undertaken as a systematic lit-
erature review following the guidelines by Kitchenham and
Charters [58], Kitchenham er al. [59], and Wohlin [60]. The
background notation necessary to follow the findings of the
review is specified in Section III-A.

In short, the study comprises three phases as established in
the research method by Kitchenham and Charters [58]: (1)
planning the review procedure; (2) conducting the review;
and (3) reporting the results. During the first phase, three
research questions (RQ), RQ2, and RQ3) were specified
(see Section III-B). Subsequently, we determined a search
strategy to retrieve primary studies, i.c., we collected all
the relevant publications investigating the research ques-
tions (see Section III-C). Then, we developed inclusion and
exclusion criteria (see Section II-D) in order to select the
studies relevant for this article. When the same publication
was retrieved from multiple sources, all-but-one instances
of the publication (duplicates) were discarded. In addition,
we identified and added manually other relevant publica-
tions extracted from the bibliography lists of the previously
selected manuscripts to ensure a maximum coverage of the
related subject areas. It is worth noting that this additional
procedure is informally known as snowballing [60]. Finally,
we extracted and synthesized the data necessary to address
the research questions (see Section IJI-E).

A. PRELIMINARY TERMINOLOGY

As has been shown in Section II, contrastive and counterfac-
tual explanations presuppose a diverse nature across various
application domains. Hence, let us now define the general
terms used henceforth in this manuscript. As we are primarily
concerned with explainability of AI algorithms, we define
explanation in terms of the observed output of such an algo-
rithm. Thus, we regard an explanation as a non-empty set of
pieces of information justifying the given algorithm’s output
for an input data instance. The explanation for the given
output on the basis of the features of the input data instance
is deemed as factual. An explanation opposing the actual
outcome to one of possible other outcomes is considered to be
contrastive (e.g., “The data instance is of class A and not B
because ...”’). An explanation containing instructions on how
the output could have been changed constitutes a counterfac-
tual explanation (e.g., ““The data instance would be of class B
if ...”’). Explanations exhibiting patterns of both contrastive
and counterfactual explanation are deemed to be contrastive-
counterfactual explanations (e.g., “The data instance is of
class A and not B because .... However, it would be of class
Bif...’’).

We distinguish between contrastive and counterfactual
explanation throughout the rest of the manuscript if and
only if only one of these two terms is used in the given
primary study. In contrast, we unify the notions of counterfac-
tual and contrastive explanation introducing the term “con-
tfactual explanation” or “contfactual” to identify potential
similarities and differences of both types of explanation

11977
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

within a broader scope of literature. This term is used here-
after wherever both terms for contrastive and counterfactual
explanation can be used interchangeably. The terms ‘“‘con-
trastive explanation” and “counterfactual explanation” are
only used when they are found in the corresponding study and
cannot be used interchangeably in the given context. Notice
that the term ‘‘contfactual explanation’’ is not equivalent
to “contrastive-counterfactual explanation” but covers both
independently used types of explanation as well as their
fusion.

A theoretical framework providing justification and a rea-
soning mechanism for obtaining a contfactual explanation is
regarded as a theory of contfactual explanation. Altogether,
we use the term contfactual explanation generation to refer
to the process of automatic composition of contfactual expla-
nations for a given output of an AI algorithm in the form of
a complementary piece of information associated to a factual
explanation.

B. RESEARCH QUESTIONS

In order to reach the three objectives of the study as formu-
lated in Section I, the following three research questions were
specified:

« RQ): How are contfactual explanations defined in the
literature?

e RQ»: What are the state-of-the-art methods of contfac-
tual explanation generation?

« RQ3: How grounded are the state-of-the-art contfac-
tual explanation generation methods on the theoretical
approaches to contfactual explanation?

C. SEARCH STRATEGY

We selected the digital libraries Scopus and Web of Science
(WoS) to retrieve relevant publications from. These libraries
do not only include research publications in computing but
also index studies across all scientific fields, which allows
for an objective analysis of the interdisciplinary literature
relevant to the research questions posed.

Subsequently, we performed six queries over the title,
abstract, and author keywords in the aforementioned libraries
(see the overall structure of the query pipeline in Fig. 1).
It is worth noting that the proximity operator NEAR is used
following the WoS notation whereas the equivalent proximity
operator W is used for the same queries in Scopus. The
following search strings were used for querying the digital
libraries:

qi = counterfactual* W/3 expla*

q2 = contrastive* W/3 expla*

ga = 91 OR go

ga = q3 AND (defin* OR theor* OR infer* OR implic*)

gs = q3 AND (generat* OR implement* OR framework*
OR develop* OR software* OR model* OR artificial intel-
ligence OR AI) AND SUBJAREA(Computer Science OR
Mathematics OR Engineering)

96 = q4 AND gs

11978

 

FIGURE 1. A pipeline of the queries executed. The queries found in the
dashed area are considered preparatory to those directly addressing the
research questions.

The search was performed on October 2”%, 2020. The
search web tools of the selected digital libraries allow
researchers to reproduce the original study. Furthermore, their
use guarantees performing equivalent queries across both
libraries. In order to capture all relevant publications, we only
used the corresponding word-stems to allow for maximal
diversity of the retrieved papers. For instance, the search item
“expla*”’ was used to cover all publications containing such
word-forms as “explanation”, “explaining’’, “explanatory”,
and so on and so forth.

Queries g; and gz embrace all the up-to-date publica-
tions containing mentions of counterfactual and contrastive
explanation, respectively, found across all subject areas.
In addition, we used a window span of three words (ie.,
“NEAR/3’’) to ensure that the attributes “counterfactual”
and “‘contrastive” relate to explanation. The resulting sets of
publications were then unified (q3).

Subsequently, the preprocessed collection of publications
was split into two overlapping subsets aiming to distinguish
the publications covering theoretical accounts of contfactual
explanation with the aim of extracting the related defini-
tions, theories (or their inferences or implications) (g4) and
existing computational frameworks for contfactual expla-
nation generation (qs). The terms “definition”, “theory”,
“inference’’, and “implication” as well as their correspond-
ing word-forms (g4) were expected to appropriately limit
the pool of the unified set of publications with the aim of
retrieving definitions as required for addressing RQ). Sim-
ilarly, we used the terms “generation’’, “implementation”,
“framework’’, “development”, “software”, “‘model’’, and
their corresponding word-forms (gs) to retrieve publica-
tions concerning contfactual explanation generation frame-
works. In addition, the terms “artificial intelligence” and
“AI” were used to ensure retrieving relevant Al-related pub-
lications. Since RQ» addresses purely technical issues of

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

state-of-the-art implementations of such tools, we further
imposed an additional restriction on gs so that it would
return only publications from such subject areas as computer
science, mathematics, and engineering. Last but not least,
the findings from ga and gs were merged to examine the
connection between the existing theories of contfactual expla-
nation and frameworks for automatic contfactual explanation
generation in the context of XAI (q).

It is important to note that publications retrieved as a
result of ga form an exhaustive set of papers address-
ing RQ). Similarly, publications obtained as a result of
qs address RQ». Finally, the papers that g¢ returned
address RQ3.

D. INCLUSION AND EXCLUSION CRITERIA

The publications retrieved during the initial search were sub-
sequently inspected on the basis of the following inclusion
and exclusion criteria. To address the epistemology of con-
tfactual explanation, we filtered the retrieved publications
to include in the collection of primary studies only those
satisfying the following criteria: (1) a publication proposes
a contrastive or counterfactual or contrastive-counterfactual
approach to explanation or (2) it contains a clearly formulated
definition of counterfactual or contrastive explanation refer-
ring to other publications in the corresponding field. In order
to capture existing computational frameworks for contfactual
explanation generation, we included publications that: (1)
present a novel approach, method, or framework for contfac-
tual explanation generation whose output can serve to explain
the reasoning of an AI algorithm and (2) are found in such
subject areas as computer science, mathematics, engineering
as well as in their sub-fields.

In contrast, we excluded duplicate reports of the same
studies appeared in both Scopus and WoS. As for the pub-
lications related to RQ,, we also removed: (1) the studies
whose contents did not introduce any contfactual theory of
explanation or (2) those containing no formal or informal
definition of contrastive or counterfactual or contrastive-
counterfactual explanation. As for the publications related
to RQ2, we discarded: (1) the publications which were
not related to AI algorithms or applications as well as
(2) those where the proposed framework did not pro-
vide any human-comprehensible contfactual explanations as
output.

E. DATA EXTRACTION AND SYNTHESIS

Table 1 shows the number of publications retrieved after each
independent query, duplicates found among them in Scopus
and WoS, as well as Candidate Primary Studies (CPS). Note
that the numbers of duplicates indicated in Table 1 refer
only to within-query duplicates, i.e., the same publications
retrieved from Scopus and WoS for the given single query.
Recall that g4 and gs exhaustively cover all the three research
questions. Hence, the numbers of CPS are calculated as asum
of the publications retrieved after g4 and qs. Furthermore,
CPS are reduced by the number of publications addressing

VOLUME 9, 2021

TABLE 1. Numbers of publications retrieved after each single query as
well as those forming the pool of candidate primary studies. The numbers
of publications making part of the primary studies are highlighted in bold.

 

a | g2 | a3 | g4 | as | a6 | CPS
Scopus 169 | 125 | 288 | 140] 67 | 22 | 185

WoS 122 | 94 | 212] 109] 22 | 7 124
In total (including | 291 | 219] 500] 249|] 89 | 29 | 309
duplicates)
Duplicates 108 | 80 | 184|] 92 | 21 | 6 107
In total (excluding | 183 | 139] 316] 157] 68 23 202
duplicates)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

[ Scopus (n = 207) [ WoS (n = 131) ]
LT

Duplicates
removed (n = 136)

 

Vv
Title, abstract, and
keyword screening
(n = 202)
Publications
excluded (n = 75)

Vv
Full-text screening
(n = 127)

Publications
excluded (n = 33)
[>_>

 

Vv
Included after
full-text screening
(n = 94)

 

 

Publications
included upon
snowballing (n = 19)
<<

 

Vv

Primary studies

(n = 113)

 

FIGURE 2. A flow diagram of the primary study selection on the basis of
queries qq and q, (nis the number of publications at each stage).

RQ3 because they are found in both sets of publications
collected for RQ; and RQ» and are therefore duplicates.

Fig. 2 displays the flow diagram of the primary study
selection. A sum of 338 publications (207 from Scopus and
131 from WoS) made up the collection of CPS addressing the
research questions. 107 within-query duplicates were identi-
fied and removed from further analysis. In addition, 29 more
duplicates were excluded when merging the sets of publica-
tions retrieved after ga and qs. Allin all, 136 duplicates were
removed.

The title, abstract, and author keywords of each candidate
primary study were screened to discard the studies irrele-
vant to the research questions posed. As shown in Fig. 2,
75 publications were deemed irrelevant and filtered out at this
stage. A deeper analysis of the remaining 127 publications

11979
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

 

 

Contfactual explanation

 

 

ee

 

Contrastive
explanation

 

 

 

(Counterfactual
explanation

Contrastive- | |
counterfactual] :
explanation

 

 

 

 

 

 

 

Theoretical frameworks

ee ee

Causal Non-causal Hybrid

 

 

 

 

 

 

' technically correct /
| pragmatically adequate

' vertical /
horizontal

: microphysical /
_macrophysical

 

 

 

 

 

' weak / strong
' empty / non-empty

imary / secondary

 

 

 

' easy / difficult

 

 

 

 

Computational frameworks

|
‘ 1 t y

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Explainability Output .
Al problem method representation Evaluation method
Classification Model-specific Numerical 1
Regression Model-agnostic Linguistic Intrinsic
Knowledge Visual Extrinsic
engineering
Multi-modal
Planning Subjective
Recommendation
ranking Objective

 

 

 

Conflict resolution

 

 

 

FIGURE 3. A taxonomy of contfactual explanation emerging from our systematic literature review.

TABLE 2. The exhaustive list of all the primary studies in relation to each
research question.

[86], [91], [93], [94], [100], [105], [122], [128]1161]

{100}, [105], [122]

 

enforced us to discard 33 studies which did not satisfy the
inclusion criteria. Finally, 19 papers were added to the review
upon inspecting the bibliography of the primary studies. As a
result, 113 unique publications formed the exhaustive pool of
primary studies.

Table 2 presents the list of primary studies selected for the
review. Thus, a collection of 74 out of 113 (65.49%) origi-
nal primary studies were found to formulate definitions for
contfactual explanation and/or address theoretical accounts
thereof (RQ}). In addition, 52 out of 113 (46.02%) publi-
cations describe frameworks (or extensions of other frame-
works) for contfactual explanation generation (RQ2). Note
that 13 out of 113 (11.50%) primary studies were found to
address both RQ; and RQ» and therefore answer RQ3.

The following data were extracted from each primary
study: title, authors, year of publication, author keywords.
In addition, all publications related to RQ, were read to ana-
lyze contfactual theories of explanation and, subsequently,
extract the sought-for definitions of contfactual explanation.
As RQ» concerned a broader number of technical character-
istics of contfactual explanation frameworks, we additionally

11980

extracted the following information: (1) the problem that the
retrieved framework aims to solve; (2) the method proposed
for contfactual explanation generation; (3) the form of output
explanation (for instance, textual or visual); and (4) the cor-
responding evaluation methods. Based on the data extracted
from the primary studies, the publications were grouped and
classified in accordance with the aforementioned criteria.

IV. RESULTS
Prior to answering the research questions, we carried out a
bibliometric analysis over the results of the general indepen-
dent queries on counterfactual and contrastive explanation (q1
and gz, respectively) as well as their union (q3). We report
the results of the bibliometric analysis in Section IV-A. The
findings related to the theoretical accounts of contfactual
explanation (RQ}) are presented in Section IV-B. The analy-
sis of the computational frameworks for contfactual explana-
tion generation (RQ>) can be found in Section IV-C. Finally,
the publications describing theoretically grounded computa-
tional frameworks (RQ3) are reported in Section IV-D.

An emerging taxonomy of contfactual explanation frame-
works is depicted in Fig. 3 and forms the core of the results
discussed in the rest of the manuscript.

A. BIBLIOMETRIC ANALYSIS

The bibliometric analysis over the queries gi, g2, and q3
allows us to obtain a big picture of the research area of cont-
factual explanation generation and spot its key characteristics.
To illustrate the state of affairs within the field, we report
annual scientific production and maps of author keywords
revealing the main problem-specific notions. The reference

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

continigentism a

>
adaptation

quasi intefventionism
mathematical explanation

count counterpossibles

counterfactuality counterfactual dependence

ee
causal explanation abqggion

construal

agent-based modeling

logical inference

counterfactual

causéljmodel

Fe
_histéry lis
boktulich ioe" commonserig@ explanation

 
   
  

© reasoning explanation : i
imadjition P wosiiard e counterfactual sets ees
A mathigratics explainable machine learning faifiess
y
f ili i . ; branching time
a counterfactual incompatibility samp OaSeT m ineleSrning
counterfactual thinking causation grounding black-be.m y digit@Jiwins
Se interactive ~ .
mechanisms tality counterfae xplanations
caudal models counterfactuals ~ “explainable artificial intelli
counterfi reasoning oats Is
causal modeling decisidgyPRakin;

reinfoMegigne learning 8

functigalism
'e possible worlds

mectighism

fore-seeability

competitive politics

FIGURE 4. The map of author keywords for the q, publications.

60

 

 

wu
°

S
o

 

 

N
o

 

Number of publications
WwW
oO

a
°
1

 

 

 

1980 1990 2000 2010 2020
FIGURE 5. Annual scientific production for the publications retrieved
after q, (the red line), gz (the blue line), and qz (the green line).

manager Mendeley was used to filter out duplicate publica-
tions. In addition, we utilized the tool VOSViewer [162] to
generate the author keyword maps.

It can be seen that contfactual explanation appears to attract
an increasing attention across all subject areas in the past two
decades. Furthermore, Fig. 5 shows a rapid rise in the number
of publications in the past three years. It is worth noting that
the number of publications in 2020 is limited to the search
date.

Author keyword maps allow us to present an overview of
the terms most relevant to those specified in the preparatory
queries (q1, q2, and q3). For illustrative purposes, non-linked
keywords were deemed to be outliers and filtered out from
the analysis. Table 3 shows the overall number of keywords
as well as that of linked keywords for each preparatory query.

VOLUME 9, 2021

TABLE 3. Numbers of linked and non-linked keywords in the preparatory
query results (q,, q2 and q;).

q1

93

 

Fig. 4 shows a graph containing the most popular author
keywords for counterfactual explanation. It can be concluded
that counterfactual explanation is often investigated in the
context of causation (pay attention to such keywords as “‘cau-
sation’, ‘‘causal inference’’ or ‘“‘causal models’’) as well as
cognitive science (as reflected by the keywords “imagina-
tion’’, “‘reasoning”’, etc.) and AI (‘machine learning”, “data
models”’, “black-box models”). Similar notions are observed
to be essential for contrastive explanation (see Fig. 6). How-
ever, a distinction between different clusters in the latter case
is visible more clearly. This is hypothesized to be due to a
more diverse usage of the term “contrastive explanation”
across various scientific areas.

A stronger impact of counterfactual explanation in the
results of the joint query g3 appears to affect significantly
the overall allocation of the related keywords in the corre-
sponding keyword map (see Fig. 7). The keywords identified
in the studies related to g3 testify that the issue of contfactual
explanation is highly interdisciplinary and finds application
in both humanities and natural sciences.

B. CONTFACTUAL EXPLANATION AS DEFINED IN RELATED
THEORIES (ANSWER TO RQ,)

As presented in Section II, the surface form of contfactual
explanation is found to preserve the same syntactic structure

11981
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

ceteris paribus laws

creative’ view
causal éxplanation
contingency
metaphysics ei.
qerounsing
circerty
implicature
models
resource-based view deepnets
calsation
probability

explanation
Ne

  
 

est explanat

counterfactual

canaligation

brun@llatour

contingentism

david bloor

criticalrealism

contrasti¥agg planation
Lis

@ plural volugery contro

heterogeneity

os, @
fr
compati
causal nga of action

europégn union

counteffactuals

ide
abstigction
contrast
ins

maniiiieber atfectigpriming acco ery

s y countergijgument
iy ge”
SF etnn@epny

-
explanations

@c trastivelexplanations
SersGouing
machin@jearning

explainable ai
-

Pea cas Gene”

FIGURE 6. The map of author keywords for the q2 publications.

counterfactuality
modality
conceivabilityand possibiliy
reasoning
counter fe ghinking
‘causal @asoning,
cognitive development

‘reditivamodel

-<
counterfactual reasoning interastive

classifigation
@ deepllearning
k-optimaly ©

interpretability

explainable aitificialfintelli
join Faking
contrS8tivagegp@ nations

adversarial examples

counterfacttal explanation
contingency

time-series

adhetence

retraduction fairness

explanataty relations accountability
causal models .
realism *i"cularityonjection

conditionals grounding

norms

i
contrast"

counterargument counterpossibles

fiction
‘ mathemati janation
S

relevance

explainable machine learning causal thag) of action

o§

ae ality ‘ex causal ¢ Nation —-construal n
causal apaljidinterpfation 5 on Sa ;
ghee 6 Cem causation COntrasti lanation
vistorige Mey rey
rea critcalrealism
exe Probability
er thesis’ céteris paribus laws
determinism
contgertism
cd faupal@osure  inapasia —_Aliositions
inddgtion e
cealon Neel
fence forebtiny

caus: ence

functional explanation
constraints,

competitive politics

FIGURE 7. The map of author keywords for the q, publications.

in general. However, a major factor discerning theoretical
approaches to contfactual explanation is found to consist
in their relation to causation. As will be shown below,
several accounts of contfactual explanation presuppose a
causal nature and establish a causal contfactual dependency
between the phenomenon to be explained and the expla-
nation itself. In contrast, other theoretical frameworks seek
purely non-causal dependencies in explanation. In addition,
several researchers attempt to unify causal and non-causal
contfactual explanation under the same paradigm. Hence,
we distinguish three main groups of contfactual explana-
tion that encompass all the retrieved primary studies: causal,

11982

TABLE 4. A diassification of approaches defining contfactual explanation.

 

 

 

 

 

 

Approach Publications

Causal [13], [21], [36], [37], [61]163], [66]-[68), [70]-[73],
[75], [771179], [82]-[84], [88]-[90], [92]-[94], [97].
[99], [101]-[103], [106], [107], [109], [113]-[121],
[123]-[127]

Non-causal [49], [64], [65], [69], [74], [80], [81], [86], [87], [91],
[96], [100], [110], [111], [122]

Hybrid [17], [22], [76], [85], [95], [98], [104], [105], [108],
[112]

 

 

 

non-causal, and hybrid. The exhaustive list of the retrieved
publications in accordance with the suggested taxonomy is
presented in Table 4 and discussed further in the following
sections.

Remind that contfactual explanation embraces contrastive,
counterfactual, and contrastive-counterfactual explanation.
Each type of contfactual explanation is present in the find-
ings, causal counterfactual making up a majority of the con-
sidered theoretical frameworks (see Fig. 8). Hence, we ana-
lyze each contfactual explanation type independently in terms
of causality in this section to draw a comparison between
different approaches. In addition, we consider (1) the issue
of quantitative evaluation of causality for causal contfactuals
as reflected in specific primary studies and (2) different sub-
categorizations of causal, non-causal, and hybrid contfactual
explanation.

1) CAUSAL CONTFACTUAL EXPLANATION
« Causal contrastive explanation is frequently found
to be designed as an answer to a why-question of
the following canonical form: ‘““Why P rather than
Q?” where P is an explanandum (i.e., the fact to be
explained), Q being a foil (i.e., one of alternative non-
occurring options) [21]. Lipton introduces the notion of

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

 

TABLE 5. Counterfactual explanation theories reflected in the primary
studies.

Non-causal 4

o ©
© ©

 

 

 

FIGURE 8. Numbers of identified theoretical contfactual explanation
frameworks with respect to causality.

“difference condition’: the contrast between the fact
and the foil is explained by identifying the cause of the
fact and proving the absence of the corresponding cause
of foil [103]. Following Lipton [103], Kean redefines
a contrastive explanation to be the difference between
the causal explanations for the question and the contrast
[93]. Barnes further requires that “P and Q be culmi-
nating events of a single type of natural causal process”’
[63]. Similarly, Day and Botterill introduce the concept
of differential inference, i.e., a form of inference based
on contrastive explanation that “can be used in order to
generate causal hypotheses” [77].

Van Fraassen formalizes a contrastive question to be the
triple (P,X,R) where P is a topic (explanandum), X
is a contrast space or contrast-class (i.e., a set of alter-
native answers to the given question), and R being the
corresponding relevance criteria [13]. Then, the answer
to a why-question must differentiate the topic from the
contrast space. Contrarily, Chien excludes the contrast
class when taking contrastive explanation as a model for
scalar implicature [75].

Hitchcock generalizes the notion of contrastive expla-
nation over all explanations bearing contrastive stress
irrespective of their syntactic structure [88]. Con-
versely, Aguilar-Palacios et al. oppose the alternative
explanation-seeking question “Why P rather than Q?”
to the congruent question “Why P but Q?” [61]. This
formulation of the question generalizes the explanation
to justify why some fact P occurs in the current situation
whereas some foil Q occurred in different circumstances
with the aim of establishing a cause and effect rela-
tion between the fact and the foil. Similarly, Tsang and
Ellsaesser claim that a contrastive explanation should
point to the importance of identifying the most relevant
factors differing the causal histories of the fact and the
foil where both fact and foil must be true [124].
Contrastive explanations may not only concern the
explanandum but also the answers to contrastive ques-

VOLUME 9, 2021

 

worlds” Stalnaker [120] [114], [117], [123]

[90]

Robin [1

tions (often referred to as explanans). Thus, Sober
stresses that the question of whether some hypothesis
H explains why a non-contrastive proposition F is true
is “incomplete until H is contrasted with an alternative
hypothesis”’ [119]. In addition, the canonical forms of
the contrastive question and the corresponding explana-
tory answer (ie., the core of contrastive explanation)
have raised a number of epistemological concerns in
philosophy of science. For instance, Dickenson refor-
mulates the contrastive explanation-seeking question to
be: ““What explains how it is possible that an agent can
act on R; other than Ro, given that Ro is present?” [78]
(where reason Rj is the cause of some action and R»
is not). The notion of contrastive explanation is further
developed in the agent-causal theory of free will. Thus,
contrastive explanation is applied to agent’s decision-
making (i.e., why the agent makes a choice refraining
from an alternative choice) [82], [101].

Campbell redefines a contrastive explanation in terms of
the so-called “structuring causes’’, i.e., the traits of the
structure of the causal system that trigger actual causes
of some event to happen [73]. A cause of this kind is
responsible for the connection between the types C and
M inasystem S. A contrastive explanation thus explains
why a system S$ is claimed to be “wired” in such a
way that an internal state of type C regularly causes
a movement of type M. Similarly, Kim et al. regard
contrastive explanation as a constraint for a system to
be satisfied by a specific set of plan traces [94].
Finally, Boulter illustrates the use of contrastive expla-
nations to distinguish between actual and non-actual bio-
logical forms [70]. Claiming all explanations in biology
to be causal, the researcher introduces the following
template for a causal relation in contrastive explanation:
“e, rather than c2 or cz of cy, causes e, rather than
€2, OF €3 OF é,,”” leaving contrasting causes implicit.
Causal counterfactual explanation. Most of the con-
sidered studies on causal counterfactual explanation
relate to either of the four theoretical milestones: Lewis-
Stalnaker’s theory of closest possible worlds [36], [120],
Pearl’s Structural Causal Models (SCM) [37], Wood-
ward’s Counterfactual Theory of Explanation (CTE)
[125], or the Neyman-Rubin Causal Model (NRCM)
[107], [115] (see Table 5).

The Lewis-Stalnaker approach codifies a counterfactual
conditional as a logical proposition where the antecedent

11983
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

and the consequent are connected by means of the
“might’’- or “would’’-conditional operator. Exploiting
the mechanism of possible world semantics, the truth-
fulness of a counterfactual is assessed by assigning to it
a binary truth-value in accordance with its proximity to
the world in question.

Following this approach, Kutach defines counterfactu-
als in natural language to be ‘“‘propositions obeying a
logic whose semantics is given in terms of a compar-
ative similarity relation among possible worlds” [99].
Similarly, Strohminger and Yli- Vakkuri assume that the
counterfactual modus ponens preserves truth-functional
possibility (“If itis possible that p and p counterfactually
implies q, then it is possible that qg’’) [123] where p
is a logical proposition and q is a conjunction of such
propositions.

Briggs extends the Lewis-Stalnaker model by apply-
ing causal modeling language to comprise logically
complex antecedents [72]. Schweder considers a coun-
terfactual to be an implicit claim within the explana-
tory answer to an explanation-seeking question [117].
In addition, Pruss and Rasmussen take into account
antecedents that are not necessarily “contrary-to-fact”’
and define a counterfactual to be a contingent proposi-
tion establishing a causal connection between a specific
description of the circumstances of a choice and a report
of an action in such circumstances [109].

Pearl’s SCM operates on a predefined causal model M =
(U,V, F) consisting of sets of background variables
determined by factors outside the model (U) and within
it (V) and a set of functions F = {f;| 1 < i < n}
mapping from U U (V \ V;) to V;, that associates each
variable V; with all the variables from U and V. Given
a set of variables X € V and a causal submodel M, =
(U,V, F,) so that F, = {f, : Vi; € X} U{X = x} and
by defining a minimal change in M required to make
a selected variable X = x (X € VY) hold true under
any u € U, a causal counterfactual is formally defined
as the solution for some subset Y € V on the set of
equations F’, [37]. Counterfactuals are thus pruned by
interventions on the antecedent component [113], which
leads to interpreting counterfactuals as non-observable
hypothetical contrasts [71].

Similarly to Pearl’s SCM, Woodward’s CTE establishes
the counterfactual dependence between the two vari-
ables by means of the intervention mechanism. Thus, for
two variables X and Y taking on some values x and y,
respectively, to explain the value of y counterfactually
is to show that Y would have taken on some value
y’ if X had taken some counterfactual value x’ [125].
In other words, some small enough change in the value
of X from x to x’ would cause a change in Y from
y to y’ in the absence of changes in values of other
variables.

Following Woodward’s theory, Schneider and Rohlf-
ing define a counterfactual as ‘‘a theoretically relevant

11984

manipulation of the observed case in order to ascertain
whether this manipulation would make a difference to
the outcome” [116]. Further, Bertossi defines a causal
counterfactual explanation to be a set of the original
feature values in the given data instance that are affected
by a minimal counterfactual intervention [67] (where
minimality is assumed to be based on a partial order
relation on counterfactual interventions).

Conversely, Andreas and Casini reconsider explanatory
counterfactuals to be “hypothetical assumptions about
the values of quantities or the values of propositions’’.
They argue that Woodward’s interventionist account of
explanation cannot handle the cases where interven-
tions are physically impossible (e.g., due to violations
of laws of nature) [62]. Applied to theorem proving,
Gijsbers leaves out the mechanism of intervention from
Woodward’s CTE. He states that a mathematical proof
has explanatory power only when the explanandum is
complemented with a contrasting claim that shows how
the mathematical object in question varies in the process
of theorem proving. Also, Fang infers counterfactual
dependencies in the form of counterfactual claims: “in
the model M, had the variable X taken such-and-such a
value x;, then the variable Y would have taken such-and-
such a value y,;” [79].

Last but not least, Holland argues that causal counterfac-
tuals are highly relevant to research in social sciences.
Thus, he follows NRCM interpreting counterfactuals
in terms of potential outcomes of a dependent causal
variable given some intervention with respect to that
variable [90].

Causal contrastive-counterfactual explanation is
sometimes considered to include “‘all kinds of subjunc-
tive conditionals, regardless of whether the antecedent is
true in the actual world or not” [126]. Thus, Kuorikoski
and Ylikoski elaborate on a contrastive counterfactual
theory of explanation claiming that the property of con-
trastiveness helps to resolve linguistic ambiguity inher-
ent in explanation [97]. In this setting, interventions
(or manipulations) specify the truth conditions of such
explanations: “‘c [c*] causes e [e*] if we can bring about
ex [e] by bringing about cx [c]” [127] (where c and cx
are causes, e and ex being the corresponding effects).
Following Kuorikoski and Ylikoski [97], Northcott
examines explanatory relevance of counterfactuals
placed in a contrastive framework [106]. Similarly,
Hohwy regards causal counterfactuals as an integra-
tive part of causal contrastive explanations. Thus, he
claims counterfactuals supported by laws are able to “go
into contrastive explanations even though unfavourable
conditions ensure that the forces they describe are not
actually occurring in the way described by any law taken
alone” [89]. On a similar note, Steglich-Petersen [121]
proposes two-level semantics of contrastive causal state-
ments requiring specific semantically complete counter-
factual justifications.

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al IEEE Access

 

« Degree of causality in causal contfactuals. Causal

relations between the variables in explanation are not
always considered binary (i.e., in the presence/absence
of a cause). There have been several attempts to measure
a degree of causality in contfactual explanation. Since
causal contrastive explanations describe a certain aspect
of the explanandum, Rips and Edwards claim them to
be partial by nature [113]. In other words, the explana-
tory power of such explanations can be quantified and
compared with that of others. In light of this assump-
tion, Northcott defines the degree of causation (ie.,
causal strength of a cause variable) to be the difference
between the values that the effect variables take on in
the actual and counterfactual cases [106]. In this regard,
he determines a counterfactual to be the value of the
effect variable. A counterfactual can thus be measured
quantitatively as the distance between the target levels
of the causal effect variables. Similarly, Ylikoski and
Kuorikoski distinguish five dimensions of explanatory
power of contrastive explanations: (1) non-sensitivity
(i.e., how sensitive the explanation is to background
conditions); (2) precision (i.e., how precisely the expla-
nation characterizes the explanandum); (3) factual accu-
racy (i.e., a proportion of true facts captured by the given
explanation in comparison with another); (4) degree
of integration (i.e., unification to a larger theoretical
framework); and (5) cognitive salience (i.e., “the ease
with which the reasoning behind the explanation can be
followed’’) [126].

Subtypes of causal contfactuals. It is worth noting that
several subcategorizations of causal contfactuals have
been suggested within some of the aforementioned the-
oretical frameworks.

As for contrastive explanation, Franklin follows
Hitchcock [88] differentiating “technically correct
contrastive explanation’ (the explanation citing
explanatory relevant information) and ‘“‘pragmati-
cally adequate/defective contrastive explanations” (the
explanation providing more information than explana-
tory relevant) [82]. Levy distinguishes between weak
contrastive explanation (if the agent is not able to explain
how the agent-causal power was exercised for reasons)
and strong contrastive explanations (otherwise) [101].
As for counterfactual explanation, Holland points to a
deceptive use of “empty” counterfactuals, i.e., coun-
terfactuals whose antecedent ‘“‘could never occur in
any real sense’? [90]. Steglich-Petersen distinguishes
between primary counterfactuals (i.e., those that relate
two events A and B as the cause and the effect) and
secondary ones (i.e., those that establish the fact that
it is event A that causes B to happen) [121]. Finally,
Schneider and Rohlfing claim counterfactuals to be
either easy or difficult [116]. From this perspective, easy
counterfactuals are ‘‘the assumptions about the outcome
of logical remainders’ that simplify theoretical expec-
tations. In contrast, the assumptions that simplify the

VOLUME 9, 2021

solution ‘but run counter to our theoretical expectations
about whether single conditions involved in a remainder
should or should not contribute to the outcome” are
assumed to be difficult.

2) NON-CAUSAL CONTFACTUAL EXPLANATION

« Non-causal contrastive explanation. Notably, non-

causal contrastive explanations can address the phys-
ical nature of a modeled system. Hence, they can be
used to explain the properties and relations inherent to
such systems. Thus, Chakravartty extends the concept of
contrastive explanation to answering non-causal what-
questions, e.g.: “What dispositions of p are relevant
to circumstances x as opposed to y?”, where p is the
object whose traits require an explanation and x and
y are the circumstances determined by the question-
dependent context [74].
In contrast to Dickenson [78] (see Sect. IV-B1), Botterill
appeals to a non-causal nature of contrastive explana-
tions [69]. Thus, the researcher argues that “‘the fact that,
in the absence of R2 but with R; still present the agent
would perform an action of some kind does not show
that when both R; and R2 are present an agent does not
act in that way because of both those reasons” (where
reason R, is the cause of some action and R2 is not).

« Non-causal counterfactual explanation. Reutlinger
develops a non-causal counterfactual theory of
explanation to apply it to Euler’s explanation? and the
renormalization group theory* [110]. This counterfac-
tual theoretical framework is subsequently extended to
capture non-causal explanations in metaphysics [111].
Driven by the assumption that physical facts and mathe-
matical models share certain features, Baron et al. apply
a structural equation modelling framework to model
counterfactuals that could explain physical facts in terms
of non-causal mathematical explanations [64]. Further,
Baron introduces the concept of the so-called “coun-
terfactual scheme’ applied to mathematical explana-
tion [65]. A counterfactual scheme is thus defined as
a triple containing (1) a counterfactual statement with
non-logical expressions substituted with variables, (2)
instructions stating which parts of the statement can
be substituted to produce a counterfactual, and (3) a
classification for evaluating the given counterfactual.
A counterfactual is then claimed explanatory if all the
instances of a counterfactual scheme are true and at
least two counterfactual schemes are distinct so that the
corresponding physical laws relevant for evaluation of
the given counterfactuals are different. Also, Hird uses

3Reutlinger refers to the phenomenon found in the city of Kénigsberg
where no-one succeeded to cross the seven bridges located in four different
parts of the city exactly once. Euler provided a non-causal explanation for
this phenomenon in terms of graph theory.

4 According to Reutlinger, renormalization group explanations are
intended ‘“‘to provide understanding of why microscopically different physi-
cal systems display the same macrobehavior when undergoing phase transi-
tions’’ [110].

11985
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

the term “counterfactual” to define projects that have
been funded in the absence of congressional committee
influence [87].

In addition, a number of definitions for non-causal coun-
terfactual explanation come from AI. In the context of
automatic decision-making, counterfactuals are found to
be most generally defined as counterarguments for an
alternative prediction [86]. Fernandez er al. refer to a
counterfactual as an effective type of explainable ML
technique that explains predictions by describing the
changes needed in a sample to flip the outcome of the
prediction [81]. More precisely, Fernandez et al. define
a counterfactual for classification tasks as a “hypothet-
ical instance similar to an example whose explanation
is of interest but with different predicted class’’ [80].
Kanehira et al. attempt to explain counterfactually video
classification output framing a (visual-linguistic) coun-
terfactual explanation in the form of the conditional
statement “X would be classified as B and not A if C
and D are not in X”’ [91] (where X is the data example
requiring an explanation, A is the class predicted for
X, B is the contrast-class in question, C and D are
specific visual patterns present or absent in the given
video frame X). On a similar note, Laugel et al. treat
a counterfactual explanation as a specific data instance,
close to the observation whose prediction is explained,
but predicted to belong to a different class [100]. Kos-
tic defines a counterfactual to be a statement describ-
ing a hypothetically different situation to the actual
state of affairs [96]. He distinguishes between vertical
and horizontal counterfactuals. Thus, a counterfactual
is considered vertical if ‘‘a global topological property
determines certain general properties of the real-world
system”. In contrast, a counterfactual is deemed hori-
zontal if ‘a local topological property determines certain
local dynamical properties of the real-world system’’.
Finally, Stepin er al. point that a counterfactual explana-
tion should refer to a set of features “‘minimally different
from those inherent to the original data point’’ [122].
Non-causal contrastive-counterfactual explanation.
Poyiadzi et al. do not distinguish between counterfactual
and contrastive explanations assuming counterfactuals
to be the new state of the considered object [49].

3) HYBRID CONTFACTUAL EXPLANATION

e« Hybrid contrastive explanation. Chin-Parker and
Bradner [17] as well as Chin-Parker and Cantelon [76]
provide a unified theoretical framework for causal and
non-causal contrastive explanation for category learn-
ing. Emphasizing the crucial importance of context for
an explanation, they consider a contrast class to be a set
of non-occurring alternates that delimits the set of poten-
tially relevant information irrespective of the inherent
causal relations.

terfactual explanation. Thus, Byrne states that ‘“‘not all
counterfactuals are about causes, and counterfactuals
that imply a causal relation differ in systematic ways
from counterfactuals that identify other sorts of rela-
tions, such as intentions’ [22]. Indeed, a large body
of research on both causal and non-causal counterfac-
tual explanation testifies that counterfactuals have a
diverse nature with respect to causality [104]. Thus,
Lowe claims counterfactuals to be causal “when the
modality involved is evidently natural or causal neces-
sity’’. Contrarily, other explanation cases such as those
arising in mathematics “clearly do not involve this
sort of necessity, but instead something like logical
necessity” [104]. Further, Knowles and Saatsi discuss
the notion of explanatory generality presuming both
causal and non-causal nature of counterfactuals argu-
ing that “explanatory counterfactuals are appropriately
directed and change-relating, capturing objective, mind-
independent modal connections that show how the value
of the explanandum variable depends on the value of the
relevant explanans variables’’ [95].

In light of this, there have been several attempts
to unify causal and non-causal counterfactuals within
one framework. Hence, a hybrid approach, originating
from monism,° has been adopted to unify causal and
non-causal counterfactual explanation. Following this
approach, Reutlinger introduces a unified explanation
framework consisting of the following elements: a state-
ment about the explanandum £, a set of generaliza-
tions (or explanans) Gj,...,Gjm, and a set of auxiliary
statements setting initial conditions for the explanatory
system [112]. A relation between an explanandum and
a set of explanans is claimed to be explanatory if and
only if at least one of the explanans supports the counter-
factual statement “had $,...,5,, been different than they
actually are (in at least one way deemed possible in the
light of the generalizations), then EF or the conditional
probability of E would have been different as well’.
At the same time, the generalizations and auxiliary state-
ments must logically entail the explanatory statement
in question. As such, both causal and non-causal expla-
nations are argued to be captured because they “reveal
counterfactual dependencies between the explanandum
and the explanans”’. Following Reutlinger’s account of
explanation, Held argues that the notion of counterfactu-
als can hardly be supported only by generalizations [85].
Furthermore, true generalizations (e.g., “all ravens are
black’’) might not allow for counterfactual situations at
all. Instead, he weakens the counterfactual dependency
to shift from generalizations to plain counterfactuals.
Mothilal er al. suggest a feature-based counterfactual
explanation generation framework where importance of
independent features is evaluated [105]. Nevertheless,

Hybrid counterfactual explanation. Explanatory plu-
ralism is as well recognized in the research on coun-

5Monism is a philosophical account of explanation that captures both
causal and non-causal explanations reducing them to a single entity [112].

11986 VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

TABLE 6. A diassification of the contfactual explanation generators by Al
problem.

[81], [86], [91], [100], [105], [122], [128]-
[131], [133]-[148], [150]-[155], [158]}-
[160]

 

they emphasize the need for causal attribution, as ignor-
ing causal relations may lead to generating unfeasible
counterfactuals. Therefore, they suggest a hybrid frame-
work for counterfactual explanation generation.

« Hybrid contrastive-counterfactual explanation.
Kuorikoski and Ylikoski point to the multifaceted nature
of contrastive-counterfactual explanation. They argue
“there exist constitutive and possibly formal counter-
factual dependencies as well as combinations of these”’
[98]. Similarly, Pexton suggests a two-level hierarchy
of explanation [108]: microphysical explanations are
non-causal and form the lower-level of the hierarchy
whereas manipulable causal explanations are placed at
the higher-level.

C. CONTFACTUAL EXPLANATIONS AS DEFINED IN
AUTOMATIC GENERATION FRAMEWORKS (ANSWER TO
RQ2)

The analysis of the primary studies related to RQ allows us
to categorize the state-of-the-art contfactual explanation gen-
eration frameworks in accordance with the following criteria:
(1) the problem the solution for which is to be explained (i.e.,
the AI problem); (2) the method employed to generate such
an explanation (i.e., the explainability method); (3) the out-
put representation of the explanation; and (4) the evaluation
method thereof.

1) Al PROBLEM

Contfactual explanations are used to justify automatic deci-
sions obtained for a variety of Al-related problems. Table 6
provides the reader with a taxonomy of the state-of-the-
art frameworks from the primary studies. It is derived from
the considered publications in terms of the domain tasks
that these frameworks are used for. As depicted in Fig. 9,
most contfactual explanation generation frameworks deal
with counterfactual explanation (31 out of 52 frameworks;
59.62%). In contrast, 17 out of 52 (32.69%) generate con-
trastive explanations. Only four studies (7.69%) fuse con-
trastive and counterfactual explanations. One of these studies
[129] deals with both classification and regression.

« Contfactuals for classification. A vast majority of
state-of-the-art AI applications that generate contfac-
tuals (42 out of 52; 80.77%) are used to explain the

VOLUME 9, 2021

 

Classification

OO

Regression

Knowledge
engineering

Planning

Recommendation
ranking

Conflict resolution

 

© ©0800

 

 

aie 3 3
a oe (os oo
co

FIGURE 9. Numbers of frameworks grouped by Al problem with respect
to the type of contfactual explanation generated.

outcome of ML-based classifiers, ie., algorithms that
learn a mapping function f : X —> Y from a training
dataset of 1 labeled examples X = {x;| 1 < i < n} to
a discrete output variable (class) Y = {y; | 1 <j < m}
where m is the number of classes. Indeed, contfactuals
are particularly suitable for informing the end-user why
a given data example is assigned a particular class label.
Thus, the outlined classification-oriented frameworks
are evaluated on classifiers based on logistic regression
[55], [136], [153], [158], decision trees [46], [80], [122],
[140], [150], [155], [159], gradient boosted decision
trees [147], support vector machines [131], [138], [146],
random forests [81], [86], [142]-[144], neural networks
[6], [48], [49], [91], [129], [130], [133], [135], [139],
[141], [145], [148], [151], or combinations of these
[100], [105], [134], [152], [154], [160]. In three studies
[67], [128], [137], the classifiers used in the experiments
are not specified.

« Contfactuals for regression. One of the classification-
oriented frameworks [129] is extended to also handle the
regression problem, i.e., learning a mapping function f
from a training dataset X to a continuous output vari-
able Y. However, the continuous output is, in this case,
subsequently converted to a lower-scale discrete value
mapped to a textual description similar to that typical of
a classification problem. The other frameworks address-
ing the regression problem aim to leverage gradient-
boosted decision trees [61] and indicate how large errors
in regression tasks could be overcome [56].

« Contfactuals for knowledge engineering. The first of
the considered frameworks (in chronological order) [93]
offers explanations by reasoning abductively over the
information extracted from a given knowledge base to
answer a specific contrastive question. In this setting,
an explanation is considered to be a consistent set of
disjunctive literals for the explanation-seeking question.
It is worth noting that the framework is not designed to
provide explanations for ML algorithms.

11987
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

TABLE 7. A diassification of the contfactual explanation generators by
explainability method.

 

Explainability
method
Model-specific

Publications

 

[46], [48], [56], [61], [80], [84], [86], [91],
(93], [94], [122], [128], [132], [138], [139], [145],
[147], [149], [153], (155]H158], [161]

[6], [49], [55], [67], [81], [100], [105], [129]-
[131], [133}[137], [140}[144], [146], [148],
[150}-{152], [154], [159], [160]

 

Model-agnostic

 

 

 

 

« Contfactuals for planning. Contfactual explanation
generation appears highly relevant to sequential tasks
in robotics such as automatic planning [94], [157],
[161]. Moreover, some of the robotics-related frame-
works found in reinforcement learning settings provide
explanations for policies that a robot selects at a given
time step [132], [156].

« Contfactuals for recommendation. Ghazimatin et al.
propose a graph-based recommendation system in a
counterfactual setup [84]. They obtain a counterfactual
explanation by removing a minimal set of user actions
so that the output recommendation changes.

« Contfactuals for conflict resolution. Mosca er al.
introduce an argumentation-based framework for social
network management [149]. They use contrastive expla-
nations to answer critical questions about agent actions
in the context of multi-user privacy conflict.

2) EXPLAINABILITY METHOD

All the frameworks generating contfactual explanations can
be classified by their explainability method as either model-
specific or model-agnostic. The former type of implemen-
tations is meant for explaining decisions of particular AI
algorithms. The frameworks of the latter type generate expla-
nations irrespective of the nature of the underlying algorithm.
Table 7 presents the publications under study grouped in
terms of the explainability method that they apply. The dis-
tribution of model-specific and model-agnostic explainabil-
ity methods for generation of different types of contfactual
explanation is shown in Fig. 10. Most frameworks deal with
counterfactual model-agnostic methods.

« Model-specific contfactual explanation generators.
Several model-specific frameworks generate counter-
factuals to explain the output of decision trees [46], [80],
[122], [155]. For instance, Fernandez er al. [80] present a
recursive algorithm which extracts counterfactuals in the
form of contrast-class decision tree nodes. The relevance
of the generated counterfactuals is then measured by cal-
culating a variant of the Gower distance. The proposed
metric penalizes the number of feature changes when
traversing the tree so that sparsity is promoted. Alterna-
tively, Sokol and Flash rely on the Manhattan distance
measuring leaf-to-leaf distance in the tree to retrieve
the most relevant counterfactuals [46], [155]. Designed
specifically for decision trees, their “Glass-box”’ frame-

11988

 

Model-specific 4 ©) () @)

Model-agnostic 4 @)

ae Ny \

oot ew? or

ot
gE

 

 

 

FIGURE 10. Numbers of frameworks grouped by explainability method
with respect to the type of contfactual explanation generated.

work is argued to be easily extendable to capture the
output of other logical (rule-based) models. Aguilar-
Palacios et al. generate contrastive explanations using
gradient boosted decision trees to forecast promotional
sales [61]. The researchers make use of the weighted
Euclidean distance to present the forecast as a contrast
to the neighbouring vectorized promotions. Stepin er al.
retrieve counterfactuals from a rule matrix where each
tule is encoded in terms of all possible feature values
[122]. Subsequently, the generated counterfactuals are
ranked using a XOR-based distance to find the most
relevant counterfactual pertinent to the given contrast
class. This method is further extended to generating
counterfactuals for fuzzy decision trees.

A number of frameworks address specific properties of
counterfactuals. Thus, Ustun er al. tackle the problem
of actionability, i.c., constraining the generated coun-
terfactuals in such a manner that the imposed changes
“do not alter immutable features” and that they “do
not alter mutable features in an infeasible way” [158].
To approach this problem, a mixed integer program-
ming method is employed. Russell et al. adopt a similar
approach to encompass continuous and discrete vari-
ables as well as the combination of the two [153]. The
main focus of the work is however placed on assessing
coherence and diversity of generated counterfactuals.
In order to guarantee the coherence of the counter-
factual data example used for explanation, an integer
programming-based method is proposed. In addition,
the generated counterfactual explanations are claimed
to be diverse, as diversity constraints are applied iter-
atively to a set of candidate counterfactuals. However,
this framework is limited to: (1) explaining predictions
of only linear classifiers and (2) a simple structure of the
textual explanation template.

A large number of frameworks are limited to explain-
ing the output of particular models due to task-specific
constraints. For instance, several explanation generators
address computer vision tasks. Hendricks er al. bind

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

an input visual image with a paired textual counterfac-
tual explanation generated by a recurrent neural net-
work [86]. In their framework, a number of candidate
explanations (both image-relevant and non-relevant) are
generated, paired, and ranked. The best counterfactual
explanation is then selected to be the most class-specific
to the counterfactual image while being the most rel-
evant to the input image. Goyal er al. argue that their
model is more faithful by design, as it generates visual
explanations directly from ‘the target model based on
the receptive field of the model’s neurons” [139].

Two model-specific frameworks are found in the con-
text of video processing. For instance, Akula er al.
[128] present an empirical study where input video
frames are paired with the corresponding AND-OR
graphs, i.e., compositional recursively defined graph-
based knowledge representations capturing contextual
information. The explanations based on such graphs
are passed on to human subjects to evaluate the con-
trastive answers to the predefined questions. Alterna-
tively, Kanehira et al. train a post-hoc explanatory model
to justify a video classifier’s output [91]. A counterfac-
tual explanation is, in this case, dependent on how likely
a selected region in the given frame is classified positive
and not negative, hence all such regions are scored and
normalized.

In accordance with the findings in the previous
Section IV-C1, contfactual explanations have a great
potential for automatic planning-related tasks. Most
explanation generators meant for planning-based tasks
are model-specific due to the problem- and approach-
specific restrictions preventing them from being used for
other AI challenges. For instance, Kim et al. employ a
Bayesian probabilistic model for generating contrastive
explanations [94]. Thus, the framework operates on a
pair of plan traces defined in terms of linear temporal
logic templates. The problem of obtaining contrastive
explanations is designed as a Bayesian inference prob-
lem, with the posterior distribution to be maximized
defined as the probability of a contrastive explanation
given a set of positive and negative plan traces. Con-
versely, Sreedharan et al. consider the task of automatic
analysis of counterfactual explanations in their “‘Hier-
archical Expertise-Level Modeling” framework [156].
A robot provides a user with a plan for the next action
to take. Then, the robot expects the user to respond with
a set of foils. The robot’s task is then to convincingly
refute the foils by offering a minimal explanation for
why the foils are not acceptable under the given cir-
cumstances. In addition, Chakraborti et al. formulate
the multi-model planning problem as a tuple consisting
of the planner’s model of the problem and the corre-
sponding human approximation thereof [132]. As plan
explicability is reformulated in terms of its comprehen-
sibility by an end-user. The robot’s model is adapted to
the updates of human’s model of the problem.

VOLUME 9, 2021

The problem of contrastive explanation generation for
planning is also found to be framed in the reinforcement
learning setting. For instance, Sukkerd et al. formulate
the planning problem as the shortest stochastic path
problem and develop the corresponding problem solver
to obtain a contrastive explanation [157]. Hence, their
objective is to find an optimal policy “that minimizes
the expected cumulative cost of reaching a goal state
over all closed policies”. The explanation is believed
to justify the rejection of the policies alternative to the
optimal one. In addition, Zhao and Sukkerd explain
an autonomous system’s behaviour modeling it as a
Markov decision process [161]. Thus, a contrastive
explanation is presented as a product of the analysis of
the optimal policy at the next time step and an opposing
policy on the basis of the objective values.
Model-agnostic contfactual explanation generators.
A large number of model-agnostic frameworks treat con-
tfactual explanation generation as an optimization prob-
lem ina post-hoc manner. Wachter et al. design a generic
counterfactual explanation framework to find the closest
point to the test data example [6]. Fixing the optimal
set of weights of a trained classifier, the objective func-
tion minimizes the distance between the nearest data
points of opposing classes. Note that counterfactual data
points can be synthesized artificially. The researchers
suggest the use of the Manhattan distance weighted by
the inverse median absolute deviation to calculate the
proximity of a counterfactual to the input data example.
Another case of counterfactual explanation generation
regarded as an optimization problem is the “Constrained
Adversarial Examples’’ framework [148]. Adversarial
examples that could serve as the basis for the counterfac-
tual explanation of the output of deep learning models
are searched for with the aim of minimizing the loss
with respect to the attributes (features) between the orig-
inal and counterfactual data examples. The researchers
attempt to find the best counterfactual explanation by
minimizing the number of attributes changed. Further-
more, the gradient direction is constrained to ensure the
ethical adequacy of the explanation generated. Dandl
et al. [134] formulate counterfactual search as a multi-
objective optimization problem using a distance metric
for mixed feature spaces aiming to obtain sparse and
most plausible counterfactuals. Labaien et al. gener-
ate contrastive explanations for time-series data [141].
The explanation generation is considered a two-fold
optimization problem of finding pertinent positives and
negatives. Pawelczyk et al. make use of an autoencoder
architecture for a pretrained classifier performing coun-
terfactual search in the nearest neighbor style [151].
Model-agnostic frameworks are largely found to use
decision trees as part of the reasoning mechanism
instead of explaining their output. In contrast to the
model-specific frameworks operating on decision tree
output, Guidotti et al. employ decision trees as part

11989
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

of reconstructing the reasoning behind any arbitrary
classifier in a post-hoc fashion [140]. In their “Local
Rule-based Explanation” framework, they generate a
local neighbourhood for the given pre-classified data
example using a genetic algorithm and subsequently
train a decision tree on that newly obtained dataset
to select a minimally distant foil within that local
neighbourhood. Similarly, van der Waa et al. randomly
sample or generate a data set in the neighbourhood
local to the data point in question [159]. A decision
tree is then trained to select the foil based on the
minimum number of nodes between the original data
point and the candidate foils. Furthermore, their “Foil-
Trees” framework provides the methodological basis
for perceptual-level contrastive explanation generation
within the “‘Perceptual-Cognitive Explanation” frame-
work [150]. Subsequently, the generated contrastive
explanations are attributed to a specific group of users
by means of ontology engineering at the cognitive level
of the framework to make the explanations adaptive. In
contrast, Martens and Provost argue that decision trees
are an inadequate tool for representing, e.g., large doc-
uments [146]. Hence, they suggest the model-agnostic
“Search for Explanations for Document Classification’”’
algorithm for retrieving counterfactual explanations.
However, it is only directly applicable to binary linear
classifiers, whereas heuristics are proposed for non-
linear models.

Several model-agnostic frameworks aim at measuring
specific properties of contfactuals. Anjomshoae et al.
[129] focus on contrastive explanations that maximize
contextual importance and contextual utility. On the
one hand, contextual importance measures the extent to
which the input feature values affect the black-box algo-
rithm’s output. On the other hand, contextual utility testi-
fies how favorable the values of the selected features are
for a given decision. Thus, the context-based values are
calculated for each feature used by a black-box model
observing the changes in the output as the input varies
across the range of all possible input values. Being based
on model-agnostic and problem-independent concepts,
this framework is shown to be universally applicable to
various classification and regression algorithms. How-
ever, the scalability of such an algorithm is limited to
the use-cases operating on a small number of features.
A similar limitation is observed due to possibly high
variability of the input. Laugel er al. raise the issue of
justification for counterfactual explanation [144]. They
argue that a synthesized counterfactual data point must
be connected to the training data. Counterfactuals are
selected from a local neighbourhood circling around
the test example with the radius of the distance to the
closest correctly predicted data point of a contrast-class.
The candidate counterfactuals are then clustered, as the
initial local neighbourhood is updated to become a more
extensive hyperspherical layer, until it can no longer

11990

be extended. Laugel et ai. [100] enhance the work on
justified counterfactual explanations. They argue that
the distance from the test instance to a counterfactual
does not sufficiently measure counterfactual’s relevance,
as the counterfactual in question may appear discon-
nected from the ground-truth data. Thus, a counterfac-
tual is deemed justified if it can be connected to an asso-
ciated ground-truth data instance without crossing the
decision boundary. Fernandez et al. introduce the notion
of counterfactual sets to enhance counterfactual diver-
sity [81]. They explain random forest predictions by
fusing different tree predictors so that the resulting coun-
terfactual set contains the most relevant counterfactual.
The other neighboring counterfactuals serve to diversify
the output explanation. Mothilal et al. are also con-
cerned with counterfactual diversity [105]. They design
a loss function with a diversity metric over the generated
counterfactuals to provide end-users with multiple rele-
vant counterfactual explanations. Kusner et al. propose
a causal model to assess the so-called ‘counterfactual
fairness”’ [55]. Itis worth noting that counterfactuals are
presented in the form of conditional distributions and not
structural equations despite the fact that the causal model
employed follows Pearl’s formalism [37].

Similarly to the model-specific frameworks, numerous
model-agnostic explanation generators are found to be
task-specific. In computer vision-related classification
tasks, Chang et al. find the smallest region in the image
whose substitution would change the classifier’s predic-
tion [133]. They employ a generative model to construct
a saliency map while masking the other regions of the
input image. Similarly, Dhurandhar er al. address an
optimization problem over a perturbation variable to
produce a contrastive explanation for the image classifi-
cation task [135]. However, the proximity of the selected
counterexample to the test point is, in this case, guaran-
teed by using an autoencoder.

3) OUTPUT REPRESENTATION

The considered frameworks output contfactual explanations
in several ways. Depending on the problem considered, cont-
factuals are presented in the form of: (1) intervals or specific
values of the appropriate feature values whose alteration
would have changed the output (i.e., numerical or feature-
based output); (2) single- or multiple-sentence coherent text
(Le., linguistic output); (3) specific regions in the input image
(ie., visual output); or (4) a multi-modal combination of
(some of) the above (see Table 8). As depicted in Fig. 11,
most frameworks focus on numerical counterfactual output.

« Numerical (feature-based) contfactual explanation.
Numerical values (or intervals of values) associated to
the most relevant features usually explain the behavior
of AI algorithms. They can be represented as logical
formulas [67], [93], [94] or in tabular form reflecting
necessary changes to affect the decision [55], [56], [61],

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

TABLE 8. A dassification of the contfactual explanation frameworks by
output representation.

 

 

 

 

 

 

 

 

 

 

 

 

 

Output Publications
representation
Numerical [55], [56], [61], [67], [80], [81], [93], [941, [100],
(feature-based) [105], [132], [136], [140], [147], [148], [151],
[152], [154], [158], [160]
Linguistic [6], [48], [84], [122], [128], [131], [146], [149],
[153], [156], [157], [159], [161]
Visual [49], [130], [133]}{135], [139], [141], [142],
[144]
Multi-modal [46], [86], [91], [129], [137], [138], [143], [145],
[150], [155]
Numerical 4 ©) @
Linguistic 4 G) @) @
Visual J @® @® @
Multi-modal 4 @ ©) @
of ey 2
oo ow
CG wee
om

FIGURE 11. Numbers of frameworks grouped by output representation
with respect to the type of contfactual explanation generated.

[81], [105], [136], [147], [148], [151], [154], [158],
[160]. They can be extracted from interpretable feature-
value pairs as a result of pruning in the search space
[132]. In addition, they can replicate the internals of the
classifier’s structure, e.g., in the form of decision tree
nodes or rules [80], [140], [152].

« Linguistic contfactual explanation is a piece of gram-
matical single- or multiple-sentence text in natural lan-
guage. Single-sentence textual explanations combine
a textual description with explicitly stated numerical
feature values [6]. Such explanations suggest feature-
value based instructions [48], [122], [131] or alterna-
tive actions for a possible output change [84], [149].
They also answer end-user’s inquiries with respect to the
automatic decision in question [128], [159]. In contrast,
multiple-sentence explanations provide end-users with
specific details for the given decision [153], [156], [157],
[161] or explain multiple decisions at once [146].

« Visual contfactual explanation. On the one hand,
visual explanations for non-visual input data (ie.,
datasets containing continuous or categorical feature-
value pairs) plot feature-value pair dependencies
[134], [141], [142]. On the other hand, visual input
data (i.e., images) are associated with saliency maps
[133] or explained by contrastive patterns between the
given data example and that of an opposing class in one

VOLUME 9, 2021

iteration [130], [144] or a series thereof [49]; by depict-
ing critical regions absent in the input data example that
determine what lacks in the image to be classified differ-
ently [135]; or by visualizing spatial regions associated
to data examples of opposed classes [139].

« Multi-modal contfactual explanation is a combination
of numerical and/or linguistic and/or visual explana-
tions. Multi-modal explanations are claimed to enhance
human-robot interaction [46]. They are often selected to
be the most appropriate where the problem addressed
is concerned with pairing a computer vision problem
with a natural language processing task such as object
detection and language grounding [86]. In addition,
visual-linguistic explanations identify counterfactuality
in videos [91] and allow for dialogic interaction [150].
Such hybrid counterfactuals (in terms of their output rep-
resentation) may as well complement each other while
addressing the same task. For instance, Gomez et al.
visualize the generated explanations in the form of bar
plots combining them with explicitly stated numerical
values [138]. Alternatively, Liu et al. combine feature
importance bar plots with visual input and output [145].
While a textual explanation summarizes the degree of
importance of the selected features, a visual explanation
may present contextual in-method metrics that justify
the classifier’s reasoning [129]. Contfactual explana-
tions, as a mixture of tabular and visual output represen-
tations, appear also in an augmented reality framework
[137]. Nevertheless, explanations of different modalities
are not necessarily merged. To ensure the universality of
the proposed approaches, specific feature values are pre-
sented for tasks with datasets containing only continuous
features, i.e., where the same method is used to output
images for a handwritten digit classification problem
[143]. Finally, interaction with users can be enhanced
by means of voice-based explanations combined with
textual explanations [46], [155].

4) EVALUATION METHOD

Evaluation of generated contfactual explanations is an issue
of main concern. Unfortunately, despite an increasingly
expanding use of contfactual explanations, no uniform set of
evaluation methods has been adopted so far. Hence, itis worth
taking a look at evaluation methods from other generation-
oriented sub-areas of AI. For instance, it is common to dis-
tinguish between intrinsic and extrinsic evaluation methods
in natural language generation [163]. Intrinsic evaluation
implies assessing the performance of a natural language gen-
eration system (or its modules) as an isolated unit. In contrast,
extrinsic (task-based) methods are designed to estimate how
successfully the system performs with respect to an exter-
nal task. In addition, Gatt and Krahmer make a distinction
between “objective” (automatic, corpus-based) and “‘subjec-
tive’ (human judgements) metrics [164]. Objective metrics
include (but are not limited to) precision- and/or recall-
oriented scores, number of insertions/deletions/substitutions,

11991
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

TABLE 9. A diassification of the contfactual explanation generators by
evaluation method.

 

Evaluation Publications
method
None [6], [46], [49], [67], [93], [129]}131], [136], [145],
[149], [150], [153], [155], [157], [158], [161]
Subjective [56], [128], [137]

Objective [55], [61], [80], [81], [86], [91], [94], [100], [105],
[122], [132]-[135], [138]-[144], [146]-[148], [151],
[152], [154], [156], [159], [160]

Hybrid [48], [84]

wml O ©
Subjective 4 @ @

Objective 4 @) ()

Hybrid 4 © ©

 

 

 

 

 

 

 

 

 

 

 

 

FIGURE 12. Numbers of frameworks grouped by evaluation method with
respect to the type of contfactual explanation generated.

etc. In turn, subjective metrics measure readability, accuracy,
relevance of the generated text, as perceived by humans.
Thanks to their methodological universality, they can be
extrapolated to other (non-linguistic) modalities of generated
explanations (e.g., numerical, visual, or multi-modal) and are
therefore used to form the basis of the evaluation method
classification in this review. It is worth noting that all the
considered frameworks are evaluated by means of intrinsic
(either subjective or objective) metrics. Hence, we only make
a clear distinction between subjective and objective evalua-
tion methods in this study (see Table 9 for details). A distinc-
tion between the use of different types of evaluation metrics
can be seen in Fig. 12. It is easy to appreciate how most
frameworks deal with objective evaluation of counterfactuals.
Let us give further details below, regarding the four groups of
publications in Table 9.

« No evaluation details provided. 17 out of 52 (32.69%)
of the considered publications do not evaluate their
frameworks, i.e., neither automatic metrics for contfac-
tual explanation generation are suggested nor a human
evaluation survey is presented in such publications.
However, whereas certain publications do not provide
any specific evaluation method, some do stress that
human evaluation should be encouraged to estimate the
quality and effectiveness of the generated counterfactu-
als [46], [150], [153].

« Subjective evaluation. The subjective methods include
human preferences for certain types of contfactual

11992

explanation over others. For instance, Akula et al. show
that that contrastive explanation-seeking questions are
in general better answered by means of contfactual
explanations [128]. They classify contrastive questions
in the following 10 categories suggesting the template
questions for arbitrary objects x, x), and x2 (all being of
some class X) and y, y;, and yo (all being of some other
class Y):

— WH-X: “Why x rather than not x?”’;

— WH-X-NOT-Y: “Why x rather than y?’’;

— WH-X1-NOT-X2: “Why x; rather than x2?”’;
-— WH-NOT-Y: “Why not y?’;

— NOTX: “Ts it x rather than not x?”:

- NOT-X1-BUT-X2: “Is it x; rather than x2?’’;

— NOT-X-BUT-Y: “Is it x rather than y?”’;

-— DO-X-NOT-Y: ‘What if it is x rather than y?’’;
- DO-NOT-X: “What if it is not x?’’.

- DO-X1-NOT-X2: ‘What if it is x; and not x2?”

It is worth noting that 6 out of 10 question types
(WH-NOT-Y, NOT-X, NOT-X1-BUT-X2, NOT-X-BUT-
Y, DO-NOT-X, and DO-X1-NOT-X2) matched with
automatically generated contfactuals are shown to be
highly preferred to factual explanations.

In addition, Ferrario et al. propose an augmented reality-
based setting to favor interactivity and facilitate explain-
ing ML algorithm output to non-experts [137].
However, the two aforementioned studies [128], [137]
lack an evaluation of the quality of the generated cont-
factual explanations themselves.

Lucic et al. asked 75 subjects to judge interpretabil-
ity, actionability, and trustworthiness of the generated
contfactual explanations [56]. They concluded contfac-
tual explanations are highly interpretable and actionable.
In addition, they help users understand why the model
makes large errors while solving a regression problem
but do not support users’ trust in the model’s output.

In addition, Hendricks et al. provide results of human
evaluation for the generated explanations [86]. However,
these only include evaluations for factual explanations
and are therefore excluded from the taxonomy group
being discussed.

Objective evaluation. A majority of the researchers
propose objective (automatic) methods for evaluating
automatically generated contfactuals. A number of the
frameworks are evaluated by means of accuracy-based
metrics [61], [91], [94], [159]. Kanehira et al. propose
one accuracy-based evaluation metric for visual and lin-
guistic explanations each: negative class accuracy and
concept accuracy, respectively [91]. Negative class accu-
racy estimates the quality of the visual explanation as
the ratio of the probability of the contrast class after the
image region in question is masked out. In turn, concept
accuracy estimates how compatible the output linguistic
explanation is to its visual counterpart. It is calculated
as the intersection over union between a given region

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

and all bounding boxes in the image. Kim et al. define
the domain-specific accuracy for the automatic planning
problem of unique contrastive explanations as a sum of
the number of traces in a positive set of traces satisfying
a contraint for a contrastive explanation and those of the
negative set where the constraint is unsatisfied over all
plan traces [94]. The consistency of output explanations
is otherwise shown by measuring their accuracy on the
basis of mean error, mean absolute error, and mean
absolute percentage error [61].

An extensive number of evaluation methods are found
to be strictly task- or approach-specific. Hendricks et al.
measure word detection (i.e., which words are not image
relevant by holding out one word at a time from the sen-
tence to determine the least relevant word in the explana-
tion) and word correction (i.e., anumber of replacements
of the foiled word with words from a set of target words)
[86]. Similarly, Martens and Provost estimate explana-
tion complexity by calculating the average number of
words in the shortest explanation and problem complex-
ity according to the overall number of generated expla-
nations [146]. Ferndndez er al. evaluate the relevance
of the generated counterfactuals measuring a Gower
distance-based metric in comparison with the number
of feature changes and minimum distance (in terms of
decision tree nodes) between the leaves in the given
decision tree classifier [80]. Van der Waa et al. evaluate
the generated explanations by means of such model-
specific metrics as the average length of the explanation
in terms of decision tree nodes and the Fj-score of the
foil-tree on the test set compared to the model’s output
[159]. Kusner et al. estimate counterfactual fairness on
the basis of the density of the predicted data for their
causal models [55]. Laugel er al. claim that understand-
ability of the generated explanations can be estimated
by means of their sparsity defined as the number of non-
zero coordinates of the explanation vector [143]. Moore
et al. measure the number of solutions, the distances to
the nearest training set data points, and the transferabil-
ity of the generated counterfactuals to other datasets and
classifiers [148]. Sreedharan et al. calculate the number
of predicates that are used to generate the model lattice
[156]. Similarly, Chakraborti et ai. calculate the number
of nodes in the search space remaining after pruning
[132]. Goyal et al. report how often the discriminative
regions lie inside the test data example segmentations
as well as relevant specific key regions [139]. Labaien
et al. calculate the number of changes to switch from
the original to the selected contrastive sample following
the dataset constraints [141]. To estimate faithfulness
of the generated counterfactuals, Pawelczyk et al. sug-
gest calculating the so-called degree of difficulty of a
counterfactual suggestion to measure how costly it is to
achieve the state of the given suggestion [151]. Aiming
to provide realistic counterfactuals, Sharma er ai. intro-
duce the counterfactual explanation robustness-based

VOLUME 9, 2021

score defined as the expected distance between the input
instances and their corresponding counterfactuals [154].
In addition, the generated counterfactuals are inspected
in terms of fairness which is calculated as the expected
distance between the input and a counterfactual over
distinct values for a specified feature set. Merrick and
Taly evaluate output explanations in terms of mean
feature attributions to show the importance of relevant
references [147]. Gomez et al. evaluate counterfactuals
in terms of data distribution, feature importance, as well
as possible and actionable changes to the input [138].
Dandl er al. use the hypervolume indicator metric to
estimate the quality of the estimated Pareto front during
counterfactual search [134]. In addition, Chang et al.
measure the weakly supervised localization error for an
image detection task — the intersection-over-union ratio
over 0.5 with any of the ground truth bounding boxes
and the saliency metric, i.e., “‘the log ratio between the
bounding box area and the in-class classifier probability
after upscaling” [133].

Several metrics can be extended to be applied to other
approaches. Lash et al. estimate how much the prob-
ability of a given prediction reduces given a feature
perturbation as determined by a contrastive explanation
[142]. Dhurandhar et al. employ the concept of pertinent
positives (i.e., “factors whose presence is minimally
sufficient in justifying the final classification” [135])
and pertinent negatives (i.e., “factors whose absence is
necessary in asserting the final classification’’) to evalu-
ate factuals and counterfactuals, respectively, for a given
classification task. Both types of evaluation methods
highlight the features supporting evidence as formulated
in the contrastive explanation on the basis of the values
that a perturbation variable takes on. Fernandez et al.
evaluate counterfactuals in terms of the average of the
pairwise distances based on the feature type and the
percentage of valid counterfactuals [81].

Mothilal et al. stress that counterfactuals should be
evaluated in terms of validity (i.e., whether a generated
counterfactual really leads to a different outcome), prox-
imity (i.e., feature-wise distance between the original
and counterfactual samples), sparsity (i.e., number of
features differing in the original and counterfactual sam-
ples), and diversity (i-e., feature-wise distance between
each pair of counterfactuals) [105]. Similarly, Stepin
et al. calculate factual and counterfactual explanation
length to estimate conciseness of the generated expla-
nations [122]. They also compute the number of coun-
terfactuals and their best minimal distance to the factual
explanation to assess the relevance of counterfactuals.
Rajapaksha er al. consider coverage (as an indicator of
representativeness of a rule for a given dataset), con-
fidence (i.e., the percentage of instances in the dataset
which contain the consequent and antecedent together
over the number of instances which only contain the
antecedent), lift (i.e., an association between antecedent

11993
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

and consequent), leverage (i.e., the observed frequency
between the antecedent and consequent), and the num-
ber of features in explanation for evaluating their frame-
work against other rule-based methods [152]. Also,
White and Garcez reintroduce fidelity to the underlying
classifier on the basis of distance to the decision bound-
ary [160].
In addition, some of the model-agnostic frameworks
[140], [159] allow for measuring how well the output
of black boxes (i.e., actual output to be explained) and
grey boxes (i.e. interpretable intermediate predictors)
mimic the local neighbourhood (i.e., fidelity) and the
data example to be explained (.e., hit). Laugel et al.
measure how justified counterfactuals are by averaging a
binary score (one if the explanation is justified following
the proposed definition, zero otherwise) over all the
generated explanations [100], [144].
It is worth noting that the run-time of explanation gen-
eration algorithms is reported in addition to the evalua-
tion metrics for several frameworks [132], [139], [146],
[152], [156], [159].

e¢ Hybrid evaluation. Two frameworks are evaluated
in terms of both automatic metrics and human judg-
ments. Ghazimatin er al. calculate explanation length
to discuss comprehensiveness of explanations as well
as estimate their usefulness and credibility by survey-
ing 500 subjects [84]. In addition, Le et al. compute
fidelity, conciseness, information gain, and influence
[48]. Automatic metrics are complemented with a user
study on intuitiveness, friendliness, comprehensibility,
and understandability of generated explanations.

D. LINKS BETWEEN THEORETICAL AND PRACTICAL
CONTRIBUTIONS TO CONTFACTUAL EXPLANATION
GENERATION (ANSWER TO RQ;)
We find that only few of the existing computational frame-
works are grounded on theories of contfactual explanation.
Indeed, only 13 out of 113 studies (11.50%) were present in
both of the pools of primary studies related to RQ; and RQ».
Table 10 summarizes the characteristics of such theoretically
grounded contfactual explanation generation frameworks.
Moreover, only 3 out of the 13 (23.08%) studies interpret
the insights from the theoretical foundations to propose their
own contfactual explanation definition for problem-oriented
purposes. Kean states that “explanation in artificial intel-
ligence is based on the inference of deduction” [93]. He
complements a deductive evidence-based explanation with a
redefined abductive contrastive explanation drawing parallels
to the ‘inference to the best explanation” [103]. He models
Lipton’s theoretical framework distinguishing two types of
contrastive explanation: non-preclusive (i.e., non-restrictive)
and preclusive. The key aspect distinguishing the two types of
contrastive explanation is in regard to how a model explains
the contrast given an explanation-seeking question. Thus,
a non-preclusive contrastive explanation is ‘‘irrelevant to the
model of explaining the contrast” being “necessary in the

11994

model of explaining the question’’. On the contrary, a preclu-
sive contrastive explanation is assumed to be restricted by a
negated model of the contrast.

Aguilar-Palacios et ai. [61] refer to Lipton’s definition of
contrastive explanation [12]. Referring to Pearl [37], Bertossi
redefines causal explanation in the context of XAI to be “a
set of feature values for the entity under classification that is
most responsible for the outcome’”’ [67].

The rest of works redefine contfactual explanation on
the basis of the problem-specific constraints without explic-
itly referring to the theoretical foundations described in
Section IV-B. Driven by the task of automatic planning, Kim
et al. define a contrastive explanation to be a constraint satis-
fied by a specific set plan traces [94]. Fernandez et al. define
a counterfactual to be a set of feature changes that turn the
given data example to be classified differently [80]. Whereas
this definition is applicable to the classification problem in
general, the applicability of the framework is restricted to
decision trees only. Similarly, Hendricks et al. explain visual
concepts for the image classification task on the basis of the
so-called counterfactual evidence (i.e., an attribute discrimi-
native enough for another class of objects in the image absent
in the given image) [86]. Ghazimatin er al. [84] define a
counterfactual on the basis of their model’s internal structure:
an explanation is deemed counterfactual if after removing the
edges from the recommendation graph, the user receives a
different top-ranked recommendation. In addition, Kanehira
et al. only specify the linguistic form of a counterfactual
explanation without defining it explicitly [91].

Finally, there is a number of marginal interpretations of
contfactuals among the RQ3-related studies. Laugel et al.
denote a counterfactual as a specific data instance that
changes the algorithm’s prediction [100]. Poyiadzi er al.
denote a counterfactual to be “the new state of the object”
[49]. Nevertheless, the most commonly acceptable definition
of a contfactual in the observed RQ3-related studies states
that a contfactual explanation is a set of minimal feature
modifications that makes the model change the prediction
[81], [105], [122].

V. DISCUSSION

The findings show that a large body of research has been
elaborated on theoretical accounts of contrastive, counterfac-
tual, and contrastive-counterfactual explanation. In addition,
the topic has recently attracted attention from researchers
in XAI (see Fig. 13). Thus, 50 out of the 52 considered
state-of-the-art contfactual explanation generation frame-
works (96.15%; RQ2) have been developed from 2017 to
2020.

The results of the study, in relation to RQ), show that a
majority of the considered theoretical accounts of contfactual
explanation (49 out of 74; 66.22%) speculate on the causal
nature of explanation. However, whereas most researchers
in philosophy of science have mainly used the concept of
counterfactuality to explain causal relations between enti-
ties in question, causal inference is poorly addressed in the

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

TABLE 10. A summary of characteristics of theoretically grounded computational frameworks for contfactual explanation generation. CT stands for
contrastive explanation, CF means counterfactual explanation, and CT-CF is contrastive-counterfactual explanation.

contfactual

CT CF

et
et
et
et
et

Kean (1998) [93]

et
et
et
et
et

Relatedness to causality

CT-CF | Causal Non-causal Hybrid

 

Computational framework properties

Al problem method

model-specific numerical

 

 

 

 

RQL:CT +@----O-O-@ ®---- OOOOOO-----©-- @OO@
RQ1:CF 7 @-----® @@--O®@ O@OOOOOOOHHD
RQ1:CT-CF 4 @O00-O-@®---@
RQ2:CT 4 @ @OOO
RQ2:CF 4 ® O08)
RQ2:CT-CF 4 OOO
so oe i i? oe oe o

FIGURE 13. Numbers of theoretical and computational contfactual explanation generation frameworks
grouped by year of publication. For illustrative purposes, only the studies published from January 1990 to

September 2020 are displayed.

pool of publications concerning computational frameworks
of contfactual explanation generation. Kean directly refers
to a causal account of contrastive explanation to address the
problem of abductive reasoning [93]. In addition, Lucic et al.
[56] explicitly specify that their method is based on previous
work on philosophical accounts of contrastive explanation
[12] as well as on causal attribution [165], [166]. Kusner et al.
[55] make use of causal inference models and the correspond-
ing tools provided by Pearl [37]. They assess how discrimi-
natory the generated counterfactual explanations are for the
given classification task output. On the other hand, Bertossi
redefines the concept of causal explanation [67]. Following
a causal account of contfactual explanation, Fernandez et al.
introduce weakly causal irreducible counterfactual explana-
tion [136]. As most of the current ML-tasks are centered

VOLUME 9, 2021

around singling meaningful patterns out from unstructured
data, establishing causal relations appears to be among the AI
problems that are yet to attract global attention. This partly
explains why most of the modern contfactual explanation
generators focus on feature perturbation when searching for
the most relevant contfactuals and not establishing causal
relations between them.

At the same time, the other computational frameworks
are primarily non-causal. Furthermore, a strikingly low num-
ber of such frameworks appear to be rooted in theoretical
accounts of explanation due to an imbalance in favor of
causality-oriented theoretical accounts. However, the amount
of publications for RQ3 may be somewhat misleading, as con-
tfactual explanations are often redefined without specifically
referring to theoretical contributors in explanation. This is

11995
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

hypothesized to be due to the problem-specific necessities
ignored in previous theoretical works from different branches
of science. For instance, Laugel et al. emphasize that the
minimal perturbations required to change the predicted class
of a given observation enable a user to understand which
features locally impact the prediction and therefore how it
can be changed [144]. In this interpretation, counterfactual
explanations are conceptually most similar to counterfac-
tuals as defined by Lewis [36]. Indeed, while the concept
of “the closest possible world” does not always appear in
the related publications, it turns out to be implicitly wired
in almost all works. Instead, other considered frameworks
do not appeal directly to any of the theoretical accounts
of explanation addressed in RQ). Hence, it is worthwhile
taking a look at how contrastive and counterfactual expla-
nations are redefined in the frameworks not appearing in
Section IV-D.

In general, a consensus among researchers has been
observed on how contfactual explanations are defined irre-
spective of the theoretical framework proposed by individual
authors. In humanities and social sciences, a major difference
in various contfactual theories of explanation is observed to
concern the causal nature of explanation and its extrapolation
to non-causal cases. In computer science and AI, notions of
counterfactual and contrastive explanation are found to be the
most dissimilar when applied to non-overlapping problems.
Nevertheless, the corresponding line of research in AI makes
little use of the rich theoretical background accumulated by
now. While some rule-based approaches used in expert sys-
tems are justified theoretically (e.g., see [93]), newly emerg-
ing tasks present novel challenges for theorists and call for
updating the theories developed so far.

More precisely, ML-specific contfactual explanations are
designed to answer the question: ““Why was the outcome Y
observed instead of Y’?” [148]. Anjomshoae et al. define
finding a contrastive explanation as “‘contrasting instance
against the instance of interest’’ [129]. Fernandez et al. spec-
ify that a counterfactual is generally regarded as a hypo-
thetical instance similar to an example whose explanation
is of interest but with a different predicted class [80]. Also,
counterfactual explanations “show a difference in a particular
scenario that causes an algorithm to change its mind” [155].

As a majority of the considered frameworks are designed
for tackling classification problems, contfactual explanations
operate on the notion of a contrast-class (e.g., see [155])
answering the question: “How is the prediction altered when
the observation changes, given a classifier and an observa-
tion?’’ Furthermore, these changes are normally expected to
be minimal [143].

However, certain application domains as well as the selec-
tion of a classifier require researchers to redefine contfactuals
imposing task-dependent constraints, which makes it nearly
impossible to connect them to any of the existing theories of
contfactual explanation. For instance, Martens and Provost
define a contrastive explanation for a document classification
task to be a minimal set of words such that removing all words

11996

within this set from the document changes the predicted class
from the class of interest [146]. In addition, Guidotti et al.
reformulate a counterfactual to be a set of split conditions of a
decision tree describing the minimal number of changes in the
feature values of a test example [140]. In image classification,
it is found necessary to detect specific regions in the given
test image. For this type of tasks, the contrastive explanation-
seeking question is formulated as follows: “Which parts of
the image, if they were not seen by the classifier, would most
change its decision? or which inputs, when replaced by an
uninformative reference value, maximally change the classi-
fier output?”’ [133], [139]. Similarly, Dhurandhar et al. ask
what should be minimally and necessarily present and absent
in the given image to justify its classification [135]. Alter-
natively, counterfactuals are viewed as ‘“‘solutions that are
guaranteed to map back onto the underlying data structure”
[153]. Redefined contrastive explanations are also found in
the domain of robotics and automatic planning. According
to Sukkerd er al., a contrastive explanation answers the
question why a generated behavior is optimal with respect
to the planning objectives of an autonomous system [157].
Alternatively, contrastive explanations are used to answer
why-not questions about the system’s behavior in which the
consequences of the counterfactuals in question are pointed
out [161].

In addition, the nature of the explanation-seeking questions
for computational frameworks deserves further discussion.
Sokol and Flash distinguish three types of counterfactual
explanations: (1) a plain counterfactual (““Why?’’) generated
as the shortest possible class-contrastive counterfactual; (2) a
counterfactual explanation not conditioned on the indicated
feature(s) (“Why despite?’’); and (3) a (partially) fixed coun-
terfactual explanation (““‘Why given?’’) which is conditioned
on a predetermined set of features [46]. Hilton proposes
different types of contrastive questions such as: (1) ‘““Why X
rather than not X?’’; (2) “Why X rather than the default value
for X?” and (3) “Why X rather than Y?” [166]. Following
this distinction, Akula et al. extend this set of contrastive
questions to formulate ten contrastive question types for
counterfactual explanation generation [128] (see Section IV-
C4). Alternatively, only linguistic templates for such expla-
nations are defined without any theoretical grounding in
accordance with any accounts described in Section IV-B. For
instance, Sokol and Flash define a counterfactual explanation
to be a piece of text following the template: “The prediction
is (prediction). Had a small subset of features been differ-
ent (foil), the prediction would have been (counterfactual
prediction) instead’’ [155].

Remarkably, a wide range of frameworks favor automatic
evaluation methods. Thus, they rarely place the end-user in
the center of the explanation evaluation process. However,
we find an increasing number of interactive frameworks that
attempt not only to present the automatically generated expla-
nations to the end-user but also interact with him or her [46],
[150], [155]. Promoting interactivity (e.g., by engaging the
end-user to participate in an explanatory dialogue with the

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

system) is expected to make explanation social and further
increase user’s trust in the system’s output.

VI. CONCLUDING REMARKS

In this work, we made two main contributions. First, we pro-
vided readers with an overview on theoretical accounts
of contrastive, counterfactual, and contrastive-counterfactual
explanation as well as frameworks of automatic generation
thereof. This overview was based on a systematic literature
review. This research methodology allowed us to carry out
an unbiased reproducible study from an interdisciplinary
topic-specific search in reputable and trustworthy sources.
Second, we proposed a two-level taxonomy of the aforemen-
tioned types of explanation with the aim of providing a well-
established tool that allows us to jointly analyze different
proposals in this research field. As a result, this taxonomy
facilitates the comparison of approaches and publications.
We expect that it raises awareness in researchers in the com-
munity about main categories (definitions, practical frame-
works, etc.) and subcategories (causal, non-causal, etc.) in the
taxonomy. Moreover, we hope that it helps properly charac-
terize the body of work and leverages a deeper collaboration
and citation among similar related work.

The findings allow us to draw the following remarks. Con-
trastive and counterfactual explanations are found to be in
great demand across various sub-fields of AI. Mainly applied
to a wide range of tasks in computer vision and natural lan-
guage processing, they present a powerful tool that enhances
human-machine interaction and allows for further person-
alization of the output generated by various AI algorithms,
including ML-based black-box algorithms.

In our systematic review, we introduced the term “cont-
factual explanation’’ to unify the aforementioned families of
explanation to subsequently analyse the existing approaches
to them from three points of view.

First, we investigated theoretical accounts of contfactual
explanation to infer the similarities and differences among
the existing theoretical approaches. Contfactuals are found
to address both causal and non-causal dependencies. Hence,
being a significant challenge, unification of causal and non-
causal explanatory engines within a contfactually-driven
framework opens new perspectives for the XAI community.

Second, existing computational frameworks for contfac-
tual explanation generation have been inspected. Despite the
fact that the notion of contfactual explanation is found to
be highly task- and domain-specific, the most commonly
accepted definition of a contfactual explanation in the context
of XAI refers to a minimal set of feature modifications that
makes the model change the prediction. A crucial short-
coming relevant to the inspected frameworks is a lack of
standardization with respect to the evaluation methods. While
designing a set of standard evaluation metrics is particularly
complicated due to a different nature of the tasks that these
explanations serve, this is hypothesized to be among major
factors preventing researchers from making faster progress
in solving the problem of contfactual explanation generation,

VOLUME 9, 2021

as it complicates a fair evaluation of newly developed frame-
works against the state-of-the-art equivalents. Furthermore,
as automatically generated explanations are meant to be user-
oriented, more effort is needed to include end-users in the
process of assessing the generated explanations.

Third, a synergy between the related theories and compu-
tational frameworks has been investigated. We find that a gap
between philosophical accounts of contfactual explanation
to scientific modeling and ML-related concepts makes the
theoretical frameworks poorly applicable to XAI. In addi-
tion, the existing methodological differences affect greatly
the definition of contfactual explanation found across various
approaches. In fact, definitions vary depending on domains of
science and even approaches used for solving specific tasks.

Finally, we believe a joint interdisciplinary effort of
researchers from both humanities and computational sciences
can be particularly fruitful for further progress in contfactual
explanation generation.

ACKNOWLEDGMENT

Ilia Stepin is an FPJ Researcher (PRE2019-090153). Jose M.
Alonso is a Ramon y Cajal Researcher (RYC-2016-19802).
Alejandro Catala is a Juan de la Cierva Researcher (IJC2018-
037522-I).

REFERENCES

[1] M. Attaran and P. Deb, “Machine leaning: The new ‘big thing’ for
competitive advantage,” Int. J. Knowl. Eng. Data Mining, vol. 5, no. 4,
pp. 277-305, 2018.

[2] M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why should i trust you?’:

Explaining the predictions of any classifier,” in Proc. 22nd Int. Conf:

Knowl. Discovery Data Mining (ACM SIGKDD). New York, NY, USA:

Association Computing Machinery, 2016, pp. 1135-1144.

N. Wang, D. V. Pynadath, and S. G. Hill, “Trust calibration within a

human-robot team: Comparing automatically generated explanations,” in

Proc. 11th ACMAEEE Int. Conf. Hum.-Robot Interact. (HRI), Piscataway,

NJ, USA: IEEE Press, Mar. 2016, pp. 109-116.

C. Rudin, “Stop explaining black box machine learning models for high

stakes decisions and use interpretable models instead,” Nature Mach.

Intell., vol. 1, no. 5, pp. 206-215, May 2019.

[5] S. Anjomshoae, A. Najjar, D. Calvaresi, and K. Framling, “Explainable
agents and robots: Results from a systematic literature review,” in Proc.
18th Int. Conf. Auto. Agents MultiAgent Syst. (AAMAS). Richland, SC,
USA: International Foundation Autonomous Agents Multiagent Systems,
2019, pp. 1078-1088.

[6] S. Wachter, B. Mittelstadt, and C. Russell, “Counterfactual explanations
without opening the black box: Automated decisions and the GDPR,”
Harvard J. Law Technol., vol. 31, no. 2, pp. 841-887, 2018.

[7] G. Schurz, “Scientific explanation: A critical survey,” Found. Sci., vol. 1,

no. 3, pp. 429-465, Sep. 1995

J.C. Pitt, Theories of Explanation. Oxford, U.K.: Oxford Univ. Press,

1988.

[9] C. B. Cross, “Explanation and the theory of questions,” Erkenntnis,

vol. 34, no. 2, pp. 237-260, Mar. 1991.

[10] T. Lombrozo, “Explanation and abductive inference,” in Oxford Hand-
book of Thinking and Reasoning. Oxford, U.K.: Oxford Univ. Press, 2012,
pp. 260-276.

[11] T. Miller, “Explanation in artificial intelligence: Insights from the social
sciences,” Artif. Intell., vol. 267, pp. 1-38, Feb. 2019.

[12] P. Lipton, “‘Contrastive explanation,’ Roy. Inst. Philosophy Suppl.,
vol. 27, pp. 247-266, Mar. 1990.

[13] C. V. F Bas, The Scientific Image. Oxford, U.K.: Oxford Univ. Press,
1980.

[14] F Sermo, J. Cassens, and A. Aamodt, “Explanation in case-based
reasoning—Perspectives and goals,” Artif’ Intell. Rev., vol. 24, no. 2,
pp. 109-143, 2005.

[3

[4

[8

11997
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

(15]

[16]

(17]

[18]

[19]

[20]
[21]
[22]

[23]

[24]

[25]

[26]

[27]

[28]
[29]
[30]
[31]

[32]

[33]

[34]

[35]
[36]
[37]
[38]
[39]

[40]

[41]

[42]

[43]

[4]

11998

N. J. Roese, ““Counterfactual thinking,” Psychol. Bull., vol. 121, no. 1,
pp. 133-148, 1997.

R. M. J. Byrne, “Mental models and counterfactual thoughts about what
might have been,” Trends Cognit. Sci., vol. 6, no. 10, pp. 426-431,
Oct. 2002.

S. Chin-Parker and A. Bradner, “‘A contrastive account of explanation
generation,” Psychonomic Bull. Rev., vol. 24, no. 5, pp. 1387-1397,
Oct. 2017.

R. Wenzlhuemer, “Counterfactual thinking as a scientific method,” His-
torical Social Res., vol. 34, no. 2, pp. 27-56, 2009.

R. Folger and C. Stein, “Abduction 101: Reasoning processes to aid
discovery,’ Hum. Resour. Manage. Rev, vol. 27, no. 2, pp. 306-315,
Jun. 2017.

C. Boutilier and V. Beche, “Abduction as belief revision,” Artif: Intell.,
vol. 77, no. 1, pp. 43-94, Aug. 1995.

P. Lipton, Inference to the Best Explanation, 2nd ed. Evanston, IL, USA:
Routledge, 2004.

R.M. J. Byrne, “Counterfactual thought,” Annu. Rev. Psychol., vol. 67,
pp. 135-157, Jan. 2016.

R. M. J. Byrne, “Cognitive processes in counterfactual thinking about
what might have been,” Psychol. Learn. Motivat.-Adv. Res. Theory,
vol. 37, pp. 105-154, Oct. 1997.

L. M. Pereira and A. B. Lopes, “Cognitive prerequisites: The special case
of counterfactual reasoning,” in Machine Ethics (Studies in Applied Phi-
losophy, Epistemology and Rational Ethics), vol. 53. Cham, Switzerland:
Springer, 2020, pp. 97-102.

J. Paik, Y. Zhang, and P. Pirolli, “Counterfactual reasoning as a key for
explaining adaptive behavior in a changing environment,” Biologically
Inspired Cognit. Archit., vol. 10, pp. 24-29, Oct. 2014.

Y. Zhang, J. Paik, and P. Pirolli, “Reinforcement learning and counter-
factual reasoning explain adaptive behavior in a changing environment,”
Topics Cognit. Sci., vol. 7, no. 2, pp. 368-381, Apr. 2015.

E. Kulakova, M. Aichhorn, M. Schurz, M. Kronbichler, and J. Perner,
“Processing counterfactual and hypothetical conditionals: An fMRI
investigation,” Neurolmage, vol. 72, pp. 265-271, May 2013.

G. Grahne, “Update and counterfactuals,” J. Log. Comput., vol. 8, no. 1,
pp. 87-117, 1998.

M. Ginsberg, “Counterfactuals,” Artif: Intell., vol. 30, no. 1, pp. 35-79,
1986.

R.J. Aumann, “Backward induction and common knowledge of rational-
ity,” Games Econ. Behav., vol. 8, no. 1, pp. 6-19, Jan. 1995.

W. Spohn, “A ranking-theoretic approach to conditionals,” Cognit. Sci.,
vol. 37, no. 6, pp. 1074-1106, Aug. 2013.

E. Kulakova and M. S. Nieuwland, “Understanding counterfactuality:
A review of experimental evidence for the dual meaning of counterfactu-
als,” Lang. Linguistics Compass, vol. 10, no. 2, pp. 49-65, Feb. 2016.
N. Hendrickson, Counterfactual Reasoning: A Basic Guide for Analysts,
Strategists, and Decision Makers. Morrisville, NC, USA: LULU Press,
2011.

H. J. Ferguson and A. J. Sanford, “Anomalies in real and counterfactual
worlds: An eye-movement investigation,” J. Memory Lang., vol. 58, no. 3,
pp. 609-626, Apr. 2008.

D. K. Lewis, On the Plurality of Worlds. Oxford, U.K.: Blackwell, 1986.
D. K. Lewis, Counterfactuals. Oxford, U.K.: Blackwell, 1973.

J. Pearl, Causality: Models, Reasoning, and Inference. Cambridge, U.K.:
Cambridge Univ. Press, 2000.

D. Hume, Enguiry Concerning Human Understanding. Oxford, U.K.:
Clarendon, 1904.

B. Kment, “Counterfactuals and explanation,” Mind, vol. 115, no. 458,
pp. 261-310, Apr. 2006.

D. E. Over, C. Hadjichristidis, J. S. B. T. Evans, 8. J. Handley, and
S.A. Sloman, “The probability of causal conditionals,” Cognit. Psychol.,
vol. 54, no. 1, pp. 62-97, Feb. 2007.

D. Edgington, “Estimating conditional chances and evaluating counter-
factuals,” Studia Logica, vol. 102, no. 4, pp. 691-707, Aug. 2014.

A. L. McGill and J. G. Klein, “Contrastive and counterfactual reason-
ing in causal judgment,” J. Personality Social Psychol., vol. 64, no. 6,
pp. 897-905, 1993.

J. Fang, Z. Huang, and F. van Harmelen, “A method of contrastive
reasoning with inconsistent ontologies,” in The Semantic Web. Berlin,
Germany: Springer, 2012, pp. 1-16.

A. Péez, “The pragmatic turn in explainable artificial intelligence
(XAI),” Minds Mach., vol. 29, no. 3, pp. 441-459, Sep. 2019.

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]
[63]
[64]
[65]

[66]

[67]

[68]

[69]

J. Kunkel, T. Donkers, L. Michael, C.-M. Barbu, and J. Ziegler, “Let
me explain: Impact of personal and impersonal explanations on trust in
recommender systems,” in Proc. CHI Conf. Hum. Factors Comput. Syst.,
May 2019, pp. 1-487.

K. Sokol and P. Flach, “One explanation does not fit all: The
promise of interactive explanations for machine learning transparency,”
in KI-Kiinstliche Intelligenz, no. 2. Berlin, Germany: Springer, 2020,
pp. 235-250.

N. Elzein, “The demand for contrastive explanations,” Philos. Stud.,
vol. 176, no. 5, pp. 1325-1339, 2019.

T. Le, S. Wang, and D. Lee, ‘““GRACE: Generating concise and informa-
tive contrastive sample to explain neural network model’s prediction,”
in Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining,
Aug. 2020, pp. 238-248.

R. Poyiadzi, K. Sokol, R. Santos-Rodriguez, T. D. Bie, and P. Flach,
“FACE: Feasible and actionable counterfactual explanations,” in Proc.
AAAI/ACM Conf. Al, Ethics, Soc., Feb. 2020, pp. 344-350.

F Bergadano, V. Cutello, and D. Gunetti, “Abduction in machine learn-
ing,” in Handbook of Defeasible Reasoning and Uncertainty Manage-
ment Systems: Volume 4 Abductive Reasoning and Learning. Norwell,
MA, USA: Kluwer Academic, 2000, pp. 197-229.

M. T. Keane and B. Smyth, “Good counterfactuals and where to find
them: A case-based technique for generating counterfactuals for explain-
able AI (XAI),” in Case-Based Reasoning Research and Development.
Cham, Switzerland: Springer, 2020, pp. 163-178.

R. M. J. Byme, “Counterfactuals in explainable artificial intelligence
(XAT): Evidence from human reasoning,” in Proc. 28th Int. Joint Conf:
Artif. Intell, Aug. 2019, pp. 6276-6282.

C. Molnar, interpretable Machine Learning. A Guide for Making Black
Box Models Explainable, 1st ed. Fletcher, NC, USA: LULU, Feb. 2019.
K. Sokol and P. Flach, “Counterfactual explanations of machine learning
predictions: Opportunities and challenges for AI safety,” in Proc. AAAI
Workshop Artif. Intell. Saf., 2019, pp. 1-4.

M. Kusner, J. Loftus, C. Russell, and R. Silva, “Counterfactual fairness,”
in Proc. 31st Int. Conf: Neural Inf. Process. Syst. (NIPS). Red Hook, NY,
USA: Curran Associates, 2017, pp. 4069-4079.

A. Lucic, H. Haned, and M. de Rijke, ‘““Why does my model fail?: Con-
trastive local explanations for retail forecasting,” in Proc. Conf. Fairness,
Accountability, Transparency, Jan. 2020, pp. 90-98.

A. B. Arrieta, N. Diaz-Rodriguez, J. D. Ser, A. Bennetot, S. Tabik,
A. Barbado, S. Garcia, S. Gil-Lopez, D. Molina, R. Benjamins, R. Chatila,
and F Herrera, “Explainable artificial intelligence (XAI): Concepts,
taxonomies, opportunities and challenges toward responsible AI,” Inj:
Fusion, vol. 58, pp. 82-115, Jun. 2020.

B. Kitchenham and S. Charters, “Guidelines for performing systematic
literature reviews in software engineering,” Softw. Eng. Group, School
Comput. Sci. Math., Keele Univ., Keele, U.K., Dept. Comput. Sci.,
Durham Univ., Durham, U.K., Tech. Rep. EBSE 2007-001, 2007.

B. Kitchenham, O. P. Brereton, D. Budgen, M. Turner, J. Bailey, and
S. Linkman, “Systematic literature reviews in software engineering—
A systematic literature review,” Inf. Softw. Technol., vol. 51, no. 1,
pp. 7-15, Jan. 2009.

C. Wohlin, “Guidelines for snowballing in systematic literature studies
and a replication in software engineering,” in Proc. 18th Int. Conf. Eval.
Assessment Softw. Eng. (EASE), 2014, pp. 1-10.

C. Aguilar-Palacios, S. Munoz-Romero, and J. L. Rojo-Alvarez, “Cold-
start promotional sales forecasting through gradient boosted-based con-
trastive explanations,” IEEE Access, vol. 8, pp. 137574-137586, 2020.
H. Andreas and L. Casini, “Hypothetical interventions and. belief
changes,” Found. Sci., vol. 24, no. 4, pp. 681-704, Dec. 2019.

E. Barnes, ““Why P rather than Q? The curiosities of fact and foil,” Phil.
Stud., vol. 73, no. 1, pp. 35-53, Jan. 1994.

S. Baron, M. Colyvan, and D. Ripley, “How mathematics can make a
difference,” Philosophers Imprint, vol. 17, no. 3, pp. 1-29, 2017.

S. Baron, “Counterfactual scheming,” Mind, vol. 129, no. 514,
pp. 535-562, Apr. 2020.

S. Baron, M. Colyvan, and D. Ripley, “A counterfactual approach to
explanation in mathematics,” Philosophia Mathematica, vol. 28, no. 1,
pp. 1-34, Feb. 2020.

L. Bertossi, “An ASP-based approach to counterfactual explanations
for classification,” Rules and Reasoning (Lecture Notes in Computer
Science), vol. 12173. Cham, Switzerland: Springer, 2020, 70-81.

A. Bokulich, “How scientific models can explain,” Synthese, vol. 180,
no. 1, pp. 33-45, May 2011.

G. Botterill, “Right and wrong reasons in folk-psychological explana-
tion,” Int. J. Phil. Stud., vol. 17, no. 4, pp. 463-488, Oct. 2009.

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

[70]

[71]

[72]
[73]

[74]

[75]

[76]

[77]
[78]
[79]

[80]

[81]

[82]

[83]

[84]

[85]

[86]

[87]

[88]
[89]

[90]

[91]

[92]

[93]

[94]

[95]

[96]

S. Boulter, “Contrastive explanations in evolutionary biology,” Ratio,
vol. 25, no. 4, pp. 425-441, 2012.

M. J. L. Bours, “A nontechnical explanation of the counterfactual
definition of confounding,” J. Clin. Epidemiol., vol. 121, pp. 91-100,
May 2020.

R. Briggs, “‘Interventionist counterfactuals,” Phil. Stud., vol. 160, no. 1,
pp. 139-166, Aug. 2012.

N. Campbell, ‘‘Self-forming actions, contrastive explanations, and the
structure of the will,” Synthese, vol. 197, pp. 1225-1240, Mar. 2018.

A. Chakravartty, ““Perspectivism, inconsistent models, and contrastive
explanation,” Stud. Hist. Philosophy Sci. A, vol. 41, no. 4, pp. 405-412,
Dec. 2010.

A. Chien, “Scalar implicature and contrastive explanation,” Synthese,
vol. 161, no. 1, pp. 47-66, Mar. 2008.

S. Chin-Parker and J. Cantelon, ‘‘Contrastive constraints guide
explanation-based category learning,” Cognit. Sci., vol. 41, no. 6,
pp. 1645-1655, Aug. 2017.

M. Day and G. S. Botterill, “Contrast, inference and scientific realism,”
Synthese, vol. 160, no. 2, pp. 249-267, Jan. 2008.

J. Dickenson, “Reasons, causes, and contrasts,” Pacific Phil. Quart.,
vol. 88, no. 1, pp. 1-23, Mar. 2007.

W. Fang, “An inferential account of model explanation,” Philosophia,
vol. 47, no. 1, pp. 99-116, Mar. 2019.

R. R. Fernandez, I. M. de Diego, V. Ace na, J. M. Moguerza,
and A. Ferndndez-Isabel, “Relevance metric for counterfactuals selec-
tion in decision trees,” in Intelligent Data Engineering and Auto-
mated Learning—IDEAL 2019 (Lecture Notes in Computer Science),
vol. 11871. Cham, Switzerland: Springer, 2019, pp. 85-93.

R. R. Ferndéndez, I. M. D. Diego, V. Acefia, A. Fernéndez-Isabel, and
J. M. Moguerza, “Random forest explainability using counterfactual
sets,” Inf. Fusion, vol. 63, pp. 196-207, Nov. 2020.

C. E. Franklin, “Agent-causation, explanation, and Akrasia: A reply to
Levy’s hard luck,” Criminal Law Philosophy, vol. 9, no. 4, pp. 753-770,
Dec. 2015.

V. Gijsbers, “A quasi-interventionist theory of mathematical explana-
tion,” Logique et Analyse, vol. 60, no. 237, pp. 47-66, 2017.

A. Ghazimatin, O. Balalau, R. S. Roy, and G. Weikum, “PRINCE:
Provider-side interpretability with counterfactual explanations in recom-
mender systems,” in Proc. 13th Int. Conf. Web Search Data Mining,
Jan. 2020, pp. 196-204.

C. Held, “Towards a monist theory of explanation,” J. Gen. Philosophy
Sei., vol. 50, no. 4, pp. 447-475, Dec. 2019.

L. A. Hendricks, R. Hu, T. Darrell, and Z. Akata, “Grounding visual
explanations,” in Computer Vision—ECCV 2018 (Lecture Notes in
Computer Science), vol. 11206. Cham, Switzerland: Springer, 2018,
pp. 269-286.

J. A. Hird, “The political economy of pork: Project selection at the
US. Army corps of engineers,” Amer. Political Sci. Rev., vol. 85, no. 2,
pp. 429-456, Jun. 1991.

C. R. Hitchcock, “The role of contrast in causal and explanatory claims,”
Synthese, vol. 107, no. 3, pp. 395-419, Jun. 1996.

J. Hohwy, “Capacities, explanation and the possibility of disunity,” Znt.
Stud. Philosophy Sci., vol. 17, no. 2, pp. 179-190, Jul. 2003.

P. W. Holland, “Causal counterfactuals in social science research,” in
International Encyclopedia of the Social Behavioral Sciences, 2nd ed.
Oxford, U.K.: Elsevier, 2015, pp. 251-254.

A. Kanehira, K. Takemoto, S. Inayoshi, and T. Harada, ‘“‘Multi-
modal explanations by predicting counterfactuality in videos,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,
pp. 8586-8594.

J. Katz, “Situational evidence: Strategies for causal reasoning from obser-
vational field notes,” Sociol. Methods Res., vol. 44, no. 1, pp. 108-144,
Feb. 2015.

A. Kean, “A characterization of contrastive explanations computation,”
in PRICAI98; Topics in Artificial Intelligence (Lecture Notes in Com-
puter Science), vol. 1531. Berlin, Germany: Springer, 1998, pp. 559-610.
J. Kim, C. Muise, A. Shah, S. Agarwal, and J. Shah, “Bayesian inference
of linear temporal logic specifications for contrastive explanations,” in
Proc. 28th Int. Joint Conf. Artif. Intell., Aug. 2019, pp. 5591-5598.

R. Knowles and J. Saatsi, “Mathematics and explanatory gener-
ality: Nothing but cognitive salience,” in Erkenninis. Dordrecht,
The Netherlands: Springer, Aug. 2019, pp. 1-19, doi: 10.1007/s 10670-
019-00146-x.

D. Kostic, “General theory of topological explanations and explanatory
asymmetry,” Philos. Trans. Roy. Soc. B, Biol. Sci., vol. 375, no. 1796,
2020, Art. no. 20190321.

VOLUME 9, 2021

[97]

[98]
[99]

[100]

[101]

[102]

[103]
[104]

[105]

[106]

[107]

[108]
[109]

[110]

(111)

(112]

113]

[114]

{115]

[116]

117]

[118]

[119]

[120]

(121)

[122]

[123]

J. Kuorikoski and P. Ylikoski, “Explanatory relevance across disciplinary
boundaries: The case of neuroeconomics,” J. Econ. Methodol., vol. 17,
no. 2, pp. 219-228, Jun. 2010.

J. Kuorikoski and P. Ylikoski, “External representations and scientific
understanding,” Synthese, vol. 192, no. 12, pp. 3817-3837, Dec. 2015.
D.N. Kutach, “The entropy theory of counterfactuals,” Philosophy Sci.,
vol. 69, no. 1, pp. 82-104, Mar. 2002.

T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, and M. Detyniecki,
“Unjustified classification regions and counterfactual explanations in
machine learning,” in Machine Learning and Knowledge Discovery
in Databases (Lecture Notes in Computer Science: Lecture Notes in
Artificial Intelligence), vol. 11907. Cham, Switzerland: Springer, 2020,
pp. 37-54.

N. Levy, “Luck and agent-causation: A response to Franklin,” Criminal
Law Philosophy, vol. 9, no. 4, pp. 779-784, Dec. 2015.

P. Lichterman and I. A. Reed, “Theory and contrastive explanation
in ethnography,” Sociol. Methods Res., vol. 44, no. 4, pp. 585-635,
Nov. 2015.

P. Lipton, Inference to the Best Explanation (Philosophical Issues in
Science). Evanston, IL, USA: Routledge, 1991.

E. J. Lowe, “What is the source of our knowledge of modal truths?” Mind,
vol. 121, no. 484, pp. 919-950, 2012.

R. K. Mothilal, A. Sharma, and C. Tan, “Explaining machine learning
classifiers through diverse counterfactual explanations,” in Proc. Conf:
Fairness, Accountability, Transparency, Jan. 2020, pp. 607-617.

R. Northcott, “Degree of explanation,’ Synthese, vol. 190, no. 15,
pp. 3087-3105, Oct. 2013.

J. Neyman and K. Iwaszkiewicz, “Statistical problems in agricultural
experimentation,” Suppl. J. Roy. Stat. Soc., vol. 2, no. 2, pp. 107-180,
1935.

M. Pexton, “Manipulationism and causal exclusion,” Philosophica,
vol. 92, no. 2, pp. 13-51, 2017.

A.R. Pruss and J. L. Rasmussen, “Explaining counterfactuals of free-
dom,” Religious Stud., vol. 50, no. 2, pp. 193-198, Jun. 2014.

A. Reutlinger, “Is there a monist theory of causal and noncausal expla-
nations? The counterfactual theory of scientific explanation,” Philosophy
Sei., vol. 83, no. 5, pp. 733-745, Dec. 2016.

A. Reutlinger, “Does the counterfactual theory of explanation apply to
non-causal explanations in metaphysics?” Eur J. Philosophy Sci., vol. 7,
no. 2, pp. 239-256, 2017.

A. Reutlinger, “Extending the counterfactual theory of explanation,”
in Explanation Beyond Causation: Philosophical Perspectives on
Non-Causal Explanations. Oxford, U.K.: Oxford Univ. Press, 2018,
pp. 74-95.

L.J. Rips andB. J. Edwards, “Inference and explanation in counterfactual
reasoning,” Cognit. Sci., vol. 37, no. 6, pp. 1107-1135, Aug. 2013.
D.-H. Ruben, “A counterfactual theory of causal explanation,” Nous,
vol. 28, no. 4, pp. 465-481, 1994.

D.B. Rubin, “Bayesian inference for causal effects: The role of random-
ization,” Ann. Statist., vol. 6, no. 1, pp. 34-58, Jan. 1978.

C. Q. Schneider and I. Rohlfing, “Case studies nested in fuzzy-set QCA
on sufficiency: Formalizing case selection and causal inference,” Sociol.
Methods Res., vol. 45, no. 3, pp. 526-568, 2016.

R. Schweder, “Causal explanation and explanatory selection,” Synthese,
vol. 120, no. 1, pp. 115-124, 1999.

C. Seelos and J. Mair, “Organizational closure competencies and scaling:
A realist approach to theorizing social enteprise,” in Social Entrepreneur-
ship and Research Methods, vol. 9. Bingley, U.K.: Emerald Group Pub-
lishing Limited, 2014, pp. 147-187.

E. Sober, “A theory of contrastive causal explanation and its implica-
tions concerning the explanatoriness of deterministic and probabilistic
hypotheses,” Eur: J. Philosophy Sci., vol. 10, no. 3, pp. 1-15, Oct. 2020.
R. Stalnaker, “‘A theory of conditionals,” in Studies in Logical The-
ory (American Philosophical Quarterly Monographs 2). Oxford, U.K.:
Blackwell, 1968, pp. 98-112.

A. Steglich-Petersen, “Against the contrastive account of singular causa-
tion,” Brit. J. Philosophy Sci., vol. 63, no. 1, pp. 115-143, Mar. 2012.

I. Stepin, J. M. Alonso, A. Catala, and M. Pereira-Farina, “Generation and
evaluation of factual and counterfactual explanations for decision trees
and fuzzy rule-based classifiers,” in Proc. IEEE Int. Conf. Fuzzy Syst.
(FUZZ-IEEE), Jul. 2020, pp. 1-8.

M. Strohminger and J. Yli-Vakkuri, “Knowledge of objective modality,”
Phil. Stud., vol. 176, no. 5, pp. 1155-1175, May 2019.

11999
IEEE Access’

|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

 

[124]

[125]
[126]
[127]

[128]

[129]

[130]

131]

[132]

[133]

[134]

[135]

[136]

[137]

[138]

[139]

[140]

141]

[142]

[143]

[144]

12000

E. W. K. Tsang and F. Ellsaesser, ‘“How contrastive explanation facili-
tates theory building,” Acad. Manage. Rev., vol. 36, no. 2, pp. 404-419,
Apr. 2011.

J. Woodward, Making Things Happen: A Theory of Causal Explanation.
Oxford, U.K.: Oxford Univ. Press, 2003.

P. Ylikoski and J. Kuorikoski, “Dissecting explanatory power,” Phil.
Stud., vol. 148, no. 2, pp. 201-219, Mar. 2010.

P. Ylikoski, Social Mechanisms and Explanatory Relevance. Cambridge,
U.K.: Cambridge Univ. Press, 2011.

A. R. Akula, S. Todorovic, J. Y. Chai, and S.-C. Zhu, “Natural
language interaction with explainable AI models,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. (CVPR) Workshops, Jul. 2019,
pp. 87-90.

S. Anjomshoae, K. Framling, and A. Najjar, “Explanations of black-box
model predictions by contextual importance and utility,” in Explainable,
Transparent Autonomous Agents and Multi-Agent Systems (Lecture Notes
in Artificial Intelligence), vol. 11763. Cham, Switzerland: Springer, 2019,
pp. 95-109.

A. Apicella, F. Isgré, R. Prevete, and G. Tamburrini, “‘Contrastive
explanations to classification systems using sparse dictionaries,” Image
Analysis and Processing—ICIAP 2019 (Lecture Notes in Artificial Intel-
ligence), vol. 11751. Cham, Switzerland: Springer, 2019, pp. 207-218.
B. R. Barricelli, E. Casiraghi, J. Gliozzo, A. Petrini, and S. Valtolina,
“Human digital twin for fitness management,’ [EEE Access, vol. 8,
pp. 26637-26664, 2020.

T. Chakraborti, S. Sreedharan, Y. Zhang, and S. Kambhampati, “Plan
explanations as model reconciliation: Moving beyond explanation as
soliloquy,” in Proc. 26th Int. Joint Conf. Artif. Intell., Aug. 2017,
pp. 156-163.

C.-H. Chang, E. Creager, A. Goldenberg, and D. Duvenaud, “Explaining
image classifiers by counterfactual generation,” in Proc. 7th Int. Conf:
Learn. Represent. (ICLR), 2019, pp. 1-13.

S$. Dandl, C. Molnar, M. Binder, and B. Bischl, “Multi-objective coun-
terfactual explanations,” Parallel Problem Solving from Nature—PPSN
XVI (Lecture Notes in Computer Science), vol. 12269. Berlin, Germany:
Springer, 2020, pp. 448-469.

A. Dhurandhar, P.-Y. Chen, R. Luss, C.-C. Tu, P. Ting, K. Shanmugam,
and P. Das, “Explanations based on the missing: Towards contrastive
explanations with pertinent negatives,”’in Proc. Neural Inf. Process. Syst.
Found., 2018, pp. 592-603.

C. Fernandez, F. Provost, and X. Han, “Counterfactual explanations for
data-driven decisions,” in Proc. 40th Int. Conf. Inf. Syst. (ICIS), 2019,
pp. 1-10.

A. Ferrario, R. Weibel, and S. Feuerriegel, “ALEEDSA: Augmented
reality for interactive machine learning,” in Proc. Extended Abstr. CHI
Conf. Hum. Factors Comput. Syst., Apr. 2020, pp. 1-8.

O. Gomez, S. Holter, J. Yuan, and E. Bertini, “ViCE,” in Proc. Int. Conf:
Intell. User Interfaces (UI), 2020, pp. 531-535.

Y. Goyal, Z. Wu, J. Ernst, D. Batra, D. Parikh, and S. Lee, ““Counterfactual
visual explanations,” in Proc. 36th Int. Conf. Mach. Learn. (ICML), 2019,
pp. 42544262.

R. Guidotti, A. Monreale, F. Giannotti, D. Pedreschi, S. Ruggieri,
and F. Turini, “Factual and counterfactual explanations for black
box decision making,” JEEE Intell. Syst., vol. 34, no. 6, pp. 14-23,
Nov. 2019.

J. Labaien, E. Zugasti, and X. D. Carlos, “Contrastive explanations for
a deep learning model on time-series data,” in Big Data Analytics and
Knowledge Discovery (Lecture Notes in Computer Science), vol. 12393.
Berlin, Germany: Springer, 2020, pp. 235-244.

M. Lash, Q. Lin, N. Street, J. Robinson, and J. Ohlmann, “‘Generalized
inverse classification,” in Proc. Int. Conf. Data Mining (SDM), 2017,
pp. 162-170.

T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, and M. Detyniecki,
“Comparison-based inverse classification for interpretability in machine
learning,” in Proc. 17th Int. Conf. Inf. Process. Manage. Uncertainty
Knowl.-Based Syst. (IPMU). New York, NY, USA: Springer-Verlag,
2018, pp. 100-111.

T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, and M. Detyniecki,
“The dangers of post-hoc interpretability: Unjustified counterfactual
explanations,” in Proc. 28th Int. Joint Conf: Artif. Intell., Aug. 2019,
pp. 2801-2807.

[145]

[146]

[147]

[148]

[149]

[150]

(151]

[152]

[153]

[154]

[155]

[156]

[157]

[158]

[159]

[160]

[161]

[162]

[163]

[164]

[165]

[166]

S. Liu, B. Kailkhura, D. Loveland, and Y. Han, “Generative counterfac-
tual introspection for explainable deep learning,” in Proc. IEEE Global
Conf. Signal Inf. Process. (GlobalSIP), Nov. 2019, pp. 1-5.

D. Martens and F. Provost, “Explaining data-driven document classifica-
tions,” MIS Quart., vol. 38, no. 1, pp. 73-100, 2014.

L. Merrick and A. Taly, “The explanation game: Explaining machine
learning models using shapley values,” in Machine Learning and Knowl-
edge Extraction (Lecture Notes in Computer Science), vol. 12279. Cham,
Switzerland: Springer, 2020, pp. 17-38.

J. Moore, N. Hammerla, and C. Watkins, “Explaining deep learning mod-
els with constrained adversarial examples,” in PRICAI 2019: Trends in
Artificial Intelligence (Lecture Notes in Computer Science), vol. 11670.
Cham, Switzerland: Springer, 2019, pp. 43-56.

FE. Mosca, S. Sarkadi, J. M. Such, and P. McBurney, “Agent EXPRI:
Licence to explain,” Explainable, Transparent Autonomous Agents and
Multi-Agent Systems (Lecture Notes in Computer Science), vol. 12175.
Cham, Switzerland: Springer, 2020, pp. 21-38.

M.A. Neerincx, J. van der Waa, F. Kaptein, and J. van Diggelen, “Using
perceptual and cognitive explanations for enhanced human-agent team
performance,” in Engineering Psychology and Cognitive Ergonomics
(Lecture Notes in Computer Science), vol. 10906. Cham, Switzerland:
Springer, 2018, pp. 204-214.

M. Pawelczyk, K. Broelemann, and G. Kasneci, “Learning model-
agnostic counterfactual explanations for tabular data,” in Proc. Web Conf.,
Apr. 2020, pp. 3126-3132.

D. Rajapaksha, C. Bergmeir, and W. Buntine, “LoRMIkA: Local rule-
based model interpretability with K-optimal associations,” Inf. Sci.,
vol. 540, pp. 221-241, Nov. 2020.

C. Russell, “Efficient search for diverse coherent explanations,” in Proc.
Conf. Fairness, Accountability, Transparency, Jan. 2019, pp. 20-28.

S. Sharma, J. Henderson, and J. Ghosh, ““CERTIFAT: A common frame-
work to provide explanations and analyse the fairness and robustness of
black-box models,” in Proc. AAAI/ACM Conf. Al, Ethics, Soc., Feb. 2020,
pp. 166-172.

K. Sokol and P. Flach, “Glass-box: Explaining AI decisions with coun-
terfactual statements through conversation with a voice-enabled vir-
tual assistant,” in Proc. 27th Int. Joint Conf. Artif: Intell., Jul. 2018,
pp. 5868-5870.

S. Sreedharan, S. Srivastava, and S. Kambhampati, “Hierarchical exper-
tise level modeling for user specific contrastive explanations,” in Proc.
27th Int. Joint Conf. Artif. Intell. (IJCAI), 2018, pp. 4829-4836.

R. Sukkerd, R. Simmons, and D. Garlan, “Towards explainable multi-
objective probabilistic planning,” in Proc. 4th Int. Workshop Softw. Eng.
Smart Cyber-Phys. Syst. Washington, DC, USA: IEEE Computer Society,
May 2018, pp. 19-25.

B. Ustun, A. Spangher, and Y. Liu, “Actionable recourse in linear classifi-
cation,” in Proc. Conf. Fairness, Accountability, Transparency, Jan. 2019,
pp. 10-19.

J. van der Waa, M. Robeer, J. van Diggelen, M. Brinkhuis, and
M. Neerincx, “Contrastive explanations with local foil trees,” in
Proc. Workshop Hum. Interpretability Mach. Learn. (WHI), 2018,
pp. 1-7.

A. White and A. S. D. A. Garcez, “Measurable counterfactual local
explanations for any classifier,” in Proc. 24th Eur. Conf: Artif. Intell.
(ECAI), 2020, pp. 2529-2535.

E. Zhao and R. Sukkerd, “Interactive explanation for planning-based sys-
tems,” in Proc. 10th ACMHEEE Int. Conf. Cyber-Phys. Syst., Apr. 2019,
pp. 322-323.

N. J. van Eck and L. Waltman, “Software survey: VOSviewer, a com-
puter program for bibliometric mapping,” Scientometrics, vol. 84, no. 2,
pp. 523-538, Aug. 2010.

K. S. Jones and J. R. Galliers, Evaluating Natural Language Processing
Systems: An Analysis and Review. New York, NY, USA: Springer-Verlag,
1996.

A. Gatt and E. Krahmer, “Survey of the state of the art in natural language
generation: Core tasks, applications and evaluation,” J. Artif. Intell. Res.,
vol. 61, pp. 65-170, Jan. 2018.

D. J. Hilton and B. R. Slugoski, “Knowledge-based causal attribution:
The abnormal conditions focus model,” Psychol. Rev., vol. 93, no. 1,
p. 75, 1986.

D.J. Hilton, “Conversational processes and causal explanation,” Psychol.
Buil., vol. 107, no. 1, pp. 65-81, 1990.

VOLUME 9, 2021
|. Stepin et af: Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Al

IEEE Access’

 

ILIA STEPIN received the Engineer degree
(Hons.) in software engineering from the Moscow
State University of Instrument Engineering and
Computer Science, Russia, in 2012, the bachelor’s
degree in linguistics from Moscow State Linguistic
University, Russia, in 2016, and the M.Sc. degree
in computational linguistics from the University
of Stuttgart, Germany, in 2018. He is currently
pursuing the Ph.D. degree with the University of
Santiago de Compostela, Spain. He is currently
a Research Assistant with the Research Center in Intelligent Technologies
(CiTIUS), Santiago de Compostela, Spain. He is also working on a doctoral
project entitled “Argumentative Conversational Agents for Explainable Arti-
ficial Intelligence” with an emphasis on counterfactual explanation genera-
tion. His research interests include (but are not limited to) natural language
processing, argumentation theory, and human-machine interaction.

 

JOSE M. ALONSO (Member, IEEE) received
the M.Sc. and Ph.D. degrees in telecommunica-
tion engineering from the Technical University of
Madrid (UPM), Spain, in 2003 and 2007, respec-
tively. Since June 2016, he has been a Postdoctoral
Researcher with the Research Centre in Intelli-
gent Technologies (CiTIUS), University of Santi-
ago de Compostela (USC). He is member of the
CiTIUS-USC Research Group on Intelligent Sys-
tems. He has published more than 140 papers in
international journals, book chapters, and in peer-review conferences. His
research interests include explainable artificial intelligence, computational
intelligence, interpretable fuzzy systems, natural language generation, and
development of software tools. He is also the Deputy Coordinator of the
H2020-MSCA-ITN-2019 Project titled “Interactive Natural Language Tech-
nology for Explainable Artificial Intelligence” (NL4XAI). He is recognized
as the Ramon y Cajal Researcher (RYC-2016-19802), the Chair of the
Task Force on Explainable Fuzzy Systems in the Fuzzy Systems Technical
Committee of the IEEE Computational Intelligence Society (IEEE-CIS),
a member of the IEEE-CIS Task Force on Explainable Machine Learning,
the IEEE-CIS Task Force on Fuzzy Systems Software, the IEEE-CIS Content
Curation Subcommittee, and the IEEE 1855 Working Group for the mainte-
nance and update of the IEEE Standard for Fuzzy Markup Language IEEE
Std 1855TM-2016, an Associate Editor of IEEE Computational Intelligence
Magazine, and a Secretary of the ACL Special Interest Group on Natural
Language Generation.

 

VOLUME 9, 2021

 

 

ALEJANDRO CATALA received the Ing., M.Sc.,
and Ph.D. degrees from the Universitat Politéc-
nica de Valéncia (UPV), in 2006, 2008, and
2012, respectively. He is currently a Postdoctoral
Researcher awarded with the Juan de la Cierva Fel-
lowship at CiTIUS. He carried out his coBOTnity
Project at the Human Media Interaction Labora-
tory, University of Twente, The Netherlands. His
research interests include human-computer inter-
action, artificial intelligence, and intelligent user
interfaces. Along his research career, he has been awarded with several
personal grants by the Spanish and Valencian governments as well as the
European Union’s Horizon 2020 Research and Innovation Program under
the Marie Sklodowska-Curie Individual Fellowship Grant (MSCA-IF). He
has contributed with more than 70 peer-review publications, and regularly
serves as a Reviewer and a Program Committee Member in conferences
related to software engineering, artificial intelligence, and human-computer
interaction.

MARTIN PEREIRA-FARINA received the B.A.
and Ph.D. degrees in philosophy and computa-
tional logic from the University of Santiago de
Compostela, Spain, in 2007 and 2014, respec-
tively. He is currently a Lecturer with the Depart-
ment of Philosophy and Anthropology, University
of Santiago de Compostela. His research inter-
ests include digital humanities, philosophy of lan-
guage, argumentation theory, and computational
models for it.

12001
Surgical Innovation

Clinical Review & Education

Explainable Artificial Intelligence
for Safe Intraoperative Decision Support

Lauren Gordon, MD, MSc; Teodor Grantcharov, MD, PhD; Frank Rudzicz, PhD

What Is the Innovation?

Intraoperative adverse events are a common and important cause
of surgical morbidity.' Strategies to reduce adverse events and miti-
gate their consequences have traditionally focused on surgical edu-
cation, structured communication, and adverse event manage-
ment. However, until now, little could be done to anticipate these
events in the operating room. Advances in both data capture in the
operating room and explainable artificial intelligence (XAI) tech-
niques to process these data open the way for real-time clinical de-
cision support tools that can help surgical teams anticipate, under-
stand, and prevent intraoperative events.

Ina systematic review, 64% of studies reported improvements
in clinical decisions with automated decision support, especially if
suggestions were provided at the same time as the task.? Machine
learning (ML) techniques can provide this real-time decision sup-
port, estimating risk automatically from patient and intraoperative
data. However, there has been hesitation to adopt ML techniques in
health care* because these systems can have rare catastrophically
incorrect predictions, and high accuracies can be achieved in unex-
pected ways, such as recognizing patterns in the manner of data
recording, rather than in the content of the data themselves.

Explainable artificial intelligence is a collection of algorithms that
improve on traditional ML techniques by providing the evidence be-
hind predictions. For example, while a traditional ML algorithm in
radiology may predict that an image contains evidence of cancer, an
XAl system willindicate what and where that evidence is (eg, 3 cm,
right lower lobe nodule).

In 2018, Lundberg et al? developed an XAI-based warning sys-
tem called Prescience that predicts hypoxemia during surgical pro-
cedures up to 5 minutes before it occurs. This system monitors
vital signs and provides the clinician with a risk score that updates
in real time. It also continuously updates the clinician with reasons
for its predictions, listing risk factors such as vital sign abnormali-
ties and patient comorbidities. This can act like an additional vital
sign, regularly updating information to warn the anesthetist in real
time about upcoming risk.

With XAl, surgeons can receive similar warnings about upcom-
ing intraoperative events to augment their clinical judgement, help-
ing to avoid complications. Our team is currently working in surgi-
cal XAI to use laparoscopic videos to warn surgeons about upcoming
bleeding events in the operating room and explain this risk in terms
of patient and surgical factors. By anticipating and avoiding ad-
verse events, surgical teams may be able to reduce operative times
and improve outcomes for patients.

What Are the Key Advantages Over Existing Approaches?

Currently, risk prediction is done predominantly in the preopera-
tive setting.® Intraoperatively, surgical teams rely almost exclu-

jamasurgery.com

sively on clinical judgement to predict patient risk and physiologic
disturbance. However, many risky situations are unanticipated:
and are managed reactively rather than preventively.?

Machine learning, by making fewer assumptions about param-
eter relationships, can use unstructured data sources such as text,
audio, and video to provide more accurate predictions than tradi-
tional statistical techniques. However, the accuracy of ML often
comes at the cost of explainability.

Predictions by ML systems have traditionally been difficult to
interpret: the prediction itself is output to users, but the logic un-
derlying that output is not. New work in XAI opens this black box.
Both algorithm-specific and broadly applicable techniques have been
developed to understand how different types of ML models make
their predictions. For example, each risk factor can be consecu-
tively removed to see how this absence affects the prediction. This
essentially produces a relative risk calculation for each factor. By pre-
senting both a real-time risk estimate and its underlying reasoning,
XAI willallow surgeons to take advantage of complex ML-based pre-
diction without losing the interpretability of logistical regression.

How Will This Affect Clinical Care?

Explainable artificial intelligence could be a powerful tool for intra-
operative decision support, used in warning systems to help clini-
cians predict and avoid adverse events that may lead to complica-
tions. The overall risk estimates and contributing factors could be
shown to the surgeon and updated with vital signs.

Using multiple data sources, including audio and video, XAI-
based decision support may be able to provide these interpretable
early warnings about intraoperative events suchas bleeding and flag
action points such as hypothermia, aberrant anatomy, or tool use
that may be linked to the risk. With these factors provided to surgi-
cal teams in the operating room, XAl can significantly augment
clinician judgement.

Clinicians’ attention can then be drawn to warnings and they can
decide whether to take action based on the XAl-provided explana-
tions: to modify risk factors by, for instance, reviewing imaging, chang-
ing the surgical approach, or requesting different instruments.

Is There Evidence Supporting the Benefits

of the Innovation?

Explainable artificial intelligence, as an emerging technology, has yet

to be broadly implemented to provide decision support for sur-

geons. Prescience, a clinical decision support tool for anesthesiolo-

gists, is one example of how XAI might be used intraoperatively.
When provided with decision support from Prescience, anes-

thesiologists predicted hypoxemia with greater accuracy than by

clinical judgement alone (area under the curve, 0.78 vs 0.66;

P< .00)1). The authors estimate that anesthesiologists predict ap-

JAMA Surgery Published online September 11,2019

© 2019 American Medical Association. All rights reserved.

Downloaded From: https://jamanetwork.com/ by a University of Toronto Libraries User on 10/31/2019

El
E2

Clinical Review & Education Surgical Innovation

proximately 15% of intraoperative events and may be able to pre-
dict 30% when using Prescience. One-fifth of the time, hypoxemic
risk was potentially related to medications provided intraoperative-
ly-a highly modifiable risk factor.

Our team is currently working to developing applications of XAI
in surgical practice: identifying intraoperative events like bleeding
using patient, team, and surgeon factors. We aim to show that if
surgeons are warned about bleeding risk and associated risk fac-
tors, bleeding events can potentially be avoided.

What Are the Barriers to Implementing

This Innovation More Broadly?

Barriers to implementing XAI include data collection, technical de-
velopment, and clinician trust. Explainable artificial intelligence re-
quires a large volume of high-quality data for algorithm training. It
can be particularly challenging to obtain this volume of data in the
operating room. In response, we developed the OR Black Box re-
cording platform,’ now implemented internationally, that makes this
high-quality, high-volume data collection feasible.

Explainable artificial intelligence is still in its relative infancy, and
no standard approach has yet emerged. The relevance of techno-
logical development to surgeons may depend on whether research-
ers focus on explainablity or interpretability. Explainability of ML
models describes the means of decision-making generally, allow-
ing for high-level oversight and auditing of accuracy. The result can
be mathematically complex and may not always be understand-

ARTICLE INFORMATION

Author Affiliations: International Centre for
Surgical Safety, Li Ka Shing Knowledge Institute of
St Michael's Hospital, Toronto, Ontario, Canada
(Gordon, Grantcharov, Rudzicz); Department of
Surgery, University of Toronto, Toronto, Ontario,
Canada (Gordon, Grantcharov); Department of
Computer Science, University of Toronto,
Toronto, Ontario, Canada (Rudzicz).

Corresponding Author: Teodor Grantcharov, MD, REFERENCES

grants from Medtronic Canada, Ethicon Canada,
Intuitive Surgical, Olympus Surgical, and Baxter
outside the submitted work. Dr Rudzicz reports
personal fees from Surgical Safety Technologies
outside the submitted work.

able to clinicians. By contrast, interpretability provides understand-
able reasons behind individual predictions that clinicians can useto
make judgements. The broader discussion of what qualifies as an
acceptable explanation is the subject of ongoing debate.

Clinicians have been hesitant to trust traditional ML systems be-
cause the internal workings are unclear: while the algorithm may gen-
erally have high accuracy, it can be difficult to evaluate if any indi-
vidual prediction is correct. Clinicians may be more willing to trust
an ML system when its logic is made transparent. Ideally, physi-
cians will use XAI to augment their clinical judgement and evaluate
whether the algorithm's logic makes sense within the clinical con-
text prior to action. Further research on XAl-based decision sup-
port, demonstrating improved clinical judgement and reduced in-
traoperative events, would help build clinician trust and facilitate
adoption of XAI.

In What Time Frame Will This Innovation

Likely Be Applied Routinely?

While we expect XAI to become more widespread within the next
5 years, its adoption in surgery will depend on technological, cultural,
and regulatory factors. Culturally, it will depend on trust, which will
increase if XAl can generate demonstrably clear and actionable inter-
pretations in, for example, intraoperative risk prediction. Similarly, clini-
caltrials using XAl, with increasing regulatory acceptance of artificial
intelligence in health care, will help spread the use of data-driven, real-
time analytics in the operating room within the coming years.

outcomes: a systematic review. JAMA. 2005;293
(10):1223-1238. doi:10.1001/jama.293.10.1223

4. Cabitza F, Rasoini R, Gensini GF. Unintended
consequences of machine learning in medicine.
JAMA, 2017;318(6):517-518. doi:10.1001/jama.2017.

Submissions: Authors should contact Justin B. 7797
Dimick, MD, MPH, at jdimick@med.umich.edu
if they wish to submit Surgical Innovation papers.

5. Lundberg SM, Nair B, Vavilala MS, et al.
Explainable machine-learning predictions for the
prevention of hypoxaemia during surgery. Nat
Biomed Eng. 2018:2(10):749-760. doi:10.1038/

PhD, International Centre for Surgical Safety,

Li Ka Shing Knowledge Institute, 209 Victoria St,
Toronto, ON M5B 1T8 , Canada
(grantcharavt@smh.ca).

Section Editor: Justin B. Dimick, MD, MPH.

Published Online: September 11, 2019.
doi:10.1001/jamasurg.2019.2821

Conflict of Interest Disclosures: Dr Gordon
reports grants from Canadian Institutes of

Health Research during the conduct of the study.
Dr Grantcharov reports IP ownership in SST Inc and

JAMA Surgery Published online September 11, 2019

1. Jung JJ, Jini P, Lebovic G, Grantcharov T.
First-year analysis of the operating roam black box
study [published online June 18, 2018]. Ann Surg.
doi:10.1097/SLA.0000000000002863

2. Ehrenfeld JM, Funk LM, Van Schalkwyk J,

Merry AF, Sandberg WS, Gawande A. The incidence
of hypoxemia during surgery: evidence from two
institutions. Can J Anaesth. 2010:57(10):888-897.
doi:10.1007/s12630-010-9366-5

3. Garg AX, Adhikari NIJ, McDonald H, et al.
Effects of computerized clinical decision support
systems on practitioner performance and patient

$41551-018-0304-0

6. Moonesinghe SR, Mythen MG, Das P, Rowan KM,
Grocott MP. Risk stratification tools for predicting
morbidity and mortality in adult patients
undergoing major surgery: qualitative systematic
review. Anesthesiology. 2013;119(4):959-981. doi:
10.1097/ALN.0b013e3182a4e94d

7. Goldenberg MG, Jung J, Grantcharov TP. Using
data to enhance performance and improve quality
and safety in surgery. JAMA Surg. 2017;152(10):972-
973. doi:10.1001/jamasurg.2017.2888

jamasurgery.com

© 2019 American Medical Association. All rights reserved.

Downloaded From: https://jamanetwork.com/ by a University of Toronto Libraries User on 10/31/2019
| Essays |

Artificial Intelligence and Black-Box

Medical Decisions:

Accuracy versus Explainability

 

BY ALEX JOHN LONDON

Ithough decision-making algorithms are not new to

medicine, the availability of vast stores of medical

data, gains in computing power, and breakthroughs
in machine learning are accelerating the pace of their de-
velopment, expanding the range of questions they can ad-
dress, and increasing their predictive power. In many cases,
however, the most powerful machine learning techniques
purchase diagnostic or predictive accuracy at the expense of
our ability to access “the knowledge within the machine.”
Without an explanation in terms of reasons or a rationale
for particular decisions in individual cases, some commen-
tators regard ceding medical decision-making to black box
systems as contravening the profound moral responsibilities
of clinicians. As William Swartout puts it, when a physician
consults an expert, “[t]he physician may question whether
some factor was considered or what effect a particular find-
ing had on the final outcome and the expert is expected to
be able to justify his answer and show that sound medical
principles and knowledge were used to obtain it... . In ad-
dition to providing diagnoses or prescriptions, a consultant
program must be able to explain what it is doing and justify
why it is doing it.”? To the extent that deep learning systems
cannot explain their findings, some have questioned whether
medical systems should avoid such approaches and “sacrifice
predictive power in favor of simplicity of a model.”

As far back as the ancient Greeks, trust has been con-
nected to the ability to explain expert recommendations. We
expect that experts can marshal well-developed causal knowl-
edge to explain their actions or recommendations, a feat that

Alex John London, “Artificial Intelligence and Black-Box Medical Decisions:
Accuracy versus Explainability,” Hastings Center Report 49, no. 1 (2019): 15-
21. DOI: 10.1002/hast.973

January-February 2019

is a reality in some modern scientific domains. Against that
background expectation, the most powerful machine learn-
ing techniques seem woefully incomplete because they are
atheoretical, associationist, and opaque. A major problem
with this view about the importance of explanation, I argue
below, is that empirical findings in medicine often have bet-
ter epistemic footing than the theories that might explain
them and that atheoretical, associationist, and opaque de-
cisions are more common in medicine than critics realize.
Moreover, as Aristotle noted over two millennia ago, when
our knowledge of causal systems is incomplete and precari-
ous—as it often is in medicine—the ability to explain how
results are produced can be less important than the ability to
produce such results and empirically verify their accuracy. I
conclude with some reasons that a blanket requirement that
machine learning systems in medicine be explainable or in-
terpretable is unfounded and potentially harmful.

Justification, Explanation, and Causation

rust in experts is often grounded in their ability to pro-

duce certain results and to justify their actions. As a
result, it is sometimes claimed that trust in computational
decision-makers must be grounded in more than predictive
or diagnostic accuracy. It also requires the ability to justify
their recommendations. As Swartout notes, “By justifica-
tions, we mean explanations that tell why an expert system's
actions are reasonable in terms of principles of the domain—
the reasoning behind the system.”* Explanations of this form
require the system, or the expert who relies on it, to reveal
how a finding or a decision is grounded in two kinds of
knowledge: a “domain model” in which causal relationships
in the domain are captured and “domain principles” that lay

HASTINGS CENTER REPORT 15
out the “how to” knowledge or the dynamics of the domain
in question.

These requirements seem reasonable, in part, because
they have a long intellectual pedigree. Already in the moral
thinking of the ancient Greeks there is recognition of differ-
ent forms of knowledge, suited to different spheres, acquired
in different ways, reliable under different circumstances, and
amenable to different demands for explanation and justifica-
tion. In the practical sphere, a techne (a productive science)
is concerned with bringing into existence particular effects in
a specific domain—intervening in the causal nexus in order
to make a particular kind of house, to produce shoes for a
particular kind of horse, or to effect a siege against a particu-
lar type of stronghold. However, these effects can often be
brought about through other means. For example, Aristotle
says that empirics—people with a lot of experience in a par-
ticular domain but who lack a theory to explain their suc-
cess—can often achieve better results than people who know
only theory.® With experience, empirics gain know-how, but
they lack an account or an explanation for why their recom-
mendations work. What sets apart techne as a form of pro-
ductive knowledge is that it includes a theory whose general
principles explain why certain actions are correct in particular
circumstances.’ Whereas the empiric knows only to prescribe
chicken to preserve health, and the mere theorist knows only
that lean meats make people healthy, the science of medicine
combines the knowledge of the particular with knowledge of
the universal—the reason to prescribe chicken is because it is
a lean meat and eating lean meats is a cause of health.

The idea that experts should be able to justify their ac-
tions by marshaling knowledge of causal relationships in their
domain of expertise also has a long intellectual history. For
Aristotle, explanations are logical arguments in which the
particular to be explained is subsumed under a more gen-
eral set of claims that clarify the causal factors responsible
for generating the particular.’ Although our understanding
of causality has developed since the time of Aristotle, the idea
that an explanation should have something like this form
persists.” When explanations involve laws that track causal
relationships, true explanations provide insight into how a
domain works and, through that insight, enhance our ability
to more effectively intervene in that system, where interven-
tion is possible.

Another reason to expect computational systems to be
able to marshal causal knowledge and provide explanations
of this form is that this appears to be a mark of expertise in
disciplines such as structural engineering. Building a bridge
across a span requires a range of decisions. Expert structural
engineers make such decisions by marshaling “domain mod-
els” and “domain principles” of the sort that commentators
like Swartout expect experts to possess. They know what fac-
tors affect the success of a bridge, such as properties of the
location, features of materials to consider, and the tolerances
of various designs and the stresses of various uses. They also
know how to assign values to these variables in particular
cases and to simulate how particular structures will behave

1G HASTINGS CENTER REPORT

under expected loads and stresses of a particular setting to
within practically relevant margins of error.’° Detailed math-
ematical models of key causal relations enable structural en-
gineers to make design decisions that incorporate stakeholder
values, such as aesthetics and cost, into the construction of
reliable structures, Moreover, they can explain particular de-
cisions by elaborating the functional and causal requirements
that constrain or determine various choices, thereby helping
nonexperts understand why certain decisions were made or
why some constraints are negotiable while others are not. To-
gether, this causal knowledge and the explanations it supports
can also guide interventions to improve a structure's integrity.

The classical model of the techne and its modern in-
stantiations in areas like structural engineering thus present
a model of decision-making that is highly rational and in
which decisions reflect causal knowledge of a domain that
can be expressed in terms that are, at least in principle, ac-
cessible to nonexperts. These explanations thus help to foster
social trust by expanding the ability of other stakeholders to
understand what is at stake in various decisions. This fosters
accountability, since understanding why a decision was made
enables stakeholders to evaluate its merits and hold experts
accountable for avoidable error. It also fosters autonomy in
the form of nondomination,” to the extent that explanations
help stakeholders see why expert decisions are not arbitrary
and do not amount to abuse of professional authority.

The Black Box of Deep Learning

A gainst this background, many of the properties of the
most powerful machine learning systems appear suspect.
For example, deep learning systems are theory agnostic in the
sense that their designers do not program into them a model
that reflects their understanding of the causal structure of the
problem to be solved. Rather, programmers construct an ar-
chitecture that “learns” a model from a large set of data. This
architecture contains layers of connected nodes, like neurons
in a brain, that activate when they detect particular features
in input data, These systems are “deep” learners in that they
contain many nested layers of such nodes. In most cases,
these systems learn when data whose classification is already
established (for example, images of retinas that display or lack
diabetic retinopathy) are fed into the system. As instances ac-
cumulate, weights on the nodes in the network are automati-
cally adjusted to construct the mathematical model that most
accurately maps inputs (such as images of retinas’ or patient
medical records!) to the correct output labels. The systems
classify images as displaying diabetic retinopathy or not, or
assign a probability for a medical event, such as suicide or
readmission, to a medical record. After the training phase, the
sensitivity, specificity, and recall of such systems can then be
tested by inputting a second set of data whose classification
is already known and then comparing output classifications
to the “ground truth.” Deep learning systems can be trained
on millions of inputs, and their resulting predictions can be
highly accurate.

January-February 2019
The opacity, independence from an explicit domain

model, and lack of causal insight associated with some

powerful machine learning approaches are not radically

different from routine aspects of medical decision-making.

Despite this accuracy, deep learning systems can be black
boxes. Although their designers understand the architecture
of these systems and the process by which they generate the
models they use for classification, the models themselves can
be inscrutable to humans. Even when techniques are used to
identify features or a set of features to which a model gives
significant weight in evaluating a particular case, the relation-
ships between those features and the output classification can
be both indirect and fragile. A small permutation in a seem-
ingly unrelated aspect of the data can result in a significantly
different weighting of features. Moreover, different initial set-
tings can result in the construction of different models."*

Despite the overwhelming attention paid to the fact that
deep learning systems are unsuited to helping human users
understand the phenomenon in question, a far more signifi-
cant limitation is that they may not directly track causal re-
lationships in the world. Even when users limit the data fed
into the system to variables believed to be causally relevant to
the decision at hand, the resulting model only reflects regu-
larities in data. How these associations relate to underlying
causal relationships is unknown. Even if we can learn that a
system associates having “cocaine test: negative” in a patient's
electronic medical record with a higher likelihood of readmis-
sion,’’ this knowledge doesn’t reveal what a negative test indi-
cates or how this could be causally related to whatever causes
readmission. As a result, understanding that a system uses a
negative cocaine test as a predictor of readmission doesn’t in-
crease our ability to more effectively intervene in the system
being modeled.

In contrast to the logical and accessible decision-making
embodied in the classical techne, machine learning systems
stoke fears of unaccountability and domination by systems
that arbitrarily restrict stakeholder autonomy and represent
a conduit for experts to covertly impose arbitrary preferences
on stakeholders.

Uncertainty and Incompleteness of Medical
Knowledge

( ven the explanatory power of productive sciences like
structural engineering and the long history of regarding

January-February 2019

medicine as a paradigmatic example of a techne, it seems rea-
sonable to expect medical experts to live up to the same stan-
dards as the structural engineer. The problem with this view
is that the explanatory power of fields like structural engi-
neering derives from the degree of comparative completeness
with which the relevant causal systems are known. Although
medicine is one of the oldest productive sciences, its knowl-
edge of underlying causal systems is in its infancy; the patho-
physiology of disease is often uncertain, and the mechanisms
through which interventions work is either not known or not
well understood. As a result, decisions that are atheoretic, as-
sociationist, and opaque are commonplace in medicine.

Medicine is a domain in which the ability to intervene
effectively in the world by exploiting particular causal re-
lationships often derives from experience and precedes our
ability to understand why interventions work—our ability to
accurately model causal relationships in a larger portion of
the systems in which we intervene. Just as Aristotle’s empiric
succeeds in promoting health by prescribing chicken for a
healthy diet, even without knowing why chicken is a healthy
food, modern clinicians prescribed aspirin as an analgesic
for nearly a century without understanding the mechanism
through which it works, Lithium has been used as a mood
stabilizer for half a century, yet why it works remains uncer-
tain. Large parts of medical practice frequently reflect a mix-
ture of empirical findings and inherited clinical culture. In
these cases, even efficacious recommendations of experts can
be atheoretic in this sense: they reflect experience of benefit
without enough knowledge of the underlying causal system
to explain how the benefits are brought about.

Randomized clinical trials (RCTs) can establish causal re-
lationships between interventions and measured end points.
But the relationship between those end points and the theo-
ries that motivate intervention development and guide de-
ployment in practice is more tenuous. The hypothesis that
amyloid plaques in the brain are part of the disease process
of Alzheimer’s disease has motivated a decade-long search
for neuroprotective interventions that disrupt the amyloid
production system. The repeated failure of these efforts may
reflect the falsity of the underlying theory or merely the
practical difficulty of effectively intervening in the amyloid

HASTINGS CENTER REPORT 17
system.'° As a result, the practical findings from rigorous em-
pirical testing are frequently more reliable and reflective of
causal relationships than the theoretical claims that purport
to ground and explain them.

Medicine is thus a sphere where current theories of disease
pathophysiology or drug mechanism are often of unknown
or uncertain value, Since animal and in vitro models are un-
reliable predictors of effects in humans, specific hypotheses
generated by these theories are subjected to testing during the
process of evaluating the interventions that they support and
motivate.” Although the ambition of contemporary drug de-
velopment is to leverage expanding knowledge about these
factors to produce a more analytical and logical development
process, roughly nine of ten drugs that enter development are
never approved for any indication—and half of the drugs that
enter phase III testing fail.* Hidden within this summary
statistic is the fact that in some areas (for example, developing
neuroprotective treatments against Parkinson’s or Alzheimer’s
disease), nothing that we try has worked. Despite widespread
expectations that megadoses of vitamins will have therapeu-
tic or preventative effects in indications from cancer to mul-
tiple sclerosis, trials routinely demonstrate no clinical value.
In fact, the Carotene and Retinol Efficacy Trial was stopped
early after it was clear that participants at high risk of lung
cancer who received high doses of beta-carotene and retinyl
palmitate had a Higher incidence of cancer and a higher mor-
tality rate than participants in the control arm.”

Although Aristotle thought that techne represented a
paradigm of productive knowledge, he also understood that
not all branches of decision-making were as well understood
as others. For this reason, he warned that we must not “de-
mand in all matters alike an explanation of the reason why
things are what they are; in some cases it is enough if the fact
that they are so is satisfactorily established.””? Although we
can explain why an arch can bear a particular load, we may
not be able to explain why a drug stabilizes mood or eases
pain. In both practical science and practical wisdom, Aristo-
tle is explicit that, where we cannot have knowledge of both
particular facts and the general principles that explain them,
knowledge of the particulars is more important because it is
more critical to success in action.”

In medicine, the overreliance on theories that explain why
something might be the case has sometimes made it more dif-
ficult to validate the empirical claims derived from such theo-
ries, with disastrous effects. The long medical preference for
radical mastectomy over less aggressive alternatives was driven
by the pathophysiological theory that removing as much tis-
sue from the breast as possible would reduce the probability
of cancer recurrence. Only after a series of clinical trials was
this theory shown to be false. The same is true for the theory
of drug action that drove the use of high-dose chemotherapy
with autologous bone marrow transplant as a treatment for
end-stage breast cancer. In such cases, the overreliance on
plausible theoretical explanations lead to treatment practices
that harmed patients and consumed scarce resources precisely
because key causal claims in those theories were false.

18 HASTINGS CENTER REPORT

Even if the efficacy of a particular intervention for a given
indication has been established in large RCTs, patients in the
clinic often differ from clinical trial populations. Clinicians,
therefore, frequently make judgments about how comor-
bidities, gender, ethnicity, age, or other factors might affect
intervention efficacy and toxicity that go beyond validated
medical evidence.” Treatments are delivered on the basis of
explicit or implicit associations between a network of clinical
characteristics. In these cases, it may not be clear what infor-
mation clinicians draw on to make these judgments, whether
the implicit or explicit models that support their judgments
are valid or accurate, or whether equally qualified clinicians
would arrive at the same conclusions in the face of the same
data. Certainly, we should try to generate reliable clinical evi-
dence that can illuminate and guide these decisions. But this
kind of uncertainty is a routine part of clinical practice, and
the clinical judgment that it involves relies on an association-
ist model encoded in the neural network in the clinician's
head that is opaque and often inaccessible to others.

As counterintuitive and unappealing as it may be, the
opacity, independence from an explicit domain model, and
lack of causal insight associated with some of the most pow-
erful machine learning approaches are not radically differ-
ent from routine aspects of medical decision-making. Our
causal knowledge is often fragmentary, and uncertainty is the
rule rather than the exception. In such cases, careful empiri-
cal validation of an intervention’ practical merits is the most
important task. When the demand for explanations of how
interventions work is elevated above careful, empirical vali-
dation, patients suffer, resources are wasted, and progress is

delayed.

Responsible Medical Decision-Making

f the goal is to secure trust among stakeholders, then the

accuracy of a system relative to viable alternatives must be
a central concern, One advantage of explicit computational
systems over the neural networks inside the heads of expert
clinicians is that the reliability and accuracy of the former
can be readily evaluated and incrementally improved. In
high-volume contexts, such as diagnostic medical imaging,
the use of tests that are less sensitive (that is, less likely to
detect true cases of a condition), less specific (less likely to
exclude only false cases), or less precise (with less likelihood
that a positive test result correlates with having the condition)
than available alternatives can result in avoidable morbidity
and mortality on the part of patients. Any preference for less
accurate models—whether computational systems or human
decision-makers—carries risks to patient health and welfare.”
Without concrete assurance that these risks are offset by the
expectation of additional benefits to patients, a blanket pref
erence for simpler models is simply a lethal prejudice.

It might be objected that explainability is too demand-
ing a requirement since even simple associationist models are
not capable of tracking causal relationships. Nevertheless,
defects in the data used by deep learning systems to construct

January-February 2019
In medicine, the ability to intervene effectively in the

world by exploiting causal relationships often

derives from experience and precedes clinicians

ability to understand why interventions work.

decision models—such as biases stemming from the over- or
underrepresentation of particular classes of individuals—can
be inherited by these systems.”” Without insight into how the
models work, critics worry that the models may incorporate
biases that are harmful enough to offset marginal gains in
predictive power. In order to ward off such possibilities, crit-
ics hold that machine learning systems must at least be inter-
pretable to humans.

Ina popular example, Rich Caruana and colleagues report
that, although a neural net was more accurate than alterna-
tives at diagnosing the probability of death from pneumonia,
it ranked asthmatic patients as having a lower probability
than the general population.” This finding is “counterintui-
tive” because patients with a history of asthma are typically
admitted directly into the intensive care unit (ICU) for ag-
gressive medical care; it is the added care that gives them a
lower probability of death. Without such aggressive care,
asthmatic patients have a higher probability of death from
pneumonia. Their score in the system is seen as misleading
because it doesn’t reflect patients’ underlying medical need.
This prompted Caruana et al. to prefer less accurate but more
transparent models in which they could adjust the weight as-
signed to “asthmatic” to reflect current medical knowledge.

It is important to point out, however, that the ascription
of bias in this example presupposes that the goal of the deci-
sion model is to optimize the allocation of medical resources
against a baseline risk of death that is independent of current
medical practice. But insofar as the training data reflect the
probability of death given standard medical practice, this is
probably an inappropriate expectation for many patients, not
just for asthmatics. Everyone’s outcomes reflect the effects of
a range of practices not captured in the data. But patients
with different medical histories or comorbidities are likely to
receive different levels of care. A data set that reflects patient
outcomes and lacks a comprehensive and granular representa-
tion of patient characteristics and treatment practices would
probably not accurately reflect probability of death prior to
any medical intervention. If given more comprehensive infor-
mation about treatments administered to individual patients,
even a simple system would learn that, without ICU admis-
sion, asthma puts a patient at high probability of death.

In contrast, if the goal is to identify patients most at
risk of dying given standard practice, then systems that rank
asthmatics at lower risk are not biased. Rather, the system

January-February 2019

is actuarially correct—patients with asthma who receive ag-
gressive medical intervention have a lower probability of death
than some nonasthmatic patients who likely receive less ag-
gressive medical care. Such a system could be used to identify
classes of patients who might benefit from care additional to,
or more aggressive than, standard practice. However, how to
improve the outcomes of different classes of patients is a dis-
tinct, causal question that we should not expect this system to
answer. Rather than illustrating the need for interpretability,
this example illustrates the importance of understanding the
kind of judgments that a data set is likely to be able to sup-
port, clearly validating the accuracy of those specific decisions
on real-world data, and then restricting the use of associative
systems to making the specific decisions for which their ac-
curacy has been empirically validated.

This example also illustrates dangers inherent in mistak-
ing the plausibility of associations in interpretable systems for
causal relationships that can be exploited through interven-
tion. Machine learning systems can leverage associations in
data sets to make highly accurate predictions and diagnoses.
Except for systems specifically designed for causal discovery,””
it is a mistake to expect those associations to track causal re-
lationships in a way that we can exploit through interven-
tion. Interpretability may thus feed a misguided expectation
that understanding a set of associations valuable for specific
diagnostic or prediction tasks will increase our ability to per-
form additional tasks to which those associations are not well
suited and for which their accuracy has not been validated.

We saw earlier that one reason explanation is seen as the
hallmark of expertise is that it involves communicating causal
relationships in the relevant domain to stakeholders. When
we lack causal knowledge in a domain, however, systems that
use complex associations to reliably make diagnostic or pre-
dictive decisions with high sensitivity and specificity can have
significant value. Because those associations do not directly
track causal relationships, the value of interpretability is not
clear.

It is also unclear what interpretability amounts to. Hu-
man decisions are often interpretable in the sense that we can
rationalize them after the fact. But such rationalizations don’t
necessarily reveal why a person made the decision, since the
same decision may be open to many different post-hoc ratio-
nalizations. As Zachary Lipton has argued,” machine learn-
ing systems are often interpretable in this sense as well.” If

HASTINGS CENTER REPORT 19
this is all that is required to satisfy the requirement of inter-
pretability, then both humans and machine learning systems
are interpretable. We would therefore lack grounds for prefer-
ring less accurate models. But interpretability of this kind is
unlikely to facilitate the goal of maintaining system reliability.

Interpretability might mean, instead, that humans should
be capable of simulating the model a system uses for decision-
making. This might involve taking “input data together with
the parameters of the model and in reasonable time step[ping]
through every calculation required to produce a prediction.”*°
In this case, complex machine learning techniques, such as
those used by deep learning systems, are currently uninterpre-
table. But so are some otherwise simple analytical approaches
(such as linear models or rule-based systems) in sufficiently
complex cases.*! Most human decisions are not interpretable
in this sense either. Given the degree of incompleteness in
our own domain knowledge and the fact that associations do
not necessarily capture causal relationships, it is not clear that
the ability to “step through” a model will provide marginal
improvements in reliability sufficient to offset marginal losses
in accuracy. It may, however, lead overconfident analysts to
use these models for purposes to which they are inherently
unsuited.

Accountability and Nondomination

I productive sciences where domain models robustly cap-
ture causal relationships, the accuracy and reliability of
diagnostic or prognostic decisions can be grounded in expla-
nations that rely heavily on relationships and reasons derived
from those models. In spheres where this knowledge is in-
complete and piecemeal, the warrant for causal claims and
assurances of accuracy and reliability must be grounded in
empirical testing.

To promote accountability and to ensure that machine
learning systems are not covert tools for arbitrary interference
with stakeholder autonomy in medicine, regulatory practices
should establish procedures that limit the use of machine
learning systems to specific tasks for which their accuracy and
reliability have been empirically validated. This promotes
accountability and freedom from domination by enabling
stakeholders to intelligently use artificial intelligence systems
to perform tasks for which they are the most efficacious al-
ternative—even if the grounds for their superior performance
remain opaque.

To create such a system, greater emphasis should be placed
on ensuring that data sets and analytical approaches are
aligned with the decisions and uses they are intended to fa-
cilitate. Much as we seek to clarify the indications for which
a drug can be prescribed, the use cases to which a machine
learning system is suited and for which its accuracy and reli-
ability have been validated should be clearly designated. Like-
ly uses for which system performance has not been validated
should be discouraged. The robustness of systems should be
tested during development by exploring windows of opera-
tion outside of which accuracy and reliability break down.

20 HASTINGS CENTER REPORT

This involves validating system performance on multiple data
sets that reflect the diversity of real-world contexts. Before
deployment in clinical practice, system performance should
also be tested against standard-of-care alternatives in prospec-
tive trials measuring impacts on clinically meaningful end
points.” This means, in part, that when machine learning
systems perform the same decision task as humans, the rela-
tive accuracy and reliability of humans and machines should
be evaluated in well-designed empirical studies. Deployment
should also include a plan for continuous quality improve-
ment in which system performance can be audited and ac-
curacy reassessed in light of changing clinical contexts.

Recommendations to prioritize explainability or inter-
pretability over predictive and diagnostic accuracy are un-
warranted in domains where our knowledge of underlying
causal systems is lacking. Such recommendations can result
in harms to patients whose diseases go undiagnosed or who
are exposed to unnecessary additional testing. They may also
encourage the use of machine learning systems for purposes
to which they are not suited if associations in highly predic-
tive models are mistakenly treated as causal relations that can
be exploited through intervention without first validating the
causal relevance of such associations.

Acknowledgments

I thank Jonathan Kimmelman for sage advice about relevant
examples, David Danks for critical feedback on several drafts of
this paper, and an anonymous referee for helpful suggestions.

1. N. Rotstein, “Challenges in Machine Learning: Cracking the Black
Box Open,” Medium, April 19, 2018, https:medium.com/maria-01/
ok-computer-but-why-dde64c22b7ba.

2. W. R. Swartout, “XPLAIN: A System for Creating and Explaining
Expert Consulting Programs,” Artificial Intelligence 21, no. 3 (1983):
285-325.

3. S. Athey, “Beyond. Prediction: Using Big Data for Policy Prob-
lems,” Science 355, no. 6324 (2017): 483-85; see also D. E. Adkins,
“Machine Learning and Electronic Health Records: A Paradigm Shift,”
American Journal of Psychiatry 174, no. 2 (2017): 93-94; R. Caruana et
al., “Intelligible Models for Healthcare: Predicting Pneumonia Risk and
Hospital 30-Day Readmission,” in Proceedings of the 21th ACM SIG-
KDD International Conference on Knowledge Discovery and Data Mining
(New York: Association for Computing Machinery, 2015), 1721-30.

4. Swartout, “XPLAIN,” 286-87.

5. Ibid., 287.

6. Aristotle, The Complete Works of Aristotle, ed. J. Barnes (Princeton,
NJ: Princeton University Press, 1984), Metaphysics, L1.98]al2-24, and
Nicomachean Ethics, Vii.

7. A. J. London, “Moral Knowledge and the Acquisition of Virtue
in Aristotle's Nicomachean and Eudemian Ethics,” Review of Metaphys-
ics 54, no. 3 (2001): 553-83.

8. D-H. Ruben, Explaining Explanation (New York: Routledge,
2015), 109.

9. W. C. Salmon, Scientific Explanation and the Causal Structure of the
World (Princeton, NJ: Princeton University Press, 1984).

10. C. C. Fu and S. Wang, Computational Analysis and Design of
Bridge Structures (Boca Raton, FL: CRC Press, 2014).

11. PB Petit, “Civic Republicanism,” (Oxford: Oxford University
Press, 1997).

12. V. Gulshan et al., “Development and Validation of a Deep Learn-
ing Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus

January-February 2019
Photographs,” Journal of the American Medical Association 316, no. 22
(2016): 2402-10.

13. A. Rajkomar et al., “Scalable and Accurate Deep Learning with
Electronic Health Records,” Digital Medicine 1, no. 1 (2018): article
18.

14. Z. C. Lipton, “The Mythos of Model Interpretability,” arXiv pre-
print, arXiv:1606.03490, 2016, https://arxiv-org/pdf/ 1606.03490.pdf.

15. M. Bayati et al., “Data-Driven Decisions for Reducing Readmis-
sions for Heart Failure: General Methodology and Case Study,” PloS
One 9, no. 10 (2014): €109264.

16. J. Kimmelman and A. J. London, “Predicting Harms and Ben-
efits in Translational Trials: Ethics, Evidence, and Uncertainty,” PLoS
Medicine 8, no. 3 (2011): 1001010.

17. J. Kimmelman and A. J. London, “The Structure of Clinical
Translation: Efficiency, Information, and Ethics,” Hastings Center Re-
port 45, no. 2 (2015): 27-39.

18. D. W. Thomas et al., Clinical Development Success Rates 2006—
2015 (San Diego, CA: Biomedtracker, 2016).

19. “Carotene and Retinol Efficacy Trial (CARET},” National Can-
cer Institute, accessed December 20, 2018, https:epi.grants.cancer.gov/
Consortia/members/caret.html.

20. Aristotle, Nicomachean Ethics, Lwvii.

21. Ibid., VI-vii.

22. A. J. London and J. Kimmelman, “Accelerated Drug Approval
and Health Inequality,” /AMA Internal Medicine 176, no. 7 (2016):
883-84; see also Kimmelman and London, “Structure of Clinical
Translation.”

23. A. J. London, “Groundhog Day for Medical Artificial Intelli-
gence,” Hastings Center Report 48, no. 3 (2018): inside back cover.

24. Caruana et al., “Intelligible Models.”

25. F Cabitza, D. Ciucci, and R. Rasoini, “A Giant with Feet of Clay:
On the Validity of the Data That Feed Machine Learning in Medicine,”
in Organizing for the Digital World (New York: Springer, 2019), 121-36;
see also D. Danks and A. J. London, “Algorithmic Bias in Autono-
mous Systems,” in Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence (Marina del Rey, CA: IJCAI, 2017),
4691-97.

26. Caruana et al., “Intelligible Models.”

27. P. Spirtes et al., Causation, Prediction, and Search (Cambridge,
MA: MIT Press, 2000).

28. Lipton, “The Mythos of Model Interpretability.”

29. Rajkomar et al., “Scalable and Accurate Deep Learning.”

30. Lipton, “The Mythos of Model Interpretability.”

31. Ibid.

32. M. D. Abramoff et al., “Pivotal Trial ofan Autonomous AI-Based.
Diagnostic System for Detection of Diabetic Retinopathy in Primary
Care Offices,” NP/ Digital Medicine 1, no. 1 (2018): article 39.

The Strange Tale of Three Identical

Strangers: Cinematic Lessons in Bioethics

 

BY BRYANNA MOORE, JEREMY R. GARRETT, LESLIE ANN McNOLTY,

AND MARIA CRISTINA MURANO

im Wardle’s 2018 documentary film Three Identical

Strangers is an exploration of identity, family, and

loss. It’s also about nature versus nurture and the
boundaries of ethically permissible research, particularly re-
search involving children. The film tells the story of iden-
tical triplets—David Kellman, Bobby Shafran, and Eddy
Galland—who were separated soon after birth in 1961. A
different family adopted each boy, without being told that
their son had two identical brothers. Through sheer coinci-
dence, at age nineteen, Bobby and Eddy met. After a local
newspaper picked up their story and published a picture of
them, David entered the fray. Their unlikely reunion became
a national feel-good sensation. Then the real story began to

unfold.

Bryanna Moore, Jeremy R. Garrett, Leslie Ann McNolty, and Maria Cristina
Murano, “The Strange Tale of Three Identical Strangers: Cinematic Lessons in
Bioethics,” Hastings Center Report 49, no. 1 (2019): 21-23. DOI: 10.1002/
hast.974

January-February 2019

The adoption agency responsible for finding the families
was collaborating with a group of researchers working on a
study about .. . something. The design, purpose, and find-
ings of the study, headed by Austrian psychiatrist and psy-
choanalyst Peter B. Neubauer, remain unpublished and are
not exactly clear. In addition to sharing some of the personal
trials and tribulations of the brothers and their families, the
film details their fight to obtain information about the study
and for closure. But while the film may have received rave
reviews, Three Identical Strangers left us feeling uneasy. We
worried that, with so much information about the study
missing, any analysis of the film might fall prey to some of
the same kinds of concerns that we had with the film. Three
Identical Strangers demands ethical scrutiny, however.

We came away with two sets of questions. One set—and
in a way, perhaps the simpler set—has to do with the story
that the film documents. Separating biologically related chil-
dren for the purpose of medical research may strike many as
ethically problematic regardless of whether the researchers

HASTINGS CENTER REPORT 21
1903.02409v1 [cs.AI] 5 Mar 2019

arXiv

A Grounded Interaction Protocol for Explainable Artificial
Intelligence

Prashan Madumal
University of Melbourne
Victoria, Australia
pmathugama@student.unimelb.edu.au

Liz Sonenberg
University of Melbourne
Victoria, Australia
Lsonenberg@unimelb.edu.au

ABSTRACT

Explainable Artificial Intelligence (<AI) systems need to include
an explanation model to communicate the internal decisions, be-
haviours and actions to the interacting humans. Successful expla-
nation involves both cognitive and social processes. In this paper
we focus on the challenge of meaningful interaction between an
explainer and an explainee and investigate the structural aspects
of an interactive explanation to propose an interaction protocol.
We follow a bottom-up approach to derive the model by analysing
transcripts of different explanation dialogue types with 398 expla-
nation dialogues. We use grounded theory to code and identify key
components of an explanation dialogue. We formalize the model
using the agent dialogue framework (ADF) as a new dialogue type
and then evaluate it in a human-agent interaction study with 101
dialogues from 14 participants. Our results show that the proposed
model can closely follow the explanation dialogues of human-agent
conversations.

KEYWORDS

Explainable AI; Interpretable Machine Learning; Dialogue Model;
Human-Agent Interaction

ACM Reference Format:

Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. 2019. A
Grounded Interaction Protocol for Explainable Artificial Intelligence. In Proc.
of the 18th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2019), Montreal, Canada, May 13-17, 2019, FAAMAS,
9 pages.

1 INTRODUCTION

In scenarios where people are required to make critical choices
based on decisions from an artificial intelligence (AI) system, it is
important for the system to able to generate understandable expla-
nations that clearly justify its decisions. An appropriate explanation
can promote trust in the system, allowing better human-AI cooper-
ation [30]. Explanations also help people to reason about the extent
to which, if at all, they should trust the provider of the explanation.

 

Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 13-17, 2019,
Montreal, Canada. © 2019 International Foundation for Autonomous Agents and
Multiagent Systems (www.ifaamas.org). All rights reserved.

Tim Miller
University of Melbourne
Victoria, Australia
tmiller@unimelb.edu.au

Frank Vetere
University of Melbourne
Victoria, Australia
f.vetere@unimelb.edu.au

As Miller [21, pg 10] notes, the process of Explanation involves
two processes: (a) a Cognitive process, namely the process of deter-
mining an explanation for a given event, called the explanandum,
in which the causes for the event are identified and a subset of
these causes is selected as the explanation (or explanans); and (b)
the Social process of transferring knowledge between explainer and
explainee, generally an interaction between a group of people, in
which the goal is that the explainee has enough information to
understand the causes of the event.

However, much research and practice in explainable AI uses
the researchers’ intuitions of what constitutes a ‘good’ explanation
rather basing the approach on a strong understanding of how people
define, generate, select, evaluate, and present explanations [21, 22].
Most modern work on Explainable AI, such as in autonomous
agents [5, 7, 15, 36] and interpretable machine learning [13], does
not discuss the interaction and the social aspect of the explanations.
The lack of a general interaction model of explanation that takes
into account the end user can be attributed as one of the shortcom-
ings of existing explainable AI systems. Although there are existing
conceptual explanation dialogue models that try to emulate the
structure and sequence of a natural explanation [2, 32], we propose
that improvements will come from further empirically-driven study
of explanation.

Explanation naturally occurs as a continuous interaction, which
gives the interacting party the ability to question and interrogate
explanations. This allows the explainee to clear doubts about the
given explanation by further interrogations and user-driven ques-
tions. Further, the explainee can express contrasting views about
the explanation that can set the premise for an argumentation based
interaction. This type of iterative explanation can provide richer
and satisfactory explanations as opposed to one-shot explanations.
Note that we are not claiming that AI explanations are necessarily
textual conversations. These interactions, questions, and answers
can occur as part of other modalities, such as visualisations, but we
believe that such interactions will follow the same model.

Understanding how humans engage in conversational explana-
tion is a prerequisite to building an explanation model, as noted by
Hilton [17]. De Graaf [11] note that humans attribute human traits,
such as beliefs, desires, and intentions, to intelligent agents, and it
is thus a small step to assume that people will seek to explain agent
behaviour using human frameworks of explanation. We hypothe-
sise that AI explanation models with designs that are influenced
by human explanation models have the potential to provide more
intuitive explanations to humans and therefore be more likely to be
understood and accepted. We suggest it is easier for the AI to emu-
late human explanations rather than expecting humans to adapt to
a novel and unfamiliar explanation model. While there are mature
existing models for explanation dialogs [32, 33], these are idealised
conceptual models that are not grounded on or validated by data,
and seem to lack iterative features like cyclic dialogues.

In this paper our goal is to introduce a dialogue model and an
interaction protocol that is based on data obtained from different
types of explanations in actual conversations. We derive our model
by analysing 398 explanation dialogues using grounded theory [16]
across six different dialogue types. Frequency, sequence and rela-
tionships between the basic components of an explanation dialogue
were obtained and analyzed in the study to identify locutions, termi-
nation rules and combination rules. We formalize the explanation
dialogue model using the agent dialogue framework (ADF) [20], then
validate the model in a human-agent study with 101 explanation
dialogues. We propose that by following a data-driven approach
to formulate and validate, our model more accurately defines the
structure and the sequence of an explanation dialogue and will
support more natural interaction with human audiences than ex-
planations from existing models. The main contribution of this
paper is a grounded interaction protocol derived from explanation
dialogues, formalized as a new atomic dialogue type [35] in the
ADF.

We first discuss related work regarding explanation in AI and
explanation dialogue models, then we outline the methodology
of the study and collection of data and its properties. We then
present the analysis of the data, identifying key components of
an explanation dialogue and gaining insight to the relationships
of these components, formalising it using ADF and comparing
with a similar conceptual model [34]. We then describe the human-
agent study and present the validation of the model. We conclude
by discussing the model with its contribution and significance in
explainable AL.

2 RELATED WORK

Explaining decisions of intelligent systems has been a topic of inter-
est since the era of expert systems, e.g. [8, 18]. Early work focused
particularly on the explanation’s content, responsiveness and the
human-computer interface through which the explanation was de-
livered. Kass and Finin [18] and Moore and Paris [23] discussed the
requirements a good explanation facility should have, including
characteristics like “Naturalness”, and pointed to the critical role
of user models in explanation generation. Cawsey’s [6] EDGE sys-
tem also focused on user interaction and user knowledge. These
were used to update the system through interaction. So, in early
explainable AI, both the cognitive and social attributes associated
with an agent’s awareness of other actors, and capability to inter-
action with them, has been recognized as an essential feature of
explanation research. However, limited progress has been made.
Indeed recently, de Graaf and Malle [11] still find the need to em-
phasize the importance of understanding how humans respond
to Autonomous Intelligent Systems (AIS). They further note how

humans will expect a familiar way of communication from AIS
systems when providing explanations.

To accommodate the communication aspects of explanations,
several dialogue models have been proposed. Walton [32, 33] intro-
duces a shift model that has two distinct dialogues: an explanation
dialogue and an examination dialogue, where the latter is used
to evaluate the success of an explanation. Walton draws from the
work of Memory Organizing Packages (MOP) [28] and case-based
reasoning to build the routines of the explanation dialogue models.
Walton’s dialogue model has three stages: opening, argumentation,
and closing [32]. Walton suggests an examination dialogue with
two rules as the closing stage. These rules are governed by the
explainee, which corresponds to the understanding of an explana-
tion [31]. This sets the premise for the examination dialogue of an
explanation and the shift between explanation and examination to
determine the success of an explanation [33].

A formal dialogical system of explanation is also proposed by
Walton [31]. This has three types of conditions: dialogue condi-
tions, understanding conditions, and success conditions. Arioua [2]
formalize and extend Walton’s dialectical system by incorporating
Prakken’s [26] framework of dialogue formalisation.

Argumentation also comes into to play in explanation dialogues.
Walton and Bex [34] introduce a dialogue system for argumentation
and explanation that consists of a communication language that
defines the speech acts and protocols that allow transitions in the
dialogue. This allows the explainee to challenge and interrogate
the given explanations to gain further understanding. Villata et al.
[30] focus on modelling information sources to be suited in an
argumentation framework, and introduce a socio-cognitive model
of trust to support judgements about trustworthiness.

This previous work on explanation dialogues is largely con-
ceptual and involves idealized models, and mostly lacks empirical
validation. In contrast, we take a grounded, data-driven approach
to determine what an explanation dialogue should look like.

3 METHODOLOGY

To address the lack of a grounded explanation interaction protocol,
we studied real conversational data of explanations. This study
consists of data selection and gathering, data analysis, and model
development, and then a validation in a lab based simulated human-
agent experiment.

We designed a bottom-up study to develop an explanation di-
alogue model. We aimed to gain insights into three areas: 1. key
components that makeup an explanation interaction protocol (lo-
cutions); 2. relationships within those components (termination
rules); and 3. component sequences and cycles (combination rules)
that occur in explanations.

3.1 Design

We formulate our design based on an inductive approach. We use
grounded theory [16] as the methodology to conceptualize and
derive models of explanation. The key goal of using grounded
theory, as opposed to using a hypothetico-deductive approach, is
to formalize a model that is grounded on actual conversation data
of various types, rather than a purely conceptual model.
The study is divided into three distinct stages, based on grounded
theory. The first stage consists of coding [16] and theorizing, where
small chunks of data are taken, named and marked manuaily ac-
cording to the concepts they might hold. For example, a segment
of a paragraph in an interview transcript can be identified as an
‘Explanation’ and another segment can be identified as a “Why
question’. This process is repeated until the whole data set is coded.
The second stage is categorizing, where similar codes and concepts
are grouped together by identifying their relationship with each
other. The third stage derives a theoretical model from the codes,
categories and their relationship.

3.2 Data

We collected data from six different data sources encompassing
six different types of explanation dialogues. Table 1 shows the
explanation dialogue types, explanation dialogues that are in each
type and number of transcripts. Here, ‘static’ is defined as when an
explainee or an explainer is the same person change from transcript
to transcript (e.g. same journalist interviewing different people). We
gathered and coded a total of 398 explanation dialogues from all of
the data sources. All the data sources! are text based, where some
of them are transcribed from voice and video-based interviews.
Data sources consist of Human-Human conversations and Human-
Agent conversations. We collected Human-Agent conversations to
analyze if there are significant differences in the way humans carry
out the explanation dialogue when they knew the interacting party
was an agent with respect to the frequency of different locutions.

Table 1: Coded data description.

 

 

Explanation Dialogue Type #Dialogue — #Scripts
1. Human-Human static explainee 88 2
2. Human-Human static explainer 30 3
3. Human-Explainer agent 68 4
4, Human-Explainee agent 17 1
5. Human-Human QnA 50 5
6. Human-Human multiple explainee 145 5

 

Data source selection was done to encompass different combi-
nations of participant types and numbers. These combinations are
given in Table 2. We diversify the dataset by including data sources
of different mediums such as verbal based and text based.

Table 3 presents the codes and their definitions. We identify
‘why’, ‘how’ and ‘what’ questions as questions that ask counterfac-
tual explanations, questions that ask explanations of causal chains,
and questions that ask causality explanations respectively. The
whole number of the code column refers to the categories the codes
belong to, where 1) Dialogue boundary; 2) Question type; 3) Expla-
nation; 4) Argumentation; 5) Return question type.

4 GROUNDED EXPLANATION INTERACTION
PROTOCOL

In this section, we present the interaction model resulting from
our grounded study, and formalize the model using agent dialogue

‘Links to all data sources (including transcripts) can be found at https://
explanationdialogs.azurewebsites.net

Table 2: Explanation dialogue type description.

 

 

 

 

 

Participants Number Medium Data source

1. Human-Human 1-1 Verbal Journalist Inter-
view transcripts

2. Human-Human 1-1 Verbal Journalist Inter-
view transcripts

3. Human-Agent 1-1 Text Chatbot conversa-
tion transcripts

4, Human-Agent 1-1 Text Chatbot conversa-
tion transcripts

5. Human-Human n-m Text Reddit AMA
records

6. Human-Human 1-n Verbal Supreme court
transcripts

Table 3: Code description.

Code Description

1.1 QE start Explanation dialogue start

1.2 QE end Explanation dialogue end

2.1 How How questions

2.2 Why Why questions

2.3 What What questions

3.1 Explanation
3.2 Explainee Affirmation
3.3 Explainer Affirmation

Explanation given for questions
Explainee acknowledges explanation
Explainer acknowledges explainee’s ac-
knowledgment

Background to the question provided by
the explainee

Counterfactual case of the how/why
question

Argument presented by explainee or ex-
plainer

An argument that starts the dialogue
Argument Affirmation by explainee or
explainer

4.4 Argument-c Counter argument

45 Argument-contrast Argumentation contrast case

case

5.1 Explainer Return ques- Clarification question by explainer

tion

3.4 Question context
3.5 Counterfactual case
4.1 Argument

4.2 Argument-s
4.3 Argument-a

5.2 Explainee Return ques- Follow up question asked by explainee
tion

 

framework (ADF) [20] as an atomic dialogue type [35]. Note that a
dialogue can range from a purely visual user interface interaction
to verbal interactions. We analyse some observed patterns of inter-
action and compare the grounded model to an existing conceptual
model.

When formalizing the model, we consider the interaction be-
tween explainer and explainee as a dialogue game. Dialogue games
are depicted as interactions between two or more players. The play-
ers can make ‘moves’ with utterances, according to a set of rules.
Dialogue game models have been used to model human-computer
interaction [4], to model human reasoning [27] and to develop
protocols for interactions between agents [12].
 

Q: Begin_Question

 

. E: explain/ Explanation
Question Stated furiher_explain Presented
| Q: affirm
Q: return_question

 

 

 

 

E: return_question

E: affirm

E: Begin_Explanation

Q: Begin_Argument

 

Argument

 

 

 

Explainee Affirmed

H

Presented

 

E: affirm_argument

 
 
 
   
 

E; further_explain E: further_explain

Argument Affirmed

E: counter_argument

 

 

 

Counter Argument

 

 

     
 

Presented

 

End_Argument

 

 

End_ Explanation

 

 

 

Figure 1: Explanation Dialogue Model

Formal dialogue models have been proposed for different di-
alogue types [35], such as negotiation dialogues [1], persuasion
dialogues [35] and a combination of negotiation and persuasion
dialogues [12]. To the best of our knowledge there is no formal
explanation dialogue game model grounded on data.

4.1 Agent Dialogue Framework

We use McBurney and Parson’s agent dialogue framework [20]
to formalize the explanation dialogue model as a dialogue game.
The Agent Dialogue Framework (ADF) provides a modular and
unifying framework that can represent and combine different types
of atomic dialogues in the typology of Walton and Krabbe [35], with
the freedom of introducing new dialogue type combinations. The
ADF has three layers: 1. topic layer; 2. dialogue layer; and 3. control
layer. In the topic layer, the topics of discussion in a dialogue game
are presented in a logical language. Then, the dialogue layer [20]
consists of a set of rules:

Commencement rules: rules under which the dialogue com-
menses.

Locutions: Rules that determine which utterances are permitted
in the dialogue-game. Typical locutions include assertions, ques-
tions, arguments, etc.

Combination rules: Rules that define the dialogical context
of the applicability of locutions. E.g. it might not be applicable to
assert preposition p and =p in the same dialogue.

Commitments: Rules that determine the circumstances where
players express commitments to a preposition.

Termination rules: Rules that determine the ending of a dia-
logue.

More formally, given a set of participating agents A, we define
the dialogue G at the dialogue layer as a 4-tuple (0,8, 7,CF),
where © denotes set of legal locutions, R the set of combinations,
7 the set of termination rules and C¥ the set of commitment
functions respectively [20].

Selection and transitions between dialogue types are handled in
the control layer. Dialogue types can be combined using iteration,
sequencing and embedding [20, pg 10]. When combined, an ADF is
given by 5-tuple (A, £, 11g, II¢, 1) where the set of agents is given
by A, logical language representation given by £, set of atomic
dialogue types given by Ig, set of control dialogues given by Ie¢,
and I] is the closure of IIg UII¢, which represents the set of formal
dialogues denoted by the 4-tuple given above. Closure is defined
under the combination rules presented by McBurney and Parsons
[20].

4.2 Formal Explanation Dialogue Game Model

In this section we present the formal explanation dialogue model
as a new atomic dialogue type [35] using the modular ADF, and
discuss how it is derived from the grounded data of explanation
dialogues according to the layers of ADF. Our analysis of the data
shows that people switch from explanation to argumentation and
back again during an explanation dialogue, in which the explainee
questions a claim made by an explainer. For this reason, our model
has two dialogue types: Explanation and Argumentation. Dialogue
games of atomic dialogue types [35] have an initial situation and
an aim (e.g persuasion dialogue having the initial situation of con-
flicting opinions of the interacting party and the aim of resolving
the conflict). For our explanation dialogue, the initial condition is
the knowledge discrepancy between explainer and explainee of the
topic p and the aim is to provide knowledge about the topic p to
the explainee.

Formally, the explanation dialogue model (ADF) is the tuple:

ADFp = (A, £, 1a, Uc, HW) (1)

where the set of agents A = {Q, E}, where labels Q and E refer
to the Questioner (the explainee) and the Explainer respectively;
£ is the set of logical representations about topics (denoted by
p. qt...) Ua = {Gg Ga}, where Gz is the explanation dialogue
and Gy is the argumentation dialogue, IIe = (Begin_Question, Be-
gin_Explanation, Begin_Argument, End_Explanation, End_Argument),
and I] is the closure of Ilg UI¢ under the combination rule set. I]
gives us the set of formal explanation dialogue G.

The Topic Layer is dependent on the particular application
domain in which the explanation dialogue is embedded, so we do
not define this further.

Dialogue Layer. The dialogue layer consists of the two dialogue
types: explanation (Gg) and argumentation (G4):

Gr = (Oz, Re, Tz.CF gz)

2
Ga = @a.RaTaCFa) @)

The set of legal locutions are defined by:
Og = (explain, affirm, further_explain, return_question)

©, = (affirm_argument, counter _argument, further_explain).
(3)

For clarity, we define the commencement rules, combination rules,
and termination rules via the state transition diagram in Figure 1.
While most codes are directly transferred to the model as states and
state transitions, codes that belonged to information category are
embedded in different states. The combination rules Rg and Ry are
defined by the individual transitions on the diagram. For example,
after a dialogue begins with a question, the next locution is either
the explainer asking for clarification using a return_question or
giving an explanation. Similarly, the set of termination rules can
be extracted from the state model as the state transitions that lead
to the termination state, giving 7g = (affirm(p), explain(p)) and
Ta = (affirm_argument(p), counter_argument(p)). We do not define
commitments CF as these were not observable in our data.

Control layer. This can be identified as state transitions that lead
to and out of the two dialogue types in Figure 1 (e.g. argue, expla-
nation_end). Argumentation occurs naturally within explanation
dialogues, meaning that this is an embedded dialogue, as defined by
McBurney and Parsons [20]. An argument can occur after an expla-
nation was given, which will then continue on to an argumentation
dialogue. The dialogue then returns to the explanation dialogue, as
shown in Figure 1. A single explanation dialogue can contain many
embedded argumentation dialogues.

Explanation dialogues can occur in sequence, which is modelled
by the external loop. Note that a loop within the explanation dia-
logue implies that the ongoing explanation is related to the same
original question and topic, while a loop outside of the dialogue
means a new topic is introduced. We coded explanation dialogues to
end when a new topic was raised in a question. Questions that ask
for follow-up explanations (return_question) were coded when the

questions were clearly identifiable as requesting more information
about the given explanation.

Example: We now go through the formal model with an exam-
ple dialogue which is taken from the human-agent experiments
discussed in Section 5.1. Example is given in Table 4 with the dia-
logue text, locutions/rules and a commentary about the dialogue.
Two agents who are explainee (player) and the explainer (agent)
participate in the dialogue given by Q and E respectively and the
topic ‘cities’ by p:

Table 4: Example: from human-agent experiments of Ticket
to Ride domain.

 

Dialogue Text Locutions/Rules Commentary

 

E:Opponentis gazing at Begin Explanation(p) - Commence ex-

Duluth to Omaha route
and will try to extend it
to Kansas City.

Q: Is he going to Pitts-
burgh?

E: Opponent will try to
Extend the path from
Pittsburgh to Houston
through Atlanta, has
been repeatedly gazing
at that path

Q: No. He is going to El
Paso.

E: Yes, now opponents
gaze is focused at El
Paso and will try to
build from Little rock to
Dallas to El Paso.

return_question(p)

further_explain(p)

Begin_Argument(p)

affirm_argument(p)

End_Argument(p)

planation dialogue
with an explana-
tion about cities
which the oppo-
nent is gazing at

- Using locution re-
turn_question avail-
able in Explanation
dialogue type, in-
quiring more infor-
mation

- providing further
explanation using
further_explain lo-
cution about topic

p.

- Argumentation
sub-dilaog begins
about topic p after.
- Argument is
acknowledged

by the Agent (E)
using
affirm_argument

- End the embedded
argument dia-
logue by using the
control dialogue
End_Argument
which also ends the
initial dialogue.

locution

 

This example shows an interaction between an agent and a
human using the explanation dialogue with an embedded argumen-
tation dialogue. The human-agent study is discussed in depth in Sec-
tion 5.1. The example demonstrates the ability of our model to han-
dle embedded dialogues and cyclic dialogues (explanation dialogues
that occurs twice with Begin_explanation and further_explain)
which similar model of explanation dialogue by Walton [34] lack.
A detailed model comparison between our model and Walton’s can
be found in Section 4.4.
4.3 Analysis

We focus our analysis on three areas to further reinforce the de-
rived interaction protocol: 1. Key components of an Explanation
Dialogue; 2. Relationships between these components and their
variations between different dialogue types; and 3. The sequence
of components that can successfully carry out an explanation dia-
logue.

 

 

= Human-Human static explainee

= = +Human-Human static explainer
Human-Explainer agent
Human-Explainee agent

—F— Human-Human QnA

—@— Human-Human multiple explainee

 
 
    

nN
T

    
 

 

 

   

 

a
T

Average Occurrence in a Dialog
T

2
a
T

 

 

 

6 yap Lammas ae
OB gad good go ao a aot as go go go pat ane? p20 ao aot
Le NE ve 3 go goo CO og oe got gt tO gue ce
ee re rs ae oO yt cy eae oe
os os we eo OF ee gee ce
pe Sacaee ss
Codes

Figure 2: Average code occurrence per dialogue in different
explanation dialogue types

4.3.1 Code Frequency Analysis. The average code occurrence
per dialogue in different dialogue types is depicted in Figure 2. In
all dialogue types, a dialogue is most likely to have multiple what
questions, multiple explanations and multiple affirmations.

Argumentation is a key component of an explanation dialogue.
The explainee can have different or contrasting views to the ex-
plainer regarding the explanation, at which point an argument can
be put forth by the explainee. An argument in the form of an expla-
nation that is not in response to a question can also occur at the
very beginning of an explanation dialogue, where the argument set
the premise for the rest of the dialogue. An argument is typically
followed by an affirmation and may include a counter argument
by the opposing party. From Figure 2, Human-Human dialogues
with the exception of QnA have argumentation but Human-Agent
dialogues lack any substantial occurrences of argumentation.

4.3.2 Explanation Dialogue Termination Rule Analysis. Partici-
pants should be able to identify when a dialogue ends. We analyse
the different types of explanation dialogues to identify the codes
that are most likely to signify termination.

From Figure 3, all explanation dialogue types except Human-
Human QnA type are most likely to end in an explanation. The
second most likely code to end an explanation is explainer affir-
mation. Ending with other codes such as explainee and explainer
return questions is presented by ‘Dialogue ending In Other’ bar in
Figure 3. It is important to note that although a dialogue is likely to
end in an explanation, that dialogue can have previous explainee
affirmations and explainer affirmations.

 
 
 

 
 

[Dialog ending in explanation
[-_] Dialog ending in Explainee Affirmation
[EG Dialog ending in Explainer Affirmation
[EE Dialog ending in Preconception

[EE Dialog ending in Argument Affirmation
Dialog ending in Argument Counter
HE Dialog ending in Other

 
   
 
 

  

     
 

 
 

 

3 k gg
S 6 8

Ending Type % per Dialog
8

Explanation Dialog Type

Figure 3: Average code occurrence in per dialogue in differ-
ent explanation dialogue types

4.4 Model Comparison

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Explainee Explainer Explainee Explainer Explainee
asks why- |» presents |} finds anomaly explains |» accepts STOP
question: story. in story anomaly. explanation.
e
SOALOGLE 5 a) Teo
— sequence oF xplainee
The story Examination ence
holds up or }¢—] dialogue examination }4_{probes.
is refuted successful y per
bath parties satisfied
evaluated. STOP

 

 

Figure 4: Argumentation and explanation in dialogue [34]

We compare the explanation dialogue model, which also contains
an argumentation sub-dialogue, by Walton [34]. Walton proposed
the model shown in Figure 4, which consists of 10 components.
This model focus on combining explanation and examination di-
alogues with argumentation. A similar shift between explanation
and argumentation/examination can be seen between our model
and Walton’s. According to the data sources, argumentation is a
frequently present component of an explanation dialogue, which is
depicted by the Explainee probing component in Walton’s Model.
The basic flow of explanation is the same between the two models,
but the models differ in two key ways. First, is the lack of exami-
nation dialogue shift in our model. Although we did not derive an
examination dialogue, a similar shift of dialogue can be seen with
respect to affirmation states. That is, our ‘examination’ is simply
the explainee affirming that they have understood the explanation.
Second is Walton’s focus on the evaluation of the successfulness
of an explanation in the form of examination dialogue, whereas
our model focus on delivering an explanation in a natural sequence
without an explicit form of explanation evaluation.

Thus, we can see similarities between Walton’s conceptual model
and our data-driven model. The differences between the two are at
a more detailed level than at the high-level, and we attribute these
i

a) =
we l

Las Veons

Figure 5: Ticket to Ride Computer Game

differences to the grounded nature of our study. While Walton
proposes an idealised model of explanation, we assert that our
model captures the subtleties that would be required to build a
natural dialogue for human-agent explanation.

5 EMPIRICAL VALIDATION

In this section we discuss the validation of the derived explanation
interaction protocol. We conducted a human-agent study in which
an agent provides explanations using our model. The purpose of the
study is to test whether the proposed model holds in a human-agent
setting, and in particular, that the human participants follow the
dialogue model when interacting with an artificial agent.

5.1 Study

We conducted our study using the Ticket to Ride online computer
game in a co-located competitive setting, previously used by Newn
et al. [24] and Singh et al. [29], in a university usability lab. The basic
layout of the game is shown in Figure 5. In this game, players must
compete to build train routes between two cities, with each player
building at least two such routes. For the purpose of the study, a
game is played between two players who we term as the player
and the opponent. The player is assisted by an intelligent software
agent that predicts the intentions and plans of the opponent. It
is important to note that for a two player game, each route can
be claimed only by one player. This allows players to block each
other deliberately or otherwise, therefore inferring the intent of
the opponent is beneficial for winning the game. We use the intent
recognition algorithm of Singh et al. [29] to predict the opponent’s
future moves. The algorithm uses gaze data from an eye tracker
and the actions of the opponent to formulate the possible plans (e.g.
most probable routes the opponent can take). The opponent’s gaze
will also appear as a heat map on top of the player’s Ticket to Ride

2https://www.daysofwonder.com/tickettoride/en/

 

game screen. The agent communicates the predictions and their
explanations to the player through a chat window.

To evaluate our model, we adopted a Wizard of OZ approach de-
scribed by Dahlbiick et al. [10], meaning that the natural language
generation of the explanation agent is played by a human ‘wizard’,
but this is unknown to the human participant. It is important to
note that only the natural language generation is delegated to the
wizard, while the agent generates and visualize plans {a set of con-
nected train routes) in a separate interface in order to assist the
wizard. Wizard has access to the visualized plans, most gazed at
routes and most gazed at cities. The argument for using Wizard
of Oz (WoZ) technique as opposed to a natural language imple-
mentation is twofold. First, to gather high quality empirical data
related to the model bypassing the limitation that exist in natural
language interfaces [10]. Second, having an interaction that closely
resembles human discourse [10] allows the human to have more
natural responses. Wizard of Oz techniques have been demonstra-
tively shown to successfully evaluate conversational agents [9, 14],
human-robot interactions [3] and automated vehicle-human inter-
actions [19, 25] .

The Wizard uses the prediction information of the agent and
translates it to a more natural dialogue, enabling us to get empirical
data on natural explanation dialogues between player and the agent.
The player is informed that he/she can communicate with the agent
in a natural discourse. A prediction of the game includes a route
that opponent might build (e.g from Pittsburgh to Houston) and a
city area opponent might be interested in (e.g Interested around
Houston). Predictions are generated from the implemented intent
recognition algorithm of Singh et al. [29]. The wizard follows a
simple natural language template: prediction followed by the ex-
planation. The explanation template can include one or more of
the following in any order: gaze explanation (e.g the opponent has
been repeatedly gazing at that path) and causal history explanation
(e.g the opponent has already built some paths along that route).

The protocol of the Wizard is outlined as follows. The Wizard
follows the locutions, termination rules and combination rules of
the dialogue model. Predictions and explanations of the agent is
translated to natural language using the template described above
by the wizard according to the nature of the locution used. Players
(experiment participants) can ask questions and present argument
in natural language. Players can initiate dialogues as well as reply
to the Wizard at any time in the game, and are in-control of the
frequency of the interaction. The wizard follows a static failure
response to any interactions that failed or unable to provide predic-
tions and explanations (e.g I’m unable to answer that). Note that
in the case of a participant using an invalid locution or a control
dialogue, the dialogue will fail and wizard will end the dialogue
with a termination rule.

Parameters of the experiments are as follows. In total, we ob-
tained 101 explanation dialogues across 14 experiments. Players
were from the same university, aged between 23 and 31 years
(M = 27.2). Players were observed through an observation room,
in which the wizard was located. The duration of each experiment
had an upper bound of 30 minutes (M = 20.15), limited by the

3Video capture of an experiment is provided in supplementary material at https:
//explanationdialogs.azurewebsites.net/supp.zip
duration of the game-play, with the ability to end early if the game
is won by either side before 30 minutes (game ends when all trains
have been used by either side). During game-play the player has
the freedom to engage in conversation with the agent or play the
game disregarding the agent, thus each experiment yields a differ-
ent number of dialogues (M = 7.21). Conversations between player
and the wizard were carried out using a chat client, through which
we recorded the dialogue data. Extracted data was then analysed
according to locutions, control dialogues and their sequences.

5.2 Results and Findings

 

25 -
[Iai iatog games
IERIE aa ciatog games

Control Dialogs

20 [BQ] - Begin_Question
[BE] - Begin_Explanation

- Begin_/
[EA] -- End_Argumentation

Locutions:

1- eplain

[AF] - affirm

IRQ] - retur_question
410F IFE] - further_explain
TAA] - affirm argument
ICA] - counter_argument

(Leal dele arable |

Dialog game occurrence %

 

 

’ x 3 BP DP a
PPP EP CPP PP PP org oP
FP set 8 of eG CLE rer
PF Pg * eo oF 9 Fee &
ere ¥ Oe v fs ew ge
eee ee & ee

© Dialon games

Figure 6: Empirical results of human-agent dialogue games.

Figure 6 illustrates the explanation dialogue games in the human-
agent study. Control dialogues and locutions are depicted by short-
ened tags, which forms a sequence when combined. An example of
a dialogue game using the tags would be [BQ][E] [AF], which cor-
responds to [Begin_question => explain = affirm] in locutions and
control dialogues. Figure 6 shows percentages a specific dialogue
game type occurred, and invalid dialogue games.

The proposed explanation dialogue model held true for 96 out
of 101 dialogue game instances we observed. Figure 6 indicates
the 5 invalid dialogue game types that occurred. These dialogues
became invalid dialogue games according to our model because of
the parallelization of dialogue combination moves. For example,
consider the dialogue game [BE][AF][RQ][BA][EA]. Here, after
affirm and return_question locution, Begin_Argumentaion control
dialogue occurs. This sequence is illegal according to the model. If
parallelization is allowed, Begin_Argumentaion control dialogue can
occur without waiting for a termination rule (e.g. affirm, explain).
We attribute this limitation to the nature of the grounded data where
parallelization cannot be accurately captured. This limitation can
potentially be rectified by introducing parallelization [20] to the
combination rules.

6 CONCLUSION

Explainable Artificial Intelligent systems can benefit from having
a proper interaction protocol that can explain their actions and

behaviours to the interacting users. Explanation naturally occurs
as a continuous and iterative socio-cognitive process that involves
two (sub)processes: a cognitive process and a social process. Most
prior work is focused on providing explanations without sufficient
attention to the needs of the explainee, which reduces the usefulness
of the explanation to the end-user.

In this paper, we propose a interaction protocol for the socio-
cognitive process of explanation that is derived from different types
of natural conversations between humans as well as humans and
agents. We formalise the model using the Agent Dialogue Frame-
work [20] as a new atomic dialogue type [35] of explanation with an
embedded argumentation dialogue, and we analyse the frequency
of occurrences of patterns. To empirically validate our model, we
undertook a human behavioural experiment involving 14 partici-
pants and a total of 101 explanation dialogues. Results indicate that
our explanation dialogue model can closely follow Human-Agent
explanation dialogues. Main contribution of this paper lies in the
formalized interaction protocol for explanation dialogues that is
grounded on data, a secondary contribution is the coded (tagged)
explanation dialogue data-set of 398 dialogues*. By following a
data-driven approach, proposed model captures the structure and
the sequence of an explanation dialogue more accurately and allow
natural interactions than explanations from existing models. The
main contribution of this paper is a grounded interaction protocol
derived from explanation dialogues, formalized as a new atomic
dialogue type [35] in the ADF. XAI systems that deal in explanation
and trust will benefit from such a model in providing better, more
intuitive and interactive explanations.

In future work, we aim introduce parallelism to the model with
respect to locutions and combination rules as founded to be present
in human-agent dialogues from the study. Further evaluation can be
done by introducing other forms of interaction modes such as visual
interactions which may introduce different forms of combination
and termination rules.

ACKNOWLEDGMENTS

The research described in this paper was supported by the Univer-
sity of Melbourne research scholarship (MRS); SocialNUI: Microsoft
Research Centre for Social Natural User Interfaces at the Univer-
sity of Melbourne; and a Sponsored Research Collaboration grant
from the Commonwealth of Australia Defence Science and Tech-
nology Group and the Defence Science Institute, an initiative of the
State Government of Victoria. We also acknowledge the support of
Joshua Newn and Ronal Singh, given in conducting the study.

REFERENCES

{1] L. Amgoud, N. Maudet, and S. Parsons. 2000. Modelling dialogues using argu-
mentation. In Proceedings Fourth International Conference on MultiAgent Systems.
31-38. https://doi.org/10.1109/ICMAS.2000.858428

[2] Abdallah Arioua and Madalina Croitoru. 2015. Formalizing explanatory dialogues.

In International Conference on Scalable Uncertainty Management. Springer, 282-

297.

Adrian Keith Ball, David C. Rye, David Silvera-Tawil, and Mari Velonaki. 2017.

How Should a Robot Approach Two People? 3. Hum.-Robot Interact. 6, 3 (Dec.

2017), 71-91. https://doi.org/10.5898/JHRI.6.3.Ball

[3

‘coded (tagged) dialogue data-set is provided in supplementary material at https:
//explanationdialogs.azurewebsites.net/supp.zip
[4]

[5

[6

(7

[3]

[9

10]

f4]

(12]

(13]

(14)

(15]

[16]

[17]

[18]

{19]

Trevor JM Bench-Capon, PAUL E Dunne, and Paul H Leng. 1991. Interacting
with knowledge-based systems through dialogue games. In Proceedings of the
Eleventh International Conference on Expert Systems and Applications. 123-140.
Joost Broekens, Maaike Harbers, Koen Hindriks, Karel Van Den Bosch, Catholijn
Jonker, and John-Jules Meyer. 2010. Do you get it? User-evaluated explainable
BDI agents. In German Conference on Multiagent System Technologies. Springer,
28-39,

Alison Cawsey. 1993. Planning interactive explanations. International Journal of
Man-Machine Studies 38, 2 (1993), 169 — 199. https://doi.org/10.1006/imms.1993.
1009

Tathagata Chakraborti, Sarath Sreedharan, Yu Zhang, and Subbarao Kambham-
pati. 2017. Plan explanations as model reconciliation: Moving beyond explanation
as soliloquy. IJCAI International Joint Conference on Artificial Intelligence (2017),
156-163. https://doi.org/10.24963/ijcai.2017/23 arXiv:1802.01013

B. Chandrasekaran, Michael C. Tanner, and John R. Josephson. 1989. Explaining
Control Strategies in Problem Solving. (1989), 9-15 pages. https://doi.org/10.
1109/64.21896

Ana Paula Chaves and Marco Aurelio Gerosa. 2018. Single or Multiple Conversa-
tional Agents?: An Interactional Coherence Comparison. In Proceedings of the
2018 CHI Conference on Human Factors in Computing Systems. ACM, 191.

Nils Dahlback, Arne Jonsson, and Lars Ahrenberg. 1993. Wizard of Oz Studies:
Why and How. In Proceedings of the 1st International Conference on Intelligent
User Interfaces (IUI 93). ACM, New York, NY, USA, 193-200. https://doi.org/10.
1145/169891.169968

Maartje M A De Graaf and Bertram F Malle. 2017. How People Explain Action
(and Autonomous Intelligent Systems Should Too). AAAI 2017 Fall Symposium
on GATJAI-HRIGAI (2017), 19-26.

Frank Dignum, Barbara Dunin-Keplicz, and Rineke Verbrugge. 2001. Agent
Theory for Team Formation by Dialogue. In Intelligent Agents VII Agent Theories
Architectures and Languages, Cristiano Castelfranchi and Yves Lespérance (Eds.).
Springer Berlin Heidelberg, Berlin, Heidelberg, 150-166.

Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Inter-
pretable Machine Learning. Ml (2017), 1-13. arXiv:1702.08608 http://arxiv.org/
abs/1702.08608

Mateusz Dubiel. 2018. Towards Human-Like Conversational Search Systems. In
Proceedings of the 2018 Conference on Human Information Interaction&Retrieval.
ACM, 348-350.

Maria Fox, Derek Long, and Daniele Magazzeni. 2017. Explainable Planning.
IJCAI - Workshop on Explainable AI (2017).

Barney G Glaser and Anselm L Strauss. 1967. The Discovery of Grounded Theory:
Strategies for Qualitative Research. Vol. 1. 271 pages. https://doi.org/10.2307/
2575405 arXiv-arXiv-gr-qc/9809069v1

Denis J. Hilton. 1991. A Conversational Model of Causal Explanation. Eu-
ropean Review of Social Psychology 2 (1991), 51-81. https://doi.org/10.1080/
14792779143000024

Robert Kass and Tim Finin. 1988. The Need for User Models in Generating Expert
System Explanations. Technical Report. University of Pennsylvania. 1-32 pages.
http://repository.upenn.edu/cis_reports/585

Karthik Mahadevan, Sowmya Somanath, and Ehud Sharlin. 2018. Communi-
cating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction. In
Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.
ACM, 429.

[20]

(21]

[22]

[23]

[24]

[25]

[30]

[31]
[32]

[33]

[34]

[35]

[36]

Peter McBurney and Simon Parsons. 2002. Games That Agents Play: A Formal
Framework for Dialogues between Autonomous Agents. Journal of Logic, Lan-
guage and Information 11, 3 (01 Jun 2002), 315-334. https://doi.org/10.1023/A:
1015586128739

Tim Miller. 2019. Explanation in Artificial Intelligence: Insights from the Social
Sciences. Artificial Intelligence 267 (2019), 1-38.

Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of
Inmates Running the Asylum; Or: How I Learnt to Stop Worrying and Love the
Social and Behavioural Sciences. In [JCAL 17 Workshop on Explainable AI (XAD.
36.

Johanna D. Moore and Cecile L. Paris. 1991. Requirements for an expert system
explanation facility. Computational Intelligence 7, 4 (1991), 367-370. https:
//doiorg/10.1111/j.1467-8640.1991.tb00409.x

Joshua Newn, Fraser Allison, Eduardo Velloso, and Frank Vetere. 2018. Looks
Can Be Deceiving: Using Gaze Visualisation to Predict and Mislead Opponents in
Strategic Gameplay. In Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems (CHI 18). ACM, New York, NY, USA, Article 261, 12 pages.
https://doi.org/10.1145/3173574.3173835

Ana Rodriguez Palmeiro, Sander van der Kint, Luuk Vissers, Haneen Farah,
Joost CF de Winter, and Marjan Hagenzieker. 2018. Interaction between pedestri-
ans and automated vehicles: A Wizard of Oz experiment. Transportation Research
Part F: Traffic Psychology and Behaviour 58 (2018), 1005-1020.

Henry Prakken. 2006. Formal Systems for Persuasion Dialogue. Knowl. Eng. Rev.

21, 2 June 2006), 163-188. https://doi.org/10.1017/S0269888906000865
Henry Prakken and Giovanni Sartor. 1998. Modelling Reasoning with Precedents

in a Formal Dialogue Game. Springer Netherlands, Dordrecht, 127-183. https:
//doi.org/10.1007/978-94-015-9010-5_5

RC Schank 1986. Explanation patterns: Understanding mechanically and cre-
atively. Erlbaum, Hillsdale, NJ.

Ronal Singh, Tim Miller, Joshua Newn, Liz Sonenberg, Eduardo Velloso, and
Frank Vetere. 2018. Combining Planning with Gaze for Online Human In-
tention Recognition. In Proceedings of the 17th International Conference on Au-
tonomous Agents and MultiAgent Systems (AAMAS '18). International Founda-
tion for Autonomous Agents and Multiagent Systems, Richland, SC, 488-496.
http://dLacm.org/citation.cfm?id=3237383.3237457

Serena Villata, Guido Boella, Dov M. Gabbay, and Leendert Van Der Torre. 2013. A
socio-cognitive model of trust using argumentation theory. International Journal
of Approximate Reasoning 54, 4 (2013), 541-559. https://doi-org/10.1016/j.ijar.
2012.09.001

Douglas Walton. 2007. Dialogical Models of Explanation. Explanation-aware
computing: Papers from the 2007 AAAI Workshop 1965 (2007), 1-9.

Douglas Walton. 2011. A dialogue system specification for explanation. Synthese
182, 3 (2011), 349-374. https://doi.org/10.1007/s11229-010-9745-z

Douglas Walton. 2016. A Dialogue System for Evaluating Explanations.
Springer International Publishing, Cham, 69-116. https://doi.org/10.1007/
978-3-319-19626-8_3

Douglas Walton and Floris Bex. 2016. Combining explanation and argumentation
in dialogue. Argument and Computation 7, 1 (2016), 55-68. https://doi.org/10.
3233/AAC- 160001

Douglas Walton and Erik C. W. Krabbe. 1995. Commitment in Dialogue: Basic
Concepts of Interpersonal Reasoning. State University of New York Press.
Michael Winikoff. 2017. Debugging Agent Programs with Why?: Questions. In
Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems
(AAMAS 17), IFAAMAS, 251-259.
® Check for updates

Received: 31 January 2021 Revised: 15 April 2021 Accepted: 10 June 2021

 

DOE: 10.1002/widm.1424

eS WIRE
OVERVIEW @® DATA MENG AND KNOWLEDGE DISCOVERY WI LEY

Explainable artificial intelligence: an analytical review

Plamen P. Angelov’?® | Eduardo A.Soares’?® | Richard Jiang’?© |
Nicholas I. Arnold’*® | Peter M. Atkinson?* ©

‘School of Computing and

Communications, Lancaster University, Abstract
Lancaster, UK This paper provides a brief analytical review of the current state-of-the-art in
“Lancaster Intelligent, Robotic and relation to the explainability of artificial intelligence in the context of recent

Autonomous Systems (LIRA) Research

advances in machine learning and deep learning. The paper starts with a brief
Centre, Lancaster, UK

3 . historical introduction and a taxonomy, and formulates the main challenges in
Lancaster Environment Centre,

Lancaster University, Lancaster, UK terms of explainability building on the recently formulated National Institute
of Standards four principles of explainability. Recently published methods

Correspondence . ee . .

Plamen P. Angelov, School of Computing related to the topic are then critically reviewed and analyzed. Finally, future

and Communications, Lancaster directions for research are suggested.

University, Lancaster LA1 4WA, UK.

Email: p.angelov@lancaster.ac.uk This article is categorized under:

Technologies > Artificial Intelligence
Edited by: Sushmita Mitra, Associate

Editor and Witold Pedrycz, Editor-in-
Chief

Fundamental Concepts of Data and Knowledge > Explainable AI

KEYWORDS

black-box models, deep learning, explainable AI, machine learning, prototype-based
models, surrogate models

 

1 | INTRODUCTION

Artificial intelligence (AI) and machine learning (ML) have demonstrated their potential to revolutionize industries,
public services, and society, achieving or even surpassing human levels of performance in terms of accuracy for a range
of problems, such as image and speech recognition (Mnih et al., 2015) and language translation (Young et al., 2018).
However, their most successful offering in terms of accuracy—deep learning (DL) (LeCun et al., 2015)—is often charac-
terized as being “black box” and opaque (Pasquale, 2015; Rudin, 2019). Indeed, such models have a huge number
(many millions or even a billion) of weights (parameters) which are supposed to contain the information learned from
training data. Not only is the number of these weights very large, but their link to the physical environment of the prob-
lem is extremely hard to isolate. This makes explaining such forms of AI to users highly problematic. Using opaque,
“black box” models is especially problematic in highly sensitive areas such as healthcare and other applications related
to human life, rights, finances, and privacy. Since, the applications of advanced AI and ML, including DL, are now
growing rapidly, encompassing the digital health, legal, transport, finance, and defense sectors, the issues of transpar-
ency and explainability are being recognized increasingly as critically important. For example, a search in Google
Trends (https://trends.google.com/trends/) reveals that in the last decade publications using the terms “DL” and
“explainable AI” (XAI) both grew significantly, but while the curve for DL is now in a saturation stage over the last
3 years or so, the curve for XAI is growing exponentially starting precisely 3 years ago when the saturation in regards to

 

 

Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any

This is an open access article under the terms of the Creative
medium, provided the original work is properly cited, the use is non-commercial and no modifications or adaptations are made.
© 2021 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.

 

WIREs Data Mining Knowl Discov. 2021;11:e1424. wires.wiley.com/dmkd 1 of 13
https://doi.org/10.1002/widm.1424

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
2of 13 WI LEY_ @ WIREs ANGELOV er aL.

DATA MINING AND KNOWLEDGE DISCOVERY

DL started to be observed as illustrated by Figure 1. This is not coincidental and demonstrates that as the huge increase
in interest towards DL starts to saturate, interest towards XAI is gearing up trying to address open research and accept-
ability questions (Arrieta et al., 2020). In this paper, this trend is analyzed and, further, a taxonomy of XAI is provided.
Moreover, we also investigate novelties in terms of XAI and include an analysis involving the Caltech-101
benchmarking dataset. Following a brief historical and critical state-of-the-art review we also consider some applica-
tions for which explainability is critically important. The paper concludes with a discussion.

2 | BRIEF HISTORICAL PERSPECTIVE

AI was closely linked to both ML and to logic and symbolic forms of reasoning from its inception in the middle of the
20th century (Samuel, 1959; Smolensky, 1987). ML and data-driven statistical techniques gained momentum in recent
years due to an unprecedented increase in the number and complexity of data available (now the majority of data are
unstructured, with many more images/videos as well as text/speech in comparison to the 20th century) (Bishop, 2006;
Goodfellow et al., 2014).

Historically, the first methods of AI, such as decision trees (Quinlan, 1990), symbolic AI (Smolensky, 1987), expert
systems, fuzzy logic, and automated reasoning (Robinson & Voronkov, 2001), as well as some forms of artificial neural
networks (ANNs), for example, radial-basis function (RBF) architectures and linguistic, prototype-based, representa-
tions were significantly more interpretable and self-explainable than the more recent and more efficient forms such as
support vector machines (SVMs) (Hearst et al., 1998) and most other forms of ANNs.

In the last few years, explainability has become an important issue not only for scientists, but also for the wider pub-
lic including, regulators, and politicians. As AI and ML (and, especially, DL) become more wide spread and intertwined
with human-centric applications, and algorithmic decisions become more consequential to individuals and society,
attention has shifted back from accuracy to explainability (Angelov & Soares, 2020; Core et al., 2006; Pedreschi
et al., 2019). Complex and “black box” (Pasquale, 2015; Rudin, 2019) types of models can easily fool users (Nguyen
et al., 2015) and, in turn, this can lead to dangerous or even fatal consequences (Stilgoe, 2020). Opening the “black box”
is critically important not only for acceptability within society, but also for regulatory purposes. (In 2019 the US Con-
gress passed the Algorithmic Accountability Act (MacCarthy, 2019) and the EU enshrined the right for an explanation
to the consumer (Core et al., 2006; Goodman & Flaxman, 2017; Pedreschi et al., 2019).) The current data-rich environ-
ment brought the temptation to take shortcuts from raw data to solutions using a very large number of abstract, purely
numerical parameters (Angelov & Soares, 2020; Rudin, 2019; Stock & Cisse, 2018), without providing a deep insight
into, and understanding of, the underlying dependencies, causalities, and internal model structures. The issue of
explainability is an open research question for some of the most successful (in terms of accuracy) forms of ML such as
SVMs, DL, and many of the ANNs (Bishop, 2006), as Figure 2 illustrates.

In the above context, the main question is not so much: Can we get an XAI solution?, but Can we get a highly accu-
rate XAI solution comparable to the accuracy that DL would provide? Table 1 illustrates some results for the Caltech-101

 

(a) Deep Learning Search interest over time (b) XAI Search interest over time
100
90
89
a » 80
2 2
2 = 70
< 7 2
= = 60
S S
S 5 50
3 a
o » 40
= =
s & 30
o a
ea “2
10
2 0.
2 Yrs, May Ma, > Pons Moy. Yor, Sep Yang, May. Man, Yon. oy, Sp, Ys, May Me, > von, Vo
07, “> “$074 025 “PQ ~O2q “Oz, 07, “C22 “$02, ~O2q “Ors “Or, “Og “Cp “M07, “02, “O2Q 0.
Date Date

FIGURE 1 _ Illustrates the interest evolution towards two terms according to Google Trends: (a) deep learning (DL), (b) explainable
artificial intelligence (XAT)

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
ANGELOV Er aL.

 

 

 

3 of 13
@ fd WIRE meee AND KNOWLEDGE DISCOVERY —_WI LEy_| 2

BS
High
37| 0 A
o
c XAI's
° bere OQ) Research Future
= Learning
° Rondom
E Oa oO
> j Decision
U Tree
© KNN
3 @
< Bayes
Low in
Low Interpretability (notional) High
FIGURE 2 _ Accuracy vs. interpretability for different machine learning models
TABLE 1 Performance comparison for the Caltech-101 dataset
Method Accuracy Time (s) #Parameters Interpretability
XDNN (Angelov & Soares, 2020) 94.31% 362 4 per class High
VGG-16 (Simonyan & Zisserman, 2014) 90.32% 18,332 138.000.000 Very low
ResNet-50 (He et al., 2016) 90.39% 12,540 23.000.000 Very low
Random forest (Breiman, 2001) 87.12% 412 ~20,000 Medium
SVM (Hearst et al., 1998) 86.64% 783 ~15,000 Low
KNN (Peterson, 2009) 85.65% 221 ~300 and all data Low
Decision tree (Quinlan, 1996) 86.42% 236 ~5 rules per class High
Naive Bayes (Rish, 2001) 54.84% 323 409,700 Medium

benchmarking dataset considering different methods and evaluation metrics (best results are highlighted with bold
fonts).

AI has been closely related to automated reasoning and mimicking human intelligence from its inception
(Angelov & Gu, 2018). ANNs, as a branch of AI that is closely related to ML, went through a roller coaster cycle of
development starting in the middle of the past century while the Second World War was still raging with the introduc-
tion of the computational perceptron—the model of a single neural cell (meuron) by Warren McCulloch and Walter
Pitts in 1943 (Bien & Tibshirani, 2011). At the time that this theory was introduced it was very close to the biological
original that inspired it—the human brain. With the power that comes from networking multiple (.e., millions or even
billions of) neurons comes predictive power, but also a complexity that tends to opaqueness. Historically, ANNs went
through their first boom during the 1980s-1990s when one of the main achievements was the introduction of popular
efficient supervised learning methods, such as the back-propagation algorithm by Werbos (1990), and architectures
such as multi-layer perceptrons (MLPs), RBF, and so on. The power of ANNs is in the layered architecture which, in
effect, performs a series of embedded mathematical transformations—somewhat analogous to the Russian dolls called

“matryoshka”:
vs oses(slou( 0)

where y, denotes the pth output; w represents the weights; j is the number of inputs/features and i is the number of
neurons at a hidden layer; f; denotes the activation function of the input layer; f, denotes the activation function of a
hidden layer; f,, denotes the activation function of the output layer.

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
4 of 13 WI LEY_ iE WIREs ANGELOV er aL.

DATA MINING AND KNOWLEDGE DISCOVERY

 

MLPs have one or more hidden layers. ANNs with multiple hidden layers are now called “deep.” ML occurs by
modifying the weights of the links between the nodes of neighboring layers. The good news is that the huge number of
weights allows one to model highly complex functions that map the inputs (in the case of classification—features) to
the output(s)—(in classification—class labels). It has been proven theoretically that an MLP can approximate arbitrarily
well any nonlinear function (Hornik et al., 1990). Indeed, ANNs are highly parametric models. In addition, the layered
structure allows the learning to be performed using error back-propagation with the relatively simple and reliable gradi-
ent family of methods combined with conveniently described single neuron activation functions, such as Gaussian or
linear. The bad news is that the potentially huge number of weights (unknown parameters) requires a lot of training
data, time, computational resources, and makes the overall model hard to explain because the link between the weights
and the physicality of the original problem is broken. In comparison, humans can learn from very few examples, or
even a single example, and understand the concept behind the classification decision as well as explain and articulate it
to others. The AI systems of the future will have to emulate such capabilities (Angelov & Gu, 2018).

Historically, interest towards ANNs dipped around the mid-90s leading to the so-called AI winter and attention
moved away. Even the success of IBMs Deep Blue chess playing computer program that was able to beat the world's
best chess player Gary Kasparov was not able to revitalize AI (Campbell et al., 2002). However, when in 2015 the UK
company DeepMind that defeated the world champion of the game Go with its AlphaGo (Chang et al., 2016) using a
deep form of ANN, interest in ANNs and AI, as well as ML, returned with full force. It does have to be acknowledged
that problems such as (so-called fair) games are well-structured logical problems and, therefore, easier to learn in com-
parison to the much more open-ended real-life problems that are of interest to various industrial and defense applica-
tions. For example, autonomous driving or aerial systems, financial, health, legal, and other real-life problems are more
complex, unpredictable, and uncertain (Nilsson, 2014).

3 | XAI TAXONOMY

In the literature, a variety of terms exist to indicate the opposite of the “black box” nature of some of the AI and ML,
and especially DL, models. We distinguish the following terms:

* Transparency: a model is considered to be transparent if, by itself, it has the potential to be understandable. In other

words, transparency is the opposite of “black-box” (Adadi & Berrada, 2018).

Interpretability: is defined as the capacity to provide interpretations in terms that are understandable to a human

(Gilpin et al., 2018).

» Explainability: is related with the notion of explanation as an interface between humans and an AI system. It com-
prises AI systems that are accurate and comprehensible to humans (Gilpin et al., 2018).

Although these terms are similar in their semantic meanings, they confer different levels of AI to be accepted by
humans. For more details, the ontology and taxonomy of XAJ at a high level can be detailed as below:

* Transparent model: Typical transparent models (Adadi & Berrada, 2018) include k-nearest neighbors (kKNN), decision
trees, rule-based learning, Bayesian network, and so on. The decisions from these models are often transparent,
although transparency, as a property, is not sufficient to guarantee that a model will be readily explainable.

* Opaque model: Typical opaque models (Pasquale, 2015; Rudin, 2019) include random forest, neural networks, SVMs,
and so on. Although these models often achieve high accuracy, they are not transparent.

* Model agnostic: Model-agnostic XAI approaches (Dieber & Kirrane, 2020) are designed with the purpose of being
generally applicable. As a result, they have to be flexible enough, so that they do not depend on the intrinsic architec-
ture of the model, thus, operating solely on the basis of relating the input of a model to its outputs.

* Model-specific: Model-specific XAI approaches often take advantage of knowing a specific model and aim to bring
transparency to a particular type of one or several models (Bach et al., 2015).

» Explanation by simplification: By simplifying a model via approximation (Tritscher et al., 2020), we can find alterna-
tives to the original models to explain the prediction we are interested in. For example, we can build a linear model
or a decision tree around the predictions of a model, using the resulting model as a surrogate to explain the more
complex one.

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
=a ® WIREs AND KNOWLEDGE DISCOVERY —_WI LEy_| =

» Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).

* Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.

» Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.

The ML literature predominantly uses the term “interpretability” as opposed to “explainability,” but according to
Burkart and Huber (2020), interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of “black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by:

Scope (local (Bach et al., 2015; Selvaraju et al., 2017) and global (Chen et al., 2019; Pedreschi et al., 2019)).

» Usage (post hoc, e.g., surrogate models (Dieber & Kirrane, 2020; Pedreschi et al., 2019; Tritscher et al., 2020) and
intrinsic to the model architecture, e.g., explainable-by-design (Soares, Angelov, Biaso, et al., 2020; Soares, Angelov,
Costa, et al., 2020)).

Methodology (focused on the features (Chen et al., 2019; Selvaraju et al., 2017) or on the model parameters (Dieber &
Kirrane, 2020)).

In recognition of the growing importance of this topic, NIST published in August 2020 Four principles of XAI
(Phillips et al., 2020), which define the following fundamental principles which an AI must honor to be considered an
XAI as follows:

+ Explanation: this principle states that an AI system must supply evidence, support; or reasoning for each decision
made by the system.

* Meaningful: this principle states that the explanation provided by the AI system must be understandable by, and meaningful
to, its users. As different groups of users may have different necessities and experiences, the explanation provided by the AI
syste must be fine-tuned to meet the various characteristics and needs of each group.

* Accuracy: this principle states that the explanation provided by the AI systern must reflect accurately the system's processes.

+ Knowledge limits: this principle states that AI systems must identify cases that they were not designed to operate in
and, therefore, their answers may not be reliable.

Figure 3 depicts the ontology of the XAI taxonomy. Transparent models can easily achieve explainability, while
opaque models require post hoc approaches to make them explainable. The categories of post hoc approaches are illus-
trated accordingly.

4 | REVIEW OF THE STATE-OF-THE-ART

Current research on XAI is still mostly limited to sensitivity analysis (Arrieta et al., 2020), layer-wise feature relevance
propagation and attribution (Tritscher et al., 2020), local pseudo explanations by LIME (Dieber & Kirrane, 2020), game-
theoretic Shapley additive explanations (Chen et al., 2019), gradient-based localization, and Grad-CAM (Selvaraju
et al., 2017) or surrogate models. In this section, some of the more widely used methods are outlined.

4.1 | Features-oriented methods

SHapley Additive exPlanation (SHAP) (Lundberg & Lee, 2017) is a game-theoretic approach to explain ML predictions.
SHAP seeks to deduce the amount each feature contributed to a decision by representing the features as players in a

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
6 of 13 WI LEY_ @ WIREs ANGELOV er aL.

DATA MINING AND KNOWLEDGE DISCOVERY

Opaque Models

Ensemble Method
Support Vector Machine
Multi-layer Neural Net

Transparent Models

   

    
     
         
   
 
     

 

Decision Trees
K-Nearest Neighbor
Rule-based Inference
Bayesian Models
Linear Regression, etc.

 
    
 

1=>.¢e) Feller elie)
Models

Convolution Neural Net
Recurrent Neural Net, etc.

 

Post-hoc Explainability

Model Specific

|
:

Sree e tela WV]

 

Model Agnostic

_ $$ __¥_¥_ > a EEE
Explanation by Visual Local
Feature Relevance Explanation Explanation

FIGURE3 The high-level ontology of explainable artificial intelligence approaches

    

Simplification

  

coalition game. The payoff of the game is an additive measure of importance, the so called Shapley value, which repre-
sents the weighted average contribution of a particular feature within every possible combination of features. As such,
local and global interpretations of a model are consistent and the average prediction is fairly distributed across all
Shapley values, meaning that contrasting comparisons between explanations are possible. However, if the model is not
additive then interpretation of the Shapley values is not always transparent, as predictive models may have non inde-
pendent pay-off splits. Furthermore, while SHAP can be considered model agnostic, optimized implementations of the
SHAP algorithm to all model types is not immediately straight forward or efficient.

Class activation maps (CAMs) are specific to CNNs. CAMs represent the per-class weighted linear sum of visual pat-
terns present at various spatial locations in an image (Zhou et al., 2016). More formally, global average pooling is
applied to the final convolutional feature map in a network, before the output layer. These pooled feature maps are
then used as the input features to a fully connected layer and output through a loss function. By projecting the weights
of the output back to the previous convolutional layer, the areas in the input image with greater influence over the
CNNs' decision are highlighted per-class and visible through a heatmap representation. CAMs cannot be applied to
pre-trained networks and networks that do not adhere to the specified fully convolutional network architecture. Addi-
tionally, spatial information can be lost by the fully connected layer and map scaling. Two generalizations of the base
CAM model, Grad-CAM (Selvaraju et al., 2017) and Grad-CAM+-+ (Chattopadhay et al., 2018), try to further increase
the explainability of CNNs.

Gradient-weighted class activation mapping (Grad-CAM) (Selvaraju et al., 2017) generalizes CAM to any arbitrary
CNN architecture and without retraining. The gradients for any target class are fed into the final convolutional layer
and an importance score computed in respect to the gradients. As with other methods, a heatmap representation of the
Grad-CAM indicates which regions of the input image were most important in the CNN's decisions. However, Grad-
CAM produces only coarse-grained visualizations and cannot explain multiple instances of the same object in an image.
Grad-Cam+-+ (Chattopadhay et al.) considers the weighted average of the gradients to overcome these drawbacks.

Feature oriented methods provide insights into where a decision is taking place in terms of the input, but fall short
of a human level explanation of how and why the model came to those decisions. Consequently, a human could not
exactly reproduce the explanations rendered by the model.

4.2. | Global methods

For features with precise semantic definitions, global attribution mappings (GAMs) Ubrahim et al., 2019) can explain a
neural network's predictions on a global level, across subpopulations, by formulating attributions as weighted conjoined
rankings. The advantages are that different subpopulations can be captured through a tuneable granularity parameter.

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
ANGELOV er aL. a 7 of 13
ET AL 9 WIREs _ _WI L Ey_| 73

W DATA MINING AND KNOWLEDGE DISC!

GAMs find a pair-wise rank distance matrix between features and a K-medoids clustering algorithm used to group simi-
lar local feature importances into clusters. The medoid of each cluster then summarizes the pattern detected in each
cluster as a global attribution. This approach is therefore relevant to feature exploration among different sub-
populations of samples.

Gradient-based saliency maps (Simonyan et al., 2013) are a visualization technique which render the absolute value
of the gradient (in respect to the input features) of the majority predicted class as a normalized heatmap. The pixels
with a high activation are highlighted and correspond to areas that are most influential (i.e., salient). The method's
explanation lies in the ability for a user to look at what features in the image are being used in the classification deci-
sion. However, the absolute value means that gradients of neurons with negative input are suppressed when propagat-
ing nonlinear layers. As with feature-oriented methods, gradient-based saliency maps do little to communicate
decisions beyond model diagnostics.

In Ancona et al. (2018), deep attribute maps are presented as a technique for rendering the explainability of
gradient-based methods. Importantly, the proposed framework illustrates evaluations between different saliencey-based
explanation models. Simply, the gradient of the output is multiplied by the respective input to generate an explanation
of a model's prediction in the form of a heatmap. Red and blue colors indicate positive and negative contributions,
respectively, to the output decision. Explanations are sensitive to noisy gradients and variations in the input. Deep attri-
bute maps alone cannot explain why two models produce similar or different results.

4.3 | Concept models

Concept activation vectors (CAVs) were introduced by Kim et al. (2021), a technique to explain globally the internal
states of a neural network by mapping human understandable features to the high-level latent features extracted by the
neural network. As such, CAVs represent the degree to which these abstract features point towards a set of human
understandable concepts chosen by a user. Of course, a certain amount of human bias is imposed, but by explaining the
associated concept it becomes possible to determine any defects in the decision-making process the model has learned;
for instance, if certain characteristics are mistakenly seen as important. Subsequently, automatic concept based expla-
nations (Ghorbani et al., 2019) extract CAVs automatically without human supervision, thereby removing human bias.
Instead of being chosen, the human understandable concepts are segmented at various spatial resolutions from in-class
images. Nevertheless, concept-based methods are reliant on the concepts being uniquely meaningful to the class, and
the effectiveness of explanation is adversely affected if a chosen concept is commonly present in multiple classes.

4.4 | Surrogate models

Local interpretable model-agnostic explanations (LIME) (Dieber & Kirrane, 2020) is a model-agnostic technique to cre-
ate locally optimized explanations of ML models. LIME trains an interpretable surrogate model to learn the local behav-
ior of a global “black box” model's predictions. For image classification, an input image is divided into patches of
contiguous superpixels (ie., an image object) and a weighted local model is then trained on a new set of permuted
instances of the original image (i.e., some superpixels are turned to gray). The intuition is then that by changing aspects
of the input data that are human understandable (spatial objects) and learning the differences between those perturba-
tions and the original observations, one can learn what about the input contributed to each class score. However, these
explanations are not always informative or reliable at a human level if the parameters that control the perturbations
are chosen based solely on heuristics.

4.5 | Local, pixel-based methods

Layer-wise relevance propagation (LRP) (Bach et al., 2015) uses predefined propagation rules to provide an explanation
of a multilayered neural network's output in respect to the input. The method renders a heatmap, thereby providing
insight into which pixels contributed to the model's prediction and the extent to which they did. Accordingly, LRP high-
lights positive contributions to a network's decision. While LRP can be applied to an already trained network, this pro-
cess is post hoc and therefore provides only a simplified distillation of the features’ role in the decision and is only

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
8 of 13 WI LEY_ iE WIREs ANGELOV er aL.

DATA MINING AND KNOWLEDGE DISCOVERY

 

applicable if the network implements backpropagation. DeconvNet (Noh et al., 2015) uses a semantic segmentation
algorithm which learns a deconvolution network and, therefore, provides insights about pixel contribution during the
classification process. Similarly, a deep belief network (Hinton et al., 2006) was proposed to improve the interpretability
of traditional neural networks.

4.6 | Human-centric methods

The above methods, despite their advantages, do not provide clear explanations understandable to humans. They rather
“barely scratch the surface” of the “black box” aiming for “damage limitation” with post hoc hints about the features
(attribute allocation) or localities within an image. This is radically different from the way people reason and make
decisions, make associations, evaluate similarities, and draw an analogy that can be articulated in court or to another
expert (e.g., in medicine, finance, law or other area). The aforementioned methods do not answer the fundamental
questions of model structure and parameters relating to the nature of the problem and completely ignore reasoning.

Recently, in Angelov and Soares (2020) a cardinally different approach to explainability was proposed which treats
it as a human-centric (anthropomorphic) phenomena rather than reducing it to statistics. Indeed, humans compare
items (e.g., images, songs, and movies) in their entirety and not per feature or pixel. People use similarity to associate
new data with previously learned and aggregated prototypes (Bien & Tibshirani, 2011) while statistics is based on aver-
ages (Bishop, 2006).

5 | EXPLAINABILITY-CRITICAL APPLICATIONS

The frequency and importance of algorithms in applications have lead regulators and official bodies to develop policies
that provide clearer accountability for algorithmic decision-making. One such example is the European Union's
General Data Protection Right, which some have interpreted as a “Right to Explanation” (Goodman & Flaxman, 2017).
Although the extent of this right is in dispute, the discourse around such topics has reinforced that automated systems
must avoid inequality and bias in decisions. Furthermore, they must fulfill the requirements for safety and security in
safety-critical tasks. Consequently, there has been a recent explosion of interest in XAI models in different areas.
Recently, it has been reported that XAI has been applied in several critical domain applications such as medicine
(Holzinger et al., 2017), the criminal justice system (Dressel & Farid, 2018), and autonomous driving (Cysneiros
et al., 2018).

In the medical domain there is a growing demand for AI approaches, most notably during the COVID-19 pandemic.
However, AI applications must not only perform well in terms of classification metrics, but need also to be trustworthy,
transparent, interpretable, and explainable, especially for clinical decision-making (Holzinger et al., 2017). Soares,
Angelov, Biaso, et al. (2020), for example, offered an explainable DL approach for COVID-19 identification via com-
puted tomography (CT) scans. The proposed approach was reported to surpass mainstream DL approaches such as
ResNet (He et al., 2016), GoogleNet (Szegedy et al., 2015), and VGG-16 (Simonyan & Zisserman, 2014) in terms of accu-
racy, Fl score and other statistical metrics of performance, but critically, this approach is based on prototypes which, in
this case, represent a CT scan that a radiologist can clearly understand. The prototypes are examples of CT scans of
patients with or without COVID. This approach can be expanded readily to include more classes, such as “mild” or
“severe” COVID, and so on, or go to the level of superpixels as in Tetila et al. (2020). Furthermore, the proposed deep
neural network has a clear and explainable architecture (with each layer having a very clear meaning and using visual
images of CT scans so the decision can easily be visualized).

Couteaux et al. (2019) proposed an explainable DeepDream approach where the activation of a neuron is maximized
by performing gradient ascent of a given image. The method has output curves that show the evolution of the features
during the maximization. This favors the visualization and interpretability of the neural network and was applied for
tumor segmentation from liver CT scans (Couteaux et al., 2019).

Another example application of XAI is the criminal justice system. In some countries such as the United States auto-
mated algorithms are being used to predict where crimes will most likely occur, who is most likely to commit a violent
crime, who is likely to fail to appear at their court hearing, and who is likely to re-offend at some point in the future
(Dressel & Farid, 2018). One such widely used criminal risk assessment tool is the Correctional Offender Management
Profiling for Alternative Sanctions (COMPAS). Although the data used by COMPAS do not include an individual's race,

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
ANGELOV er aL. a 9 of 13
ET AL 9 WIREs _ _WI L EY_| 28

W DATA MINING AND KNOWLEDGE DISC!

TABLE2 _ XAl critical applications—summary

Method Application

Holzinger et al. (2017) Medicine

Dressel and Farid (2018), Soares and Angelov (2019) Criminal justice system
Soares, Angelov, Biaso, et al. (2020) COVID-19 identification
Couteaux et al. (2019) Tumor segmentation
Mathews (2019) NLP

Smith-Renner et al. (2019), Xie and Philip (2018) Anomaly and fraud detection
Soares et al. (2019) Novelty detection

Soares, Angelov, Costa, et al. (2020) Autonomous vehicles

other aspects of the data may be correlated to race that can lead to racial biases in the predictions. Therefore, explana-
tions of such critical decisions are necessary to favor fairness and reduce racism during the decisions (Dressel &
Farid, 2018). As discussed by Soares and Angelov (2019), prototype-based algorithms can be a solution to reduce bias
and favor fairness as one can check and balance the prototypes generated to guarantee a fairer decision. Moreover, the
approach proposed in Soares and Angelov (2019) also provides human explainable rules to assist specialists during
decision-making.

Applications based on NLP also benefit from XAI. Mathews (2019) presented an explainable approach for tweet data
classification based on LIME. XAI techniques for anomaly and fraud detection are also explored by different authors as
a means of enhancing users’ trust (Smith-Renner et al., 2019; Xie & Philip, 2018).

Another application domain in which there is a growing number of applications and interest towards XAI is defined
as autonomous systems (these may be airborne, maritime or land-based individual vehicles with a control system or
swarms). Self-driving vehicles, for example, are automated systems that are expected to be used in possibly an unknown
environment (Das & Rad, 2020). In this context, the trust and acceptance of such systems require transparency, in con-
trast to “black-box” solutions. For example, a recent crash (on 18 March 2018) by an autonomous car owned by Uber
led to the operator being charged with negligent homicide (Stilgoe, 2020) two and a half years later. It is, therefore, criti-
cally important (not only from the point of view of public perception and trust which can make or break market per-
spectives, but also from a purely regulatory and legal perspective) to have transparent, interpretable, and explainable,
non-”black-box” models in use. This can lead to more reliable systems which are necessary to guarantee safety and
meet regulations (Das & Rad, 2020). Recently, examples of prototype-based approaches were published in which XAI
was used for understanding the visual scene (Soares et al., 2019) and the situation awareness of a self-driving car on a
highway/motorway/autobahn through the so-called vector of affordance indicators (relative velocities and distances to
the neighboring vehicles) (Soares, Angelov, Costa, et al., 2020). Not only were the accuracy, F1 score and other statisti-
cal measures reported to be comparable with, or surpass conventional DL methods, but the model was clearly explain-
able to a human in the form of linguistic rules and visual means. Moreover, for cases when the situation on the road is
deemed to be generated from a class that was never used in training (a completely new type of scene) it was reported
that conventional DL methods can make an incorrect prediction with a high confidence, which may have very damag-
ing consequences for autonomous vehicles, passengers, legal outcomes, and trust. Instead, Soares et al. (2019) proposed
a self-evolving approach, which can pro-actively learn from new situations due to its prototype nature, and also provide
explainable rules. These safety mechanisms are very important for critical applications such as autonomous driving.
Table 2 summarizes the applications mentioned in this section:

6 | FURTHER DISCUSSION

XAI aims to help humans to understand why a machine decision has been reached and whether or not it is trustworthy.
Consequently, XAI is inevitably a paradigm on how to bridge machine intelligence and human intelligence, with the
goal being to enable and widen the acceptance of AI systems by human subjects. In this sense, XAI can be interpreted
as “AI for people.”

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
10 of 13 WI LEY_ (3 WIREs ANGELOV er aL.

DATA MINING AND KNOWLEDGE DISCOVERY

6.1 | Critical importance of XAI

Even though intelligent systems offer great possibilities, the research initiative of XAI raises concerns of giving such
intelligent systems too much power without the ability to explain the decision-making process lying underneath
such complex systems to domain experts (e.g., medics, lawyers, financial experts, etc.) in terms, and in a form, under-
standable to them. This not only helps understand specific decisions made by such systems, but also encourages
researchers to create more human-like (anthropomorphic) solutions as well as inspiring the study and increased under-
standing of the brain as a natural information processing phenomenon. Moreover, since machines are taking over the
decision process in many daily situations, user rights have to be protected. Intelligent machines still mostly cannot
process abstract information or real-world knowledge unless it is converted to a form understandable by the algorithm
(features, outputs, and labels).

The above critical issue has become extremely important in many AI application areas. For example, the decision
from an automated diagnosis system may influence the treatment plan of a patient, and doctors need to understand
why such a decision was made and evaluate the underlying risks. If we consider farming-assisting autonomous drones,
the farmers need to know why, when, and where drones decide to perform automated spraying of water or pesticides.
Thus, a trustworthy XAI system becomes a critical prerequisite for AI to be applied to practically any real-world prob-
lem. Much research is now being focused on how to handle such kinds of problems.

6.2 | Bridge the gap between DL and neuroscience via XAI

DL as the state-of-the-art AI technique has its roots in the emulation of the human brain. To make deep neural net-
works explainable, an ultimate goal is to find a way to match human intelligence and find a way to build a human-
made “brain” that can interpret the neuronal activities in the human brain or at least, at a functionally higher level,
map the deep architectures to the layered information processing units in the brain.

There are two important differences between the features of current mainstream DL and the human brain. First,
the human brain is more like an analogue circuit without the ability to store high precision parameters. Second, neu-
rons in the human brain are highly interconnected instead of the carefully “handcrafted” architectures of the current
mainstream DL. It is curious, therefore, that the mainstream DL literature is very critical of so-called “handcrafted” fea-
tures (Goodfellow et al., 2014), but is slow to accept that the architectures it is pushing forward are “handcrafted”,
highly problem-specific and with multiple meta-parameters such as stride, kernel sizes, number of layers, and so on.

With the above concerns, XAI can help bridge the gap between DL and neuroscience in a mutually beneficial way.
On one side, neuroscience and psychology can help build rationalized XAI models that are more easily understood by
humankind (Byrne, 2019; Taylor & Taylor, 2020). On the other side, XAI models derived from deep neural networks
can also help in understanding the mechanisms of intelligence in the human brain (Fellous et al., 2020; VU
et al., 2018). The ultimate goal of XAI could be redefined as the pursuit of fully understanding how human intelligence
originates from neurons.

6.3 | Future directions

One promising direction for future research is to focus on prototype-based models (Angelov & Gu, 2018; Angelov &
Soares, 2020) rather than on abstract and highly embedded architectures. Prototype-based models are not new as such
(Bien & Tibshirani, 2011)—starting with the simplest (and highly efficient example of kNN), through RBF types of
ANNs and IF...THEN rules. The power of prototype-based models was noted by Tibshirani in (Bien & Tibshirani, 2011),
but so far these were not developed in the context of DL where they can combine a deeper architecture with a clearly
explainable form of representation. Despite its efficiency, the kKNN method is, strictly speaking, not a learning method,
because it requires all the data to be available and stored. Some sparsity is needed which can result from simple
unsupervised forms of learning such as clustering or more complex end-to-end auto-encoders. There is an established
misconception that the only form of learning is parametric learning through optimization (minimization) of a cost
(or loss) function. In fact, people learn by acquiring prototypes from data samples using similarity. Following this logic,
the learning in prototype-based models revolves around the position and properties of the prototypes in the feature/data
space as opposed to the parameters/weights-centered approach that dominates the mainstream. In addition, there is a

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
ANGELOV . 11 of 13
— eS fd WIRE meee AND KNOWLEDGE DISCOVERY —_WI LEY. °

principle difference between similarity and statistical learning (i-e., the two alternative approaches to evaluate the dif-
ference and divergence between two data items). Similarity can be defined over a pair of data items/samples while sta-
tistical measures require a large (theoretically infinite) number of independent data observations.

Another promising direction is to build Turing’s type-B random machines (or unorganized machines) Jiang &
Crookes, 2019; Webster, 2012), also random Boltzmann machines, which can possibly lead to a generalized AI. The
inclusion of new neuro-scientific findings into XAI models will make research on XAI more rationalized, and vice
versa: such a cross-disciplinary exploitation will make XAI not only meaningful for AI researchers but also help solve
century-old challenges on how to understand human intelligence, ultimately. Open research questions in this area
include: (i) how best to determine the network/model architecture?; (ii) how best to extract and represent features?;
(iii) what are the best distance metrics and what are the implications?; (iv) which is the best optimization method?; and
(v) how to determine the best set of prototypes that represent the data best (if a prototype-based method is being used)?

 

CONFLICT OF INTEREST
The authors have declared no conflicts of interest for this article.

DATA AVAILABILITY STATEMENT
Data sharing is not applicable to this article as no new data were created or analyzed in this study.

AUTHOR CONTRIBUTIONS

Plamen Angelov: Conceptualization; data curation; formal analysis; methodology; supervision; writing-original draft;
writing-review & editing. Eduardo Soares: Conceptualization; data curation; formal analysis; methodology; software;
validation; visualization; writing-review & editing. Richard Jiang: Conceptualization; formal analysis; investigation;
methodology; writing-review & editing. Nicholas I. Arnold: Conceptualization; formal analysis; investigation;
methodology; writing-review & editing. Peter M. Atkinson: Conceptualization; formal analysis; investigation;
methodology; writing-review & editing.

ORCID

Plamen P. Angelov © https://orcid.org/0000-0002-5770-934X
Eduardo A. Soares © https://orcid.org/0000-0002-2634-8270
Richard Jiang © https://orcid.org/0000-0003-1721-9474
Nicholas I. Arnold © https://orcid.org/0000-0003-3968-6233
Peter M. Atkinson © https://orcid.org/0000-0002-5489-6880

RELATED WIREs ARTICLE
Causability and explainability of artificial intelligence in medicine

 

REFERENCES

Adadi, A., & Berrada, M. (2018). Peeking inside the black-box: A survey on explainable artificial intelligence (XAI). IEEE Access, 6,
52138-52160.

Ancona, M., Ceolini, E., Oztireli, C., & Gross, M. (2018). Towards better understanding of gradient-based attribution methods for deep neural
networks. http://arxiv.org/abs/1711.06104

Angelov, P., & Soares, E. (2020). Towards explainable deep neural networks (xDNN). Neural Networks, 130, 185-194.

Angelov, P. P., & Gu, X. (2018). Toward anthropomorphic machine learning. Computer, 51, 18-27.

Arrieta, A. B., Diaz-Rodriguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcia, S., Gil-Lépez, S., Molina, D., Benjamins, R.,
Chatila, R., & Herrera, F. (2020). Explainable artificial intelligence (XAT): Concepts, taxonomies, opportunities and challenges toward
responsible AI. Information Fusion, 58, 82-115.

Bach, S., Binder, A., Montavon, G., Klauschen, F., Miiller, K.-R., & Samek, W. (2015). On pixel-wise explanations for non-linear classifier
decisions by layer-wise relevance propagation. PLoS One, 10, e0130140.

Bien, J., & Tibshirani, R. (2011). Prototype selection for interpretable classification. The Annals of Applied Statistics, 5, 2403-2424.

Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.

Breiman, L. (2001). Random forests. Machine Learning, 45, 5-32.

Burkart, N., & Huber, M. F. (2020). A survey on the explainability of supervised machine learning. arXiv preprint arXiv:2011.07876.

Byrne, R. M. (2019). Counterfactuals in explainable artificial intelligence (XAI): Evidence from human reasoning. In Proceedings of the
twenty-eighth international joint conference on artificial intelligence IJCAI-19) (Vol. 1, pp. 6276-6282).

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
12 of 13 WI LEY_ (3 WIREs ANGELOV er aL.

DATA MINING AND KNOWLEDGE DISCOVERY

Campbell, M., Hoane, A. J., Jr., & Hsu, F.-H. (2002). Deep blue. Artificial Intelligence, 134, 57-83.

Chang, H. S., Fu, M. C., Hu, J., & Marcus, S. I. (2016). Google deep mind's alphago. OR/MS Today, 43, 24-29.

Chattopadhay, A., Sarkar, A., Howlader, P., & Balasubramanian, V. N. (2018). Grad-CAM++: Generalized gradient-based visual explana-
tions for deep convolutional networks. In 2018 IEEE Winter conference on applications of computer vision (WACV) (pp. 839-847).

Chen, H., Lundberg, S., & Lee, S.-I. (2019). Explaining models by propagating Shapley values of local components. arXiv preprint arXiv:
1911.11888.

Core, M. G., Lane, H. C., Van Lent, M., Gomboc, D., Solomon, S., & Rosenberg, M. (2006). Building explainable artificial intelligence systems.
In AAAI (pp. 1766-1773).

Couteaux, V., Nempont, O., Pizaine, G., & Bloch, I. (2019). Towards interpretability of segmentation networks by analyzing DeepDreams. In
Interpretability of machine intelligence in medical image computing and multimodal learning for clinical decision support (pp. 56-63).
Springer.

Cysneiros, L. M., Raffi, M., & do Prado Leite, J. C. S. (2018). Software transparency as a key requirement for self-driving cars. In 2018 IEEE
26th international requirements engineering conference (RE). IEEE (pp. 382-387).

Das, A., & Rad, P. (2020). Opportunities and challenges in explainable artificial intelligence (XAI): A survey. arXiv preprint arXiv:2006.11371.

Dieber, J., & Kirrane, S. (2020). Why model why? Assessing the strengths and limitations of lime. arXiv preprint arXiv:2012.00093.

Dressel, J., & Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism. Science Advances, 4, eaao5580.

Fellous, J. M., Sapiro, G., Rossi, A., Mayberg, H., & Ferrante, M. (2020). Explainable artificial intelligence for neuroscience: Behavioral neuro-
stimulation. Frontiers in Neuroscience, 13, 1346.

Ghorbani, A., Wexler, J., Zou, J., & Kim, B. (2019). Towards automatic concept-based explanations. http://arxiv.org/abs/1902.03129

Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L. (2018). Explaining explanations: An overview of interpretability of
machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). IEEE (pp. 80-89).

Goodfellow, I, Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative adversarial
nets. Advances in Neural Information Processing Systems, 3, 2672-2680.

Goodman, B., & Flaxman, S. (2017). European union regulations on algorithmic decision-making and a “right to explanation”. AI Magazine,
38, 50-57.

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition (pp. 770-778).

Hearst, M. A., Dumais, S. T., Osuna, E., Platt, J., & Scholkopf, B. (1998). Support vector machines. IEEE Intelligent Systems and their Applica-
tions, 13, 18-28.

Hinton, G. E., Osindero, S., & Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527-1554.

Holzinger, A., Biemann, C., Pattichis, C. S., & Kell, D. B. (2017). What do we need to build explainable AI systems for the medical domain?
arXiv preprint arXiv:1712.09923.

Hornik, K., Stinchcombe, M., & White, H. (1990). Universal approximation of an unknown mapping and its derivatives using multilayer
feedforward networks. Neural Networks, 3, 551-560.

Ibrahim, M., Louie, M., Modarres, C., & Paisley, J. (2019). Global explanations of neural networks: Mapping the landscape of predictions. In
Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19. Association for Computing Machinery (pp. 279-287).
https://doi.org/10.1145/3306618.3314230.

Jiang, R., & Crookes, D. (2019). Shallow unorganized neural networks using smart neuron model for visual perception. IEEE Access, 7,
152701-152714.

Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., & Sayres, R. (2021). Interpretability beyond feature attribution: Quantitative
testing with concept activation vectors (TCAV). http://arxiv.org/abs/1711.11279

LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521, 436-444.

Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. Advances in Neural Information Processing Sys-
tems, 30, 4765-4774 https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html

MacCarthy, M. (2019). An examination of the algorithmic accountability act of 2019. Available at SSRN 3615731.

Mathews, S. M. (2019). Explainable artificial intelligence applications in NLP, biomedical, and malware classification: A literature review. In
Intelligent computing-proceedings of the computing conference (pp. 1269-1292). Springer.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G.,
Petersen, S., Beattie, C., Sadik, A., Antonoglou, I, King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level con-
trol through deep reinforcement learning. Nature, 518, 529-533.

Nguyen, A., Yosinski, J., & Clune, J. (2015). Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.
In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 427-436).

Nilsson, N. J. (2014). Principles of artificial intelligence. Morgan Kaufmann.

Noh, H., Hong, S., & Han, B. (2015). Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE international
conference on computer vision (pp. 1520-1528).

Pasquale, F. (2015). The black box society. Harvard University Press.

Pedreschi, D., Giannotti, F., Guidotti, R., Monreale, A., Ruggieri, S., & Turini, F. (2019). Meaningful explanations of black box AI decision
systems. In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, pp. 9780-9784).

Peterson, L. E. (2009). K-nearest neighbor. Scholarpedia, 4, 1883.

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
ANGELOV . 13 of 13
— eS fd WIRE meee AND KNOWLEDGE DISCOVERY —_WI LEY. °

Phillips, P. J., Hahn, C. A., Fontana, P. C., Broniatowski, D. A., & Przybocki, M. A. (2020) Four principles of explainable artificial intelligence.

Quinlan, J. R. (1990). Decision trees and decision-making. IEEE Transactions on Systems, Man, and Cybernetics, 20, 339-346.

Quinlan, J. R. (1996). Learning decision tree classifiers. ACM Computing Surveys (CSUR), 28, 71-72.

Rish, I. (2001). An empirical study of the naive Bayes classifier. In IMCAI 2001 workshop on empirical methods in artificial intelligence (Vol.
3, pp. 41-46).

Robinson, A. J., & Voronkov, A. (2001). Handbook of automated reasoning (Vol. 1). Gulf Professional Publishing.

Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature
Machine Intelligence, 1, 206-215.

Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, 3, 210-229.

Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-CAM: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE international conference on computer vision (pp. 618-626).

Simonyan, K., Vedaldi, A., & Zisserman, A. (2013). Deep inside convolutional networks: Visualising image classification models and saliency
maps. arXiv:1312.6034 [cs].

Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.

Smith-Renner, A., Rua, R., & Colony, M. (2019). Towards an explainable threat detection tool. In IUI workshops.

Smolensky, P. (1987). Connectionist AI, symbolic AI, and the brain. Artificial Intelligence Review, 1, 95-109.

Soares, E., & Angelov, P. (2019). Fair-by-design explainable models for prediction of recidivism. arXiv preprint arXiv:1910.02043.

Soares, E., Angelov, P., Biaso, S., Froes, M. H., & Abe, D. K. (2020). SARS-Cov-2 CT-scan dataset: A large dataset of real patients CT scans for
SARS-Cov-2 identification. medRxiv.

Soares, E., Angelov, P., Costa, B., & Castro, M. (2019). Actively semi-supervised deep rule-based classifier applied to adverse driving scenar-
ios. In 2019 international joint conference on neural networks (IICNN). IEEE (pp. 1-8).

Soares, E. A., Angelov, P. P., Costa, B., Castro, M., Nageshrao, S., & Filev, D. (2020). Explaining deep learning models through rule-based
approximation and visualization. IEEE Transactions on Fuzzy Systems, 1, 1-10.

Stilgoe, J. (2020). Who killed Elaine Herzberg? In Who's driving innovation? (pp. 1-6). Springer.

Stock, P., & Cisse, M. (2018). ConvNets and ImageNet beyond accuracy: Understanding mistakes and uncovering biases. In Proceedings of
the European conference on computer vision (ECCV) (pp. 498-512).

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., & Rabinovich, A. (2015). Going deeper with
convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).

Taylor, J. E. T., & Taylor, G. W. (2020). Artificial cognition: How experimental psychology can help generate explainable artificial intelli-
gence. Psychonomic Bulletin and Review, 28, 6276-6282.

Tetila, E., Bressem, K., Astolfi, G., Sant'Ana, D. A, Pache, M. C., & Pistori, H. (2020). System for quantitative diagnosis of
COVID-19-associated pneumonia based on superpixels with deep learning and chest CT. ResearchSquare, 1, 1-13. https://doi-org/10.
21203/rs.3.rs-123158/v1

Tritscher, J., Ring, M., Schlr, D., Hettinger, L., & Hotho, A. (2020). Evaluation of post-hoc XAI approaches through synthetic tabular data. In
International symposium on methodologies for intelligent systems (pp. 422-430). Springer.

VU, M., Adah, T., Ba, D., Buzsdki, G., Carlson, D., Heller, K., Liston, C., Rudin, C., Sohal, V., Widge, A., Mayberg, H., Sapiro, G., &
Dzirasa, K. A. (2018). A shared vision for machine learning in neuroscience. Journal Neuroscience, 18, 1601-1607.

Webster, C. S. (2012). Alan turing’s unorganized machines and artificial neural networks: His remarkable early work and future possibilities.
Evolutionary Intelligence, 5, 35-43.

Werbos, P. J. (1990). Backpropagation through time: What it does and how to do it. Proceedings of the IEEE, 78, 1550-1560.

Xie, S., & Philip, S. Y. (2018). Next generation trustworthy fraud detection. In 2018 IEEE 4th international conference on collaboration and
internet computing (CIC). IEEE (pp. 279-282).

Young, T., Hazarika, D., Poria, S., & Cambria, E. (2018). Recent trends in deep learning based natural language processing. IEEE Computa-
tional Intelligence Magazine, 13, 55-75.

Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. (2016). Learning deep features for discriminative localization.

 

How to cite this article: Angelov, P. P., Soares, E. A., Jiang, R., Arnold, N. I, & Atkinson, P. M. (2021).
Explainable artificial intelligence: an analytical review. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 11(5), e1424. https://doi.org/10.1002/widm.1424

 

asuaary] SUOLLULOS aANRAID o[gEoT{dde oWp fq pouioaa’ a sole WO ‘asn Jo Sol Joy Areqr] OUT[UG AaTtAA UO (SUONIPUOS-pUR-sULIA/UKOS’Ao[LM A reIqifoUT|UO//:sdny) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO ArEAQrT oUTTUD AaTt A “T2eUST eUEIYDO; Aq FCI UEPLR/ZOO TO L/loppuios Kopi Areiqrauruo'sourmyysdny wioxy popeotumog *¢ “1ZOZ ‘S6LPZPET
Building Explainable Artificial Intelligence Systems

Mark G. Core, H. Chad Lane, Michael van Lent, Dave Gomboc, Steve Solomon and

Milton Rosenberg
The Institute for Creative Technologies, The University of Southern California
13274 Fiji Way, Marina del Rey, CA 90292 USA
core,lane,vanlent,gomboc,solomon,rosenberg @ict.usc.edu

Abstract

As artificial intelligence (AD systems and behavior
models in military simulations become increasingly
complex, it has been difficult for users to understand the
activities of computer-controlled entities. Prototype ex-
planation systems have been added to simulators, but
designers have not heeded the lessons learned from
work in explaining expert system behavior. These new
explanation systems are not modular and not portable;
they are tied to a particular AI system. In this paper,
we present a modular and generic architecture for ex-
plaining the behavior of simulated entities. We describe
its application to the Virtual Humans, a simulation de-
signed to teach soft skills such as negotiation and cul-
tural awareness.

Introduction

The complexity of artificial intelligence (AI) systems in sim-
ulations and games has made it difficult for users to under-
stand the activities of computer-controlled entities. In simu-
lations and games used for training, the explainability of be-
haviors is essential. Students must understand the rationale
behind actions of simulated entities and how their actions
affect the simulated entities.

The military has traditionally used live exercises for train-
ing; the principal tool for learning from these exercises has
been the after-action review (AAR). US Army Field Manual
25-101, “Battle Focused Training”, gives recommendations
on conducting an AAR, and states that “The OPFOR [op-
posing forces] can provide valuable feedback on the training
based on observations from their perspectives...the OPFOR
can provide healthy insights on OPFOR doctrine and plans,
the unit’s action, OPFOR reactions to what the unit did.”
(Army 1990)[Appendix G] OPFOR are often played by in-
structors and participate in the AAR giving their viewpoints
not only as opponents in the exercise but also as teachers
assessing trainee performance. Friendly forces also partici-
pate in the AAR, and provide insight on how trainee orders
translate into the behavior of units and individual soldiers.
Despite the importance of AARs, military simulators cur-
rently do not provide this type of experience. Such simu-
lators can present mission statistics (e.g., casualties, tactical

Copyright © 2006, American Association for Artificial Intelli-
gence (www.aaai.org). All rights reserved.

1766

objectives achieved) and replay events from the simulation,
but neglect the causes behind these statistics and simulation
events. What is missing in the AAR are the simulated enti-
ties (both OPFOR and friendly forces) to present their view-
point on what happened and why, and an AAR leader to en-
sure students are asking the right questions and learning how
to improve their performance.

Military simulations are also used as predictive tools; as
such, their AI systems must be validated as behaving realis-
tically and according to military doctrine. Detailed specifi-
cations are drafted for such AI systems; the resulting behav-
iors are put under heavy scrutiny. In most cases, because
observers have no way to question Al-controlled entities,
the observer’s only recourse is to watch numerous simula-
tion runs, looking for cases where faulty reasoning results in
an incorrect action. For example, to detect an error where a
rifleman entity fails to check its rules of engagement before
firing its weapon, the observer must watch simulation runs
until a situation occurs where the rifleman fires its weapon
but its rules of engagement forbid this action. A better ap-
proach would be to question the entity about a single weapon
fire event, and see whether checking the rules of engagement
is part of its reasoning.

Figure 1 shows a screenshot of the user interface of our
explanation system for the One Semi-Automated Forces
Objective System, a military simulation (Courtemanche &
Wittman 2002), and introduces the concept of an explana-
tion system for simulated entities. Users select a time point
to discuss, an entity to be questioned, and the question it-
self. Some of the questions are specific to the particular en-
tity (e.g., what is your health?) while others concern a larger
group (e.g., what is your unit’s task?).

Two of the first explanation systems for simulation were
Debrief (Johnson 1994) and Explainable Artificial Intelli-
gence for Full Spectrum Command, FSC (van Lent, Fisher,
& Mancuso 2004). In Debrief, Johnson built explanation
capabilities into Soar (Laird, Newell, & Rosenbloom 1987)
agents that were part of battlefield simulations. van Lent et
al. added explanation capabilities to a training aid developed
by commercial game developers and academic researchers.
van Lent et al. coined the term “explainable artificial intelli-
gence” (XAJI) to describe the ability of their system to sum-
marize the events of the game/simulation, flag key events,
and explain the behavior of computer controlled entities.
 

eXplainable Artificial Intelligence

nie, Mark tone, by eal Le, Slave Sik non -aseiie Millon) Resende, Bill Smeaton.

 

Question | Entity -|

   

Figure 1: Interface to XAI for OOS

While novel, this research did not address how lessons
learned from explaining expert system decisions (Swartout
& Moore 1993) applied to explaining the behavior of sim-
ulated entities. Swartout and Moore advocated building a
high-level knowledge base containing facts about the do-
main and problem-solving strategies, and using an automatic
program writer to build an expert system from this specifica-
tion. The problem is more complicated for XAI because of
technical challenges (i.e., the executable code must interface
with an external simulation) and real world constraints (i.e.,
we have little or no control of the simulated entities - they
have already been built or designed).

In the follow-on project to XAI for FSC, we faced these
challenges but sought a middle ground between building an-
other simulation-specific explanation system and demand-
ing to build the simulated entities ourself (as Swartout and
Moore built their own expert systems). Following this phi-
losophy, we re-engineered our system to support:

e domain independence - supporting reuse and the devel-
opment of common behavior representations while being
flexible enough to deal with simulation-specific idiosyn-

crasies
the ability to explain the motivations behind entity actions

modularity - allowing external components such as GUIs,
natural language generators, and tutors to interface with
the system

In the related work section, we discuss how the John-
son and van Lent et al. work falls short in addressing these
goals; we then present our XAI architecture designed specif-
ically to meet these needs. Our first XAI system using
this new architecture works with the One Semi-Automated
Forces Objective System (OOS), a tactical military simula-
tor and is described in the papers, (Gomboc et al. 2005;
Core et al. 2005).

The focus of this paper is our second instantiation of the
new architecture, the Virtual Humans simulator (Traum et
al. 2005). This was not designed as a tactical simulation,

1767

but rather for the teaching of soft skills such as leadership,
teamwork, negotiation and cultural awareness. The current
scenario has one character, a virtual doctor who communi-
cates with students through spoken input/output as well as
generating animated gestures. The student plays the role of
a U.S. Army Captain whose unit is planning an operation
against insurgents in the same neighborhood as the clinic
run by the doctor. In order to minimize the risk to civilians,
the Captain is ordered to move the clinic to a safer location.
The doctor is not interested in moving; the Captain’s task is
to convince the doctor to move the clinic.

Although the core structure of our architecture has not
changed since we built XAI for OOS, we are continu-
ally making improvements such as the replacement of hard
coded questions with logical forms. In this paper, we present
an abbreviated case study describing the process of connect-
ing this architecture to the Virtual Humans simulator. It il-
lustrates the steps necessary to connect a simulation to an
external explanation system as well as giving an idea of what
parts of the system are reusable and what must be authored.
We then present a sample dialogue with interactions between
a student, automated tutor, and the XAI system. We do not
focus on the details of this tutor, but include it in this pre-
sentation to illustrate the usefulness of XAI in a pedagogical
context. In the future work section, we discuss taking fea-
tures specific to XAI for OOS and XAI for Virtual Humans
and making them domain-independent functions that can be
reused in new target simulations.

Related Work

In this section, we discuss two pieces of previous work on
generating explanations of entity actions during a simula-
tion: Debrief (Johnson 1994) and explainable artificial in-
telligence (XAJ) for Full Spectrum Command (FSC) (van
Lent, Fisher, & Mancuso 2004). Debrief works with enti-
ties controlled by Soar (Laird, Newell, & Rosenbloom 1987)
agents in a tactical air combat domain. FSC was a collabo-
tation between game developers and academic researchers
who developed their own artificial intelligence system to
control simulated entities in this training tool for command-
ing a light infantry company.

Debrief uses Soar’s learning mechanism to save the
agents’ states during the simulation. After the simulation,
an agent gives a textual description of what happened and
the user is allowed to ask questions. To answer these ques-
tions, Debrief performs reflection by rerunning the agent and
selectively changing elements of its state to determine the
cause of its actions (including actions such as inserting a be-
lief into memory).

FSC has logging facilities that save each entity’s state:
simple values such as ammo status and a representation of
the task being executed (including its subtasks and how the
platoon is broken into squads). After the simulation is over,
FSC presents mission statistics and flags “interesting” time
points (an entity is wounded or killed, the first contact with
the enemy, task start and end times). The user selects a
time and entity and is able to ask questions about the entity’s
state.
Both systems were important first steps in adding expla-
nation capabilities to simulated entities, but neither approach
was suitable for our goals of domain independence and mod-
ularity. Debrief and XAI for FSC are specific to the AI sys-
tems controlling the simulated entities in those applications,
and not directly applicable to other AI systems.

(Gomboc et al. 2005) was the first description of our
domain-independent XAI architecture; it described our ef-
forts to connect this architecture to the tactical simulations,
the One Semi-Automated Forces Objective System, OOS
and Full Spectrum Command. We noted that simulations
differ as to their “explanation-friendliness”, and one of the
key issues is how the simulation represents behaviors. Simu-
lations may encode behaviors directly in a programming lan-
guage, use planning operators, use a declarative rule format,
or combinations of the three. We argued against a purely
procedural representation because it provides no represen-
tation of the motivations underlying the behavior. Consider
the example of sending a fire team to clear a room. Once
the fire team is in position outside the room, the grenadier
throws a grenade before the team enters the room. This
could be encoded as a procedure (the step before entering
the room is always throwing the grenade) in which case, the
system cannot explain why the grenade was thrown. In the
next section, we summarize the XAI architecture presented
in (Gomboc et al. 2005) as well as discussing what options
are available when dealing with an explanation-unfriendly
simulation.

XAI Architecture

Figure 2 shows our domain-independent XAI architecture.
The (Gomboc er ai. 2005) paper focused on the left side of
diagram and the problem of importing data from the simu-
lation. We use a relational database to store this informa-
tion because tactical military simulations typically produce
large log files; they record data at a fine granularity (e.g.,
OOS records the values of variables about once a second).
The Virtual Humans currently log a much smaller amount
of data, but the full power of the relational database may be
useful later if the logging in Virtual Humans becomes more
fine-grained. We focus on a subset of the virtual human’s be-
haviors: if we covered the entire range, then we would see
log files of comparable size as those in tactical simulations.

Assuming XAI is able to import the necessary informa-
tion from the simulation, the next step is to inspect the data
looking for “interesting” facts. The definition of interest-
ing will change with each application. For OOS, we used a
placeholder definition that highlighted when an entity fired
its weapon, or when it began, was half-way through, or com-
pleted a task. Collaboration with domain experts will help us
refine this definition. Because we developed an automated
tutor for the Virtual Humans, our definition of interesting re-
ferred to events that the tutor should discuss with the student
and it is the responsibility of the tutor to identify them. Al-
though we currently hand-annotate these teaching points, we
are automating the process using a heuristic-based approach
to identifying teachable moments in an exercise.

Once the XAI system is initialized, users select an entity
with which to speak and the time point in the simulation

1768

they wish to discuss first. Users then query the entity about
the current time point by selecting questions from a menu.
The dialogue manager orchestrates the system’s response;
first using the reasoner to retrieve the relevant information,
then producing English responses using the natural language
generator (NLG). NLG fills slots in natural language tem-
plates with information from the database. NLG is coded
in XSL templates and takes advantage of XSL features such
as iteration and procedure calls (common tasks such as state
descriptions are shared among templates).

As noted in (Gomboc et al. 2005), this is a best-case sce-
nario where the simulation makes available a rich behavior
representation containing, for example, entity goals and ac-
tion preconditions and effects. However, this does not mean
that if the simulation does not have such a representation we
cannot attempt to explain entity behavior.

Previous work in explaining expert system behavior (sum-
marized in (Swartout & Moore 1993)) dealt with a similar
problem; if the expert system’s inference engine contains
special features (not represented in the system’s knowledge
base) then the output of these features can still be explained
by hard coded explanation routines. However, if changes
are made to the special features without also updating the
explanation routines, there is a potential for inaccurate ex-
planations. Thus, explanations can still be made but at a
cost to the maintainability and robustness of the system.

In the case of representing simulation behaviors for XAI,
there are three options with associated costs and benefits:

1. automatically import the behaviors. cost: requires a rep-
resentation of goals and the preconditions and effects of
behaviors. benefit: high maintainability and robustness.

target: plan-based representations

. semi-automatically import the behaviors. cost: must be
able to find goals, preconditions, and effects in behav-
ior representation. benefit: more maintainable and robust
than option 3, and makes fewer assumptions than option
1. target: rule-based representations

. hand-build the XAI representation of the behaviors. cost:
low maintainability - any change in the behavior must be
duplicated in the XAI representation. Need subject matter
expert to author missing goals, preconditions, and effects.
benefit: makes no assumptions about the simulation’s be-
havior representation. target: procedural representations

The reason that we target rule-based representations with
option 2 is that some elements on the left hand side of rules
are preconditions (e.g., you must have ammunition to fire
your weapon), but other elements may be abort conditions
(e.g., do not fire your weapon when a friendly entity is in
the path) or internal bookkeeping (e.g., setting internal vari-
ables, making sure a rule does not fire twice). Similarly,
not all elements on the right hand side of rules are effects
and may instead also be internal bookkeeping. With such a
representation, we can hand-annotate the preconditions and
effects in these rules, then automatically import the behav-
ior. Although there is no guarantee that annotations will be
updated as developers change entity behaviors, at least this
meta-data is co-located with the original behavior represen-
tation.
    
     
   
  

 

  

—_—->

 

 

Natural
Language
Generator

Reasoner

 

 

 

 

 

a

 

Dialogue Manager

tt

 

 

 

 

 

 

 

 

 

 

Entity Al
Scenario
alos {
Al Behaviors XAl
Database
Simulation Environment
- —
Behavior
Translation
Process

 

XAI User Interface

Questions Answers

 

 

 

 

 

 

 

Figure 2: XAI Architecture

Collecting log files and scenario information is not trivial,
but here the options are simple: if the simulation does not
make information available for export, the XAI system can-
not answer questions about it. In our XAI for OOS system,
we entered some scenario and log information by hand to
build a proof-of-concept system, but this data is specific to
a particular simulation run, and some of the hand data-entry
must be repeated for each new simulation run. It is more fea-
sible to hand-author behavior representations because these
do not change between simulation runs.

Building a new XAI System
Connecting our current XAI system to a new simulation re-
quires several steps:

1. study the behavior representation and choose one of the

three approaches to importing behaviors as discussed in
the previous section.

2. implement data import for behaviors and log files
3.

3a. write the logical form (LF) of each question
3b. write the query LF

specify the question list for the domain

4. augment the natural language generator to support the
new questions and their potential answers

create GUI

Specifying the question list for a new simulation requires
two steps. The first step is writing the logical form of the
question which is used to generate the English form of the
question. For the Virtual Humans, we had 110 distinct ques-
tions so by using the natural language generator to produce
the questions we could change how they were phrased with-
out rewriting all 110 questions. The second step is writing
the query to retrieve the answer from the database; we use
an abstract language called the query logical form to encode
queries (see below for more details).

The last step in connecting XAI to a new simulation is
building a new GUI or reusing a GUI from a previous XAI

5.

1769

system. Although every XAI system will have the same
basic GUI components (ways to select entities, times, and
questions, and displays of dialogue between user, XAI, and
tutor), to support replay of the simulation requires support
from the target simulation, and if XAI is a feature integrated
into the simulation, it will share the simulation’s GUI. Be-
cause of these constraints, we designed the GUIs of XAI for
OOS and XAI for Virtual Humans as separate components
that communicate with the rest of the system through XML
messages. Our abstract message format facilitates this play-
and-plug functionality. The messages convey the content of
menus such as the question list and list of time points as well
as user selections from these menus. The messages also up-
date the state of the dialogue between the student and tu-
tor and the dialogue between the student and XAI. The GUI
can display these menu choices and text in whatever widgets
(e.g., radio buttons, drop-down menus) it chooses.

XAI for Virtual Humans

Following the steps enumerated in the previous section, the
first task in connecting XAI to the Virtual Humans was to
study the behavior representation. In this case, the sim-
ulation developers not only had to model physical behav-
iors such as treating patients but also the behaviors under-
lying the utterances produced by the student and doctor
(e.g., committing, insisting), and the doctor’s mental rea-
soning (e.g., making the decision to help the Captain). The
model of physical actions contained preconditions and ef-
fects explaining the relationships between the actions (e.g.,
you need supplies to treat the patients). In importing this
model we found some bugs in the model, so it is more accu-
rate to say that we semi-automatically imported the physical
behaviors. Now that the bugs are fixed, we should be able to
fully automate this process.

The non-physical behaviors were implemented with hun-
dreds of rules developed in the Soar cognitive architecture
(Laird, Newell, & Rosenbloom 1987). Given enough time,
it should be possible to hand-annotate the goals, precondi-
name=‘the’
variable=‘x’

quant
tense=‘present’
modality=‘none’

proposition frame (name=‘negotiation-stance’
agent=‘doctor
theme='‘captain’
time=CURRENT
value=x

Figure 3: LF for “What is your negotiation stance?”

tions and effects in all these rules. As an initial step, our
implementation focused on the rules governing trust since
teaching trust building is one of the pedagogical goals of
the 2005 Virtual Humans system. As described in (Traum
et al. 2005), the Virtual Humans model trust as influenced
by three factors: familiarity, credibility, and solidarity. All
three have direct positive relationships to trust; the more the
doctor feels he knows you, feels you speak the truth, and
feels that you share common goals, the more he trusts you
(and vice versa). In our current prototype, we model single
steps of the doctor’s reasoning by linking rules to English
paraphrases (e.g., “the negotiation failed because you lost
the trust of the doctor’).

Once we designed the database format for the target sim-
ulation and wrote the code to import the data, the next step
involved encoding the questions to be answered by XAI in
the target domain (i.e., specifying the question itself, encod-
ing the relevant database queries, and necessary changes to
the natural language generation templates). Questions are
encoded in a logical form (LF), an abstract representation of
their content. A simplified graphical version of our XML
representation for the question, “What is your negotiation
stance?” is shown in figure 3. The logical form was designed
to support future plans to generate syntactic features of the
character’s language such as tense and modality rather than
hard coding them in templates. The other feature to note
is the variable, CURRENT which is substituted at runtime
with the current line being discussed. It is obvious that we
would not author separate questions such as “what is the ne-
gotiation stance at line 1” and “what is the negotiation stance
at line 2”. However, this same mechanism also allows us to
have one logical form for the questions, “why did the nego-
tiation fail?” and “why are you avoiding the negotiation?”.
Here, the runtime variable is the negotiation stance.

The logical form of the question is accompanied by an
abstract representation of the query (we call it the query LF)
to retrieve the answer. It also uses runtime variables so that
authors only have to write one query LF for the questions
“why did the negotiation fail?” and “why are you avoiding
the negotiation?”. The XAI reasoner translates the query LF
into the SQL query which is sent to the database. An area of
future work is to derive the query LF automatically from the
LF of the question.

1770

The next step is modifying the set of XSL templates so
that the English form of the question can be generated as
well as the range of potential answers. Templates can be
reused to support new questions and their potential answers.
For example, there is one set of templates that generates
English descriptions of states and tasks. These are used to
describe states and tasks in questions as well as answers. In
future work, we intend to make our natural language genera-
tion more domain-independent by hard coding less English,
and adding templates encoding domain-independent aspects
of language such as syntax and morphology.

Sample Dialogue with XAI for Virtual
Humans

As mentioned in the introduction, after action reviews
(AARs) are the Army’s primary tool for learning from live
training exercises, and our goal is building AAR experiences
for simulated exercises. Our AAR GUI for the Virtual Hu-
mans is shown in figure 4. We currently lack a replay of
the simulation and instead show a transcript of the inter-
action between the student and the virtual doctor that took
place during the simulation (upper left of figure 4). Cur-
rently our prototype works with the following sample dia-
logue (C=Captain and D=Doctor):

2

Hello Doctor Perez

Hello

I have orders to move this clinic to another location
You want to move the clinic

Yes

Do you see that girl? She lost her mother today.
It is not safe here. We cannot protect you.
Protect me? Protect me from what?

You are going to attack?

Yes

I would have to refuse this decision

My patients need my attention now

VyQggayVaAyNys

The dialogue between the student and tutor appears in the
lower left of the AAR screen. In this session, the tutor asks
the student to use XAI to investigate what happened at line
10. The student then asks questions of the doctor using the
question list on the bottom right of the screen, and the dia-
logue between student and doctor appears in the upper right
of the screen. This is a prototype interface and we are cur-
rently experimenting with merging the two dialogue win-
dows, adding playback capabilities, and developing a more
usable question list.

Before the AAR begins, the tutor must analyze the dia-
logue searching for teaching points. Intuitively we can see
that the dialogue is unsuccessful. In line 10, the student re-
vealed secret information, and triggered the doctor’s ending
of the conversation. Examining the log files showing the
doctor’s mental state confirms that the dialogue was unsuc-
cessful and that line 5 (where the Captain asked the doctor to
move the clinic) and line 10 decreased trust. Our knowledge
of the rules governing the doctor helps us see opportunities
the student missed to gain trust in lines 3 and 10.

After loading the log files and behavior data into our XAI
system, we hand-annotate these teaching points. Because
eure
Simu at on History

 

1 Utterance

DOCTOR: Hello.

(CAPTAIN: | have orders to move this clinic to another location.
DOCTOR: You want to move the clinic.

CAPTAIN: Yes.
“DOCTOR: Do you see that
Peaevan a is not safe here, We cannot protect you.

Time List &

girl? She lost her mother today.

 

 

    

Transcri t rotect ct me from what?
posto var are going to attack?

DOCTOR: | would have to refuse this decision

<=> DOCTOR: My patients need my attention now.

‘
[_ Submit
Turcring Dialogue
doctor?

USER: He didn't want to move the clinic

Tutor-Student

D i al og ue have him move the clinic inline 3.
There is room for improvement in line 10.
===> In the XAI window, please ask the doctor some questions and
find out how this utterance contributed to the failure of the

negotiation.

Click on Submit in the simulation history window to begin the
investigation.

TUTOR: We first need to know why the negotiation failed.

Enter your resporee

 

_ CAPTAIN: Hello Doctor Perez.

TUTOR: That's right. Why do you think this utterance upset the

TUTOR: That's right. You abruptly admitted your goal was to

>

 

XAT Dialogue’

USER: Let's start the investigation at line 10.

DOCTOR: OK.

USER: What is your negotiation stance?

DOCTOR: The negotiation is over and | refuse to move the
clinic.

USER: What are the preconditions of running the clinic here?
DOCTOR: having supplies here, the patients being safe here,
the patients being here, the clinic being here and being
neutral.

USER: What actlon(s) could change being neutral?

DOCTOR: negotiating with the Captain, cooperating with the
Captain and the Americans moving the clinic.

USER: Why did the negotiation fall?

DOCTOR: | no longer trusted the Captain.

USER: How was your solidarity with the Captain affected here?
DOCTOR: It decreased.

XAl|-Student
Dialogue

<——

Questions:
ID. Question
1 Whats your negotiation stance? ia
2 Why did the negotiation fail?
3. Whatis your trust of the Captain?
5_Howwas your solidarity with the Captain affected here?
6 Why did your solidarity with the Captain decrease?
7_How was the Captain's credibility affected here?
9 How was your familiarity with the Captain affected here?
11_ [How important do you feel being neutral i
12 What action(s) could cause being neutral?
effects of running the clinic here?
14. What action(s) could change being neutral?
15 What are the preconditions of running the clinic here?
16 What actions have being neutral as a precondition?

Question List
—_

 

 

 

 

Tack: [run-clinic-here ©) State: [neutrality _ »

Lsebrit |

 

 

 

Figure 4: Interface to XAI for Virtual Humans

these are not subjective judgments, we anticipate that it will
be fairly simple to encode these intuitions, and annotate
teaching points automatically. The losses of trust and failure
of the negotiation appear directly in the log file. Determining
what the student should have done requires recognizing rules
that increase trust and checking their preconditions (e.g., ut-
tering a pleasantry such as “nice to meet you” is possible at
line 3 and would have increased trust).

Our automated tutor is a prototype and we experimented
with a hierarchical task network planner, JSHOP2 (a Java
derivative of SHOP (Nau er al. 1999)) as its control sys-
tem. In ongoing work, we are implementing a more reac-
tive approach because relying solely on a planner for control
meant that the planner had to be rerun after each student in-
put (we could not predict the student’s action), resulting in
an inefficient system. In this section, we will focus on using
XAI as teaching aid; the tutor also has the ability to question
students directly and evaluate their answers using key-word
spotting. Below are lines of a sample dialogue between tutor
and student (T=tutor and S=student); the student is given the
opportunity to identify his own errors and the student picks
line 10. The tutor determines that line 10 is best taught by
having the student investigate the problem using XAI.

1. T: Inthe simulation history window, can you select a
line where you think you could have done better?
S: Line 10
T: That’s right. There is room for improvement in
line 10.
In the XAI window, please ask the doctor some
questions and find out how this utterance

contributed to the failure of the negotiation.

2.
3

1771

While using XAI for the Virtual Humans, students are
presented with a question list like the one in figure 5. The
questions reflect information logged and our model of the
doctor’s behavior. The missing questions (q2, q4, etc.) are
“why” questions associated with ql through q9. We illus-
trate how “why” questions work in the continuation of the
tutoring session below. The user asks the doctor about his
negotiation stance, and now a new question, q2, is available
(ie., “why did the negotiation fail?”). The dialogue manager
keeps track of these even numbered questions and every time
we ask about line 10 this new question will be available.

4. S: Let’s start the investigation at line 10.

5. D: OK

6. S: What is your negotiation stance?

7. D: The negotiation is over and I refuse to move the
clinic

8. S: Why did the negotiation fail?

9. D: Ino longer trusted the Captain

When the tutor instructs the student to use XAI, it acti-
vates a component called the investigation model tracer to
track the student’s progress with XAI. Currently, we define
an ideal investigation as a series of questions whose answers
provide the lessons to be learned. For this example the ideal
investigation is very simple and consists of q2 (which was
just asked) and q6. q6, “Why did your solidarity with the
Captain decrease?”, is made available after the student asks
qs.

One reason the ideal investigation is so simple is that the
student did not get very far in his negotiation. If the stu-
dent had done better, he would have needed to know about
the preconditions and effects of actions that concerned the
What is your negotiation stance?

What is your trust of the Captain?

How was your solidarity with the Captain affected
here?

How was the Captain’s credibility affected here?

How was your familiarity with the Captain affected
here?

How important do you feel being neutral is?”

What action(s) could cause being neutral?

What are the effects of running the clinic here?

What action(s) could change being neutral?

What are the preconditions of running the clinic here?

What actions have being neutral as a precondition?

qll:
ql2:
ql3:
ql4:
ql5:
ql6:

“These virtual humans use utility theory to evaluate actions
and states of the world. This question allows the user to query
the relative importance of different states of the world from the
doctor’s perspective.

Figure 5: Sample question list for XAI for Virtual Humans

doctor such as running the clinic. Questions 11-16 allow the
student to learn about the doctor’s model of physical actions.
In the current version of the system, there are 16 actions and
17 states of the world that the user can ask about. Users
select actions and states from menus, and questions 11-16
change to match the selection.

The question list in figure 5 was generated when the cur-
rent task was “running the clinic in its current location”, and
the current state was “the doctor’s neutrality”. Running the
clinic in its current location, and running it in a new location
are the two most important tasks in the doctor’s world. Neu-
trality is important as we see in the dialogue continuation
below, because it is a prerequisite for running the clinic. If
the initial dialogue with the doctor had been more success-
ful, the student would need to consider hiring locals to move
the clinic instead of using U-S. troops.

10 S: What are the preconditions of running the clinic
here?

11 D: having supplies here, the patients being safe here,
the patients being here, the clinic being here
and being neutral

12 S: What action(s) could change being neutral?

13 D: negotiating with the Captain, cooperating with
the Captain and the Americans moving the clinic

14 S: What action(s) could cause the clinic being
[moved] there?

15 D: the Americans moving the clinic, locals moving

the clinic, and me moving the clinic

While the student was asking questions, the investigation
model tracer watched his progress. After several turns have
passed and the student has not asked q6, the tutor gives a
hint as shown below (actually the hint appears in a differ-
ent window). We use a standard model of hinting where the
hints get more specific until the tutor gives away the answer.
In this case, because we have not authored any hints, the tu-
tor gives away the answer. Our mechanism for “unlocking”
questions does not encode the relationships between the con-

1772

tent of questions. So the tutor does not know that in order
to know why solidarity decreased, the student must know
that solidarity did decrease. We are working on fixing this
problem in our next version of the system. Let’s assume that
the student realizes he must unlock q6 and asks q5 as shown
below. Once the student has asked q6, the tutor recognizes
that the investigation is complete and reengages the student
in dialogue (in the tutor-student dialogue window).!

16 T: Please ask question #6

17S: How was your solidarity with the Captain
affected here?

18 D: It decreased.

19 S: Why did your solidarity with the Captain
decrease?

20 D: The Captain is committing to performing an
undesired act.

21 T: Good job. You found out that admitting to the

planned attack decreased the doctor’s trust of you,
which caused the negotiation to fail...

Future Work

In this section, we focus on the issues of domain indepen-
dence and modularity, comparing our two systems, XAI for
OOS and XAI for Virtual Humans. We discuss plans to take
domain-dependent aspects of these systems and make them
available as general features that can be activated or deac-
tivated as appropriate. For example, XAI for OOS has the
feature of removing non-applicable questions from its ques-
tion list; if an entity did not fire its weapon at the current
time point then the question, “what are you shooting at?”’,
will be removed from the question list. However, there are
pedagogical ramifications to this “feature”. Consider a ques-
tion from the Virtual Humans domain, “How was your sol-
idarity with the Captain affected here?”. We may want to
display this question even if solidarity did not change be-
cause we want the student to realize that solidarity could
have changed. Thus, we will allow the feature to be deacti-
vated on a question-by-question bias.

Another feature present in XAI for OOS is the ability to
use HTML formatting to include links to entities and times.
Because entities and times are often the topic of conversa-
tion, we added the ability for users to click on mentions of
entities and times to change the entity or time as opposed to
selecting them from the entity or time menu. The XAI-for-
OOS GUI interprets tags in the natural language generatot’s
output in order to make this linkage. We will formalize this
convention in our XML message format.

Other features are associated with dialogue context; in the
sample dialogue, we saw that asking certain questions of the
virtual doctor made new questions available (i.e., question
unlocking). In XAI for OOS, entities “introduce” them-
selves when you first select them (e.g., “I am the fire team
leader of Blue Fireteam 1...””), and in some contexts, users
can ask the question, “can you give me more detail?”. Cur-
rently XAI for Virtual Humans and XAI for OOS use ad-

‘Although not shown in these excerpts, the goal is to encour-
age the student to be honest but vague (e.g., "I cannot discuss our
operational plans’).
hoc models of context limited to the information needed to
support features such as question unlocking. We will build
a general model of context storing each line of the student-
XAlI-tutor dialogue and who produced it. To enable question
unlocking, we will store the logical form of questions asked
and answers produced by the system. Questions will refer-
ence the dialogue context by listing applicability conditions
rather than relying on a domain-specific context checking
mechanism. This model of context will also be useful for
the natural language generator, allowing it to tailor its output
based on the context and produce more natural and readable
text.

Conclusion

Rather than simply writing an explanation system that only
worked for its target AI system and simulator, we used our
generic and modular architecture for explainable artificial
intelligence (XAI) systems in building XAI for the One
Semi-Automated Forces Objective System, a tactical mili-
tary simulation (Courtemanche & Wittman 2002). This ar-
chitecture continued to evolve as we worked on XAI for Vir-
tual Humans, a simulation designed to teach soft skills such
as leadership, teamwork, negotiation, and cultural aware-
ness. In this paper, we presented an abbreviated case study
on how to connect an explanation system to a target simula-
tion and in particular, model behaviors and add support for
new questions. We also showed how our prototype tutor uses
XAI as a teaching tool, getting the student to understand his
simulated negotiation partner’s reasoning and mental state.

The key point of the XAI architecture is its domain inde-
pendence and modularity. Every XAI system will have the
basic components in figure 2 but their exact implementation
will vary based on project requirements. For example, our
reliance on a relational database and SQL queries was based
on the requirement of handling a large dataset size and our
short development times. The creation of the query logical
form was a step toward a more declarative representation
and future work may include more powerful reasoning and
storage components.

We are currently continuing to work in the domain of ne-
gotiation with cultural awareness and are collaborating with
a project called ELECT (Enhanced Learning Environments
with Creative Technologies) here at the Institute for Cre-
ative Technologies. The goal of ELECT is to build a sys-
tem including explanation and tutoring to be used directly
in Army training and involves several external collaborators
contributing subject matter expertise. We plan to work with
the Army Research Institute to evaluate the effectiveness of
our tutor and the XAI system.

Acknowledgments

We thank William Swartout and David Traum for their guid-
ance and support. The project described herein has been
sponsored by the U.S. Army Research, Development, and
Engineering Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position or the
policy of the U.S. Government; no official endorsement
should be inferred.

1773

References

1990. FM 25-101: Battle Focused Training. Headquarters
Department of the Army. Washington D.C.

Core, M. G.; Lane, H. C.; van Lent, M.; Solomon, S.;
Gomboc, D.; and Carpenter, P. 2005. Toward question an-
swering for simulations. In Proc. of the IJCAI 2005 Work-
shop on Knowledge and Reasoning for Answering Ques-
tions (KRAQOS).

Courtemanche, A., and Wittman, R. 2002. OneSAF: A
product-line approach for a next-generation CGF. In Proc.
of the Eleventh SIW Conference on Computer-Generated
Forces and Behavioral Representations, 349-361.

Gomboc, D.; Solomon, S.; Core, M. G.; Lane, H. C.; and
van Lent, M. 2005. Design recommendations to support
automated explanation and tutoring. In Proc. of the Four-
teenth Conference on Behavior Representation in Model-
ing and Simulation.

Johnson, W. L. 1994. Agents that explain their own actions.
In Proc. of the Fourth Conference on Computer Generated
Forces and Behavioral Representation, Orlando, FL.

Laird, J. E.; Newell, A.; and Rosenbloom, P. 1987. Soar:
An architecture for general intelligence. Artificial Intelli-
gence 33:1-64.

Nau, D. S.; Cao, Y.; Lotem, A.; and noz Avila, H. M. 1999.
Shop: Simple hierarchical ordered planner. In Proceedings
of the Sixteenth International Joint Conference on Artificial
Intelligence (IJCAI99), 968-973.

Swartout, W. R., and Moore, J. D. 1993. Explanation in
second generation expert systems. In David, J.; Krivine,
J. P; and Simmons, R., eds., Second Generation Expert
Systems. Springer-Verlag.

Traum, D.; Swartout, W.; Marsella, S.; and Gratch, J. 2005.
Fight, flight or negotiate: Believable strategies for convers-
ing under crisis. In Proc. of the 5th International Working
Conference on Intelligent Virtual Agents.

van Lent, M.; Fisher, W.; and Mancuso, M. 2004. An ex-
plainable artificial intelligence system for small-unit tacti-
cal behavior. In Proc. of the Sixteenth Conference on Inno-
vative Applications of Artificial Intelligence. Menlo Park,
CA: AAAT Press.
Explainable Artificial Intelligence and Machine Learning: A
reality rooted perspective

Frank Emmert-Streib’?, Olli Yi-Harja*, and Matthias Dehmer®

‘Predictive Society and Data Analytics Lab, Faculty of Information Technology and Communication
Sciences, Tampere University, Tampere, Finland *
“Institute of Biosciences and Medical Technology, Tampere University of Technology, Tampere, Finland
“Institute for Intelligent Production, Faculty for Management, University of Applied Sciences Upper
Austria, Steyr Campus, 4040 Steyr, Austria

January 26, 2020

Abstract

We are used to the availability of big data generated in nearly all fields of science as
a consequence of technological progress. However, the analysis of such data possess vast
challenges. One of these relates to the explainability of artificial intelligence (AI) or machine
learning methods. Currently, many of such methods are non-transparent with respect to
their working mechanism and for this reason are called black box models, most notably deep
learning methods. However, it has been realized that this constitutes severe problems for a
number of fields including the health sciences and criminal justice and arguments have been
brought forward in favor of an explainable AI. In this paper, we do not assume the usual
perspective presenting explainable AI as it should be, but rather we provide a discussion
what explainable AI can be. The difference is that we do not present wishful thinking but

reality grounded properties in relation to a scientific theory beyond physics.

1 Introduction

Artificial intelligence (AI) and machine learning (ML) have achieved great successes in a number of
different learning tasks including image recognition and speech processing [1-3]. However, many of
the best performing methods are too complex (abstract) prohibiting a straight forward explanation
of the obtained results in simple words. The reason therefore is that such methods process high-
dimensional input data in a non-linear and nested fashion to reach probabilistic decisions. This
convolutes a clear view, e.g., on what information in the input vector is actually needed to arrive

at certain decisions. As a result, such models are non-transparent or opaque and are typically

 

*frank.emmert-streib@tuni.fi
regarded as black box models [4]. Importantly, not only deep neural networks (DNNs) are suffering
from this shortcoming but also support vector machines (SVMs), random forests (RFs) or ensemble
models (e.g. Adaboost) [5-8].

This black box character establishes problems for a number of fields. For instance, when making
decisions in a hospital about the treatment of patients or at the court about the sentencing of
a defendant, such decisions should be explainable [9, 10]. In an endeavor to address this issue
the field explainable AI (XAJI) has recently re-emerged [11]. While previous work in this area
focused on specific problems of deep learning models, defining explainable AI or the taxonomy of
XAJI [12-14], our approach presents a different perspective as follows. First, instead of describing AI
systems with desirable properties making them explainable, we present a reality rooted perspective
showing what XAI can deliver. Put simply, instead of presenting explainable AI as it should be
we show what explainable AI can be. Second, we derive thereof limitations of explainable AI.
Such limitations may be undesirable but they are natural and unavoidable. Third, we discuss
consequences of this for our way forward.

Our paper is organized as follows. In the next section, we briefly describe the current state
of explainable AI. Then we present different perspectives on learning methods and discuss the
definition of a scientific theory. This allows us to conclude some limitations of an even perfect
versions of explainable AI. Finally, we discuss reasons for the confusion about explainable AI and

present ways forward. The paper finishes with concluding remarks.

2 Current state of explainable Al

For our following discussion, it is important to know how explainable AI is currently defined. Put
simply, one would like to have explanations of internal decisions within an AI system that lead
to an external result (output). Such explanations should provide insight into the rationale the AI
uses to draw conclusions [15].

A more specific definition of explainable AI was proposed in [16].

Definition 1 (explainable AI). 1) produce more explainable models while maintaining a high level
of learning performance (e.g., prediction accuracy), and 2) enable humans to understand, appro-

priately trust, and effectively manage the emerging generation of artificially intelligent partners.
Another attempt of a definition of explainable AI is given in [13].

Definition 2 (explainable AI). Explainable Artificial Intelligence is a system that produces details

or reasons to make its functioning clear or easy to understand.

Furthermore, it is argued that the goals of explainable AI are trustworthiness, causality,
transferability, informativeness, confidence, fairness, accessibility, interactivity and privacy aware-
ness [13, 17].

In general, there is agreement that an explainable AI system should not be opaque (or a
black box) that hides its functional mechanism. Also, if a system is not opaque and one can even
understand how inputs are mathematically mapped to outputs then the system is interpretable [15].
Taken together, this implies model transparency. The terms interpretability and explainability
(and sometimes comprehensibility) are frequently used synonymously although the ML community
seems to prefer the former while the AI community prefers the latter [14].

From a problem-oriented view, in [18] different types of interpretability, for instance perceptive
interpretability, interpretability via mathematical structure, data-driven interpretability or inter-
pretability by utilities, and explainability are discussed. They found that many journal papers in
the ML and AI community are algorithm-centric, whereas in medicine risk and responsibility re-
lated issues are more prevalent. Similar discussions for different interpretations of interpretability
can be found in [19].

Overall, at to this point one can conclude the following. First, there is no single definition of
explainable AI available that would be generally accepted but many descriptions are overlapping
with each other in the above discussed ways. Second, the characterizations of explainable AI
state what XAI should be. That means they form desirable properties of such an AI system
without deriving these from higher principles. Hence, these characterizations can be seen as
wishful thinking. Before we can formulate reality rooted attainable goals of explainable AI, we

need to discuss different perspectives on models and the general capabilities of a scientific theory.

3 Perspective of statistics

In statistics, one can distinguish between two main types of models. The first type, called infer-
ential or explanatory model, provides a causal explanation of the data generation process whereas
the second type, called predictive model, just produces forecasts [20,21]. Certainly, an inferential
model is more informative (i.e. theory-like, see below) than a predictive model because also an
explanatory model can be used to make predictions but the predictive model does not provide
(causal) explanations for such predictions. An examples for an explanatory model is a causal
Bayesian network whereas a random forest is a prediction model. Due to the complementary
capabilities of predictive and inferential models they are coexisting next to each other and each is
useful in its own right.

The general problem for creating causal models from data is that their inference from observa-
tional data is very challenging requiring usually also experimental data (generated by perturbations

of the system). Currently, most data in the health and social sciences are observational data ob-
tained from merely observing the behavior of the system because performing experiments is either

not straight forward or ethically prohibited.

4 Perspective of artificial intelligence

In [22] it was argued that explainable AI is not a new field but has been already recognized and
discussed for expert systems in the 1980s. This is understandable considering that from about
the 1950s to the 1980s the dominant paradigm of AI was symbolic artificial intelligence (SAI) and
SAI used high-level and human-readable representations and rules for manipulating these, e.g.,
by using expert systems. Hence, not only the need for explainable systems has been realized but
some AI systems could also accomplish near optimal explainable goals due to the nature of SAI.

With the renewed interest in neural networks in recent years in the form of deep neural networks
(DNN), the question of explaining and interpreting models has been re-surfaced. One reason for
this is that deep neural networks, in contrast to methods used for SAI, are not symbol-based but
connectionist, i.e., they are learning (possibly high-dimensional) features from data and store them
in the weights of the network [23,24]. Although, more powerful in practice the price payed for
this comes in form of a higher complexity of the representation used, which is no longer human-
readable. Recently, it has been argued that AI systems should not only solve pattern recognition
problems but provide causal models of the world that support explanation and understanding [25].
This demand connects directly to the statistics perspective because causal models are exploratory,
see above.

After clarifying how explainable models are understood by different communities we take a
step back to see what would be the ultimate goal achievable of an explainable AI. For this reason,

we discuss the meaning of a theory.

5 What is a scientific theory?

In science, the formal definition of a theory is difficult but commonly it refers to a comprehensive
explanation of a subfield of nature that is supported by a body of evidence [26-28]. In physics,
the term theory is generally associated with a mathematical framework derived from a set of
basic axioms or postulates which allows to generate experimentally testable predictions for such a
subfield of physics. Typically these systems are highly idealized, in that the theories describe only
certain aspects. Examples include classical field theory and quantum field theory. An important
aspect of a theory is that it is falsifiable [29]. That means experiments can be conducted for testing
the predictions made by a theory. As long as such experiments do not contradict the predictions

a theory is accepted, otherwise rejected.
With respect to the stringency with which theories have been quantitatively tested, theories in
physics, e.g., general relativity or quantum electrodynamics, are certainly what can be considered
the best scientific theories. This implies that such theories provide answers to all questions that can
be asked within the scope of the theory. Furthermore, the theory provides also an explanation of
the obtained results. However, these explanations do not come in the form of a natural language,
e.g., English, but are mathematical formulas. Hence, the mathematical formulas need to be
interpreted by means of a natural language as good as possible. This may seem as a minor issue
but the severity of this may be exemplified by the interpretation of quantum mechanics because
so far there is no generally accepted interpretation, although the Copenhagen interpretation is
the most popular one [30]. Interestingly, the reason for this is ascribed to personal philosophical
prejudice [30]. The latter point hints to the incompleteness of any natural language in interpreting
a mathematical formalism of quantum mechanics. For completeness, we would like to mention
that even in physics not everything is covered by the existing theories because so far there is no

theory unifying gravity and quantum mechanics [31].

6 Expected limitations of an explainable AI

From this discussion, we can conclude some limitations even a perfect version of an explainable
AI will have. Considering that essentially all applications of AI and ML are beyond physics, e.g.,
in biology, medicine and health care, industrial production or human behavior, one cannot expect
to have a simpler theory for any of these fields than what we have for physics. Hence, even the
interpretability of such a theory is expected to be more problematic than an interpretation of, e.g.,
quantum mechanics.

In order to make this point more clear let’s consider a specific example. Suppose a theory of
cancer would be known, in the sense of a theory in physics discussed above. Then this cancer theory
would be highly mathematical in nature which would not permit a simple one-to-one interpretation
in any natural language. Hence, only highly trained theoretical cancer mathematicians (in analogy
to theoretical physicists) would be able to derive and interpret meaningful statements from this
theory. Regardless of the potential success of such a cancer theory, this implies that medical doctors
- not trained as theoretical cancer mathematicians - could not understand nor interpret such a
theory properly and, hence, from their perspective the cancer theory would appear opaque or non-
explainable. Without our discussion, such a result would appear undesirable and unacceptable,
however, given the context we provided above this appears unavoidable and natural. A similar
discussion can be provided for any other field than cancer showing that even a perfect version
of an explainable AI theory would not be interpretable or explainable for managers or general

practitioners for natural reasons.
The next optimistic but less ambitious assumption would be to suppose AI could provide
a description for fields outside of physics. Interestingly, physicists realized already decades ago
that such an expansion of a theory beyond the boundaries of physics is very challenging. For
instance, severe problems encountered are due to the arising of emergent properties and non-
equilibrium systems [32,33]. For this reason, phenomena outside of physics are usually addressed by
"approaches" collectively named as complex adaptive systems (CAS) [34,35]. We used intentionally
the word "approaches" and not "theories" because the used models and the obtained descriptions
are qualitatively very different thereof. Whereas it is unquestionable that valuable insights have
been gained into complex phenomena from economy, society and sociology [36,37] a theory for such
fields is still absent and currently not in sight. Hence, it seems fair to conclude that even the most
powerful AI system of CAS would be far from being a theory and for this reason lack explanatory
capabilities. However, this translates directly into a principle incompleteness of questions such an
AI system could answer and reduces further what could be delivered by an explainable AI.

Finally, we come to the most realistic view on an AI system which views its purpose as a
system to analyze data. Depending on the AI system and the available data it is clear that the
understanding that can be obtained from such a system is even further limited in the answers that
can be provided as well as in the level of explanations it can give. This is also true if the Al system
would be based on a perfect method because the data represent only an incomplete sample of all
possible data and is as such inherently limited in the explanations it can provide translating in an
unavoidable uncertainty of statements about the population it studies.

In Fig. 1, we summarize the above discussion graphically. The shown coordinate systems
indicate the qualitative relation between the influence of the distance from a theory and the
comprehensiveness of the description (left) and the influence of the sample size on the uncertainty
of statements or explanations about the population (right). The yellow arrow on the left indicates
the distance from physical theories which corresponds also to the x-axis of the left coordinate
system. A similar meaning has the purple arrow on the right for the diameter of the random

sample and x-axis of the right coordinate system

7 Reasons for the confusion

It is interesting to ask how utopian, idealistic requirement for an explainable AI, as discussed in
Section 2 above, could be demanded when reality looks quite differently. We think the reason for
this is twofold. First, statistical models and machine learning methods have been introduced due to
the lack of general theories outside of physics because these allow a quantitative analysis studying
experimental evidence. Hence, even in this unsatisfactory situation, systematic and quantitative

approaches are available for extending our knowledge of complex phenomena. Second, in recent
QM: quantum mechanics
QFT: quantum field theory

CAS: complex adaptive systems

      
 

population
uncertainty

sample size

comprehensiveness of the
description

    

random sample of data

distance from theory methout => uncertain statements
about population

Figure 1: An overview describing the limitations of explainable AI with respect to attainable
goals. Left: Different scientific fields are arranged according to their increasing complexity [32]
starting from the best (most comprehensive) theories of physics in the center. The further the
distance from these theories the less comprehensive are the models describing subjects of CAS
(left coordinate system). Right: Any AI system analyzes a random sample of data drawn from a
population. One source of uncertainty is provided by the sample size of the data (right coordinate
system) that translates directly into uncertain statements about the population.

years we have been experiencing a data surge which gives the impression that every research
question should start with "data". This led even to the establishment of a new field called data
science [38]. Taken together, this may have given the impression that AI and ML methods are
more powerful than physical theories because they can (approximately) reach areas - due to the
availability of methods and data - that are blocked for physics. However, these methods are
not intended as theories but merely as practical instruments to deal with complex phenomena.
Importantly, the questions that can be answered need to have answers that can be found within
the data. Every lack of such qualitative data, e.g., due to a limited sample size, translates directly

into a lack of answers and is for this reason an inherent uncertainty of any AI system.

8 Discussion

Having realized the limitations of explainable AI with respect to attainable goals, what does this
mean for the way to go forward? In the following, we present a discussion of practical remedies
that are based on the insights gained in the first part of our paper. We want to emphasize that
these remedies do not bring us back to the delusional view of an idealized explainable AI but
provide means to realistically formulate achievable goals.

An Al system may not need to be explainable in a natural language as long as its generalization

error does not exceed an acceptable level. An example for such a practice are nuclear power plants.
Since nuclear power plants are based on physical laws (see the discussion of quantum mechanics
above) the functioning mechanisms of such plants are not explainable to members of the regulatory
management or politicians responsible for governing energy politics. Nevertheless, nuclear power
plants have been operated since many decades contributing to our energy supply. In a similar way,
one could envision an operational policy for medicine, e.g., utilizing deep learning methods.

Deep learning may not be necessary to analyze a given data set. Nowadays, many people
are using deep learning methods because they seem to think such methods are needed without
exploring alternative approaches. However, in this way problems regarding the interpretability
and explainability of the results are encountered. While it may be possible that future deep
learning methods may be less opaque, if currently available methods solve the same problems in
a satisfactory way and do not suffer from such limitations, e.g., decision trees, they should be
preferred and used.

The similarity (or difference) of the predictiveness of AI systems needs to be quantified in an
explainable way. This point is related to the previous one because if one can quantitatively assesses
the similarity (or the difference) between two AI systems one can compare an explainable with a
non-explainable AI system to evaluate one benefit over the other. Hence, even if an explainable
AI system does not fully solve a given problem, e.g., compared to a deep learning approach, it
may be sufficient to use. Importantly, even when an AI system itself may not be explainable the
comparison of different systems can be understandable. This way the lack of an explainability
maybe compensatable. A challenge of such an approach is that a quantification should not only
be based on prediction errors [39] but an assessment of risk and utility [40]. This latter issue is
clear in a clinical context.

Partial insights into the interpretability and explainability of AI systems should be developed. It
may not be feasible to convert deep learning models into fully transparent systems, however, this
may be achievable in part. For instance, certain aspects of an analysis could be understandable
which are integrated to achieve the complete model. Given the fact that in general data science
problems present themselves as a process [41] there should be ample opportunity to identify

subproblems deemable as an explainable AI.

9 Conclusions

In this paper, we shed some light on the current state of explainable AI and derived limitations
with respect to attainable goals from clarifying different perspectives and the capabilities of well

tested physical theories. The main results can be summarized as follows:

1. An Al system does not constitute a theory but an instrument (a model) to analyze data. >

The comprehensiveness and the explainability of an even perfect AI system are inherently
limited by the random sample of data used.

. The more comprehensive (i.e. theory-like) an AI system becomes in predicting CAS the more

complex becomes its underlying mathematics. = There is no simple one-to-one translation

into a natural language to explain the results or the working mechanism.

. The most powerful but opaque AI systems (e.g. deep learning) should not be preferred and

applied by default but a comparison to alternative explainable methods should be conducted
and differences should be quantified. = An explainable and quantifiable reason can be
derived by integrating prediction error, risk and utility for weighing the pros and cons for

each model.

We hope our results can contribute to formulating realistic goals for an explainable AI that

are also attainable [42].

Conflict of Interest Statement

The authors declare that the research was conducted in the absence of any commercial or financial

relationships that could be construed as a potential conflict of interest.

Author Contributions

All authors contributed to all aspects of the preparation and the writing of the manuscript.

References

[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep

[2

3

[4

convolutional neural networks. In Advances in neural information processing systems, pages

1097-1105, 2012.

Sepp Hochreiter and Jiirgen Schmidhuber. Long short-term memory. Neural computation,

9(8):1735-1780, 1997.

Stuart J Russell and Peter Norvig. Artificial intelligence: a modern approach. Malaysia;
Pearson Education Limited,, 2016.

Brent Mittelstadt, Chris Russell, and Sandra Wachter. Explaining explanations in ai. In
Proceedings of the conference on fairness, accountability, and transparency, pages 279-288.

ACM, 2019.
[5] Vladimir Naumovich Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
[6] L. Breiman. Random Forests. Machine Learning, 45:5-32, 2001.

[7| R.E. Schapire and Y. Freund. Boosting: Foundations and Algorithms. Adaptive Computation
and Machine Learning series. MIT Press, 2012.

(8

Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. Chapman and Hall/CRC,
2012.

(9

Andreas Holzinger, Chris Biemann, Constantinos $ Pattichis, and Douglas B Kell. What
do we need to build explainable ai systems for the medical domain? arXiv preprint

arXiv: 1712.09923, 2017.

[10] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions
and use interpretable models instead. Nature Machine Intelligence, 1(5):206-215, 2019.

[11] Feiyu Xu, Hans Uszkoreit, Yangzhou Du, Wei Fan, Dongyan Zhao, and Jun Zhu. Explainable
ai: a brief survey on history, research areas, approaches and challenges. In CCF International
Conference on Natural Language Processing and Chinese Computing, pages 563-574. Springer,
2019.

[12] Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal.
Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE

bth International Conference on data science and advanced analytics (DSAA), pages 80-89.
IEEE, 2018.

[13] Alejandro Barredo Arrieta, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham
Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lépez, Daniel Molina, Richard Ben-
jamins, et al. Explainable artificial intelligence {xai): Concepts, taxonomies, opportunities

and challenges toward responsible ai. arXiv preprint arXiv:1910.10045, 2019.

[14] Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A survey on Explain-
able Artificial Intelligence (XAI). IEEE Access, 6:52138-52160, 2018.

[15] Derek Doran, Sarah Schulz, and Tarek R Besold. What does explainable AI really mean? A
new conceptualization of perspectives. arXiv preprint arXiv:1710.00794, 2017.

[16] David Gunning. Explainable artificial intelligence (XAI). Defense Advanced Research Projects
Agency (DARPA), nd Web, 2, 2017.

[17] Zachary C Lipton. The mythos of model interpretability. Queue, 16(3):30-57, 2018.

10
[18] Erico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Towards

medical xai. arXiv preprint arXiv:1907.073874, 2019.

[19] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and
Dino Pedreschi. A survey of methods for explaining black box models. ACM computing
surveys (CSUR), 51(5):93, 2019.

[20] Galit Shmueli et al. To explain or to predict? Statistical science, 25(3):289-310, 2010.

[21] Leo Breiman et al. Statistical modeling: The two cultures. Statistical science, 16(3):199-231,
2001.

[22] Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, Simone
Stumpf, Peter Kieseberg, and Andreas Holzinger. Explainable ai: the new 42? In Inter-

national Cross-Domain Conference for Machine Learning and Knowledge Extraction, pages

295-303. Springer, 2018.

[23] Jerome A Feldman and Dana H Ballard. Connectionist models and their properties. Cognitive

science, 6(3):205-254, 1982.

[24] Stephan I Gallant. Connectionist expert systems. Communications of the ACM, 31(2):152-
170, 1988.

[25] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building

machines that learn and think like people. Behavioral and brain sciences, 40, 2017.
[26] Root Gorelick. What is theory? Ideas in Ecology and Evolution, 4, 2011.
[27| Hans Halvorson. Scientific theories. In The Oxford Handbook of Philosophy of Science.

[28] Patrick Suppes. What is a scientific theory? US Information Agency, Voice of America
Forum, 1964.

[29] K.R. Popper. The logic of scientific discovery. Basic Books, New York, 1959.

[30] Maximilian Schlosshauer, Johannes Kofler, and Anton Zeilinger. A snapshot of foundational
attitudes toward quantum mechanics. Studies in History and Philosophy of Science Part B:
Studies in History and Philosophy of Modern Physics, 44{3):222-230, 2013.

[31] Carlo Rovelli. Quantum gravity. Cambridge university press, 2004.
[32] Philip W Anderson. More is different. Science, 177(4047):393-396, 1972.

[33] F. Emmert-Streib. Aktive Computation in offenen Systemen. Lerndynamiken in biologischen

Systemen: Vom Netzwerk zum Organismus. PhD thesis, University of Bremen, 2003.

11
[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

H.G. Schuster. Complex Adaptive Systems. Scator Verlag, 2002.

Simon Levin, Tasos Xepapadeas, Anne-Sophie Crépin, Jon Norberg, Aart De Zeeuw, Carl
Folke, Terry Hughes, Kenneth Arrow, Scott Barrett, Gretchen Daily, et al. Social-ecological
systems as complex adaptive systems: modeling and policy implications. Environment and

Development Economics, 18(2):111-132, 2013.

Per Bak. How nature works: the science of self-organized criticality. Springer Science &

Business Media, 2013.

R. N. Mantegna and H. E. Stanley. Scaling behaviour in the dynamics of an economic index.

Nature, 1995.

F Emmert-Streib and M Dehmer. Defining data science by a data-driven quantification of

the community. Machine Learning and Knowledge Extraction, 1(1):235-251, 2019.

F Emmert-Streib, 5 Moutari, and M Dehmer. A comprehensive survey of error measures
for evaluating binary decision making in data science. Wiley Interdisciplinary Reviews: Data

Mining and Knowledge Discovery, page e1303, 2019.
David Kreps. Notes on the Theory of Choice. Routledge, 2018.

F. Emmert-Streib, 5. Moutari, and M. Dehmer. The process of analyzing data is the emergent

feature of data science. Frontiers in Genetics, 7:12, 2016.

Marvin Minsky. Steps toward artificial intelligence. Proceedings of the IRE, 49(1):8-30, 1961.

12
Sociology QM: quantum mechanics
QFT: quantum field theory

CAS: complex adaptive systems

   
      
 

population

Biology uncertainty

Physics

sample size
comprehensiveness of the
description
random sample of data
distance from theory method[ 1 => uncertain statements

about population
City Research Online

 

UNIVERSITY OF LONDON
EST 1894

 

 

City, University of London Institutional Repository

 

Citation: Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S. & Yang, G-Z. (2019). XAI-
Explainable artificial intelligence. Science Robotics, 4(37), eaay7120. doi:
10.1126/scirobotics.aay7120

This is the accepted version of the paper.

This version of the publication may differ from the final published version.

 

Permanent repository link: httos://openaccess.city.ac. uk/id/eprint/23405/

Link to published version: https://doi.org/10.1126/scirobotics.aay7120

Copyright: City Research Online aims to make research outputs of City,
University of London available to a wider audience. Copyright and Moral Rights
remain with the author(s) and/or copyright holders. URLs from City Research
Online may be freely distributed and linked to.

Reuse: Copies of full iterns can be used for personal research or study,
educational, or not-for-profit purposes without prior permission or charge.
Provided that the authors, title and full bibliographic details are credited, a
hyperlink and/or URL is given for the original metadata page and the content is
not changed in any way.

 

 
 

City Research Online: http://openaccess.city.ac.uk/ publications @city.ac.uk

 
Focus

XAI - Explainable Artificial Intelligence

David Gunning’, Mark Stefik, Jaesik Choi, Timothy Miller, Simone Stumpf, Guang-Zhong Yang

Explanations are essential for users to effectively understand, trust, and manage powerful artificial intelligence
applications.

1. Introduction

Recent successes in machine learning (ML) have led to a new wave of artificial intelligence (Al)
applications that offer extensive benefits to a diverse range of fields. However, many of these systems
are not able to explain their autonomous decisions and actions to human users. Explanations may not
be essential for certain Al applications, and some Al re- searchers argue that the emphasis on expla-
nation is misplaced, too difficult to achieve, and perhaps unnecessary. However, for many critical
applications in defense, medicine, finance, and law, explanations are essential for users to understand,
trust, and effectively manage these new, artificially intelligent partners [see recent reviews (1-3)].

Recent Al successes are largely attributed to new ML techniques that construct models in their internal
representations. These in- clude support vector machines (SVMs), ran- dom forests, probabilistic
graphical models, reinforcement learning (RL), and deep learning (DL) neural networks. Although these
models exhibit high performance, they are opaque in terms of explainability. There may be in- herent
conflict between ML performance (e.g., predictive accuracy) and explainability. Often, the highest
performing methods (e.g., DL) are the least explainable, and the most explainable (e.g., decision trees)
are the least accurate. Figure 1 illustrates this with a notional graph of the performance- explainability
tradeoff for some of the ML techniques.

 

1 David Gunning — Defense Advanced Research Agency (DARPA), 675 N. Randolph St., Arlington VA 22201, now at Facebook Al Research, 770 Broadway,

New York, NY 10003, E-mail: dgunning@fb.com,. Mark Stefik - Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. E-mail:
stefik@parc.com. Jaesik Choi - Department of Computer Science and Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of
Korea, 44919. E-mail: jaesik@unist.ac.kr. Timothy Miller - School of Computing and Information Systems, The University of Melbourne, Victoria 3010
Australia, E-mail: tmiller@unimelb.edu.au. Simone Stumpf - Centre for HCl Design, School of Mathematics, Computer Science and Engineering. City,
University of London, London EC1V OHB, UK. E-mail: Simone.Stumpf.1@city.ac.uk . Guang-Zhong Yang — The Hamlyn Centre, Imperial College London,

London SW7 2AZ, also The Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China, E-mail: g.z.yang@imperial.ac.uk
 

 

Interpretable Models
Techniques to learn more structured,

> Future Techniques
° interpretable, causal models

 

 

*. Explainability

 

Deep Learning
Improved deep learning techniques to
learn explainable features

 

 

 

 

Learhing Techniques

 

 

 

 

 

oe
Neural Nets “Graphical
“Models Ensemble
Deep — Methods
Learnigg BaliSfNets Random Model Agnostic
SRL _ _ .
CRPSS=FENs, rarest Techniques to infer an explainable model
MINs a from any model as a black box
Statistical “OSS = oeesion
jarkov
Models sys Models

 

 

Figure 1. Learning techniques and explainability. Concept adapted from (9). (B) Interpretable models: ML AQ4
techniques that learn more structured, interpretable, or causal models. Early examples included Bayesian rule
lists, Bayesian program learning, learning models of causal relationships, and using stochastic grammars to
learn more interpretable structure. Deep learning: Several design choices might produce more explainable
representations (e.g., training data selection, architectural layers, loss functions, regularization, optimization
techniques, and training sequences). Model agnostic: Techniques that experiment with any given ML model, as a
black box, to infer an approximate explainable model.

2. What is XAI

The purpose of an explainable Al (XAl) sys- tem is to make its behavior more intelligible to humans by
providing explanations. There are some general principles to help create effective, more human-
understandable Al systems: The XAl system should be able to explain its capabilities and understandings;
explain what it has done, what it is doing now, and what will happen next; and dis- close the salient
information that it is acting on (4).

However, every explanation is set within a context that depends on the task, abilities, and expectations
of the user of the Al system. The definitions of interpretability and ex- plainability are, thus, domain
dependent and may not be defined independently from a domain. Explanations can be full or partial.
Models that are fully interpretable give full

and completely transparent explanations. Models that are partially interpretable reveal important
pieces of their reasoning process. Interpretable models obey “interpretability constraints” that are
defined according to the domain (e.g., monotonicity with respect to certain variables and correlated
variables obey particular relationships), whereas black box or unconstrained models do not neces- sarily
obey these constraints. Partial expla- nations may include variable importance measures, local models
that approximate global models at specific points and saliency maps.

3. XAI— Expectation from users

XAI assumes that an explanation is provided to an “end user” who depends on the decisions,
recommendations, or actions produced by an Al system yet there could be many different kinds of
users, often at different time points in the development and use of the system (5). For example, a type
of user might be an intelligence analyst, judge or an operator. However, other users who demand an
explanation of the system might be a developer or test operator who needs to understand where there
might be areas of improvements. Yet another user might be policy-makers, who are trying to assess the
fairness of the system. Each user group may have a preferred ex- planation type that is able to
communicate information in the most effective way. An effective explanation will take the target user
group of the system into account, who might vary in their background knowledge and needs for what
should be explained.

4. __ Explainability - Evaluation and Measurement

A number of ways of evaluating and measuring the effectiveness of an explanation have been proposed,
however, there is currently no common means of measuring if an XAl system is more intelligible to a
user than a non-XAl system. Some of these measures are subjective measures from the user’s point of
view, such as user satisfaction which can be measured through a subjective rating of the clarity and
utility of an explanation. More objective measures for an explanation’s effectiveness might be task
performance, i.e., does the explanation improve the user’s decision-making. Reliable and consistent
measurement of the effects of explanations is still an open research question. Evaluation and measure-
ment for XAI systems include evaluation frameworks, common ground [different think- ing and mutual
understanding (6)], common sense, and argumentation [why (7)].

5. XAlI-— Issues and Challenges

There remain many active issues and challenges at the intersection of machine learning and explanation.
These include but are not limited to:

1) Starting from computers versus starting from people (8). Should XAI systems tailor explanations to
particular users? Should they consider the knowledge that users lack? How can we exploit explanations
to aid interactive and human-in-the-loop learning, including enabling users to interact with explanations
to provide feedback and steer learning?

2) Accuracy versus interpretability. A major thread of XAl research on explanation explores techniques
and limitations of interpretability. Interpretability needs to consider tradeoffs involving accuracy and
fidelity and to strike a “sweet spot” between accuracy, interpretability, and tractability.

3) Using abstractions to simplify explanations. High-level patterns are the basis for describing big plans
in big steps. Automating the discovery of abstractions has long been a challenge, and understanding the
discovery and sharing of abstractions in learning and explanation are at the frontier of XAI research
today.

4) Explaining competencies versus explaining decisions. A sign of mastery by highly qualified experts is
that they can reflect on new situations. It is necessary to help end users to understand the competencies
of the Al systems in terms of what competencies a particular Al system has, how the competencies
should be measured, and whether an Al system has blind spots; that is, are there classes of solutions it
can never find?
From a human-centered research perspective, research on competencies and knowledge could take XAI
beyond the role of explaining a particular XAI system and helping its users to determine appropriate
trust. In the future, XAls may eventually have substantial social roles. These roles could include not only
learning and ex- plaining to individuals but also coordinating with other agents to connect knowledge,
developing cross-disciplinary insights and common ground, partnering in teaching people and other
agents, and drawing on previously discovered knowledge to accelerate the further discovery and
application of knowledge. From such a social perspective of knowledge understanding and generation,
the future of XAI is just beginning.

8. References

1. W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, K. R. Muller, Explainable Al: interpreting, Explaining
and Visualizing Deep Learning (Springer Nature, 2019).

2.H.J. Escalante, S. Escalera, |. Guyon, X. Bard,
Y. Gucluturk, U. Guclu; M. van Gerven, Explainable and interpretable Models in Computer Vision and
Machine Learning (Springer, 2018).

3. O. Biran, C. Cotton, Explanation and justification in machine learning: A survey, paper presented at
the IJCAI-17 Workshop on Explainable Al (XAI), Melbourne, Australia, 20 August 2017.

4. V. Bellotti, K. Edwards, Intelligibility and accountability: Human considerations in context-aware
systems. Hum. Comput. Interact. 16, 193-212 (2009).

5. T. Kulesza, M. Burnett, W. Wong, S. Stumpf, Principles of explanatory debugging to personalize
interactive machine learning, in Proceedings of the 20th International Conference on intelligent User
Interfaces (ACM, 2015), pp. 126-137.

6. H. H. Clark, S. E. Brennan, Grounding in communication, in Perspectives on Socially Shared Cognition,
L. B. Resnick, J. M. Levine, S. D. Teasley, Eds. (American Psychological Association, 1991), pp. 127-
149.

7.D. Wang, Q. Yang, A. Abdul, B. Y. Lim, Designing theory-driven user-centric explainable Al, in
Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (ACM, 2019), paper
no. 601.

8. T. Miller, Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267, 1-38
(2018).

9. D. Gunning, Explainable artificial intelligence (XAIl), DARPA/I20;
www.cc.gatech.edu/~alanwags/DLAI2016/ (Gunning)%20ICAI-16%20DLAI%20WS. pdf.
IEEE Access:

Multidiscipinary Rapid Review £ Open Access Journal

 

Received October 3, 2020, accepted October 7, 2020, date of publication October 15, 2020, date of current version October 26, 2020.

Digital Object Identifier 10.1109/ACCESS.2020.3031477

Gaining Insight Into Solar Photovoltaic Power
Generation Forecasting Utilizing Explainable
Artificial Intelligence Tools

MURAT KUZLU™!, {Senior Member, IEEE), UMIT CALI©2, (Member, IEEE),
VINAYAK SHARMA”?, AND 6ZGUR GULER*

'Department of Engineering Technology, Old Dominion University, Norfolk, VA 23529, USA

2Department of Electric Power Engineering, Norwegian University of Science and Technology, 7491 Trondheim, Norway
Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC 28223, USA
4eKare Inc., Fairfax, VA 22031, USA

Corresponding author: Murat Kuzlu (mkuzlu @odu.edu}

This work was supported in part by the Commonwealth Cyber Initiative, an investment in the advancement of cyber research and
development, innovation and workforce development in Virginia.

ABSTRACT Over the last two decades, Artificial Intelligence (AD approaches have been applied to various
applications of the smart grid, such as demand response, predictive maintenance, and load forecasting.
However, AI is still considered to be a ‘“‘black-box’’ due to its lack of explainability and transparency,
especially for something like solar photovoltaic (PV) forecasts that involves many parameters. Explainable
Artificial Intelligence (XAI) has become an emerging research field in the smart grid domain since it
addresses this gap and helps understand why the AI system made a forecast decision. This article presents
several use cases of solar PV energy forecasting using XAI tools, such as LIME, SHAP, and ELI5, which
can contribute to adopting XAI tools for smart grid applications. Understanding the inner workings of
a prediction model based on AI can give insights into the application field. Such insight can provide
improvements to the solar PV forecasting models and point out relevant parameters.

INDEX TERMS Explainable artificial intelligence (XAI), solar PV power generation forecasting, explain-
ability and transparency.

NOMENCLATURE Pace The accumulated value

y The predicted value of y Pave The average value

L Fidelity function PV Photovoltaic

Q Complexity measures RFR Random Forest regressor

dj Feature attribution for a feature j RMSE Root-mean Square Error

Tx Proximity measure Ss A set of non-zero indexes in 2’

é LIME explanation model SHAP SHapley Additive exPlanations

(4 Explanation for the model SP Surface pressure

ELIS Explain Like ’'m 5 SSRD Surface solar rad down

f The model being explained STRD Surface thermal rad down

G A set of interpretable model g € G TCC Total cloud cover

HOUR | The hour of the day TCIW Total column ice water

HUM _ Relative humidity TCLW Total column liquid water

k Feature TEMP 2 metre temperature

LIME Local Interpretable Model-agnostic TP Total precipitation

M The number of input features TSR Top net solar rad

N The number of sample U 10 metre U wind component

V 10 metre V wind component
The associate editor coordinating the review of this manuscript and XAI Explainable Artificial Intelligence

approving it for publication was Long Wang. XGBoost eXtreme Gradient Boosting
187814 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020
M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

IEEE Access’

 

y The actual value
Z_ A set of all input features

Zz The coalition vector

L_ INTRODUCTION

In recent decades, the world’s energy consumption has been
on the rise. This has led to a global concern regarding future
energy demand as well as shifting to more sustainable sources
of energy to meet this growing need [1]. Steps have been
taken to tackle the concerns of the modern electric grid and
to increase the efficiency and reliability of the electric grid.
One such step is to use Artificial Intelligence (AI) methods in
smart grid applications. AI methods have been used in electric
network operation and control [2], energy management and
control [3], demand response [4], predictive maintenance [5],
energy generation [6] and load forecasting [7]. AI methods
have been critical in the modernization of the electrical grid
and making it a “smart” grid. Nevertheless, AI is still con-
sidered as a black-box method because of the absence of a
simple understanding of the inner workings of the fundamen-
tal models. Numerous utility engineers in the energy industry
are hesitant to deploy AJ-based techniques considering their
lack of insight and explainability, which can help under-
stand their dynamic decision-making procedure. However,
Explainable AI (XAI) addresses this concern by increasing
the explainability and transparency of the AI models and
thus opening the black-box. An extensive review of XAI was
provided in [8] as far as concepts, taxonomies, opportunities,
challenges, and adopting XAI tools. The Defense Advanced
Research Projects Agency (DARPA), in 2017, introduced an
XAI initiative with the aim to deliver AI techniques with
more explainable models in order to understand, trust and
adequately deal with rising AI applications [9]. Numerous
applications of AI in the smart grid can be found in the
literature. The authors in [10] proposed a high-precision deep
neural network model, i.e., PVPNet, utilizing meteorological
information, such as temperature, solar radiation, and histori-
cal PV system output data, for day-ahead solar PV generation
forecasting. The purpose of this study is to focus on solar
PV forecasting using XAI tools. There are three reasons for
focusing on this application. To start with, there has been a
remarkable rise in the adoption of solar PV in the U.S., with
more than 1 million solar installations, totalling to 71.3 GW
in capacity [11]. This increasing installation capacity of solar
PV has led to a need to reassess traditional forecasting algo-
rithms, which do not consider weather conditions, such as
cloud cover, irradiance, etc. Those can drastically affect the
accuracy of PV forecasts [12]. Secondly, not many studies
have explored XAI in energy forecasting, and third, XAI has
mostly been explored for text and image data and not so much
with time-series data.

XAT has been explored more in areas where explainability
and transparency of the model’s working is critical, such
as healthcare-stroke detection [13], cybersecurity-Intrusion
Detection Systems (IDS) [14], military-target classifica-
tion [15], and finance-risk management [16]. XAI based

VOLUME 8, 2020

models have also been used in smart grid applications. In [17],
the authors present a short-term electricity load forecast-
ing study with generalized additive models to enable the
integration of both a regressive part with explanatory vari-
ables (weather, calendar variables, and global trends) and an
auto-regressive part with lagged loads. Authors in [18] pro-
pose an agent-based deep reinforcement learning algorithm
using an XAI approach to control an energy storage system.
The learning progression of an agent was explained, i.e., an
efficient dispatch algorithm for the energy storage device
under variable tariff-structures. In [19], the authors applied
XAI techniques to interpret the load forecasting output of
a gradient boosting algorithm (XGBoost) [20] and show
the analysis in SHAP (SHapley Additive exPlanations) [21].
In this article, we present multiple forecasting methods to
predict solar PV energy generation using three XAI tools,
namely, LIME, SHAP, and ELIS. The XAI tools help explain
how much an input feature contributes to the forecast. The
use of feature engineering is employed in the preprocessing
stage. XAI brings more insights and explanations in other
stages of the process, such as during model operation and
after (post-modelling). A joint approach, explainable feature
engineering, provides the additional potential to manage the
dimensions and effectiveness of the input parameters by con-
sidering the AI/ML model and domain-specific details.

AI has been used with the context of energy systems and
the energy market domain for at least two decades. It is indi-
cated that many practitioners, academicians, and researchers
in the domain consider the Al-based system as a “black-
box,” which might lead them to oversee many important and
influencing parameters in their modelling framework. Solar
forecasting can also be considered a good sample territory
where XAI may bring additional insights and explanations
regarding various variables, which will transform the “black-
box”’ model to a type of “‘grey-box’’ model. According to
the comprehensive literature review, this study is one of the
first publicly available resources, which proposes XAI-based
solar PV power generation forecasting.

Il. MACHINE LEARNING MODELS AND EXPLAINABLE
ARTIFICIAL INTELLIGENCE TOOLS

This section discusses the machine learning models and XAI
tools used in this study.

A. MACHINE LEARNING MODELS

1) RANDOM FOREST REGRESSION (RFR)

Random forest is an ensemble machine learning technique
used for supervised learning. It can be implemented for
classification as well as regression problems. Multiple deci-
sion trees are trained using randomly sampled data from the
input. The meta-estimator combines the predictions from the
decision trees [22]. For this article, a random forest regres-
sor is implemented to forecast PV power generation using
the Scikit-learn library in Python [23]. Random forests are
known to work well with tabular data. Random forest models

187815
IEEE Access’

M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

 

can produce accurate results without having to aggressively
fine-tune the model’s hyperparameters [24].

B. EXPLAINABLE ARTIFICIAL INTELLIGENCE TOOLS

Due to the increasing use of artificial intelligence and
machine learning and recent dependence on these tech-
nologies, XAI has become an emerging field of study.
A variety of XAI tools have been developed by researchers
across various fields to help understand AI-based black-box
models. LIME (Local Interpretable Model-Agnostic Expla-
nations) [25], uses an interpretable model to approximate
any AI model. SHAP (Shapley Additive exPlanations) [26]
helps to explain models, and the features that are important
in building the model using Shapely values. ELIS [27] helps
to explain various regression and classification models and
their implementations in Python. MLxtend (machine learning
extensions) [28] offers a solution to better understand popular
machine learning libraries in Python. Skater [29] provides
a solution to understand the learning structure of various
machine learning libraries in Python. InterpretML [29] is a
tool developed by Microsoft that helps explaining machine
learning models as well as provides a new model called
Explainable Boosting Machine (EBM). TreeInterpreter [30]
is a tool to understand tree-based ensemble models. Alibi [31]
provides explanations and insights into machine learning
models. In this work, we explore and implement LIME,
SHAP, and ELI5 for solar PV power forecasting.

1) LOCAL INTERPRETABLE MODEL-AGNOSTIC
EXPLANATIONS

As the name suggests, Local Interpretable Model-agnostic
Explanations (LIME) is a tool to understand and interpret the
underlying machine learning model while remaining model-
agnostic. LIME was introduced by [25], with the idea of
approximating the machine learning model with a model that
can be understood. This is done locally since it can be easier
to understand and approximate complicated machine learning
models globally [32]. The explanations provided by LIME
will enable users to understand and interpret the model. LIME
defines explanations in the following manner:

(x) = argmin£ (f, g, tx) + Q(g) (1)
geG

where, G is the set of interpretable models, Q2(g) defines
the complexity of the explanation of all g ¢ G. The aim
should be to have low Q(g) so as to have a simple model
that can be interpretable. The black-box model that is being
explained is denoted by f. 7, is the proximity measure,
which defines the size of the neighborhood around instance x,
and £ (f, g, 7) is the measure of how close the explanation
model g to the prediction of the original model f, i.e., fidelity.
The final goal is to minimize £ (f, g, 7x) and to get a inter-
pretable approximation of the black-box model. As the name
suggests, LIME tries to minimize L (f, g, 7.) while being
model-agnostic. It presents local models that approximate the
black-box model globally [25], [33].

187816

2) SHapley ADDITIVE exPlanation

Introduced in [34], SHapley Additive exPlanation (SHAP)
uses Shapely values to explain the contribution of each fea-
ture to the prediction [35]. SHAP uses the coalitional game
theory defining how well each group (or coalition) of agents
can do for itself. SHAP is defined as:

M
e(z') = do + > bz) (2)

j=l
where @; is the attribute of the feature j, z’ denotes the
coalition vector, i.e., if the feature is present (z’ = 1) or
absent (z’ = 0). M denotes the number of input features.
e gives the explanation for the model. SHAP, in order to
compute shapely values, assumes only some features values
are playing, i.e., present, and some are not, i.e., absent
[35], [36]. By doing this, SHAP identifies how much each
feature contributes to the prediction. To compute SHAP val-
ues for model f, with S denoting a subset of features, with
Z denoting the set of all input features, with (7 = 1)
and E[f(x) | xs] denoting expected value of the function
conditioned on a subset S of the input features, SHAP values
from game theory to attribute @; values to each feature can be

defined as a value function of players in S [36]:

“YM —|S|—1)!

6= ETS hou fo &

SSZ\ i}

SHAP has a Python implementation, which offers a visu-
alization tool for each feature and its importance. It works
with tree-based models from Scikit-learn package in Python
as well.

3) ELIS

ELIS is a Python package that aims to explain black-box
machine learning models in Python. ELI5 gives the weights
associated with each feature to depict the feature’s impor-
tance in the machine learning model. ELIS is implemented for
most of the commonly used Python-based machine learning
packages, such as Scikit-learn, Keras, and XGBoost [27].
Unlike LIME, ELIS is not model-agnostic, and it has its own
implementation of XGBoost [37].

Ill DATA COLLECTION AND PREPROCESSING
A popular open-source benchmark dataset from the
Global Energy Forecasting Competition (GEFCOM) held
in 2014 [38] is used for this work. The reason for selecting an
open-source dataset is to make the work easily reproducible.
The data consists of hourly solar power generation data and
corresponding numerical weather forecasts from April 1%,
2012 to July 1%, 2014. In this work, the data contains the
following weather variables from the European Centre for
Medium-Range Weather Forecasts (ECMWF):

1) TCLW (kg m**-2)

2) TCIW (kg m**-2)

3) SP (Pa)

4) HUM (%).

VOLUME 8, 2020
M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

IEEE Access’

 

5) TCC

6) U (ms**-1)
7) V(ms**-1)
8) TEMP (K)
9) SSRD (J m-2)
10) STRD (J m-2)
11) TSR (J m-2)
12) TP (m)

13) HOUR (h)

A. DATA PREPROCESSING

SSRD, STRD, TSR, and TP are accumulated fields, i.e., the
values are accumulated hourly throughout the day. The values
need to be preprocessed in order to get average values. The
following formula is used to get average values:

Pace (k + 1) ~~ Pacclk)
Pe(k) = ee 4
‘ave( ) 3600 ( )
where, Paye(k) is the average value and P,,- denotes the

accumulated value [39].

B. TRAIN AND TEST DATASET AND VALIDATION

In order to select the best parameters for the model and
prevent overfitting, the dataset was split into two primary
sets, i.e., training and testing set. We extracted out 30% of
our data as test data, while the remaining 70% was used as
training data. The test data was not used during the training
phase, except for final performance evaluation (error) of the
applied models. Root Mean Squared Error (RMSE) is used as
the error metric for the experiments. It is defined as:

N
Y On) — Hay)?
n=1

   

RMSE = (5)

N

where y(1) denotes the actual solar power generation at
time-step 1 and }(1) denotes the solar power forecast value
at time n while N is the number of samples.

IV. IMPLEMENTATION OF XAI ON SOLAR PV
GENERATION FORECASTING AND VALIDATION

The objective of this article is to apply XAI methods on solar
PV power generation forecasting and to interpret “black-
box’’ machine learning models so that it can be used in smart
grid applications with a proper acceptance. The Random
Forest Regressor (RFR) is considered as the base black-box
model for this article. The RFR model is trained using the
training data explained in the previous section. The hyper-
paremeters of the RFR model are tuned to get maximum
accuracy. The final RFR model is trained with 50 estimators.
The base RFR model gives an RMSE of 7.23%, which is
a decent result for this model. Figure 1 shows the plot of
actual test data points versus the forecast. However, the main
objective of this article is to use XAI tools to understand
the underlying model and the impact of each feature on
the forecasting results rather than the forecast itself. Below,
we present the following XAI techniques, LIME, SHAP,

VOLUME 8, 2020

 

— Forecast
— Actual

 

 

 

02-14 00 02-14 12 02-15 00

Date

02-15 12 02-16 00

FIGURE 1. Actual solar PV power generation vs predicted solar PV power
generation.

and ELIS, which can be employed for model interpreta-
tions and make the machine learning models understandable.
Figure 2 shows the methodology of the work in the form of a
flowchart. It includes three steps: (1) Solar PV data collection
and preprocessing, (2) Black-box AI-based forecasting mod-
els, and (3) Applying XAI tools, ie., LIME, SHAP, and ELIS.

A. APPLYING LIME XAI TOOL TO SOLAR PV FORECASTING
The LIME tool helps to identify an interpretable model
over the interpretable representation locally. When we apply
LIME for an explanation of individual predictions, it shows
the solar PV power output forecasting results with each fea-
ture, as shown in Figure 3. Please note that LIME provides the
explainability locally, i.e., explanation in the neighborhood of
the prediction. SSRD, HOUR, and TSR are the most important
features, while TCWL, U, and TP are the lowest in terms
of numerical contribution. The contribution of each feature,
either positive or negative, can be seen in the explanations,
e.g., SSRD has a positive effect, while TCIW has a negative
effect on predictions. Please note that the results obtained
from LIME can be slightly different when we train the data
again due to the stochastic nature of machine learning.

LIME is also capable of local interpretability of the models.
Figures 4-6 show the local explanations for 3 hours of a day,
i.e., 6, 7, and 8". LIME results consist of three parts:
(1-Left) Prediction probabilities of solar PV power generation
forecasting, (2-Middle) The LIME explanation of selected
features, and (3-Right) The original feature values. According
to the results, the solar PV forecasted values are 0.731, 0.647,
and 0.686, while the actual values are 0.774, 0.758, and 0.712,
respectively. It provides a pretty good result for this instance.
It can also be extended with more instances. For example,
as shown in Figure 5, the solar PV power generation is
predicted as 0.647. SSRD, HOUR, TSR, and TCIW in orange
have positive impacts, i.e., increasing the prediction, while
STRD, U, TEMP, and V in blue have a negative impact,
i.e., decreasing the prediction, for this instance.

B. APPLYING SHAP XAI TOOL TO SOLAR PV FORECASTING

SHAP computes the global feature importance by taking an
average of the magnitude of the SHAP values across the

187817
IEEE Access’

M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

 

Solar PV Data Collection
and Preprocessing

Black-box Al-based

Forecasting Models

XAI Tools
LIME/SHAP/ELI5

 

 

   
 

 

 

 

 

 

!

Explanation

»> x

User

    
 

y

sll

  

 

 

 

 

 

FIGURE 2. Flowchart showing the methodology applied in the work.

Local explanation

 

SSRD > 0.65 4
HOUR <= -0.94 4
TSR > 0.73 +

TCIW > -0.19 5
TEMP > 0.64 4

STRD > 0.63 4

SP <= -0.76 7

HUM <= -0.87 7

V <= -0.66 4

-0.32 < TCC <= 1.047
TCWL > -0.11 5
-0.66 < U <= -0.03 4
TP <= -0.30 4

 

 

 

0.00 «0.05 «=O .20 0.25

FIGURE 3. LIME feature importance results.

dataset. In this study, each SHAP value provides informa-
tion about the contribution of each feature, either positively
or negatively, towards predicting solar PV power. Figure 7
shows the SHAP values for the Random Forest regression
model. It represents each feature’s importance while remain-
ing visually concise. The higher the SHAP value of a feature
the higher is the impact on the model output, either negatively
or positively. As per the SHAP values, SSRD has the highest
impact on the model output, around 0.5. Each forecast is
run through the model, and a dot is created for each feature
attribute value. Therefore, one result gets one dot on each
feature’s line. This reveals, for example, that a rise in the
SSRD increases the solar PV output. Dots are colored by the
feature’s value for that forecast and pile up vertically to show
density.

The other advantage of the SHAP XAI tool is to provide
a partial dependence plot. It helps to understand how the
marginal effect of one or two features have on the predicted
outcome of the model. We can plot the SHAP value of the
target feature to explore the relationship with other features,
iLe., linear, monotonic, or more complex. For instance, verti-
cal dispersion at a single value of SSRD represents interaction

187818

effects with HOUR, as shown in Figure 8. The plot shows
that there is a positive and approximately linear correlation
relationship between target variable, i.e., SSRD, and HOUR
interacting frequently.

SHAP is also capable of local interpretability of the mod-
els. The results in Figures 9-11 are generated by applying the
SHAP algorithm on the same 3 hours of a day as done in the
previous section with LIME. We predicted three instances
as 0.720, 0.680, and 0.690, while the actual values are
0.774, 0.758, and 0.712, respectively. According to three
predictions, all features show similar trends for all selected
data points. Feature values causing increased predictions are
in red, and their visual size shows the magnitude of the
feature’s effect. Feature values decreasing the prediction are
in blue. The biggest impact comes from SSRD, HOUR, and
TSR for the prediction. For example in Figure 10, we pre-
dicted the solar PV power generation as 0.680, here SSRD,
HOUR, and TSR in red have positive impacts, i.e., increasing
the prediction, while STRD in blue has a negative impact,
i.e., decreasing the prediction, for this instance.

C. APPLYING ELI5 XAl TOOL TO SOLAR PV FORECASTING

ELIS is a Python library that allows to visualize and
debug various ML models, such as Scikit-learn, XGBoost,
LightGBM, CatBoost, Sklearn-crfsuite, and Keras. ELIS uses
an approach based on interpreting-random-forest feature
weights. These weights are calculated by following the deci-
sion paths in trees of an ensemble. Each node of the tree has an
output score, and the contribution of a feature on the decision
path is how much the score changes from parent to child.
Table 1 shows the weight for each feature in the model. SSRD
has the highest weight, i.e., it is the most important feature.
The prediction can be described as the sum of the feature
contributions + the “bias”, i.e., the mean given by the top
most region that covers the entire training set. According to
the results, SSRD has the highest contribution to final solar
PV predictions. HOUR also has a significant contribution
along with TSR and TCIW. The intercept (often labeled the
constant) is the expected mean value of the predicted results
when all features are zero.

VOLUME 8, 2020
M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

IEEE Access’

 

Predicted value negative
0.00 (ET) 0.76
(min) 0.73 (max)

 

ositive
P Value

Feature

 

FIGURE 4. LIME explanation for hour 6 of a day where Actual:0.77461539, Predicted:0.73 156154.

Predicted value negative positive
SSRD 0.68 Feature Value
0.00 TT | 077 o27
(min) 0.65 (max) = -0.94
TSR > 0.73
|-0.32 < TCIW <= -0,.32
STRD > 0.63
0.01
U <= -0.66

-0.66 < V <= -0.08

  

0.001

FIGURE 5. LIME explanation for hour 7 of a day where Actual:0.75833333, Predicted:0.64756026.

 

: negative ositive
Predicted value & ssRp = 2 65 Feature Value
0.00 TT | 0.7 027
(min) 0.69 (max) = -0,94

ITSR > 0.73

-0,32 < TCIW <= -0.19

0.01

STRD > 0.63

 

FIGURE 6. LIME explanation for hour 8 of a day where Actual:0.71262821, Predicted:0.6868576.

To understand how the model works, individual predictions
are examined in ELIS. We examine this for three instances
of each model, i.e., three consecutive hours of a day shown
in Tables 2-4. In terms of accuracy, the solar PV predicted
power generation values are 0.717, 0.677, and 0.689, while
the actual values are 0.774, 0.758, and 0.712, respectively.
Tables show that SSRD has the highest positive effect, while
STRD has the highest negative effect. For example, as given
in Table 3, contributions of SSRD, and STRD to the predicted
value, i.e., 0.677, are 0.474, and —0.051, respectively.

VOLUME 8, 2020

D. MODELS WITH SUBSET OF FEATURES USING THE
ANALYSIS FROM ELIS, LIME AND SHAP

XAI tools help us understand complex black-box machine
learning models. With the knowledge that we get about the
inner working of the ML models, our goal is to improve
the models performance by providing it with better inputs
and take away the input features that negatively impact the
model’s performance. In this section, we look at the feature
importance provided by LIME, SHAP and ELIS, and elimi-
nate the two least important features according to each of the

187819
IEEE Access: M. Kuzlu et a/.: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAl Tools

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

High TABLE 2. ELIS explanation for the 6th hour of a day.
SSRD —————
HOUR cafe Actual 0.774615 Predicted 0.717422
Target Feature Weight Value
TSR '--——-- PV Power _SSRD 0.468908 2.575425
Tew a PV Power <BIAS> 0.178025 1
STRD - te PV Power HOUR 0.085265 -1.51686
® PV Power TCC 0.029704 —_-1.10524
Hum ——> 8 PV Power TCIW 0.012997 -0.32074
TCC “i 2 PV Power HUM 0.007081 -1.61374
TEMP jm 8 PV Power TSR 0.004176 2.508821
PV Power SP 0.00322 -1.08073
u —-- . PV Power TCWL 0.001611 -0.34857
v —— PV Power TP 0.000852 -0.29901
1p ae : PV Power V -0.00583 0.050971
PV Power TEMP -0.01019 2.061648
sP 7“ PV Power U -0.01932_-1.39247
TCWL +. PV Power STRD -0.03908 1.445211
Low
02 “Shae valine (impact on mradel output) oa TABLE 3. ELI5 explanation for the 7th hour of a day.
FIGURE 7. SHAP values with impact on model. Actual 0.758333 Predicted 0.677499

 

15 Target Feature Weight Value
PV Power SSRD 0.474194 —- 2.953836
PV Power <BIAS> 0.178025 1
PV Power HOUR 0.083735 —-1.3724

 

 

0.4

 

1.0

 

 

 

 

 

 

 

 

 

 

 

 

0.3 PV Power TCC 0.01109 -1.09629

5 0.5 PV Power HUM 0.006946 -1.69948
ga 0.2 « PV Power TSR 0.001394 2.851593
ae 00 2 PV Power TCWL 0.001183 ——--0.34819
a Y oa x= PV Power V 0.001053 -0.19898
& “os PV Power TP 0.000796 —--0.29901
. PV Power SP -0.00049 -1.16186

oe PV Power __TCIW -0.0049___—-0.31923

-1.0 PV Power U -0.00848 -1.40341

01 PV Power TEMP -0.01566 2.251754

“as PV Power STRD 0.05138 1.551317

 

 

05 00 O65 10 #15 20 25 3.0

SSRD
TABLE 4. ELIS explanation for the 8th hour of a day.

FIGURE 8. Dependence plot of SSRD.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

TABLE 1. ELIS explanations. Actual 0.71262821 Predicted —_0.6890705127
Target Feature Weight Value

Feature Weight Std PV Power SSRD 0.475717 2.823923
SSRD 0.778945 0.102629 PV Power <BIAS> 0.178025. «1

HOUR 0.075221 __(0.018073 PV Power HOUR 0.048827 ——_—-1.22794
TSR 0.053688 0.103161 PV Power TCC 0.037107 -0.82766
TCIW 0.015678 0.003519 PV Power HUM 0.005721 __-1.70496
STRD 0.013434 0.003318 PV Power TSR 0.002227 __—-2.759754
TCC 0.013019 0.006075 PV Power TP 0.001239 ——-0.29901
HUM 0.011854 0.002545 PV Power TCIW -0.00034_—~-0.26052
U 0.008606 0.002205 PV Power TEMP 0.00165 2.346532
TEMP _0.006973 0.002058 PV Power V -0.00314___—-0.02194
V 0.006887 (0.00189 PV Power SP -0.00469 1.26207
SP 0.006249 0.001437 PV Power U -0.00492——-1.3612
TCWL 0.005549 (0.001944 PV Power TCWL -0.00921_—_—--0.30173
TP 0.003896 0.001722 PV Power STRD 0.03585 1.673153

 

XAI tool. Table 5 shows RMSE results associated with all TABLE 5. RMSE results from each model.
features and removing the two least important features as per

 

 

 

 

 

the three XAI models. SHAP XAI model provides a better Model —___ Features removed Re (%)

performance in terms of RMSF, i.e., from 7.236 to 7.216. LIME TCWL, TCC 78
SHAP SP, TCWL 7.216

E. OBSERVATIONS ELIS TP, TCWL 7.235

 

This study gives a few pointers on how to implement an XAI

tool, ie., LIME, SHAP, and ELIS. Each XAI tool/package Table 6 shows the execution time for LIME, SHAP and ELIS.
has its own strengths and limitations, in terms of comput- The LIME XAI model provides the best efficiency in terms
ing cost, explanation locally/globally, feature weight, etc. of execution time, i.e., 34.3 milliseconds. The following

187820 VOLUME 8, 2020
M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

IEEE Access’

 

rc
TSR = 2.509 | HOUR = -1.517

FIGURE 9. SHAP explanation for the 6th hour of a day.

higher = lower

0.72 ) 778 0 778
«

STRD = 1.445

SSRD = 2.575

higher = lower

-0.2222 -0.02217 ).1778 ).3778 0.5778 0.68 7778 9778
a! K
TSR = 2.852 'HouR =-1.372 SSRD = 2.954 STRD = 1.554
FIGURE 10. SHAP explanation for the 7th hour of a day.
higher = lower
22 2217 0.1778 3778 0.5778 0.69 0.7778 0.977
TSR = 2.76 | HOUR = -1.228 SSRD = 2.824 STRD = 1.673

FIGURE 11. SHAP explanation for the 8th hour of a day.

TABLE 6. Time taken to run LIME, SHAP and ELI5.

Model — Time

LIME = 34.3 milliseconds
SHAP 9.4 minutes
ELIS 47.32 milliseconds

observations regarding LIME, SHAP, and ELIS tools can be
made from results.

« Observation |: The key limitation of all tools is that they
need to run many evaluations of the original model.

e Observation 2: All tools support regression and classi-
fication models (in this study, we focus on regression
models).

« Observation 3: LIME is a locally surrogated model,
which explains the prediction at local boundaries.

e Observation 4: LIME is model agnostic, which means
that it can be applied to any machine learning model.

e Observation 5: LIME does not guarantee a perfect dis-
tribution of the effects.

« Observation 6: The SHAP value is the only method
to deliver a full explanation and considers all possible
predictions, for instance, using all possible combinations
of inputs.

« Observation 7: LIME is faster than SHAP since the
calculation of SHAP values is very time-consuming as
it checks all the possible combinations.

« Observation 8: The SHAP value is the contribution of a
variable to the difference between the actual prediction
and the mean prediction.

VOLUME 8, 2020

« Observation 9: SHAP can guarantee properties like con-
sistency and local accuracy.

e Observation 10: SHAP provides more detailed informa-
tion and results, such as visualizing, explaining multiple
predictions, dependence and summary plots, and feature
importance with SHAP values.

e Observation 11: ELI5 is the simplest of the three XAI
tools.

e Observation 12: ELIS does not support true model-
agnostic interpretations and support for models.

« Observation 13: ELIS is mostly limited to tree-based and
other parametric-linear models.

« Observation 14: A prediction by ELI5 can be described
simply, i.e., the sum of the feature contributions + the
“bias’’.

e Observation 15: ELIS provides weights for each feature
depicting how influential it might have been in contribut-
ing to the final prediction decision across all trees as well
as individual data-point predictions.

F. TEST ENVIRONMENT

The experiments presented in this study were implemented on
Python version 3.8. The workstation used for the work runs
an Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz processor
with 16 GB RAM, and NVIDIA GeForce GTX 1070 GPU
with 8 GB memory.

V. CONCLUSION
This article presents the application of a random forest

forecasting model to predict solar PV power generation.
Furthermore, XAI tools, such as LIME, SHAP and ELI5,

187821
IEEE Access’

M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

 

are applied to the random forest AI model to understand
and explain the reasons for a particular prediction as well
as to contribute to the adoption of XAI tools to smart grid
applications. The data used for this article is a public dataset
from GEFCOM 2014. According to the results, XAI tools
can provide detailed information to interpret the model fea-
tures and results as well as improvement of the model’s
results through explainability and transparency. This study
has given a few pointers on how to choose an XAI tool, such
as LIME, SHAP and ELIS. Each XAI tool/package has its
own strengths and limitations, in terms of the computing cost,
explanation locally/globally, feature weights, etc.

The utilities are willing to create next generation control
centers with visualization technologies and business analytics
tolls, which support emerging technologies, such as AI and
mixed reality, but at the same time they want to simplify
the usability for employees who have less expertise in these
technologies. XAI based PV solar forecasting systems and
similar tools provide a very productive play-ground for the
utilities. It is expected that this study can benefit utility engi-
neers and researchers working on power generation and load
forecasting by providing an insight into the XAI potentials
and availability in smart grid applications.

REFERENCES

{1] M--J. Santofimia-Romero, X. del Toro-Garcia, and J.-C. Lépez-Lépez,
“Artificial intelligence techniques for smart grid applications,” in Green
ICT: Trends Challenges. 2011, pp. 41-44.

[2] J. A. Momoh, “Smart grid design for efficient and flexible power networks

operation and control,” in Proc. IEEE/PES Power Syst. Conf. Expo.,

Mar. 2009, pp. 1-8.

H. Zhou, M. Rao, and K. T. Chuang, “Artificial intelligence approach

to energy management and control in the HVAC process: An evaluation,

development and discussion,’ Develop. Chem. Eng. Mineral Process.,

vol. 1, no. 1, pp. 42-51, May 2008.

A. K. Pathak, D. S. Chatterji, and M. S. Narkhede, “Artificial intelligence

based optimization algorithm for demand response management of res-

idential load in smart grid,” Int. J. Eng. Innov. Technol., vol. 2, no. 4,

pp. 136-141, 2012.

[5] M. De Benedetti, F Leonardi, EF Messina, C. Santoro, and A. Vasilakos,

“Anomaly detection and predictive maintenance for photovoltaic sys-

tems,” Neurocomputing, vol. 310, pp. 59-68, Oct. 2018.

A. Mellit and S. A. Kalogirou, “Artificial intelligence techniques for pho-

tovoltaic applications: A review,” Prog. Energy Combustion Sci., vol. 34,

no. 5, pp. 574-632, Oct. 2008.

[7] M. Q. Raza and A. Khosravi, “A review on artificial intelligence based

load demand forecasting techniques for smart grid and buildings,” Renew.

Sustain. Energy Rev., vol. 50, pp. 1352-1372, Oct. 2015.

A. Barredo Arrieta, N. Diaz-Rodriguez, J. Del Ser, A. Bennetot, S. Tabik,

A. Barbado, S. Garcia, 8. Gil-Lopez, D. Molina, R. Benjamins, R. Chatila,

and F. Herrera, ““Explainable artificial intelligence (XAT): Concepts, tax-

onomies, opportunities and challenges toward responsible AI,” Inj: Fusion,

vol. 58, pp. 82-115, Jun. 2020.

D. Gunning, “Explainable artificial intelligence (XAT),”” Defense Adv. Res.

Projects Agency (DARPA), nd Web, vol. 2, p. 2, Nov. 2017.

[10] C.-J. Huang and P.-H. Kuo, “Multiple-input deep convolutional neural net-
work model for short-term photovoltaic power forecasting,” IEEE Access,
vol. 7, pp. 74822-74834, 2019.

[11] US. Solar Market and 15 States See Best Quarter Ever for Res-
idential Solar. [Online]. Available: https://www.seia.org/news/us-solar-
market-and-15-states-see-best-quarter-ever-residential-solar

[12] Y. Wang, N. Zhang, Q. Chen, D. S. Kirschen, P. Li, and Q. Xia, “Data-
driven probabilistic net load forecasting with high penetration of Behind-
the-Meter PV,” IEEE Trans. Power Syst., vol. 33, no. 3, pp. 3255-3264,
May 2018.

[3

[4

[6

[8

[9

187822

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]
[22]

[23]

[24

[25]

[26]

27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

N. Prentzas, A. Nicolaides, E. Kyriacou, A. Kakas, and C. Pattichis, “Inte-
grating machine learning with symbolic reasoning to build an explainable
AI model for stroke prediction,” in Proc. IEEE 19th Int. Conf. Bioinf:
Bioeng. (BIBE), Oct. 2019, pp. 817-821.

D. L. Marino, C. S. Wickramasinghe, and M. Manic, “An adversar-
ial approach for explainable AI in intrusion detection systems,” in
Proc. 44th Annu. Conf. IEEE Ind. Electron. Soc. (IECON), Oct. 2018,
pp. 3237-3243.

Mandeep, H. S. Pannu, and A. Malhi, “Deep learning-based explainable
target classification for synthetic aperture radar images,” in Proc. 13th Int.
Conf: Human Syst. Interact. (HSI), Jun. 2020, pp. 34-39.

J. Adams and H. Hagras, “A Type-2 fuzzy logic approach to explainable
Al for regulatory compliance, fair customer outcomes and market stability
in the global financial sector,” in Proc. IEEE Int. Conf. Fuzzy Syst. (FUZZ-
IEEE), Jul. 2020, pp. 1-8.

A. Pierrot and Y. Goude, “Short-term electricity load forecasting with
generalized additive models,” in Proc. ISAP Power, 2011, pp. 1-6.

H. Kumar, P. Mary Mammen, and K. Ramamritham, “‘Explainable AI:
Deep reinforcement learning agents for residential demand side cost
savings in smart grids,” 2019, arXiv:1910.08719. [Online]. Available:
http://arxiv.org/abs/1910.08719

Y.-G. Lee, J.-Y. Oh, and G. Kim, “Interretation of load forecasting using
explainable artificial intelligence techniques,” Trans. Korean Inst. Elect.
Eng., vol. 69, no. 3, pp. 480-485, 2020.
XGBoost Documentationg. [Online].
readthedocs.io/en/latest/

Explainersq. [Online]. Available: https://shap.readthedocs.io/en/latest/

L. Breiman, “Bagging predictors,’ Mach. Learn., vol. 24, no. 2,
pp. 123-140, Aug. 1996.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, and V. Dubourg, “Scikit-
learn: Machine learning in Python,” J. Mach. Learn. Res., vol. 12,
pp. 2825-2830, Oct. 2011.

J. Moon, K.-H. Kim, Y. Kim, and E. Hwang, “‘A short-term electric load
forecasting scheme using 2-Stage predictive analytics,” in Proc. IEEE Int.
Conf. Big Data Smart Comput. (BigComp), Jan. 2018, pp. 219-226.

M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why should I trust you?’:
Explaining the predictions of any classifier,” in Proc. 22nd ACM SIGKDD
Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1135-1144.

K. Sriwong, T. School of Computer EngineeringSuranaree University
of Technology (SUT}111 University AvenueMuangNakhon Ratchasima
30000, K. Kerdprasop, and N. Kerdprasop, “Post-operative life expectancy
of lung cancer patients predicted by Bayesian network model,” Int.
J. Mach. Learn. Comput., vol. 8, no. 3, pp. 280-285, Jun. 2018.
TeamHG-Memex. Teamhg-Memex/eli5. [Online]. Available:
/github.com/TeamHG-Memex/eli5

S. Raschka, “MLxtend: Providing machine learning and data science
utilities and extensions to Python’s scientific computing stack,” J. Open
Source Softw., vol. 3, no. 24, p. 638, 2018.

D. Dataman. (Mar. 2020). Explain Your Model With Microsoft's Inter-
pretmi. [Online]. Available: https://medium.com/analytics-vidhya/explain-
your-model-with-microsofts-interpretml-5daab1d693b4

EF. Revert. (Feb. 2019). Interpreting Random Forest and Other Black Box
Models Like XGboost. [Online]. Available: https://towardsdatascience.
com/interpreting-random-forest-and-other-black-box-models-like-
xgboost-80f9cc4.a3c38

Getting Startedq. [Online]. Available: https://docs.seldon.io/projects/alibi/
en/latest/overview/getting_started.htm]

Sameer Singh Marco Tulio Ribeiro. (Aug. 2016). Local Interpretable
Model-Agnostic Explanations (Lime): An Introduction. [Online]. Avail-
able: https://www.oreilly.com/content/introduction-to-local-interpretable-
model-agnostic-explanations-lime/

T. Peltola, ‘“‘Local interpretable model-agnostic explanations of
Bayesian predictive models via kullback-leibler projections,’ 2018,
arXiv: 1810.02678. [Online]. Available: http://arxiv.org/abs/1810.02678
S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model
predictions,” in Proc. Adv. Neural Inf. Process. Syst.,2017, pp. 4765-4774.
C. Molnar, “Interpretable machine leaming,’ in A Guide for
Making Black Box Models Explainable. 2019. [Online]. Available:
https://christophm. github. io/interpretable-ml-book/

S. M. Lundberg, G. G. Erion, and S.-I. Lee, “Consistent individualized
feature attribution for tree ensembles,” 2018, arXiv: 1802.03888. [Online].
Available: http://arxiv.org/abs/1802.03888

Available: — https://xgboost.

https:

VOLUME 8, 2020
M. Kuzlu et af: Gaining Insight Into Solar PV Power Generation Forecasting Utilizing XAI Tools

IEEE Access’

 

[37] Posted By: OnClick360 and OnClick360. (Feb. 2020). Interpretable
Machine Learning With Lime Eli5 Shap Interpretml. [Online]. Available:
https://www.onclick360.com/interpretable-machine-learning-with-lime-
eli5-shap-interpret-ml/

[38] T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and
R. J. Hyndman, “Probabilistic energy forecasting: Global energy
forecasting competition 2014 and beyond,” Int. J. Forecasting, vol. 32,
no. 3, pp. 896-913, Jul. 2016

[39] M. Abuella and B. Chowdhury, “Solar power probabilistic forecasting
by using multiple linear regression analysis,” in Proc. SoutheastCon,
Apr. 2015, pp. 1-5.

MURAT KUZLU (Senior Member, IEEE) received
the B.Sc., M.Sc., and Ph.D. degrees in electron-
ics and telecommunications engineering, in 2001,
2004, and 2010, respectively. He joined the
Department of Engineering Technology, Old
Dominion University (ODU), as an Assistant Pro-
fessor, in 2018. In 2006, he joined the TUBITAK
MAM Energy Institute (Scientific and Techno-
logical Research Council of Turkey-Marmara
Research Center), where he worked as a Senior
Researcher. Before joining ODU, he worked as a Research Assistant Pro-
fessor with the Virginia Tech’s Advanced Research Institute. His research
interests include smart grid, demand response, the Internet of Things (IoT),
machine learning with a particular applications on energy savings, and
wireless communication.

 

UMIT CALI (Member, IEEE) received the B.E.
degree in electrical engineering from Yildiz Tech-
nical University, Istanbul, Turkey, in 2000, and
the M.Sc. degree in electrical communication
engineering and the Ph.D. degree in electrical
engineering and computer science from the Uni-
versity of Kassel, Germany, in 2005 and 2010,
respectively. He joined the Department of Elec-
tric Power Engineering, Norwegian University of
Science and Technology, Norway, in 2020, as an
Associate Professor. He worked with the University of Wisconsin—Platteville
and the University of North Carolina at Charlotte as an Assistant Professor,
in 2013 and 2020, respectively. His current research interests include energy
informatics, artificial intelligence, blockchain technology, renewable energy
systems, and energy economics. He is serving as the Active Vice Chair for
the IEEE Blockchain in Energy Standards WG (P2418.5).

 

VOLUME 8, 2020

 

 

VINAYAK SHARMA received the M.S. degree
in applied energy and electromechanical systems
with a thesis on deterministic and probabilistic
forecasting for renewable energy. He is currently
pursuing the Ph.D. degree in electrical engineering
with the University of North Carolina at Charlotte.
His research interests include applying statistical
and machine learning techniques for load, wind,
solar, and natural gas forecasting, power systems
planning, and optimization of energy storage.

6ZGUR GULER received the B.S. degree in com-
puter science and the M.S. degree in computer
science with a focus on image-guided surgery
from the University of Innsbruck, Innsbruck,
Austria, and the Ph.D. degree from the Medical
University of Innsbruck, Austria, with a focus
on image-guided diagnosis and therapy. He is
currently an Imaging Scientist and AI Researcher
specialized in 3-D chronic wound imaging and
computer vision. Prior to joining eKare Inc.,
he was a Researcher with the Sheikh Zayed Institute (SZI) for Pediatric
Surgical Innovation Center, Washington, DC, where he developed the seg-
mentation and classification algorithms that laid the groundwork of the eKare
inSight system.

187823
2006.00093v4 [cs.AI] 12 Oct 2020

arXiv

Explainable Artificial Intelligence: a Systematic Review

Giulia Vilone, Luca Longo

School of Computer Science, College of Science and Health,
Technological University Dublin, Dublin, Republic of Ireland

 

Abstract

Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few
years. This is due to the widespread application of machine learning, particularly deep learn-
ing, that has led to the development of highly accurate models but lack explainability and in-
terpretability. A plethora of methods to tackle this problem have been proposed, developed and
tested. This systematic review contributes to the body of knowledge by clustering these meth-
ods with a hierarchical classification system with four main clusters: review articles, theories
and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and
recommends future research directions.

Keywords: Explainable artificial intelligence, method classification, survey, systematic
literature review

 

1. Introduction

The number of scientific articles, conferences and symposia around the world in eXplainable
Artificial Intelligence (XAI) has significantly increased over the last decade [1, 2]. This has led
to the development of a plethora of domain-dependent and context-specific methods for dealing
with the interpretation of machine learning (ML) models and the formation of explanations for
humans. Unfortunately, this trend is far from being over, with an abundance of knowledge in
the field which is scattered and needs organisation. The goal of this article is to systematically
review research works in the field of XAI and to try to define some boundaries in the field. From
several hundreds of research articles focused on the concept of explainability, about 350 have
been considered for review by using the following search methodology. In a first phase, Google
Scholar was queried to find papers related to “explainable artificial intelligence”, “explainable
machine learning” and “interpretable machine learning”. Subsequently, the bibliographic sec-
tion of these articles was thoroughly examined to retrieve further relevant scientific studies. The
first noticeable thing, as shown in figure 2 (a), is the distribution of the publication dates of se-
lected research articles: sporadic in the 70s and 80s, receiving preliminary attention in the 90s,
showing raising interest in 2000 and becoming a recognised body of knowledge after 2010. The
first research concerned the development of an explanation-based system and its integration in a
computer program designed to help doctors make diagnoses [3]. Some of the more recent papers
focus on work devoted to the clustering of methods for explainability, motivating the need for
organising the XAT literature [4, 5, 6]. The upturn in the XAI research outputs of the last decade
is prominently due to the fast increase in the popularity of ML and in particular of deep learning
(DL), with many applications in several business areas, spanning from e-commerce [7] to games

Preprint submitted to Elsevier October 13, 2020
[8] and including applications in criminal justice [9, 10], healthcare [11], computer vision [10]
and battlefield simulations [12], just to mention a few. Unfortunately, most of the models that
have been built with ML and deep learning have been labelled ‘black-box’ by scholars because
their underlying structures are complex, non-linear and extremely difficult to be interpreted and
explained to laypeople. This opacity has created the need for XAI architectures that is motivated
mainly by three reasons, as suggested by [12, 13]: i) the demand to produce more transparent
models; ii) the need of techniques that enable humans to interact with them; iii) the require-
ment of trustworthiness of their inferences. Additionally, as proposed by many scholars [13, 14]
[15, 16], models induced from data must be liable as liability will likely soon become a legal re-
quirement. Article 22 of the General Data Protection Regulation (GDPR) sets out the rights and
obligations of the use of automated decision making. Noticeably, it introduces the right of expla-
nation by giving individuals the right to obtain an explanation of the inference/s automatically
produced by a model, confront and challenge an associated recommendation, particularly when
it might negatively affect an individual legally, financially, mentally or physically. By approving
this GDPR article, the European Parliament attempted to tackle the problem related to the prop-
agation of potentially biased inferences to society, that a computational model might have learnt
from biased and unbalanced data.

Many authors surveyed scientific articles surrounding explainability within Artificial Intelli-
gence (AI) in specific sub-domains, motivating the need for literature organisation. For instance,
[17, 18] respectively reviewed the methods for explanations with neural and bayesian networks
while [19] clustered the scientific contributions devoted to extracting rules from models trained
with Support Vector Machines (SVMs). The goal was, and in general is, to create rules highly
interpretable by humans while maintaining a degree of accuracy offered by trained models. [20]
carried out a literature review of all the methods focused on the production of visual represen-
tations of the inferential process of deep learning techniques, such as heat-maps. Only a few
scholars attempted to make a more comprehensive survey and organization of the methods for
explainability as a whole [1, 21]. This paper builds on these efforts to organise the vast knowl-
edge surrounding explanations and XAI as a discipline, and it aims at defining a classification
system of a larger scope. The conceptual framework at the basis of the proposed system is rep-
resented in Figure 1. Most of the methods for explainability focus on interpreting and making
the entire process of building an AI system transparent, from the inputs to the outputs via the
application of a learning approach to generate a model. The outcome of these methods are expla-
nations that can be of different formats, such as rules, numerical, textual or visual information, or
a combination of the former ones. These explanations can be theoretically evaluated according
to a set of notions that can be formalised as metrics, usually borrowed from the discipline of
Human-Computer Interaction (HCD [22].

The remainder of this paper is organised as it follows. Section 2 provides a detailed descrip-
tion of the research methods employed for searching for relevant research articles. Section 3
proposes a classification structure of XAI describing top branches while Sections 5-4 expand
this structure. Eventually, section 8 concludes this systematic review by trying to define the
boundaries of the discipline of XAI, as well as suggesting future research work and challenges.
Explainable Artificial Intelligence

 

Methods for Explainability Evaluation approaches

 

 

= = | output “IF... THEN... &
cma : rs 2 Ee
z (At fk

explanators
knowledge X

 

 

 

 

 

 

ante-hoc post-hoc

 

 

 

 

 

Figure 1: Diagrammatic view of Explainable Artificial Intelligence as a sub-field at the intersection of Artificial Intelli-
gence and Human-Computer Interaction

2. Research methods

Organizing the literature of explainability within AI in a precise and indisputable way as well
as setting clear boundaries is far from being an easy task. This is due to the multidisciplinarity
surroundings of this new fascinating field of research spanning from Computer Science to Math-
ematics, from Psychology to Human Factors, from Philosophy to Ethics. The development of
computational models from data belongs mainly to Computer Science, Statistics and Mathemat-
ics, whereas the study of explainability belongs more to Human Factors and Psychology since
humans are involved. Reasoning over the notion of explainability touches Ethics and Philosophy.
Therefore, some constraints had to be set, and the following publication types were excluded:

e scientific studies discussing the notion of explainability in different contexts than AI and
Computer Science, such as Philosophy or Psychology;

e articles or technical reports that have not gone through a peer-review process;

e methods that could be employed for enhancing the explainability of AI techniques but
that were not designed specifically for this purposes. For example, the scientific litera-
ture contains a considerable amount of articles related to methods designed for improving
data visualization or feature selection. These methods can indeed help researchers to gain
deeper insights into computational models, but they were not specifically designed for pro-
ducing explanations. In other words, those methods developed only for enhancing model
transparency but not directly focused on explanation were discarded.

Taking into account the above constraints, this systematic review was carried out in two phases:

1. papers discussing explainability were searched by using Google Scholar and the following
terms: ‘explainable artificial intelligence’, ‘explainable machine learning’, ‘interpretable
machine learning’. The queries returned several thousands of results, but it became imme-
diately clear that only the first ten pages could contain relevant articles. Altogether, these
searches provided a basis of almost two hundred peer-reviewed publications;

2. the bibliographic section of the articles found in phase one was checked thoroughly. This
led to the selection of one hundred articles whose bibliographic section was recursively
analysed. This process was iterated until it converged and no more articles were found.

3
3. Classification of scientific articles on explainability

After a thorough analysis of all the selected articles, four main categories were extracted as
depicted in Fig. 2 and as listed below:

e reviews on methods for explainability - it includes either literature or system-
atic reviews of those methods devoted to the proposal and/or testing of solutions for the
explainability of data- and knowledge-driven models;

e notions related to the concept of explainability - it includes studies focused
on the definition of those notions related to the concept of explainability and on the deter-
mination of the main characteristics as well as the requirements of an effective explanation;

e development of new methods for explainability - it includes articles that pro-
pose novel and original methods for enhancing the explainability of data/knowledge-driven
models;

e evaluation of methods for explainability - it includes articles reporting the re-
sults of scientific studies aiming at evaluating the performance of different methods for
explainability.

1975-1977 | 1 :

1978-1980 | 1 -—>Reviews (53) ~_

1981-1983 | 1 ——
19

    
  

Reviews

   
  

Notions (85) —

 

 

| oe
Evaluations (59) built upon

 

(a) (b) (©)

Figure 2: Proposed classification of the XAI literature with (a) the distribution of published scientific articles over time,
(b) the root of our hierarchical classification system representing the main four categories and the percentage of articles
in each , and (c) the salient relations between these categories that have emerged.

Following the proposed classification, it was possible to design a map of the XAI literature
in form of a tree whose root contains the above four categories (figure 2, part b). This tree
expands into branches of different depth where leaves represent scientific articles. Figure 2,
part b, also shows the percentage of articles grouped by each category, clearly highlighting the
distribution of the research efforts towards the development of methods for explainability. Note
that, a paper might appear in multiple branches of this classification, as it might cover multiple
dimensions. Figure 2, part c, depicts the dependencies of the main four categories. In general,
scholars would not be able to carry out reviews of the XAI literature without the existence and
consideration of relevant notions and methods for explainability as well as the approaches for
evaluating the performances of these methods. Evaluation approaches naturally followed the
creation of methods for explainability which have been engineered to meet as many requirements
of an effective explanation as possible.
4. Reviews of the XAI literature

This category contains literature and systematic reviews devoted to specific classes of solu-
tions for explainability, such as systems generating textual explanations [23], or constrained to
specific AI techniques as, for instance, neural networks [24] (summary in table A.2 and figure
3). These reviews provide an entry point for researchers to acquire information and get familiar
with the key aspects of the rapidly growing body of research related to explainability. They also
attempt to summarise the main techniques for explainability and to highlight their strengths and
limitations. Seven clusters emerged based on distinct aspects of explainability covered by these
reviews:

e application fields - reviews on methods for explainability in a specific field of appli-
cation;

* construction approaches - reviews on methods for explainability specifically designed
to explain the inferential process of models. This category has been further divided into:

— data-driven approaches which focus on extracting new knowledge from trained
models from data, but without accounting for the prior knowledge of domain experts.

— knowledge-driven approaches focused on capturing an expert’s knowledge and
logic, often embedded in the notion of agent;

e theories & concepts - reviews of the notions related to the concept of explainability;

¢ output formats - reviews on methods for explainability focused on generating specific
formats of explanations, such as visual or rules;

problem types - review articles on methods designed to explain the logic of data and
knowledge-driven models applied to a specific type of problem, namely regression or clas-
sification;

® generic reviews - generic reviews that cover a wide range of data/knowledge-driven
models as well as their methods for explainability and cannot be placed within any other
category.

In the application fields cluster, the assumption of the methods for explainability is that it
is not possible to accept the inference made by a model without understanding its function-
ing because a decision, supported by a wrong prediction, can have a dramatic impact on peo-
ple’s lives [1]. The second cluster, construction data-driven approaches, contains reviews of
methods for explainability for specific data-driven learning approaches, mainly neural networks
[25, 20, 26, 27, 28, 29, 17, 30], bayesian networks [18] and SVMs [31, 19], not constrained to
a specific type of input data for the approach or a particular output format for an explanation,
such as images or texts. Other scholars instead focused on reviewing methods for knowledge-
based approaches such as Expert Systems (ES) [32] and Intelligent Systems [33]. In particular,
these surveys analysed what types and formats of explanations were tested on these systems and
which ones work better than others. For instance, [34] showed that rich explanations, based on
a combination of information regarding users, items and features, are very effective, while [33]
claimed that explanations should be context-specific to be effective. The third cluster contains
those reviews focused on objectively defining the concept of explainability and its set of related

5
. ons &, %, Py
» Finance, military, transportation "8, he
=—* Healthcare Yen ?
Application fields + iy,
—> Law "tp,

—> Human-computer interaction

 

Bayesian networks
AY hy
*data-driven » Neural networks (22.22%) Visual soe
. ~data Neural networks (81.25%)
Support vector machines driven

R Construction
E approach mesant systems Reviews
53 articl
Vv knowledge- based “G+ Intelligent systems wo paacoa Coe “o .
: proaches

| Theories & concepts Recommender systems 0 - knowledge "Ya,
E > Textual 1 based Sy, Pe,
w Output formats —~c——> visual |< —> Classification . & ak, “he, "hing
s —— Rules — Regression Rs a ws “tea

Problem type s recommen cS ‘et aay

» Generic reviews systems (16.67%) “S

Figure 3: Hierarchical classification of the review articles on explainable artificial intelligence and machine learning
interpretability (left) and distribution of the review articles across categories (right).

notions, which are discussed in depth in section 5.1. One of these studies presented an overview
of different theories of explanation borrowed from the cognitive science and philosophy disci-
plines, contextualised within case-based reasoning [35]. In details, it is believed that, in order to
be effective, an AI system should: (I) explain how it reached the answer and (II) why it is a good
answer, (IIT) why a question is relevant or not, (IV) clarify the meaning of the terms used in the
system that might not be understood by the users and, lastly, (V) teach the user about the domain.
In short, the goals that an explanation must achieve depend on the domain under consideration,
the underlying model and end-users. Similarly, [23] suggested that explanations should take into
account the preferences and preconceptions of end-users. This can be achieved by incorporating
more findings from the behavioural and social sciences into the newly emerging field of XAI.
For example, people explain their behaviour based on their beliefs, desires and intentions hence
these elements must be considered in an explanation. Eventually, explanations based on coun-
terfactual examples should help end-users to understand the logic of an underlying model by
leveraging on people’s capability to infer general rules from a few examples. Counterfactuals
add also something new to what is already known from the existing data and provide additional
information on how a model behaves in novel, unseen situations [36]. The fourth cluster con-
tains reviews of methods for explainability generating a specific output format for an explanation
(further discussed in section 6). Methods generating textual explanations are surveyed in [32]
and compared according to some requirements about the structure and content of the explana-
tions to adapt them to the users’ needs and knowledge. [37] focused on written explanations
generated from fuzzy rules integrated with natural language generation tools. The underlying
reasonable assumption is that the understandability of these rules cannot be given for granted.
Researchers studied the capabilities of ‘data-to-text’ approaches that automatically create lin-
guistic descriptions from a complex dataset by means of aggregation functions, implemented as
fuzzy rules, that aggregate ‘computational perceptions’. A computational perception is “a unit
of meaning for the phenomenon under analysis and is identified by a set of linguistic expressions
and their corresponding validity values given a situation.” Some methods combine Logical AI
and Statistical AI to generate textual explanations [38]. The former is concerned with ‘formal
languages’ to represent and reason with qualitative specifications, while the latter is focused on
learning quantitative specifications from data. However, the authors claimed that the search for

6
an effective way to learn representations of the inferential process of data-driven models is still
open [38]. A body of literature focused on the visual explanation of deep learning models. Ex-
planators generating salient masks were investigated in [20, 30] whilst [20, 39] reviewed methods
that graphically represent the inner structure and functioning of neural networks with flow-charts
or other explanatory graphs. An interesting alternative was proposed in [40] whereby methods
based on nomograms, rule induction, fuzzy logic, graphical models and topographic mapping
can be utilised to explain data-driven models and learning techniques. Similarly to textual ex-
planation, the problem of visually inspecting data-driven models has not been resolved and there
are still challenges and open questions to be answered. Some reviews summarised the methods
for explainability that generate sets of rules from underlying trained models [41] by extracting
frequent relations from a dataset using fuzzy logic and fuzzy rules [42, 43, 40], the integration
of symbolic logic with the neural networks [44, 45] and, more generally, the usage of automated
reasoning to shed a light over the inferential process of automatically constructed data-driven
models [46]. The fifth cluster contains reviews that analysed the methods for explainability for
either regression [47] or classification [31, 48, 49] problems. They have a broader scope than the
previous reviews as they range over several fields, AI techniques and explanation types. Their
goal was to summarise the important issues, still unresolved, of interpreting prediction models for
both problem types and encouraging researchers to improve the existing or discover novel meth-
ods for explainability. Eventually, some reviews have a more generic scope. They are aimed at
proposing a comprehensive way of organizing the several methods for explainability [1, 50, 21]
or describing them [50, 51, 52]. A group of these reviews tried to evaluate the performances of
various methods. This is done by comparing the explanations automatically produced by these
methods [19] or by measuring how much they fulfil certain notions of explainability, such as
completeness, through the use of either quantitative or qualitative metrics[53] (further discussed
in section 7).

5. Notions related to the concept of explainability

Explaining a model induced from data by employing a specific learning technique is not a
trivial goal. A body of literature focused on achieving such a goal by investigating and attempting
to define the concept of explainability, leading to many types of explanation and the formation of
several attributes and structures. To organise these, the specific following clusters are proposed:

e attributes of explainability - it contains criteria and characteristics used by schol-
ars to try to define the construct of ‘explainability’ ;

e types of explanation - it includes the different ways scholars reported explanations
for their ad-hoc applications, what pieces of information are included or left out;

e structure of an explanation - it contains the various components an explanation
can be constructed on, such as causes, context, and consequences of a model’s prediction
as well as their ordering.

5.1. Attributes of explainability

One of the principal reasons to produce an explanation is to gain the trust of users [54].
Trust is the main way to increase users’ confidence with a system [55] and to make them feel
comfortable while controlling and using it [56]. Besides trust, researchers determined other

7
positive effects brought by explainability. According to [57], it is part of human nature to assign
causal attribution of events. A system that provides a causal explanation on its inferential process
is perceived more human-like by end-users as a consequence of the innate tendency of human
psychology to anthropomorphism. Thus, several scholars spoke at length about causality which is
considered a fundamental attribute of explainability [12, 58, 59, 56, 23]. Explanations must make
the causal relationships between the inputs and the model’s predictions explicit, especially when
these relationships are not evident to end-users. Data-driven models are designed to discover
and exploit associations in the data, but they cannot guarantee that there is a causal relationship
in these associations. As pointed out in [56], the task of inferring causal relationships strongly
depends on prior knowledge, but some associations might be completely unexpected and not
explainable yet. Scientists can use these associations to generate hypotheses to be tested in
scientific experiments; however, this is outside the scope of the methods for explainability. Other
four reasons supporting the necessity to explain the logic of an inferential system or a learning
algorithm were suggested in [1]:

e explain to justify - the decisions made by utilising an underlying model should be explained
in order to increase their justifiability;

e explain to control - explanations should enhance the transparency of a model and its func-
tioning, allowing its debugging and the identification of potential flaws;

e explain to improve - explanations should help scholars improve the accuracy and efficiency
of their models;

e explain to discover - explanations should support the extraction of novel knowledge and
the learning of relationships and patterns.

Despite the widely recognised importance of explainability, researchers are striving to de-
termine universal, objective criteria on how to build and validate explanations [22]. Numerous
notions underlying the effectiveness of explanations were proposed in the literature (as sum-
marised in table 1). [22] surveyed 250 articles from the fields of Philosophy, Psychology and
Cognitive Science to analyse in depth how people define, generate, select, evaluate and present
explanations. The author also presented an interesting definition of XAI as a human-agent inter-
action problem where the agent reveals the underlying causes to its or another agent’s decision
process. In other words, XAT is believed to be a subset of the human-agent interaction field that
can be defined as the intersection of AI, social science and HCI.

Two studies on explainability demonstrated that this concept is utilised in several fields, span-
ning from Mathematics, Physics, Computer Science to Engineering, Psychology, Medicine and
Social sciences [63, 75]. Explainability is often replaced with the notion of interpretability, con-
sidered as synonyms within the general AI community, and in particular by those scholars in
automated learning and reasoning, whereas it seems that the software engineering community
prefers the term understandability [63]. Generally speaking, interpretability is often defined as
the capacity to provide or bring out the meaning of an abstract concept and understandability
as the capacity to make the model understandable by end-users (see table 1). However, other
definitions are proposed in the literature. Explainability or interpretability is defined in [26] as
“the degree to which a human observer can understand the reason behind a decision (or a pre-
diction) made by the model”. An interesting distinction between the concepts of interpretation

8
Table 1: Definition of the notions related to the concept of explainability

 

 

Notion Description & Reference

Algorithmic The degree of confidence of a learning algorithm to behave ‘sensibly’ in general [26, 2]

transparency

Actionability The capacity of a learning algorithm to transfer new knowledge to end-users [60, 61]

Causality The capacity of a method for explainability to clarify the relationship between input and output
[12, 58, 57, 59, 56, 23]

Completeness The extent to which an underlying inferential system is described by explanations [53, 60, 61]

Comprehensibility The quality of the language used by a method for explainability [62, 63, 64, 65, 66, 13, 67, 68, 69]

Cognitive
relief
Correctability

Effectiveness
Efficiency

Explicability

Explicitness
Faithfulness
Intelligibility

Interactivity
Interestingness

Interpretability

Informativeness
Justifiability
Mental Fit

Monotonicity
Persuasiveness
Predictability

Refinement
Reversibility
Robustness

Satisfaction

Scrutability /
diagnosis
Security
Selection /
simplicity

Sensitivity
Simplification
Soundness
Stability
Transparency

Transferability

The degree to which an explanation decreases the “surprise value” which measures the amount
of cognitive dissonance between the explanandum and the user’s beliefs. The explanandum is
something unexpected by the user that creates dissonance with his/her beliefs [58]

The capacity of a method for explainability to allow end-users make technical adjustments to an
underlying model [60, 61]

The capacity of a method for explainability to support good user decision-making [70, 71, 72]
The capacity of a method for explainability to support faster user decision-making [70, 55, 71]
The degree of association between the expected behaviour of a robot to achieve assigned tasks or
goals and its actual observed actions [73]

The capacity of a method to provide immediate and understandable explanations [74]

The capacity of a method for explainability to select truly relevant features [74]

The capacity to be apprehended by the intellect alone [75, 76, 5, 77, 78]

The capacity of an explanation system to reason about previous utterances both to interpret and
answer users’ follow-up questions [79, 80]

The capacity of a method for explainability to facilitate the discovery of novel knowledge and to
engage user’s attention [64, 81, 67, 65, 82]

The capacity to provide or bring out the meaning of an abstract concept [64, 50, 83, 66, 13, 22,
29, 84, 85, 4, 6, 86]

The capacity of a method for explainability to provide useful information to end-users [56]

The capacity of an expert to assess if a model is in line with the domain knowledge [1, 64, 50, 33]
The ability for a human to grasp and evaluate a model [64, 87]

The relationship between a numerical predictor and the predicted class that occurs when increas-
ing the value of the predictor leads to either always increase or decrease the probability of an
instance’s membership to the class [88]

The capacity of a method for explainability to convince users perform certain actions [70, 55, 71]
The capacity to anticipate the sequence of consecutive actions in a plan [73]

The capacity of a method to guide experts in improving the performance/robustness of a model
[89]

The capacity to allow end-users to bring a ML-based system to an original state after it has been
exposed to an harmful action that makes its predictions worse [60, 61]

The persistence of a method for explainability to withstand small perturbations of the input that
do not change the prediction of the model [90, 89]

The capacity of a method to increase the ease of use and usefulness of a ML-based system [70,
55,71]

The capacity of a method for explainability to inspect a training process that fails to converge or
does not achieve an acceptable performance [89, 70, 55]

The reliability of a model to perform to a safe standard across all reasonable contexts [91]

The ability of a method for explainability to select only the causes that are necessary and sufficient
to explain the prediction of an underlying model [23]

The capacity of a method for explainability to reflect the sensitivity of the underlying model with
respect to variations in the input feature space [92, 93]

The capacity to reduce the number of variables under consideration to a set of principal ones [94]
The extent to which each component of an explanation’s content is truthful in describing an un-
derlying system [60, 61]

The consistency of a method to provide similar explanations for similar/neighboring inputs [74]
The capacity of a method to explain how the system works even when it behaves unexpectedly
[76, 26, 13, 95, 84, 14, 15, 70, 55, 16, 96, 86]

The capacity of a method to transfer prior knowledge to unfamiliar situations [56]

Understandability The capacity of a method of explainability to make a model understandable [75, 63, 64, 89, 97]

 

9
and explanation was proposed in [29]. On one hand, an interpretation is the mapping of an ab-
stract concept (as a predicted class) into a domain that the human can make sense of, such as, for
instance, images or texts that can be inspected and classified by people. On the other hand, an
explanation is the collection of features of an interpretable domain that contributed to produce a
prediction for a given item. The authors of [29] did not specify how to determine this collection
of features. The selection criteria are to be decided by researchers according to several factors
like the type of input data and the degree of refinement in the explanation demanded by end-users.
An expansion of the definition of interpretability through the determination of its main charac-
teristics was presented in [74, 22, 85]. In detail, [74] suggested the following requirements: (I)
fidelity - the representation of inputs and models in terms of concepts should preserve and present
to end-users their relevant features and structures, (II) diversiry - inputs and models should be
representable with few non-overlapping concepts, and (IID grounding - concepts should have an
immediate human-understandable interpretation. These requirements were further expanded in
[22] by listing a set of characteristics that an explanation should possess:

© contrastive nature of explanations - people seek for an explanation when they are presented
with counterfactual and/or counter-intuitive events;

e selectivity of explanations - people usually do not expect that an explanation contains the
actual and complete list of the causes of an event, but only a selection of the few causes
deemed to be necessary and sufficient to explain it. Authors point out the risk that this
selection might be influenced by cognitive biases;

© social nature of explanations - explanations are part of a dialogue aiming at transferring
knowledge, therefore, they are based on the beliefs of both the explainer and explainee;

© irrelevance of probabilities to explanations - referring to the occurrence probabilities of
events or to the statistical relationships between causes and events does not produce a
satisfactory and intuitive explanation. Explanations are more effective when they refer to
the causes and not to their likelihood.

Four further requirements for enhancing the interpretability of visual explanations were added
in [85]: i) graphical integrity - the representations should highlight the features that contribute
the most to the final predictions and distinguish those with positive and negative attribution, ii)
coverage - a large fraction of the most important features should be visible in the representation,
iii) morphological clarity - the important features should be clearly displayed, their visualization
cannot be ‘noisy’, and iv) layer separation - the representation cannot occlude the raw image
which should be visible for human inspection. Other two notions strongly correlated with in-
terpretability are comprehensibility [64] and intelligibility [75]. However, scholars highlighted
some differences. [66] proposed to distinguish between interpretable systems, systems in which
end-users can mathematically analyse algorithms, and comprehensible systems that “emit sym-
bols enabling user-driven explanations of how a conclusion is reached”. Two studies [75, 78]
defined intelligibility as an attribute of user-centric reasoned explanations that are easily inter-
pretable by end-users and that draws from foundational concepts of other disciplines such as
Philosophy and Cognitive Psychology. Additionally, both studies recommended exploiting the
experience and knowledge of the HCI community in making interfaces that empower people to
assure that intelligibility will be one of the core requirements of the next generation of AI sys-
tems. Other authors focused on breaking some of the notions identified in table 1 into sub-notions

10
or on assigning further requirements. For example, three sub-notions related to transparency that
should be achieved by any learning model were defined in [26, 56]:

e simulatability - the capacity of a model to allow a user to understand its structure and
functioning entirely;

e decomposability - the degree to which a model can be decomposed into its individual
components (input, parameters and output) and of their intuitive explainability;

© algorithmic transparency - the degree of confidence of a learning algorithm to behave
’sensibly’ in general (see also table 1).

However, according to [56], it is not possible to achieve algorithmic transparency in neural
networks because of the current incapacity of experts to understand the inferential process of
these models and to prove that they work correctly on new, unseen observations. Scholars at-
tempted to overcome this shortcoming by finding methods to trace the predictions of a model
to the most influential features of the input. Examples of these methods are heat-maps [98]
which are created by back-propagating the predictions of a model to the input space and high-
lighting relevant pixels. Alternatively, [99] proposed a solution to satisfy the simulatability and
decomposability properties by substituting black-box models with Generalized Additive Mod-
els (GAMs). GAMs are linear combinations of simple models trained on a single feature of an
input dataset, thus allowing end-users to quantify the contribution of each feature to the out-
come. However, transparency must be handled with caution because it can be dangerous under
certain circumstances, as highlighted in [96]. Requiring that data and models are fully visible
to end-users prevents the creation of intellectual properties; this can significantly slow down the
development of new technologies. Moreover, data can contain sensitive or personal informa-
tion which cannot be made public without affecting people’s privacy. Finally, the displaying of
more information might push a researcher to optimise a model on specific instance(s) but deteri-
orating its overall performance and degree of generalisability. Scholars extensively investigated
sensitivity [92, 93]. In this context, sensitivity is considered as the sensibility of explanations
to variations in the input features, model implementation and, subsequently, in the model’s pre-
dictions. [92] introduced the requirement of input invariance meaning that a method for ex-
plainability must mirror the sensitivity of the underlying model with respect to transformations
of the inputs in order to ensure a reliable interpretation of their contribution to each prediction.
[93] focused on the sensitivity of methods for explainability specifically designed for neural net-
works, in particular those that quantify the contribution of input features to the predictions, such
as DeepLift [100] and Layer-wise Relevance Propagation (LRP) [101]. In this case, a method for
explainability satisfies the sensitivity requirement if it assigns a non-zero contribution to an input
feature when two instances, in the input space, differ in that feature only but lead to different
predictions. According to [93], methods for explainability must also fulfill the requirement of
implementation invariance. This suggests that a method applied to functionally equivalent neural
networks should assign identical contributions to the features of the input. Two neural networks
are functionally equivalent if their predictions are equal for all inputs despite having different
implementations and architectures. Finally, scholars identified various factors that might affect
the interestingness of a model, in particular of the rule-based ones [81, 67]. First, rule size is the
number of instances satisfied by a rule. Usually, small size rules are undesirable as they explain
only a few instances. The main aim is to discover rules that cover a large portion of the input
data. However, there are situations where small rules might capture exception occurring in the

11
data that can be of interest for scientists. Second, imbalance of class distributions occurs when
the instances belonging to a class are more frequent than those of another class. It might be
more difficult, hence more interesting, to discover those rules aimed at predicting the minority
classes. Attribute costs represent the cost to get access to the actual value of an attribute of the
data. For example, it is easy to assess the gender of a patient but the determination of some
health-related attributes can require an expensive investigation. Rules that utilise only ‘cheap’
attributes are more interesting. Eventually, the interestingness of a rule must take into account
the misclassification costs. In some domain of application, the erroneous classification of an in-
stance might have a significant impact, not only in terms of money. In case of medical diagnosis,
classifying as healthy a patient affected by a lethal disease might lead to premature death. In-
terestingness was also examined for Reinforcement Learning (RL) agents which are designed to
take actions in a specific environment with the aim to maximize a cumulative reward [82]. The
authors proposed a framework to make the behaviour of these agents explainable by analysing
their historical interactions with the environment and extracting a few interestingness elements.
Examples of interesting elements of these interactions are the portion of environment observed
by the agent, the frequency of certain types of interactions and the cost (in terms of a reward) of
the interactions carried out.

5.2. Types of explanations

Researchers tried to create a classification system for the types of explanation suitable for
interpreting the logic of learning algorithms. A method for explainability should answer several
questions to form an exhaustive explanation. The two most common questions are why and how
the model under scrutiny produces its predictions/inferences [102, 103, 2, 7]. However, scholars
identified other questions that might arise and that require different answers, thus different types
of explanations [104]. Additionally, as pointed out in [105, 106], distinct behaviours, distinct
problems and distinct types of users require distinct explanations. This has led to many ad-hoc
classifications that are domain-dependent and are hard to be merged into one. For example, [107]
focused on the types of users of methods for explainability. They proposed a two-class system
consisting of traced-based explanations, useful for system designers, that accurately reflects the
reasoning implemented within a model, and reconstructive explanations, designed for end-users,
based on an active, problem-solving approach. A reconstructive explanation tends to build a
‘story’ exposing the input features contributing to a prediction. For instance, an image of a bird
was assigned to a certain class because of the colour of the bird. However, the model might
have analysed other features that did not influence the final assessment, like the image’s back-
ground. These characteristics can be included in the traced-based explanations but excluded from
the reconstructive explanations. The same scholars also developed Reconstructive EXplanation
(REX) [107, 108], an explanatory tool capable of producing reconstructive textual explanations
for expert systems. REX is built on a model that maps the execution of the expert system onto
a textbook representation of the domain. A textbook representation presents the domain knowl-
edge in human-understandable explanations, much of which comes from domain textbooks. The
explanation consists of mapping over key elements from the execution trace and expanding on
them using the more structured textbook knowledge, which is a collection of relationships be-
tween cues, hypotheses and goals as illustrated by this example: “The presence of damages to
the drainage pipes is a sign that the cause of an excessive high uplift pressures on a concrete dam
is internal erosion of soil under the dam. Erosion would lead to broken pipes, therefore slowing
drainage and causing high uplift pressures”. The goal is to determine the cause of high uplift

12
pressure on a concrete dam, the cues consist of the presence of broken pipes and the hypoth-
esis is the erosion of soil. Another classification of the types of explanations was proposed in
[109] for intelligent systems which include intelligent agents, such as those AI assistants utilised
in customer support chats, or other support decision systems like those for medical diagnoses.
Here, traced-based explanations were defined as mechanistic explanations and correspond to the
answer of the question “How does it work?”. Hence, they must offer insights into the causes
and consequences of events and how these events and the different components of the intelli-
gent systems interact to give rise to complex actions. Reconstructive explanations were instead
called ontological explanations and describe the structural properties of the intelligent systems:
its components, their attributes, and how they are related to each other. [109] also added a third
category, referred to as operational explanations which respond to the question “How do I use
it?” by relating goals to the mechanics designed to realise them. A more articulated classification
of the types of explanations was introduced in [110] and it is based on five types of explanations
that intelligent systems should produce. The first one, teaching explanations, aims at informing
humans about the concepts learned by the system such as, for example, the presence of some
physical constraints (walls or other obstacles) that can limit its actions. Introspective tracing
explanations have the goal of finding the cause of and the solution to a fault whilst introspective
informative explanations aim at explaining predictions based on the reasoning process to im-
prove human-system interaction. The last two types of explanations, post-hoc explanations and
execution explanations, are respectively focused on explaining the decisions and their execution
without necessarily following the same reasoning process and directly linking them with the in-
puts. An example of post-hoc and execution explanation is a robot describing the path it wants
to follow to go from point A to point B and all the movements it must do to cover that path. This
explanation can mention the characteristics of the surrounding environment that have been con-
sidered while planning the path, but it does not mention that alternative paths were considered
and discarded and the reasons beyond these decisions. Finally, [111] presented a classification of
the types of knowledge intrinsically embedded in an explanation. Explanations based on reason-
ing domain knowledge focus on the domain knowledge needed to perform reasoning, including
rules and terminology. Communication domain knowledge is instead about the domain knowl-
edge needed to inform, clearly and comprehensively, end-users about the underlying domain, and
it might include additional information not strictly necessary for reasoning. Eventually, domain
communication knowledge focuses on how to communicate within a certain domain of applica-
tion and it deals with practical aspects of the communication process, such as the language to
be used, the most effective strategies for effective explanations and the communication medium.
This knowledge must be tuned to the prior knowledge and cognitive state of the hearer.

5.3. Structures of explanations

The most effective way to structure explanations is still an open problem despite being tackled
by several scholars. As highlighted in [112], two properties of the structure of an explanation can
have a significant effect on learning, namely the capacity to “accommodate novel information in
the context of prior beliefs and do so in a way that fosters generalization”. As prior beliefs greatly
vary according to the application field and the domain knowledge of end-users, researchers ex-
amined and proposed different structures for explanations which are domain-dependent. The first
studies on the most suitable and effective structures of textual explanations were carried out in
the 80-90s and focused on interpreting the inferential process of expert models. Most of these ex-
planations were planned as dialogues where end-users were allowed to ask a (limited) number of
questions via an explanatory tool. Blah [113], an example of these tools, was primarily concerned

13
with structuring explanations so that they do not appear too complex. It was based on a series of
psycho-linguistic studies that analyzed how human beings explain decisions, choices, and plans
to one another. Different ways to structure a conversational explanation, or dialogue, to success-
fully transfer knowledge from an explainer to an explainee were listed in [114, 115, 116, 117, 80].
All these studies proposed to split a dialogue into three stages: opening, explanation and clos-
ing stage. Each stage has to obey a set of rules to ensure that the knowledge about the model’s
inferential process can be successfully transferred to end-users. On one hand, [80] grounded
this three-stage formal protocol on the data collected from almost four hundred real dialogues
which were examined to detect the key components of an explanation, the relationships between
them and their order of occurrence. These main components can be synthesised by a set of ques-
tions (mainly how, why and what) and the relative arguments presented by an explainer to an
explainee who, respectively, answer the questions and acknowledge the explanation or challenge
it with counterfactual examples. On the other hand, [115, 116, 117] focused on the most effective
set of rules to manage interactive dialogues with interruptions from the user while maintaining
coherence between the different sections of an explanation. They also developed a tool, called
EDGE, that generates dialogues based on these rules. EDGE updates assumptions about the
user’s knowledge based on his/her questions and uses this information to influence the further
planning of the explanation. Other studies on interactive dialogues [118, 119, 120, 121, 122]
focused on the structure, the language and main components (what pieces of information must
be included) of these dialogues. Based in these early studies, [123, 124, 125, 126] proposed a
modular architecture for explaining the behavior of simulated entities in military simulations. It
consists of three modules: a reasoner, a natural language generator and a dialogue manager. The
user can stop simulation and query about what happened at the current time point by selecting
questions from a list. The dialogue manager orchestrates the system’s response: firstly, by using
the reasoner to retrieve the relevant information from a relational database, then producing En-
glish responses using the natural language generator. More recently, interactive dialogues were
used as the explanation format of choice in knowledge-based systems other than expert systems.
AutoTutor [127], designed to be integrated into tutoring systems, is grounded on learning theo-
ries and tutoring research. It simulates a human tutor by holding a conversation with the learner
in natural language.

The explanations of task planning systems, according to [12, 128], must contain information
on (I) why a planner choose an action, (ID) why a planner did not choose another action, (IID)
why the decisions of a planner are the best among a set of possible alternatives, (IV) why certain
actions cannot be executed and (V) why one needs or does not need to change the original
plan. The criterion of episodic memory was added to the above list by [128], whereby an agent
should remember all the factors that influenced the generation and execution of a plan such as
“states, actions, and values considered during plan generation, traces of plan execution in the
environment, and anomalous events that led to plan revision”. A formal framework to generate
preferred explanations of a plan was introduced in [129]. Preferences over explanations must
be contextualized with respect to complex observational patterns. Actions might be affected
by several causes and requires reflecting on the past, meaning that explanations must take into
consideration previous events and information.

14
6. Development of new methods for explainability

More than 200 scientific articles were found that aim at developing new methods for explain-
ability. Over time, researchers have tried to comprehend and unfurl the inner mechanics of data-
driven, knowledge-driven models in various ways. From an examination of these articles, two
main criteria exist for discriminating methods for explainability:

© scope - it refers to the scope of an explanation that can be either global or local. In the
former case, the goal is to make the entire inferential process of a model transparent and
comprehensible as a whole. In the latter case, the objective is to explicitly explain each
inference of a model [130, 26, 56, 17];

e stage - it refers to the stage at which a method generates explanations. Ante-hoc methods
are generally aimed at considering explainability of a model from the beginning and during
training to make it naturally explainable whilst still trying to achieve optimal accuracy
or minimal error [13, 99, 131]; post-hoc methods are aimed at keeping a trained model
unchanged and mimic or explain its behaviour by using an external explainer at testing
time [13, 56, 29, 97].

Taking into account the articles examined in this systematic review, and inspired by the clas-
sification system in [21], we propose additional criteria:

problem type - methods for explainability can vary according to the underlying problem:
classification or regression;

e input data - the mechanisms followed by a model to classify images can substantially
differ from those used to classify textual documents, thus the input format of a model
(numerical/categorical, pictorial, textual or times series) can play an important role in
constructing a method for explainability;

e output format - similarly, different formats of explanations useful for different circum-
stances can be considered by a method for explainability: numerical, rules, textual, visual
or mixed.

Figure 4 depicts the main branches of methods for explainability and shows the distribution
of the articles across these branches. Each of the many methods for explainability retrieved from
the scientific literature can be robustly described by using the five categories of figure 4 (stage,
scope, problem type, input data and output format). Additionally, as it is possible to notice
from Figure 4, the post-hoc methods are further divided into model-agnostic and model-specific
methods [21]. The former methods do not consider the internal components of a model such
as weights or structural information, therefore they can be applied to any black-box model. The
latter methods are instead limited to specific classes of models. For example, the interpretation of
the weights of a linear regression model is specific to the learning approach (linear regression).
Similarly, methods that only work with the interpretation of neural networks are model-specific
[25, 26, 30]. Model agnosticity and specificity do not usually apply to the class of ‘ante-hoc’
methods because their goal is to make the functioning of a model transparent, so almost all them
are intrinsically model-specific [13]. Some post-hoc methods for explainability can be applied
both at a global or local scope [132] and can work for either regression or classification problems
[133].

15
  
   

pax (260982)

»Ante-hoc ,*Model agnostic

  

->Stage +0
—+Post-hoc —~ >Model specific
" ~Scope _.g “Global » Classification
—+6
T Local ‘Regression
H —-O-Problem type
° ~Numerical/ ;——Numerical
D Categorical
s »Pictorial ~Rules
"Input data “9 xtual i Textual
‘—+Time serie Visual
»Mixed

— Output format
8
®
&

Figure 4: Classification of methods for explainability (left) and distribution of articles across categories (right).

The following sections try to succinctly describe the main classes of methods for explainabil-
ity found during this systematic review, accompanied by tables for reporting their stage, scope,
problem type, input data and output format and sorting them in alphabetic order. Given the large
number of methods found, it was decided to group them into five thematic classes.

6.1. Output formats

Visual explanations are probably the most natural way of communicating things and a very
appealing way to explain them. Visual explanations can also be used to illustrate the inner func-
tioning of a model via graphical tools. For instance, heat-maps can highlight specific areas of
an image or specific words of a text that mostly influence the inferential process of a model
by using different colours [134, 135]. Similarly, a graphical representation can be employed
to represent the inner structure of a model, such as the graphs proposed in [136] where each
node is a layer of the network and the edges the connections between layers. Another intuitive
form of explanation for humans are textual explanations, natural language statements that can
be either written or orally uttered. An example is the phrase “This is a Brewer Blackbird be-
cause this is a blackbird with a white eye and long pointy black beak” shown by an explainer
of an image classification model [137]. A schematic, logical format, more structured than vi-
sual and textual explanations but still intuitive for humans, are rules that can be used to ex-
plain the inferences produced by models induced from data. Rules can be in the form of ‘IF
... THEN’ statements with AND/OR operators and they are very useful for expressing combi-
nations of input features and their activation values [138, 139]. Technically, rules of these type
employ symbolic logic, a formalized system of primitive symbols and their combinations (ex-
ample: ‘(Country = US A) A (28 < Age <= 37) > (Salary > 50K)’ [140]). The parts before
and after the — logical operator are respectively referred to as antecedent and consequent. Given
this logic, rules can be implemented as fuzzy rules, linking one or more premises to a conse-
quent that can be true to a degree, instead of being entirely true or false. This can be obtained
by representing each antecedent and consequent as fuzzy sets [43]. Combining fuzzy rules with
learning algorithms can become a powerful tool to perform reasoning and, for instance, explain
the inner logic of neural networks [141]. Similarly, the combination of antecedents and conse-
quent can be seen as an argument in the discipline of argumentation, and a set of arguments can

16
be put together in a dialogical structure by employing attacks, the link between arguments that
model conflictuality [142, 143]. Arguments and attacks form a complex structure but with high
explanatory power, suitable for explaining the inner functioning of data-driven models. Expla-
nations can also be constructed by only employing numerical formats as crisp values, vectors of
numbers, matrices or tensors as in Probe [144] and Concept Activation Vectors (CAVs) [145],
two methods for explainability. A Probe consists of a linear classifier fitted to the features,
treated independently, learned by each layer of a neural network. Probes are engineered to better
understand the roles and dynamics of the internal layers. The numerical explanations are the
probability scores assigned by the probes to each class [144]. CAVs separates the activation val-
ues of a neural network’s hidden layer relative to instances belonging to a class, forming a set,
from those generated by the remaining part of the input dataset, forming a second set. Subse-
quently, a binary linear classifier is trained to distinguish the activation values of the two sets.
Then, CAVs computes directional derivatives on this classifier to measure the sensitivity of the
model to changes in inputs towards the class of interest. This is a scalar quantity, calculated for
each class over the whole dataset, which quantifies how important a user-defined concept is to
classify the input instances in the class under analysis. For example, CAVs measures how sensi-
tive the class ‘zebra’ is to the presence of stripes in an input image. Eventually, the most powerful
format of explanations are those that employ one or more of the formats described so far (visual,
textual, rules, numeric). An example of a combination of visual and numerical explanation is
utilized by Important Support Vectors and Border Classification [146] that provide insight into
local classifications produced by a Support Vector Machine (SVM). The former method returns
the support vectors which influence the most the final classification for a particular instance. The
latter determines which features of a data point would need to be altered (and by how much) to
be placed on the separating surface between two classes. The explanations are in the form of an
interactive interface where the user can select a point and the tool shows the attributes that had
the largest effect on classifying it and the closest border value. The user can modify the selected
point’s attributes to see how the SVM treclassifies it. Image Caption Generation with Attention
Mechanism [147] is an example of visual and textual explanations jointly employed. It returns
attention maps for a combination of a Convolutional Neural Network (CNN) and a Long-Short
Term Memory (LSTM) network where the CNN performs object recognition in images and the
LSTM generates their captions.

6.2. Model agnostic methods for explainability

Several methods for explainability were designed to work with any learning technique. How-
ever, this does not mean that they can be universally applied as they might be constrained to the
types of inputs of the technical problem they try to solve and the explanation they try to provide.

6.2.1. Numeric explanations

A few model agnostic methods for explainability produce numerical explanations (see table
A.3 and figure 5). Most of them focus on measuring the contribution of an input variable (or a
group of them) with quantitative metrics. Distill-and-Compare [148] trains a transparent, sim-
pler model, called student, on the output obtained from a large, complex model, considered as a
teacher, to mimic its inferential process. In this study, the student model was constrained to be
GAMs which allow to easily assess the contribution of each feature in a numerical format. Simi-
larly, SHapley Additive exPlanations (SHAP) [149] utilizes additive feature attribution methods,
basically linear combinations of the input features, to build a model which is an interpretable

17
approximation of the original model. Some methods for explainability are based on an ‘input
perturbation’ approach and, generally speaking, they work by modifying the reported values of
the variables of an input instance to cause a change in the model’s prediction. Explain and Ime
[150, 151] assess respectively the contribution of a particular input variable or a set of vari-
ables. This is done by replacing the actual values of the variables describing each input instance
with other values sampled from the same variable(s) and measuring the differences in the output
probability scores. The assumption is that the larger the difference in the outcome, the more rel-
evant the variable is for the prediction process. Similarly, the Global Sensitivity Analysis (GSA)
method [152, 153] ranks input features by quantifying the effects on the predictions of a given
model when they are varied through their range of values. [154, 155, 156, 157, 158] proposed
a method to explain the prediction of a model at instance level also based on the contribution
of each feature estimated by comparing the model output when all the features are known and
when one or more of them are omitted. The contribution is positive for the features that lead
to the prediction towards a class, negative for those that push the prediction against a class and
zero when they don’t have influence. Four methods, Quantitative Input Influence (QI) func-
tions [159], Gradient Feature Auditing (GFA) [160], Influence functions [161] and Monotone
Influence Measures [162], utilize influence functions to assess the contribution of each feature
to certain predictions. An influence function is a classic technique from statistics [161] measur-
ing the sensitivity of a model to changes in the distributions of the independent variables. The
perturbation of the input can be done in different ways such as applying a constant shift (Influ-
ence functions [161]), obscuring parts of the input (GFA [160]), rotating, reflecting or randomly
assign labels to the input (Monotone Influence Measures [162]). Feature Importance [163] and
Feature Perturbation [164] are also based on algorithms that modify subsets of the input features
to find groups of interacting attributes used by different classifiers and to determine the extent to
which a model exploits such interactions.

 

 

  

 

 

 

 

  

item Oo Oe OM 0.00 0.05, O10 On 020
— re 1 1
Za ; Fe suprates [an
ioe a :
o : Twalsdirdoae [uasiooe
q =
. bedecdly vobleaciity
dooml
‘pedaniiy
resiualsigar
os
= ic nel freesulturdioxide
i jew sid Cersty :
5 Totes cibites a
f x sala
— 1 1
aco one OMG 0.00 0.05. O10 On5 020
(a) Distill-and-Compare [148] (b) GSA [152] (c) GFA [160]

Figure 5: Examples of numerical explanations generated by model-agnostic methods for explainability.

18
6.2.2. Rule-based explanations

A few model-agnostic methods for explainability produce rule-based explanations by exploit-
ing several rule-extraction techniques (see table A.5 and figure 6), such as automated reasoning-
based approaches. The method presented in [165] extracts logical formulas as decision trees by
combining split predicates along paths from inputs to predictions into logical conjunctions and
all the paths related to an output class into logical disjunctions. These rules can be analyzed with
logical reasoning techniques to extract information about the decision-making process. Sim-
ilarly, Genetic Rule EXtraction (G-REX) [166, 167] employed genetic algorithms to generate
IF-THEN rules with AND/OR operators. Anchor [140] uses two algorithms to extract IF-THEN
rules which highlight the features of an input instance, called ‘anchors’, that are sufficient for a
classifier to make a prediction. In an analogical manner, the words “not bad” are often used in
sentences expressing a positive sentiment, and thus can be considered anchors in sentiment anal-
yses. These two algorithms, a bottom-up formation of and a beam-search for anchors, identify
the candidate rules with the highest estimated precision over a dataset where precision is equal
to the fraction of correct predictions. The first algorithm starts from an empty set of rules and
adds, at each iteration, a rule for each feature predicate. The second one instead starts from a set
containing all the possible candidate rules and then selects the best ones in terms of precision.
Model Extraction [168] and Partition Aware Local Model (PALM) [169] utilize decision trees
(DTs) to approximate complex models with the assumption that, as long as the approximation
quality is good, the statistical properties of the complex model are reflected in the interpretable
ones. End-users have also the faculty to examine the DT’s structure and determine whether the
rules match intuition. Model Extraction generates DTs by using the Classification And Regres-
sion Trees algorithm (CART) and trains them over a mixture of Gaussian distributions fitted to
the input data using expectation maximization. PALM uses a two-part surrogate model: a meta-
model, constrained to be a DT, that partitions the training data, and a set of sub-models fitting
the patterns within each partition.

28 < Age < 37
Workclass = Private
Education = High School grad
Marital Status = Married
Occupation = Blue-Collar
Relationship = Husband
Race = White
Sex = Male
Capital Gain = None
Capital Loss = Low
Hours per week < 40.00
Country = United

 

 

ates

 

P(Salary > $50K) = 0.57

 

(a) G-REX [166] (b) Anchor [140]

Figure 6: Examples of rule-based explanations generated by model-agnostic methods which can be visualized as (a) a
decision tree (b) a list of rules accompanied by textual and visual examples.

6.2.3. Visual explanations

Visual explanations try to explain the inner functioning of a model via graphical aids and
many model-agnostic methods exploit them (table A.6 and figure 7). One of the most widely
used among these aids is represented by ‘salient masks’ that are efficient ways to point out what

19
parts of input, especially when images or texts are treated, most affect a model’s prediction by
superimposing a mask highlighting them. Layer-Wise Relevance Propagation (LRP) [101] was
developed as a model-agnostic solution to the problem of understanding image classification pre-
dictions by pixel-wise decomposition of nonlinear classifiers. In its general form, LRP assumes
that the classifier can be decomposed into several layers of computation and it traces back contri-
butions of each pixel to the final output, layer by layer, to attribute relevance to individual inputs.
The pixel contributions can be visualized as heat-maps. Spectral Relevance Analysis (SpRAy)
[8] consists of spectral clustering on a set of LRP explanations in order to identify typical and
atypical decision behaviours of an underlying data-driven model. For example, to analyse the
inferential process of a classifier trained on a dataset of images of animals, SpRAy produces an
LRP heat-map for each image. Then, it checks if the heat-maps highlight the area representing
the animal or if, for a specific animal, the classifier is focusing on other parts, such as the pres-
ence of a rider in case the animal is a horse. Image Perturbation [170] produces explanations
in the forms of saliency maps by blurring different areas of the image and checking which ones
most affect the prediction accuracy when perturbed. Similarly, the Restricted Support Region Set
(RSRS) Detection method [171] visualizes a set of size-restricted and non-overlapping regions
of an image that are critical to classification. This means that if any of them is removed, then
the image is wrongly classified. The explanation consists of the original image with its critical
regions determined by RSRS greyed out. The I VisClassifier [172] is based on linear discriminant
analysis (LDA). It attempts at reducing the dimension of the input data and produces heat-maps
that gives an overview of the relationship among clusters in terms of pairwise distances between
cluster centroids both in the original and reduced dimensional spaces. The Saliency Detection
method [173] utilizes a U-Net neural network trained to generate a saliency map, in a single
forward pass, for any image and classifier received as inputs. The output map then highlights the
parts of the image that are considered important by the classifier.

Some methods use other visual aids, like graphs and scatter-plots, to generate visual expla-
nations. The Sensitivity Analysis method [174] generates explanations that correspond to local
gradients. These gradients indicate how a data point must be moved to change its predicted la-
bel. The explanations can be either scatter-plots of the gradient vectors or heat-maps showing
which parts of the inputs must be modified to change the predicted class. Individual Conditional
Expectation (ICE) plots [175] are line charts graphing the functional relationship between a pre-
dicted response and a feature for each individual observation when keeping all the other features
fixed and varying the value of the feature under analysis. [176] proposed two alternatives to ICE
plots, called Partial Importance (PI) and Individual Conditional Importance (ICI) plots, which
visualize the feature importance rather than its prediction. Both plots are aimed at showing how
changes in a feature affect model performance. PI works at the global level by visualizing the
point-wise average of all ICI curves across all observations, whereas ICI works at the local level
by presenting changes for each observation. The importance of each feature is assessed using the
Shapley Feature Importance measure which fairly distributes the model’s performance among
them according to their marginal contribution. Explanation Graph [177] is based on the pertur-
bations of the input features. It works by training a model on both the original and the perturbed
data. Subsequently, a comparison of the original and perturbed input-output pairs is performed to
infer causal dependencies between input and output. This method was tested across several word
sequence generation tasks in Natural Language Processing (NLP) applications. The perturbed
input contains statements that are semantically similar to the originals but differ in some elements
(words and punctuation) and their order. The inferred dependencies are shown in graphs where

20
the nodes contain the words of the original and perturbed inputs and their relative outputs and the
edges represent the connections between them. A Worst-Case Perturbation [178] corresponds in-
stead to the smallest perturbation such that the perturbed input leads to an incorrect answer with
high confidence. This method was applied only to images and the explanation consists of the
perturbed images. Class Signatures [179] is a visual analytic interface that allows end-users to
detect and interpret input-output relationships by presenting a mix of charts (line, bar charts and
scatter plots) and tables organised in such a way that relationships become evident. Similarly,
ExplainD [180] was designed to explain predictions made by classifiers that use additive evi-
dence, such as linear SVMs and regressors. The graphs produced by this method represent the
contribution of each feature to the prediction and how the prediction changes when the value of
a feature varies across their value ranges. Manifold [181] and MLCube Explorer [182] are two
visual analytical tools that provide comparative analysis for multiple models. They also enable
end-users to define instance subsets using feature conditions, to identify instances that gener-
ate erroneous results so to explain potential reasons of these errors, and to iteratively refine the
performance of a model by using different graphical aids such as scatter-plots, bar and line charts.

   
   

(6) Origin

  

Hest Map Image

set gaten| eae (te arse asset

Ly Siime'sdee

(€) Support Region 1 (8) Suppore Region 2

 

States ey ese nat

 

(a) Explanation Graph [177] (b) RSRS [171] (c) SpRAy [8] (d) PI and ICI plots[ 176]

Figure 7: Examples of visual explanations generated by model-agnostic methods as (a) graphs, (b) restricted support
regions, (c) heat-maps, or (e) plots.

6.2.4. Mixed explanations

There are many methods for explainability that produce numerical explanations along with
graphical representations to make them more interpretable for lay people (see table A.4 and fig-
ure 8). The Functional ANOVA decomposition [183] quantifies the influence of non-additive
interactions within any set of input variables and depict them with Variable Interaction Network
(VIN) graphs where the nodes represent the variables and the edges the interactions. The Justi-
fication Narratives method for explainability [184] consists of a simple model-agnostic mapping
of the essential values underlying a classification (identified with any feature selection method)
to a semantic space that automatically produces these narratives and realizes them visually (as
bar-charts reporting the assessed relevance value of each variable) or textually. ExplAIner [133]
and Rivelo [185] are two user interfaces showing mixes of numerical, visual and textual expla-
nations. ExplAIner was designed to display visual and textual explanations of ML models which
are the outcome of an iterative workflow of three stages: model understanding, diagnosis, and
refinement. Using TensorBoard (a visualization tool developed by Google for machine learning)
as a starting point, ExplAIner produces an interactive graph view of the model to be explained.
The nodes of the graph represent the model’s components, such as inputs, parameters and out-
puts, accompanied by textual definitions, and the edges represent the relationships between the

21
components. There are also other visual explanatory tools in support of the model’s graph, such
as line-charts of metrics, like loss and accuracy, and examples of input data together with their
relative heat-maps generated with other visual methods for explainability. Rivelo works ex-
clusively with binary classification problems and binary input features. It enables end-users to
understand the causes behind predictions by interactively exploring a set of visual and textual
instance-level explanations which lists the most relevant input features (words or image areas in
a document/image), their frequency, number of instances with the feature with positive labels
and are correctly/wrongly classified.

Other mixed explanations-based methods utilize a selection of prototypes, which are samples
from the input that are correctly predicted by the model and can be considered as positive and
iconic examples, or adversarial examples, which are samples misrepresented by the model and
are used to generate contrastive explanations (see Section 5.1). This subset helps end-users un-
derstand the model by leveraging on the human ability to induce principles from a few examples.
Being a subset of a training dataset, these explanations were classified as mixed as their for-
mat depends on the nature of the input data. The Bayesian Teaching methods for explainability
[186] selects a small subset of prototypes that would lead the model to the correct inference as
if trained on the overall dataset. [187] proposed to use Sequential Bayesian Quadrature (SBQ)
in conjunction with Fisher kernels to select salient training data points. All the instances in a
training dataset are firstly embedded in the space induced by the Fisher kernels. This provides
a way to quantify the closeness of pairs of instances which, if close enough, should be treated
similarly by a model. The embedded instances are inputted into SBQ, an importance-sampling-
based algorithm that estimates the expected value of a function under a distribution using discrete
samples drawn from it. Set Cover Optimization (SCO) [188] aims at selecting prototypes in such
a way that they capture the full structure of the training examples in each class of the dataset, no
points have a prototype of a different class in its neighbourhood and the prototypes are as few as
possible. This leads to a set cover optimization problem that can be solved approximately with
standard approaches such as, for instance, ‘linear program relaxation with randomized rounding’ .
Neighbourhood-Based Explanations [189] is based on a Case-Based Reasoning (CBR) approach.
It presents to end-users the entries of a training dataset that are the most similar to the new in-
put instance that needs to be explained. Similarity is measured through the Euclidean metrics
applied to all the input features. Adversarial examples are instead used in Evasion-Prone Sam-
ples Selection [190], Maximum Mean Discrepancy (MMD)-critic [191] and Pertinent Negatives
[192]. Evasion-Prone Samples Selection aims at detecting the instances closed to the classifica-
tion boundaries that can be easily misclassified if slightly perturbed whereas MMD-critic utilizes
the maximum mean discrepancy and an associated witness function to identify the portions of
the input space most misrepresented by the underlying model. Pertinent Negatives highlights
what should be minimally and necessarily absent to justify the classification of an instance. For
example, the absence of glasses is a necessary condition to say if a person has a good sight.
The input data are modified by removing some parts and the pertinent negatives are identified
as those perturbations that maximise the prediction accuracy. eventually, some methods for ex-
plainability produce mixed explanations by approximating a black-box model with simpler, more
comprehensible models that the end-users can inspect to assess the contribution of each feature.
Local Interpretable Model-Agnostic Explanations (LIME) [193, 134] explains the prediction of
any classifiers by learning a local self-interpretable model (such as linear models or decision
trees), sometimes referred to as ‘white-box’ modes, trained on a new dataset which contains in-
terpretable representations of the original data. These representations can be the binary vectors

22
representing the presence or absence of certain characteristics, such as words in texts or super-
pixels (contiguous patch of similar pixels) in images. The black-box model can be explained
through the weights of the white-box estimator which does not need to fully work globally, but it
should approximate the black-box well in the vicinity of a single instance. However, the authors
proposed the Sub-modular Pick (SP-LIME) to select, from an original dataset, a representative
non-redundant explanation set of instances that is a global representation of the model.

Soren

   
   

ona 0-103 gradients

Cerv2dt

   

 

einel bias

5 eal 9 r —_

(a) ExplAIner [133] (b) MMD-critic [191]

 

     

 

+.

 

 

 

 

 

 

 

es

 

Figure 8: Examples of mixed explanations generated by model-agnostic methods for explainability which consists of a
combination of visual and textual explanations in (a) interactive interfaces or (c) a selection of prototypes from inputs.

6.3. Model-specific methods for explainability based on neural networks

A considerable portion of the reviewed scientific articles about new methods for explainabil-
ity is focused on interpreting deep neural networks (DNNs). This is not surprising giving the
momentum of Deep Learning. Most of these methods produce visual explanations (table A.7),
mostly in the form of salient masks and scatter-plots (figure 9), some as other visual aids (figure
10), rules (table A.8 and figure 11), textual and numerical explanations (table A.9 and figure 12)
or a combination of them (table A.10 and figure 13).

6.3.1. Visual explanations as salient masks

CLass-Enhanced Attentive Response (CLEAR) [194] produces attention maps for image
classification applications by back-propagating the activation values of the output layer. CLEAR
was designed to return the attentive regions responsible for the prediction, along with their at-
tentive levels to understand their influence and the dominant output class associated with these
regions. DeepResolve [195] and GradCam [196] are two gradient ascent-based methods. Deep-
Resolve computes and visualizes intermediate layer feature maps that summarize how a network
combines elemental layer-specific features to predict a specific class. GradCam instead uses the
gradients of any target concept (say ‘dog’ for instance) flowing into the final convolutional layer
to generate a heat-map highlighting the influential regions in the image for predicting that con-
cept. Heat-maps are generated by the last convolutional layer because the fully-connected layers

23
do not retain spatial information and it is expected that it has the best compromise between
high-level semantics and detailed spatial information. Stacking with Auxiliary Features (SWAF)
[197] utilizes heat-maps generated by GradCam to interpret and improve stacked ensembles for
visual question answering (VQA) tasks. VQA includes answering a natural language question
about the content of an image by returning, usually, a word or phrase or, in this case, a heat-map
highlighting the relevant regions for a prediction. Guided BackProp and Occlusion [198] find
what part of an input (pixels in images or words in questions) the VQA model focuses on while
answering the question. Guided BackProp is another gradient-based technique to visualize the
activation values of neurons in different layers of CNNs. It computes the gradients of the prob-
ability scores of predicted classes but restricts negative gradients from flowing back towards the
input layer, resulting in sharper images showcasing the activation. Occlusion consists of mask-
ing, or occluding, subsets of an input (either a region of the image or a word of the question),
then forward propagating it through the VQA model and computing the change in the probability
of the answer predicted with the original input. A similar method, Occlusion Sensitivity [199],
maps those features considered relevant in the intermediate layers of a DNN, by projecting the
top nine activation values of each layer down to the input pixel space and masking the rest of
the image. Net2Vec [200] maps instead semantic concepts to corresponding individual DNN
filter responses. It returns images that are entirely greyed out except in the region related to a
semantic concept, such as for instance the area representing a door of a building. The pixels
of this region generate activation values that are above a threshold, corresponding to the 99.5th
percentile of the distribution of all the activation values. Inverting Representations [201] inverts
the representations of images produced by the inner layers and projects them on the input image
as heat-maps. A representation can be thought of as a function of the image that characterise the
image information. By reconstructing an approximate inverse function, it should be possible to
reproduce the representations built by the layers. This method is based on the hypothesis that the
layers consider only the relevant features and discard the irrelevant differences between images
(such as, for instance, illumination or viewpoint) and consists of a reconstruction problem solved
by optimizing an objective function with gradient descent.

Similarly, Guided Feature Inversion [202] generates an inversion image representation con-
sisting of the weighted sum between the original image and another noisy background image,
such as a grey-scale image with each pixel set to an average colour, a Gaussian white noise or a
blurred image. The weights are calculated in such a way to highlight the smallest area that con-
tains the most relevant features and to blur out everything else, especially things that might lead to
an erroneous prediction, like objects belonging to other classes. SmoothGrad [203] was designed
to sharpen in two ways gradient-based sensitivity maps, which are often visually noisy as they
highlight pixels that, to a human, seem randomly selected. The first approach considers an image
of interest along with sample similar images. The second approach generates a perturbed version
of the image of interest by adding Gaussian white noise. Both approaches generate individual
saliency maps with other methods for explainability such as GradCam, for instance, and take the
average of the resulting maps. Deep Learning Important FeaTures (DeepLIFT) [100] computes
the importance scores of features based on the difference between the activation of each neuron to
a ‘reference activation’ value, computed by propagating a ‘reference input’ through the network.
This represents a default or neutral input, such as a white image, chosen according to the problem
at hand. According to the authors, this difference-from-reference approach has two advantages
over the other methods producing saliency maps: (I) it can propagate importance signals even
when the gradient is zero, avoiding artifacts caused by discontinuities in the gradient and (II)

24
it can reveal dependencies missed by other approaches because it can separately consider the
effects of positive and negative contributions. Thus, the saliency maps produced by DeepLIFT
contains all and only the important features that support or go against a certain prediction. Sim-
ilarly, Integrated Gradients [93] attributes the prediction of a DNN to specific parts of the input.
The attribution is measured as the cumulative sum of the gradients of the classification function
representing the network calculated at all points along the straight-line path from a baseline input
(a black image or an empty text, for example) to a specific input instance.

Feature Maps [204] and Prediction Difference Analysis [205] produce respectively feature-
and heat-maps highlighting areas in an input image that gives evidence for or against a predicted
class. Feature Maps utilizes a loss function that pushes each filter in a convolutional layer to
encode a distinct and unique object part, exclusive of the object class under analysis. Predic-
tion Difference Analysis instead is based on Explain [150], which was designed to evaluate the
contribution of a feature at a time. In this case, a feature should correspond to a pixel of the
image, but the authors proposed to consider patches of pixels. The assumption is that the value
of each pixel is highly dependent on the surrounding pixels. The patches are overlapping so that,
ultimately, an individual pixel’s relevance is calculated as the average relevance of the different
patches it was in. Two studies proposed variations of LRP, namely LRP with Relevance Conser-
vation [206] and LRP with Local Renormalization Layers [207]. LRP was used in conjunction
with the Pixel-wise Decomposition methods for explaining the automated image classification
process of neural networks [101]. In both studies, the authors wanted to extend LRP to DNNs
with non-linearities, such as LSTM models that have multiplicative interactions within their ar-
chitecture [206] or networks with local renormalization layers [207]. [206] proposed a strategy
to back-propagate the relevance of the neurons in the output layer back to the input layer through
the two-way multiplicative interactions between lower-layer neurons of the LSTM. The algo-
rithm sets to zero the relevance related to the gate neuron and propagate the relevance of the
source neuron only. The extension of LRP proposed in [207] is based on first-Taylor expansion
for non-linearities in the renormalization layers. [98] proposed to generate saliency maps by
computing the first-order Taylor expansion of the function that links each pixel of an input image
to the function, representing the neural network, that assigns a probability score to each output
class.

Similarly, [208] analysed the use of Taylor decomposition for interpreting generic multi-layer
DNNs by decomposing the network’s output classification into the contributions of its input el-
ements and back-propagating them from the output to the input layer, which are then visualized
as heat-maps. Receptive Fields [209] focused on visualizing the input patterns, called precisely
receptive fields, that are most strongly related to individual neurons by reconstructing these from
the highest activation values of each layer. PatternNet and PatternAttribution [210] aim at mea-
suring the contribution of the input ‘signal’ dimension, which is the part of the input that contains
information about the output class, to the prediction as well as how good the network is at fil-
tering out the ‘distractor’, which is the rest of the input (like the image background). PatterNet
yields a layer-wise back-projection of the estimated signal to the input space whereas PatternAt-
tribution produces explanations consisting of neuron-wise contributions of the estimated signal to
the classification scores. Relevant Features Selection [211] automatically identifies the relevant
internal features of a neural network via a two-step algorithm. First, a set of relevant layer/filter
pairs are identified for every class of interest by finding those pairs that reduce at the minimum
the differences between the predicted and the actual labels. This results in a relevance weight for

25
every filter-wise response computed internally by the network. Then, an image is pushed through
the network producing the class prediction and it generates a heat-map by taking into account
the internal responses and relevance weights for the predicted class. A combination of a Neural
Network and Case Base Reasoning (CBR) Twin-systems was proposed in [212]. This method
maps the features’ weights from the DNN to the CBR system to find similar cases from a training
dataset that explain the prediction of the network of a new instance. To extract the weights of
features, the authors proposed the Contributions Oriented Local Explanations (COLE) technique
which is based on the premise that the feature contributions to the model’s predictions are the
most sensible basis to inform CBR explanations. COLE uses saliency maps methods, such as
LRP and DeepLift, to estimate these contributions. This was tested on image classification prob-
lems with explanations generated in the form of similar images whose discriminating features
were highlighted by saliency maps. Compositionality [213] consists of building the meaning of
a sentence from the meanings of single words and phrases. This method is designed for visual-
izing compositionality in neural models trained for NLP tasks by plotting the salience value of
each word as saliency maps. The salience values indicate the contribution of the words to the
sentence meaning. For instance, the word ‘hate’ and ‘boring’ in the phrase ‘I hate the movie
because the plot is boring’ can be considered the two most relevant ones in a sentiment analysis
problem. The OpenBox method [214] computes exact and consistent interpretations for the fam-
ily of Piecewise Linear Neural Networks (PLNN) by transforming them into a mathematically
equivalent set of linear classifiers. Subsequently, each linear classifier is interpreted by the fea-
tures that dominate its prediction and the decision boundaries of each feature can be determined
and visualized as scatter-plots (for numeric inputs) or heat-maps (for images).

6.3.2. Visual explanations as scatter-plots

The Convolutional Neural Network Interpretation method (Cnn-Inte) [215] uses a two-level
k-means clustering algorithm to split into clusters the activation values of the neurons of hidden
layers relative to each input feature. Clusters might contain the activation values of instances
belonging to different classes. A random forest algorithm is then trained on each cluster. The
results are visually displayed using scatter plots to show how a specific test instance is classified.
[216] instead presented a method based on Principal Component Analysis (PCA) for analyz-
ing the variation of features generated by CNNs to scene factors that occur in images such as
object style, colour and lighting configuration. It analyzes CNN feature responses (or activa-
tion values) in the different layers by decomposing them as a linear combination of uncorrelated
components associated to the different factors of variation and visualizing them into scatter-
plots by using PCA. t-Distributed Stochastic Neighbor Embedding (t-SNE) maps [217] analyzes
Deep Q-networks (DQNs) in reinforcement learning applications, in particular for agents that
autonomously learn, for instance how to play video-games. This method extracts the neural
activation values of the last DQN layer and apply t-SNE for dimensionality reduction and for
generating cluster plots where each dot correspond to a particular learning phase. Similarly, Hid-
den Activity Visualization [218] uses t-SNE to visualize the projections of the activation values
of the hidden neurons as a 2D scatter-plot with points coloured according to the class of the
instances originating them. The distribution of the points in the scatter-plot gives a graphical
representation of the data distribution, relationships between neurons and the presence of clus-
ters in the activation values. Finally, TreeView [219] consists of a scatter plot representation of
a DNN via hierarchical partitioning of the feature space. Features are clustered according to the
activation values of the hidden neurons in such a way that each cluster comprised of a set of
neurons with similar distribution of activation values across the whole training set.

26
  
  

  

Question : What kind of bird is
perched on the sil?
| Predicted Answer: parrot

(b) Guided BP [198]

Query Nearest Neighbors of Query

 

 
     

ft:
# sae = oF

a so ‘ a e oe oe
(On eh
SERS bk

   

 

(a) GradCam [196] (c) Neural network-CBR(d) Compositionality (e) PCA [216] — (f) t SNE maps [217]
Twin system [212] [213]

Figure 9: Examples of visual explanations, as salient masks (a-d) and scatter-plots (e-f).

6.3.3. Visual explanations - miscellaneous

A few other methods use alternative visualization tools. Generative Adversarial Network
(GAN) Dissection [220] was designed to understand the inferential process of GANs at different
levels of abstraction, from each neuron to each object, and the relationship between objects, by
identifying units (or groups of units) that are related to semantic classes (doors, for example).
This method intervenes on them by adding or removing these objects from the image and ob-
serving how the GAN network reacts to these changes. These reactions are represented as a new
version of the input image where other objects or areas of the background are modified. For
instance, if a door is intentionally removed from a building, the GAN might substitute it with a
window or bricks. The Important Neurons and Patches method [221] analyzes the predictions of
a DNN in terms of its internal features by inspecting information flow through the network. For
instance, given a trained network and a test image, important neurons are selected according to
two metrics, both measured over a set of perturbed images (each pixel is multiplied by a Gaus-
sian noise): (I) the magnitude of the correlation between the neuron activation and the network
output which approximates the influence of each neuron on the output, and (IT) the precision of
the activation of a neuron, which estimates the generalizability of the feature(s) encoded by it,
by selecting those neurons whose activation values were not significantly affected by the per-
turbations. Given a rank of neurons, the top N are selected and their related image patches are
determined by using a multi-layered deconvolutional network and enclosed in bounding boxes
applied to the input image. [222] and [223, 224] proposed two similar methods, based on Acti-
vation Maximization, which modify the input images in such a way to maximise the activation
of a given hidden neuron with respect to each pixel. These modified images should provide a
good representation of what a neuron is doing. [225] instead presented a method to generate
Activation maps which show what features activate the neurons in the penultimate layers. It is
based on the idea that the final prediction of a DNN is dominated by the most highly-weighted
neuron activations of this layer. Shifting from pictorial to textual inputs, Cell Activation Val-
ues [226] is a method of explainability for LSTMs and uses character-level language models as
an interpretable test-bed for understanding the long-range dependencies learned by LSTMs by
highlighting sequences of relevant characters.

27
A group of methods that produce visual explanations in the form of graphs. The method pro-
posed in [136] generates data-flow graphs to visualize the structure of DNNs created and trained
in Tensorflow. Similarly, Explanatory Graph [227] produces graphs from CNNs where each node
represents a ‘part pattern’, which correspond to the peak activation in a layer related to a part of
the input, and each edge connects two nodes in adjacent layers to encode co-activation relation-
ships and spatial relationships between patterns. [228] instead added to CNNs a new Symbolic
Graph Reasoning (SGR) layer which performs reasoning over a group of symbolic nodes whose
outputs explicitly represent different properties of each semantic in a prior knowledge graph.
To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal
local-to-semantic voting module where the features of all symbolic nodes are generated by vot-
ing from local representations; b) a graph reasoning module that propagates information over
the knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping
module that learns new associations of the evolved symbolic nodes with local representations,
and accordingly enhances local features. Lastly, And-Or Graph (AOG) [229] is a method to
grow a semantic AOG on a pretrained CNN. An AOG is a graphical representation of the re-
duction of problems (or goals) to conjunctions (AND) and disjunctions (OR) of sub-problems
(or sub-goals). The AOG is used for parsing the part of the input images which corresponds to
a semantic concept and the output explanation consists of the input image where the semantic
part is included in a bounding box. Many scholars studied ways to exploit the visual explanatory
tools, described so far, to create interactive interfaces for the lay audience. For example, [230]
studied the usage of saliency maps as the building blocks of interactive interfaces to explain the
inferential logic of CNNs. ActiVis [231] is an interactive visualization system for DNNs that
unifies instance- and subset-level inspection by using flowcharts that show how neurons are acti-
vated by user-specified instances or instance subsets. Deep Visualization Toolbox [232] is based
on two visualization tools. The first one depicts the activation values produced, while processing
an image or video, on every layer of a trained CNN as heat-maps. The second tool modifies the
input images via regularised optimization methods to enable a better visualization of the learned
features by individual neurons at every layer. Deep View (DV) [233] measures the evolution
of a DNN by using two metrics that evaluate the class-wise discriminability of the neurons in
the final layer and the output feature maps. iNNvestigate [234] compares different methods for
explainability, namely PatternNet, PatternAttribution and LRP. LSTMVis [135] is a visual anal-
ysis tool for recurrent neural networks, LSTM in particular, that facilitates the understanding of
their hidden state dynamics. It is based on a set of interactive graphs and heat-maps of relevant
words. A user can select a range of text in the heat-maps, which results in the selection of a
subset of hidden states visualized in a parallel coordinate plot where each state is a data item,
time-steps are the coordinates, and the tool then matches this selection to similar patterns in the
dataset for further statistical analysis. Seq2seq- Vis [235] is similar to LSTM Vis but it focuses on
sequence-to-sequence models, also known as encoder-decoder models, for automatic translation
of texts. Seq2seq-Vis allows interactions with trained models trough each stage of the translation
process intending to identify the learned pattern, detect errors and probe the model with coun-
terfactual scenarios. Finally, N?VIS [236] is an interactive visualization tool for feed-forward
neural networks trained with evolutionary computation which allows end-users to adjust training
parameters during adaptation and to immediately see the results of this interaction. It considers
graphs representing the network topology, connection weights and activation levels for specific
inputs and weight volatility to facilitate the process of understanding the inferential process of a
neural network and to improve its performances in terms of efficiency and prediction accuracy.

28
a ESTHZ E68)

  

‘com =

(b) Activation (d) SGR [228] (e) Cell Activation [226] (f) Data-flow graphs [136]
Max [223]

Figure 10: Examples of miscellaneous visual explanations generated by methods for explainability for neural networks.

6.3.4. Rule-based explanations

Several methods for explainability are focused on rule-based explanations of the inferential
process of neural networks, usually in the form of IF-THEN rules. Scholars divided these meth-
ods into three classes [41, 132]: (J) decompositional methods work by extracting rules at the level
of hidden and output neurons by analysing the values of their weights, (ID pedagogical methods
treat an underlying neural network as a black-box and the rule extraction consists of mimicking
the function computed by the network; weights are not subjected to analysis, and (III) eclectic
methods that are a combination of the decompositional and pedagogical ones.

Regarding the decompositional methods, Discretizing Hidden Unit Activation Values by
Clustering [237] generates IF-THEN rules by clustering the activation values of hidden neu-
rons and replacing them with the cluster’s average value. The rules are extracted by examining
the possible combinations in the outputs of the discretised network. Similarly, Neural Network
Knowledge eXtraction (NNKX) [238] produces binary decision trees from multi-layered feed-
forward sigmoidal artificial neural networks by clustering the activation values of the last layer
and propagating them back to the input to generate clusters. Interval Propagation [141] is an
improved version of Validity Interval Analysis (VIA) [239] to extract IF-THEN crisp and fuzzy
rules. VIA consists of finding a set of validity intervals for the activation range of each unit (or a
subset of units) such that the activation values of a DNN lie within these intervals. The precon-
dition of each extracted rule is given by a set of validity intervals and the output is a single target
class. According to [141], VIA has two shortcomings: it fails sometimes to decide whether a rule
is compatible or not with the network and the intervals are not always optimal. Interval Propa-
gation overcomes these limitations by setting intervals to either the input or output and feed- or
back-propagating them through the network. However, this method has still a drawback. Some
neural networks require a big number of crisp rules to be approximated and to reach similar per-
formances in terms of prediction accuracy. Then, [141] proposed to compact these crisp rules
into fuzzy rules by using a fuzzy interactive operator which introduces the OR operators between
rules. Discretized Interpretable Multi-Layer Perceptrons (DIMLP) [139, 240, 241, 132] returns
symbolic rules from Interpretable Multi-Layer Perceptrons (IMLP) which are CNNs where each

29
neuron of the first hidden layer is connected to only an input neuron and its activation function is
a step function while the remaining hidden layers are fully connected with a sigmoid activation
function. In DIMPL, the step activation function becomes a staircase function that approximates
the sigmoid one. The rule extraction is performed after a max-pool layer by determining the
location of relevant discriminative hyperplanes, which are the boundaries between the output
classes. Their relevance corresponds to the number of points passing through each hyperplane as
they move to a different class. An example of a ruleset generated with DIMLP from a neural net-
work with thirty neurons, represented as x; with i = 1,...,30, in a unique hidden layer and three
output neurons is: Rule 1 - (+x3) (4x8) (417 > 0.0061) (419 < 0.151) (42; > 0.065) Class_l,
Rule 2: (417 > 0.0061) (x2; < 0.065) Class2, Default: Class_3. Rule Extraction by Re-
verse Engineering (RXREN) [242] relies on a reverse engineering technique to trace back input
neurons that cause the final result, whilst pruning the insignificant ones, and to determine the data
ranges of each significant neuron in respective classes. The algorithm is recursive and generates
hierarchical rules where conditions for discrete attributes are disjoint from the continuous ones.
Rule Extraction from Neural Network using Classified and Misclassified data (RxNCM) [243] is
a modification of RxREN. It incorporates also the input instances correctly classified in the range
determination process, not only the misclassified ones as done by RXREN. Most of the rule-based
methods for explainability are monotonic, that means they produce an increasing set of rules, thus
the prepositions that can be derived. However, sometimes adding new rules might lead to the in-
validation of some conclusion inferred by other rules, as in [244] where a method that captures
non-monotonic symbolic rules coded in the network was presented. The rule extraction algorithm
starts by partially ordering the vectors of a training dataset according to the activation values of
the output layer. Then, it determines the minimum input point that activates an output neuron
and creates a rule whose antecedents are based on the feature values of the selected instance.
Thus, the expected set of rules has the following form: L1,...,£n,~ Lnu,---.~ Ln 2 Lei
where L;(1 < i < m) represents a neuron in the input layer, L,,,; represents a neuron in the
output layer, ~ stands for default negation and — means causal implication. Finally, [245] and
[246] proposed two algorithms that extract DTs from the weights of a DNN. The former method
produces a soft DT trained by stochastic gradient descent using the predictions of a neural net-
work and its learned filters to make hierarchical decisions on where to split the data and how to
create the paths from the root to the leaves. The latter, which is designed only for image clas-
sification tasks, aims at explaining an underlying CNN semantically, meaning that the nodes of
the tree should correspond to parts of the objects that can be named. Nodes near the root should
correspond to parts shared by many images (such as the presence of four legs in images show-
ing animals) whereas the nodes close to the leaves should represent characteristics of minority
images (a peculiar characteristic of each animal). To build such DTs, the network’s filters are
forced to represent object parts by a special modification of the loss function. The DT is then
built on the part/filter pairs recursively on an image by image basis.

In regard to the pedagogical methods, Rule Extraction From Neural Network Ensemble
(REFNE) [247] extracts symbolic rules from instances generated by neural network ensembles.
The algorithm randomly selects a categorical attribute and checks if there is a value satisfying the
condition that all the instances possessing such a value fall into the same class. If the condition is
satisfied, a rule is created with the value as antecedent; otherwise, the algorithm selects another
categorical attribute and examines all the combinations of the two attributes. When all the cat-
egorical attributes are analysed, continuous attributes are considered and the process terminates
when no more rules can be created. Rules are limited to only three antecedents. Continuous

30
attributes are discretised and a fidelity evaluation mechanism checks that this process does not
compromise the relationship between the attribute and the output classes. An alternative method
to extract IF-THEN rules from neural network ensembles, called C4.5Rule-PANE [248], uses
the C4.5 rule induction algorithm. After a neural network ensemble was trained on a dataset,
the original labels of the training dataset are replaced by those predicted by the ensemble. Sub-
sequently, C4.5Rule-PANE extracts a ruleset from the modified dataset to mimic the inferential
process of the ensemble. The DecText method [249] also extracts high fidelity DTs from a DNN.
It sorts input instances by increasing the value of a feature and split an input dataset by placing
the cutpoint at the midpoint of the range. Then, four splitting algorithms check the partitions
such created and choose the best ones according to four criteria. The first, SetZero, chooses the
most discriminative features of the target variable. The other three, SSE, ClassDiff and Fidelity,
respectively select the feature which maximizes the possibility that a single class dominating
each partition is created, the quality of the partition and the fidelity between the DNN and the
tree. According to the authors, these algorithms have comparable, if not better, prediction perfor-
mance that a tree extraction technique based on entropy gain split. TREPAN [250, 251] induces a
DT that, like DecText, maintain a high level of fidelity to a DNN while being comprehensible and
accurate. It queries an underlying network to determine the predicted class of each instance and
selects the splits for each node of the tree by using the ‘gain ratio criterion’ and by considering
the previously selected splits that lie on the path from the root to that node as constraints. Tree
Regularization [252] consists of a penalty function of the parameters of a differentiable DNN
which favours models whose decision boundaries can be approximated by small binary DTs. It
finds a binary DT that accurately reproduces the network’s prediction and measures its complex-
ity as the ‘average decision path length’. It then maps the parameter vector of each candidate
network to an estimate of the average-path-length and chooses the shortest one. Word Impor-
tance Scores [253] visualizes the importance of specific inputs for determining the output of a
LSTM. By searching for consistently important phrases and calculating their importance scores,
the method extracts simple phrase patterns consisting of one to five words. To concretely validate
these patterns, they are inputted to a rule-based classifier which approximates the performance
of the original LSTM. Iterative Rule Knowledge Distillation [254] and Symbolic Logic Inte-
gration [255] are the only ante-hoc methods producing rule-based explanations for DNNs. The
former combines DNNs with declarative first-logic rules to allow integrating human knowledge
and intentions into the networks via an iterative distillation procedure that transfers the structured
information of logic rules into the weights of DNNs. This is achieved by forcing the network to
emulate the predictions of a rule-based teacher model and evolving both models throughout the
training. The latter instead encodes symbolic knowledge in an unsupervised neural network by
converting background knowledge, in the form of propositional IF-THEN rules and first-order
logic formulas, into confidence rules which can be represented in a Restricted Boltzmann Ma-
chine.

6.3.5. Textual and numerical explanations

Textual and numerical explanations are jointly less used than other methods for explainabil-
ity. InterpNET [256] makes use of a DNN to generate natural language explanations of clas-
sifications done by external models and produces statements like “This is an image of a Red
Winged Blackbird because...”. The explanations are built upon the activation values of this net-
work. Most-Weighted-Path, Most-Weighted-Combination and Maximum-Frequency-Difference
[257] are three methods for explainability that generates textual statements. Most-Weighted-Path
starts from the output neuron and selects the corresponding input passing, layer-by-layer, via the

31
  
 
 

        
   
 
 

 

  

= Ro apa ength is great than approximately 2.160 : Twice Vint fad the bas
i dh is greater than approximately 0.770 ni eT waflles with fresh whipped
me 92245870 12388 ‘OR cream , they wee AMAZIN !
ie la greater than approximately 1.465
oo es act Perfect ex acs

greater than approximately | 282 the terrace
ewe call. me SPOIed this susti is
» gross and the orange chi

well itwas so thinidon “t think ithad chicken

greater than approximately 1.438 init. go somewhere else

 

fer than approximately 1.201

 

n
‘ex than approximately 1.874

   

(a) DT extraction [245] (b) Interval Propagation [141] (c) Word Importance Scores [253]

Figure 11: Examples of rule-based explanations generated by methods for explainability for neural networks and visual-
ized as (a) decision trees, (b) list of rules or (c) by showing the most relevant input.

neuron connected with the highest weight. Then, it auto-generates a natural language expla-
nation indicating the most relevant feature for predicting the output category. Most-Weighted-
Combination works similarly, but it selects the two most-weighted input features. Maximum-
Frequency-Difference retrieves from the training dataset the most similar cases for each instance,
then perform the difference between the percentages of cases that have the same output and those
with different output. The explanation is generated according to the input with the highest dif-
ference and it is a statement like “the smart kitchen estimates that you are sad because you are
eating chocolate, which is 50% more frequent in this emotional state than people in other emo-
tional states”. Rationales [258] is a method for justifying the predictions made by DNNs in
natural language processing tasks, such as sentiment analysis, by extracting pieces of the input
text as justifications, or rationales. These rationales must contain words that are interpretable and
lead to the same prediction as to the entire input text. These words are selected via the combina-
tion of a ‘rationale generator’ function, which works as a tagging model assigning to each word
a binary tag saying whether it must be included in the rationale, and an ‘encoder’ function that
maps a string of words to a target class. Relevance and Discriminative Loss [259, 137] generates
textual explanations for an image classifier like “The bird in the photo is a White Pelican be-
cause...”. It consists of a CNN that extracts visual features from the images, such as colours and
object parts, and two LSTMs which produce a description of each image conditioned on visual
features. The training process aims at reducing two loss functions, called respectively ‘Rele-
vance’ and ‘Discriminative’, which assure that the generated sentences are both image relevant
and category-specific.

A few methods for explainability produce pure numerical explanations. Concept Activation
Vectors (CAVs) [145] separates the activation values of a hidden layer relative to the instances
belonging to a class of interest from those generated by the remaining part of the dataset. Then
it trains a binary linear classifier to distinguish the activation values of the two sets and compute
directional derivatives on this classifier to measure the sensitivity of the model to changes in
inputs towards the class of interest. This is a scalar quantity calculated over the whole dataset
for each class. Probe [144] consists of a linear classifier is fitted to a single feature learned
by each layer of a DNN to predict the original classes. The numeric explanations consist of
the predictions made by the Probes. Singular Vector Canonical Correlation Analysis (SWCCA)
[260] returns the correlation matrix of the neurons’ activation vectors, calculated over the entire
dataset, of two instances of a given DNN trained separately. The first network’s instance is
obtained at the end of the training process, whereas the latter consists of multiple snapshots of
the network during training. Every layer of the first instance is compared to every other layer

32
of the other instance to calculate correlation factors between pairs of layers. Causal Importance
[261] is calculated by summing up the variations in the output when the values of a variable are
perturbed instance by instance whilst all the other variables are kept fixed. Firstly, the algorithm
suppresses the irrelevant variables by measuring their predictive importance via a metric based
on the absolute difference between the predictions made by a DNN with the original and the
perturbed variables. The network is then trained with the relevant variables only and data are
clustered according to their hidden layer representation. This is done by training an unsupervised
Kohonen map on the matrix containing the activation values of each pair neuron/input instance.
Finally, causal importance is measured on a cluster by cluster basis, meaning that it is calculated
only on the instances belonging to the cluster under analysis. [262] proposed to measure the
Contextual Importance and Contextual Utility of input on the output variable. The former metric
is the ratio between the range of the output values covered by varying a variable throughout its
own range of values and the whole output space. For example, a neural network was trained to
predict the price of a car over a set of variable, among which there is the engine size. By varying
the engine size alone, the price varies only within a certain range. Contextual Utility represents
the position of the actual output within Contextual Importance. So the price of cars with big
engines, produced by the same manufacturer, are towards the upper end of the manufacturer’s
price range. The ideal values for these two metrics are domain-dependent. They are designed
just to help end-users understand where each variable and instance lie within the input space,
but they can be used to generate rule-based or textual explanations by structuring the domain
in intermediate concepts which attach a positive or negative outlook to certain subsets of the
output space. A certain price range can be deemed very good for a manufacturer and all its cars
in that range can be considered as a deal. Finally, REcurrent LEXicon NETwork (RELEXNET)
[263] combines the transparency of lexicon-based classifiers with the accuracy of recurrent neural
networks. Lexicons are linguistic tools for classification and feature extraction which take the
form of a list of terms weighted by their strength of association with a given class. RELEXNET
uses lexicons as inputs of a naive gated recurrent neural network which returns the probability
that the input belongs or not to a certain class.

   
 

(a) InterpNET [256]
Preference value

 

absmax.

SAAB 900 $ 2.0-16 i,
dynav (ej) af eeeroasss ssa w an ono soso ness sesescecececesseceeece }

nw 5 ——__

 

——

 

 

 

 

dynminjc)) ——~
absmin e900 132340 173780 218220 256660 298100",
Price
(b) Contextual Importance and Utility [262] (d) CAVs [145]

Figure 12: Examples of textual (a) and numerical (b-d) explanation generated by a method for explainability for neural
networks.

33
6.3.6. Mixed explanations

To help end-user interpret a model, some scholars have proposed the use of multiple types
of output formats for explanations. The Attention Alignment method [264] produces explana-
tions in the form of attention maps. These maps highlight parts of a scene that mattered to a
control DNN utilized in self-driving cars in combination with a perception DNN. The perception
DNN combines the data received from cameras and other sensors, like radars and infrared, to
‘understand’ the environment and to generate manoeuvring commands like steering angles or
braking. The control DNN is trained to identify the presence of specific objects, such as road
signs, and obstacles like pedestrians and bikers, that influence the output of the perception net-
work. Attention Alignment consists of an attention-based video-to-text algorithm that produces
textual explanations of the model predictions such as “The car heads down the street because it
is clear”? Similarly, Pointing and Justification Model (PJ-X) [265] and Image Caption Genera-
tion with Attention Mechanism [147] are two multi-modal methods for explainability, designed
for VQA tasks, that provides joint textual rationale generation and attention-map visualization.
The attention-maps are extracted from a CNN, which performs the object recognition in im-
ages, whereas the textual justifications are produced by an LSTM network as image captions.
The word(s) in the caption related to the attention region is underlined. According to [265], the
two explanations support each other in achieving high quality. The visual explanations help to
generate better textual explanations which lead to better visual pointing. Image Caption Gener-
ation with Attention Mechanism is based on two algorithms: (D a ‘soft’ deterministic attention
mechanism trainable by standard back-propagation methods and (ID) a ‘hard’ stochastic atten-
tion mechanism trainable by maximizing an approximate variational lower bound. The word(s)
in the caption related to the attention region is/are underlined. [266] and [267] proposed two
methods for replacing a DNN with a deterministic finite automaton that can be visualized as a
graph where each node represent a cluster of values in the output space and the edges represent
the presence of shared patterns in a network’s internal layers between these clusters. Lastly, the
method in [268] uses prototypes to explain the prediction of a new instance. The prototypes
are selected according to the activation values of hidden neurons related to training data. For a
new observation, it is possible to foresee and justify its prediction by identifying the three most
similar training samples based on cosine distance of their hidden activation values.

6.4. Model-specific methods for explainability related to rule-based systems

Explainable Artificial Intelligence was ignited by the interpretability problem of machine
learning, in particular of the deep learning models. However, the problem of explainability ex-
ists even before the advent of neural networks. Many rules-based learning approaches already
existed and the majority of these were interpreted with ante-hoc methods that act during the
model training stage to make it naturally explainable (see table A.11 and figure 14). An Ant
Colony Optimization (ACO) algorithm was proposed in [269] to create interpretable rules. It fol-
lows a sequential covering strategy, one-rule-at-a-time or also known as separate-and-conquer,
to generate unordered sets of IF-THEN classification rules which can be inspected individually
and independently from the others, thus they are easier to be interpreted. At each step, ACO
creates a new unordered set of rules and compare it with those of previous iterations. If the
new set contains fewer rules or has a better prediction accuracy, it replaces the previous one.
Experiments run on thirty-two publicly available datasets showed that ACO gave the best re-
sults in terms of both predictive accuracy and model size, outperforming state-of-the-art rule

34
induction algorithms with statistically significant differences. Another method based on an ACO
algorithm, called AntMinter+ [270], constructs monotonic rulesets by allowing the inclusion of
domain knowledge via the definition of a directed acyclic graph representing the solution space.
The nodes at the same depth in the graph represent the splitting values related to an input vari-
able; the edges represent which values of the following variable can be reached from a node.
AntMinter+ uses an iterative max-min ant system to construct a set of IF-THEN rules, starting
from an empty set. A rule represents a path from the start to the end nodes of the graph. The algo-
rithm stops adding rules when either a predefined percentage of training points is covered or when
the addition of new rules does not improve the performance of the classifier. AntMinter+ can be
combined with a non-linear SVM, in a method called Active Learning Based Approach (ALBA),
to generate comprehensible and accurate rule-based models. Exception Directed Acyclic Graphs
(EDAGs) [271] is an empirical induction tool that generates rules from the knowledge base of
expert systems to create comprehensible knowledge structures in the form of graphs where nodes
are premises, some of which have attached conclusions, leaves are conclusions and edges repre-
sent exceptions to some node. The ‘meaning’ of each node can be easily determined by following
its path back to the root and by inspecting its child nodes, whilst the rest of the graph is irrelevant.

 

 

ex — Vehicle controller ————- | Explanation generator u

(acceleration, change of course) | attention alignment t (Textual descriptions+ explanations)

 

 

Vehicle
Controller's
Attention map

&xample of textual descriptions + explanations:
Ours: “The car is driving forward + because there are no other cars in its lane”
Human annotator: “The car heads down the street + because the street is clear.”
(a) Attention Alignment [264]

Q: Is this a zoo?

 

 

‘Awomanis throwing a frisbee na park. A dog is standing on a hardwood floor.

 

... because the zebras are __... because there are

 

A ile git siting ona bea wtn
standing in a green field. animals in an enclosure 2 e007 ea
(b) PJ-X [265] (c) Attention Mechanism [147]

Figure 13: Examples of mixed explanations, consisting of combinations of images and texts, generated by the following
methods for explainability for neural networks.

The Interpretable Decision Set [272] and the Bayesian Rule Lists (BRL) [273, 274, 275] are
two methods that creates unordered sets of IF-THEN rules. Interpretable Decision Set is based
on an objective function that simultaneously optimizes accuracy and interpretability by learning
short and non-overlapping rules that cover the whole feature space and pay attention to small
but important classes. BRL produces a posterior multinomial distribution over permutations of

35
rules, starting from a large set of possible rules, to assess the probability of predicting a certain
label from the selected rules. The prior is the Dirichlet distribution and the permutation that max-
imises the posterior is included in the final decision set. The Bayesian Rule Sets (BRS) method
[276, 277] is similar to BRL but it uses different probabilities, with the posterior as a Bernoulli
distribution, and the prior a Beta distribution whose parameters can be adjusted by end-users to
guide BRS toward more interpretable solutions by specifying the desired balance between size
and length of rules. First Order Combined Learner (FOCL) [278] inductively constructs a set of
rules in terms of predicates used to describe examples. Each clause body consists of a conjunc-
tion of predicates that cover some positive and no negative examples. The rules are displayed
in a tree where the nodes are the predicates, the edge are the conjunctions and the leaves are
the conclusions. Non-monotonic argumentation-based approaches for increasing explainability
and dealing with conflictual information were proposed in [142, 143, 279]. They are based upon
the concepts of defeasible arguments, in the form of rules, each composed by a set of premises,
an inference rule and a conclusion as well as the notion of attacks between arguments to model
conflictuality and the retraction of a final inference. Argumentation-based approaches are be-
lieved to have a higher explainability as the notions of arguments and conflictuality are common
to the way human reason. Four methods based on fuzzy reasoning to generate interpretable
sets of rules that clearly show the dependencies between inputs and outputs were presented in
[280, 281, 282, 283]. Both [280, 283] examine the interpretability-accuracy tradeoff in fuzzy
rule-based classifiers. A multiobjective fuzzy Genetics-Based Machine Learning (GBML) al-
gorithm, analyzed by [280], is implemented in the framework of evolutionary multiobjective
optimization (EMO) and consists of a hybrid version of Michigan and Pittsburgh approaches.
Each fuzzy rule is represented by its antecedent fuzzy sets as an integer string of fixed length
and the resulting fuzzy rule-based classifier, consisting of a set of fuzzy rules, is represented as
a concatenated integer string of variable length. Multi-Objective Evolutionary Algorithms based
Interpretable Fuzzy (MOEAIF) [283] consists instead of a fuzzy rule-based model engineered
to classify gene expression data from microarray technologies. GBML and MOEAIF maximize
the accuracy of rule sets, measured by the number of correctly classified training pattern, and
minimize their complexity, measured by the number of fuzzy rules and/or the total number of
antecedent conditions of fuzzy rules. The method in [281] is based on a five-step algorithm.
First, it generates fuzzy rules that cover the extrema directly from data. Second, it checks rule
similarity to delete the redundant and inconsistent rules. Third, it optimizes the rule structure
using genetic algorithms based on a local performance index. Fourth, it performs further training
of the rule parameters using gradient-based learning method and deletes the inactive rules. Last,
it improves interpretability by using regularization. The method presented in [282] generates
fuzzy rules by starting from a set of relations and properties, selected by an expert, of an input
dataset. It then extracts the most relevant ones employing a frequent itemset mining algorithm.
The authors do not provide a specific metric for evaluating the relevancy of a relation, but they
suggest using “measures like the number of relations and properties in the antecedent or the value
of their support”. Interpretable Classification Rule Mining CRM) [284] consists of a three-step
evolutionary programming algorithm producing comprehensible IF-THEN classification rules,
where comprehensibility is achieved by minimizing the number of rules and conditions. First, it
creates a pool of rules composed of a single attribute-value comparison. Second, it utilizes evolu-
tionary processes, designed to use only relevant attributes which are to discriminate a class from
the others and improve the accuracy of the ruleset, based on the Iterative Rule Learning (IRL)
genetic algorithm (also known as the Pittsburgh approach). IRL returns a rule per output class
with the exception of one class which is set as default. The third step optimizes the accuracy of
36
the classifier by maximising the product of sensitivity and specificity. Linear Programming Re-
laxation [285, 286] is a method for learning two-level Boolean rules in conjunctive normal form
(AND-of-ORs) or disjunctive normal form (OR-of-ANDs) as a type of human-interpretable clas-
sification model. A first version uses a generalization of a linear programming relaxation from
one level to two-level rules whose objective function is a weighted combination of the total num-
ber of errors and features used in the rule. In a second version, the 0-1 classification error is
replaced with the Hamming distance between the current rule and the closest rule that correctly
classifies a sample instance. The main advantages for explainability of the Hamming distance
is that it avoids identical clauses in the ruleset, thus repetitions, by training each clause with a
different subset of input instances.

All the above methods were intrinsically ante-hoc, but other methods exist for post-hoc ex-
plainability. For example, Mycin [3], probably the first method for explainability ever devel-
oped, is a backward chaining expert system based upon a knowledge-based of IF-THEN rules
composed by an expert, a database of the context set of facts that satisfy the condition part of
the rules, and an inference engine that interpret the rules. It also includes a natural language
interface that allows end-users to interact with the system independently of the expert by ask-
ing English questions, and the system can respond to them by using its inference engine and
performing the reasoning involved in composing an answer to them. In details, it searches for
facts that match the condition part of the productions that match the action part of the question.
This method allows the system to explain its reasoning and final inferences by using AND/OR
trees created during the production system reasoning process, thus showing an element of ex-
plainability. Similarly, the Sugeno-type fuzzy inference system proposed in [287] consists of
an explicit declarative knowledge representation of rules, which are fired at the same time by a
given input, and produce a final inference. Besides this, the system includes an explanatory tool
which shows a numerical representation of the input variables, the set of co-fired rules and an
English statement exposing the reasoning process. In an example taken from the application to
an Unmanned Aerial Vehicle (UAV) sent on a fight mission, is a statement like “UAV aborted
the mission because the weather was a thunderstorm and the distance from the enemy was too
close”. Another method, the Fuzzy Inference-Grams (Fingrams) [288] produces inference maps
of sets of fuzzy rules which graphically depict the interaction between co-fired rules and gives
support to detect redundant or inconsistent rules as well as it identifies the most significant ones,
by using network scaling methods that simplify the maps while maintaining their most important
relations. Fingrams also quantifies the comprehensibility of the ruleset, measured as the propor-
tion of the co-fired rules. The assumption is that the larger the number of rules which are co-fired
for a given input, the smaller the comprehensibility of the ruleset. ExpliClas [289] is a visual
interface designed to explain, in an instance-based manner, rule-based classifiers (such as those
algorithms extracting DTs from data, like C4.5 or CART, for instance) with visual and textual
explanations. The rules are graphically displayed as DTs and a natural language generation ap-
proach returns textual explanations of the fired rules. ExpliClas was tested on in the context of
recognising the role of basketball players from some physical characteristics and game statistics.
An example of textual explanation produced in this case is “The player is a Point-Guard because
he is medium-height and he has a small number of rebounds.” and it is accompanied by a graph
of the DT structure and bar-charts of the player’s information.

37
 

 

 

 

 

(b) ExpliClas [289] (c) Fingrams [288] (e) Fuzzy Inference Systems
[287]

Figure 14: Examples of explanations generated by methods for explainability for rule-based inference systems.

6.5. Other model-specific methods for explainability

Scholars proposed other methods for explainability that are not strictly based on neural net-
works or rule-based classifiers (see table A.12 and figure 15).

6.5.1. Ensembles.

Some methods were designed to interpret the logic followed by ensembles. [290] introduced
an algorithm to tweak the input features to change the output of a tree ensemble classifier. It
modifies a variable (or a set of variable) of an input instance by applying a linear shift, capped
to a global tolerance value, until all the trees in the ensemble assign it to another target class.
The delta between the original and the tweaked value represents the ‘effort’, or ‘tweaking cost’,
required to move the instance into the target class and provides a measure of the sensitivity of
the model to changes to that particular feature(s). This information can be used to rank the vari-
ables according to their tweaking costs and inform end-users on how a particular instance must
be modified to change its output label and at what cost this can be achieved. Three methods
for extracting a single DT from ensemble models, and for generating global explanations were
presented in [291, 292, 293]. In detail, [292] uses the solution obtained from combining the
several hypotheses (or models) of the ensemble as an oracle, and it selects the single hypothesis
that is most similar to the oracle. The similarity is measured according to three formal metrics:
‘6 — measure’ which determines the probability that both classifiers agree, ‘x — measure’ which
assesses the probability that two classifiers agree by chance and ‘Q — measure’ which assigns
values between 0 and | to classifiers that correctly predict the same input instances and values
between -1 and 0 to classifiers that commit errors on different instances. Instead, [293] proposed
to generate a tree from the ensemble by using a divide-and-conquer algorithm analogous to C4.5.
Similarly, [291] combined an opaque learning algorithm (random forest), with a more transpar-
ent and inherently interpretable algorithm (decision tree). On one hand, the opaque algorithm
represents the ‘oracle’ which search for the most relevant output. On the other hand, a natural
language generation approach is aimed at composing a textual explanation for this output which
is the interpretation of the inference process carried out by the correspondent decision tree, if the

38
output of both the learning algorithms coincides. [294] proposed instead a method to efficiently
merge a set DTs, each trained independently on distributed data, into a single tree to overcome
the lack of interpretability of the distributed models. The algorithm consists of three steps. First,
each DT is converted into a ruleset where each rule replicates a path from the root to a leaf and
defines a region in the output space. All the regions are disjoint and they cover the entire space.
In the second phase, the regions are combined by using a line sweep algorithm which sorts the
limits of each region and merges overlapping and adjacent ones. Finally, a DT is extracted from
the regions with an algorithm that mimics the C5.0 and uses the values in the regions as examples.

A similar approach is at the basis of the Factorized Asymptotic Bayesian (FAB) inference
method [295] which consists of a bayesian model selection algorithm that simplified and opti-
mized a tree ensemble. FAB estimates the model’s parameters and the optimal number of regions
of the input space (ensemble methods often splits the input space into a huge number of regions)
to derive a simplified model with appropriate complexity and prediction accuracy. inTrees [296]
extracts, prunes and selects rules from a tree ensemble. The algorithm starts from the set of all
the rules in the ensemble and excludes those covering a small number of instances. At each itera-
tion, the algorithm selects the rule with the minimum error and shorter condition, then it removes
the instances satisfying this rule from the dataset and it updates the initial ruleset according to
the instances left, by discarding rules that at this stage cover just a few, if not none, instances
and recalculating their error. Discriminative Patterns [297] aims at interpreting a random forest
model that classifies sentences according to their contents by extracting a ruleset that enables
interpretation and gains insight of useful information in texts which corresponds to discrimina-
tive sequential patterns of words, or sentences that determine the predicted class. Tree Space
Prototypes (TSP) [298] selects prototypes from a training dataset to explain the prediction made
be ensembles of DTs and gradient boosted tree models on a new observation. To measure the
similarity between the new instance and the prototypes, the authors proposed a metric based on
the weighted average of the number of trees in the ensemble that assigns the points to the same
output class to quantify the contribution of the predictions made by each DT to the overall pre-
diction. By following the path root-to-leaf of the most relevant DT, it is possible to determine the
values of the features deemed important by the tree for predicting the class of the new instance
and select a prototype having the same values.

6.5.2. Support Vector Machines.

ExtractRule [138] converts hyperplane-based linear classifiers, like SVMs, into a set of non-
overlapping symbolic rules which are human-understandable because they display, in a com-
pact format, the inferential process of the underlying classifier. An example of a rule extracted
from a classifier trained to distinguish between malign and benign tumors is “(Cell Size <
3) A (Bare Nuclei < 1) A (Normal Nucleoli < 7) = mass is benign”. Each rule can be
seen as a hypercube in the multidimensional space generated by the input variables with edges
parallel to the axis. To define these hypercubes, each iteration of this algorithm is formulated as
one of two possible optimization problems. The first formulation seeks to maximize the volume
covered by each rule whereas the second formulation maximizes the number of samples cov-
ered. Important support vectors and Border classification [146] are two methods for providing
insight into local classifications produced by a SVM. The former reports the most influential
support vectors for the final classification of a particular data instance, thus determining the most
similar instances to the test point that belong to the same class. As in the previous methods
based on prototypes, presenting this subset helps users understand the model by leveraging on

39
the human ability to induce principles from a few examples. The latter determines which fea-
tures of a testing data instance would need to be altered (and by how much) to be classified
on the separating surface between the two classes, thus providing, as in the feature tweaking
method, a measure of the cost required to change a model’s prediction. SVM-+Prototypes [299]
is based on a clustering algorithm to detect the prototype vectors for each class, after the decision
function is determined to employ a SVM. These vectors are combined with the support vectors
using geometric methods to define ellipsoids in the input space, which are later transferred to
IF-THEN rules as the mathematical equations that defined the ellipsoids, so a rule looks like “If
AX? + BXS + CX|X2 + DX, + EX, + F < G Then Class,”. Weighted Linear Classifier [300]
generates weighted linear SVM classifiers or random hyperplanes to obtain models whose ac-
curacy is comparable to that of a non-linear SVM classifier and whose results can be readily
visualized by projecting them on separating hyperplanes and decision surfaces. These projec-
tions are considered as a sort of explanations. A method based on Self-Organizing Maps (SOM)
used to visualise SVMs was proposed in [301]. It is a specific unsupervised network aimed at
investigating a high-dimensional space of data for a cluster of points by projecting these clusters
onto a 2-dimensional map, but trying to preserve their topologies. Thus, it allows visualising
both data and the SVM models, providing an overall overview of the support vector decision
surface which is not possible with other visualization approaches. [302, 303, 304] introduced a
method for automatically generating nomograms as the graphical tool for visual explanations of
the inferential mechanisms of SVM and naive bayesian-driven models. A nomogram is a two-
dimensional diagram designed to allow approximating graphical computation of mathematical
functions by showing a set of scales, one for each variable (dependent and independent) in an
equation. By drawing a line connecting specific values of all the scales related to the indepen-
dent variables, it is possible to calculate the value of the dependent variable from the intersection
point between the line and the variable’s scale. The advantages for explainability of nomograms
are simplicity of presentation and clear display of the effects of individual attribute values.

6.5.3. Bayesian and hierachical networks.

Explaining Bayesian network Inferences (EBD) [305] produces DT rules to show how the
variables of a bayesian network interact to make predictions. In detail, EBI explains the value
of a target node in terms of the influential nodes in the target’s Markov blanket which include
the target’s parents, children and the children’s other parents. Working backwards from the tar-
get node, EBI shows the derivation of each intermediate node and explains how missing and
erroneous values are compensated by displaying these causal relationships in a DT hierarchy.
[306] instead proposed an explanation method for understanding bayesian networks in terms of
scenarios. Narrative approaches to reasoning with legal evidence, for instance, are based on the
formulation of alternative scenarios which are subsequently compared according to two aspects:
the relations with the evidence and the quality that depends on the completeness, internal con-
sistency and plausibility of the scenario itself. The aim is to explain the content of the bayesian
network by reporting which scenarios were modelled and evaluating their evidential support and
quality. Probabilistically Supported Arguments (PSA) [307] is a two-phase method for extracting
probabilistically explanatory supported arguments from a bayesian network. In the first phase, a
support graph is constructed from a bayesian network representing the structure for a particular
variable of interest. In the second phase, given a set of observations, arguments are built from that
support graph. To do so, the algorithm defines a logical language and a set of rules built from the
support graph by following its edges and nodes. The parents of a node are the rule conditions, the
node itself is the rule’s outcome. Only the parents supported by pieces of evidence are consid-

40
ered. Then, the ASPIC+ framework for structured argumentation is instantiated. Arguments can
attack each other on the conclusion variable and defeat can be based on the inferential strength
of the arguments which can be computed with two types of measures: ‘incremental measures’
which assign a number to the weight of the evidence (the Likelihood Ratio is an example of
these measures) and ‘absolute measures’ which assign strength based on posterior probability,
such as the posterior for instance. Such arguments can help interpret and explain the relation-
ship between hypotheses and evidence that is modelled in the Bayesian network. Contribution
propagation [308] is a per-instance method for hierarchical networks that explain which compo-
nents of the input were responsible (and to what degree) for its classification. The central idea is
that a node was important to the classification if it was important to its parents, and its parents
were important to the classification. The contribution of each input component is visualized as
heat-maps.

 

   

 

 

 

 

 

s. et | sere | a aT
a.
zs
pont jee +=
B * ie
gon ie I
EG} 4? i ~ bol oD
- 1 Sow podtion
Yeu A x3 AAs ~9 xo OEY ==_—__————_aa={==—_"
4 Sm eee oo
o EH 8B ——
bts} 1s 7 oo) ELI ae

(a) Decision tree extraction [292]

B B “ “ uo

  
    

‘tire marks.after.S.curve.suggest 1oss.of control over_vehicle

a “ ™ “
true c false 7.992. 108

     
  
 
 

sic 100)"
a a oM(S37)*

 
     

2 yooM
sachny amy ‘= ae | [el [em
A
|
(b) Self-Organizing Maps [301] (d) PSA [307]

Figure 15: Examples of explanations generated by methods specifically designed to interpret ensembles (a), Support
Vector Machines (b-c) and bayesian networks (d).

6.6. Self-explainable and interpretable methods

Naturally interpretable models, sometimes referred to as ‘white-box models’, are ‘ante-hoc’
(see table A.13 and figure 16). Their output format depends on their architecture and inputs
format. Bayesian Case Model (BCM) [309] is a method for explainability for bayesian case-
based reasoning, prototype classification and clustering. BCM learns prototypes, corresponding
to the observations that best represent clusters in a dataset, by performing joint inference on
cluster labels, prototypes and important features. Gaussian Process Regression (GPR) [310] is
a powerful, but amenable to analysis, data-driven model. GPR is a non-parametric regression
algorithm, meaning that it does not make any assumption on the estimator function as linear and
logistic regression algorithms do, robust to missing data and interpretable because the weights
assigned to each feature provide a measure of its relevance. Generalized Additive Models [99]
and their extension with pairwise interactions (GA*Ms) [311, 131] are linear combinations of

4l
simple models, called ‘shape functions’, trained on a single feature (GAMs) or up to two fea-
tures (GA?Ms). Their simple structure allows end-user to easily understand the contribution of
individual features to the predictions and to visualize them, together with the shape functions,
with bar- and line-charts. Oblique Treed Sparse Additive Models (OT-SpAMs) [312] are in-
stances of region-specific predictive models. They divide feature spaces into regions with sparse
oblique tree splitting and assign local sparse additive experts to individual regions. Transparent
Generalized Additive Model Tree (TGAMT) [313] was proposed as an explainable and transpar-
ent method that uses a CART-like greedy recursive search to grow the tree. Multi-Run Subtree
Encapsulation, which comes from the genetic programming (GP) realm, was proposed in [314]
as a way to generate simpler tree-based GP programs. If the tree contains sub-trees of different
makeup but evaluating the same vector of results, they are to be considered as the same sub-tree.
This reduces, according to the authors, the complexity of the entire tree structure and the result-
ing expressions, in favour of explainability.

Probabilistic Sentential Decision Diagrams (PSDD) [315] can be described as circuit repre-
sentations where each parameter represents a conditional probability of deciding the input vari-
ables and each node is either a logical AND gate with two inputs or a logical OR gate with an
arbitrary number of inputs. The PSDD structure can be visualized as an easily-interpretable bi-
nary tree. Mind the Gap Model (MGM) [316] is a method for interpretable feature extraction and
selection. The goal is to split the observation into clusters while returning the list of dimensions
that are important for distinguishing them. The results are presented as a mix of numbers, which
are the relevance values of each dimension, texts and graphs that represent the dimensions them-
selves. For example, in a classification problem of images representing the four seasons, MGM
returns samples of images belonging to each class (spring, summer, autumn and winter) together
with the list of their relevant features (like snow, sun and flowers) and the relevance values of
each feature per target class (snow has a high relevance value for the class ‘winter’). Supersparse
Linear Integer Model (SLIM) [317] generates a scoring system from an input dataset by assign-
ing a score to each variable that contributes to the prediction. These scores are multiplied by a set
of coefficients inferred from the training dataset and then added, subtracted, and/or multiplied to
make a prediction. Scores are generated by minimising the 0-1 loss to reach a high level of accu-
racy and to produce a classifier that is robust to outliers by applying a £9 — penairy to encourage
a high level of sparsity and a set of interpretability constraints which restricts coefficients to a
user-defined set of meaningful and intuitive values. Eventually, Unsupervised Interpretable Word
Sense Disambiguation [318] produces interpretable word sense disambiguation models that cre-
ate clusters, or inventories, of words. For example, an inventory can be the collection of all the
words related to ‘furniture’ (such as table, chair and bed). The words are clustered according to
their co-occurrence and relative position in a text, where close words are assumed to be highly
correlated, and their syntactic dependency extracted from the Stanford Dependencies (represen-
tation of grammatical relations between words in a sentence.) The resulting word groups can be
interpreted at three levels: (1) word sense inventory where each sense of the word under analy-
sis is displayed as a separate network-graph where the nodes are the semantically related words
and the edges represent their semantic relationships. For example, the word ‘table’ is connected
to two networks corresponding to ‘furniture’ and ‘data’ senses; (2) sense feature representation
characterized by a list of sparse features (which consists of words) ordered by relevance; (3) re-
sults of disambiguation in context by highlighting the most discriminative words that caused the
prediction. Words like ‘cookie’ and ‘website’ indicate that ‘table’ refers to a collection of data
and not as a piece of furniture.

42
   
   

OL it Rain
0.7 if -Rain

(1 ifaain A sur
oO .

  
     
   

Pr(abow | 8,8)

(c) Equivalent distributior

 

sun bow “Sn 7Sun “Rov © Sen Row
(a) PSDD circuit (b) Verce

 

 

 

 

 

 

waren

 

(b) SLIM [317] (d) GA?Ms [311] (f) Unsupervised Interpretable Word
Sense Disambiguation [318]

Figure 16: Examples of ante-hoc methods for explainability designed to generate self-explainable models.

7. Evaluation of methods for explainability

The proposal of many methods for explainability pushed authors to focus also on their evalu-
ation. Different evaluation metrics were proposed and found in the literature as well as different
types of evaluation were conducted, as summarised in figure 17. A thorough review of these stud-
ies led to the identification of two main ways to evaluate methods for explainability, as shown in
figure 17.

e objective evaluations - it includes research studies that employed objective metrics
and automated approaches to evaluate methods for explainability;

e human-centred evaluations - it contains those studies that evaluated methods for ex-
plainability with a human-in-the-loop approach by involving end-users and exploited their
feedback or informed judgement.

The same dual categorisation system was suggested in [64], but they named the two classes
heuristic-based and user-based metrics. The former includes quantitative measures which con-
sist of mathematical entities such as, for example, the size of models [26, 88, 319, 320, 269].
This is a simple explainability indicator and it is based upon the assumption that the bigger the
size of a model, the harder it becomes for the users to understand it. However, this assump-
tion was proved false. One of the outcomes of the human-centred evaluation study conducted
in [88] was that users found some larger models to be more comprehensible than some smaller
models because larger models provided more classification-relevant information and users are
unlikely to accept weaker, simpler models when the underlying modelled concept is believed to
be complex. An alternative categorisation was presented in [2] with three classes: application-
grounded, functionally-grounded and human-grounded evaluation metrics where functionally-
grounded and human-grounded metrics respectively corresponds to the heuristic-based and user-
based metrics proposed in [64]. Application-grounded metrics assess the quality of machine-
produced explanations of data-driven models by comparing the increase in productivity of a few

43
> Distance function robot vs human plans
Explanation completeness
oe Image classification metrics
Objective .
Local risk assessment
Rules set cardinality
Sensitivity to input perturbation
Sensitivity to parameter perturbation
Human-centred
Text quality metrics

Trade off model accuracy/size

w2Z20O-4RP CrrPEm

Qualitative User preference metrics

“_. quantitative ” Productivity performance metrics

(10.64%) Rule-based classifiers Explanation completeness (4.88%)

\ Image classification metrics (7.32%)

\ Local risk assessment (4.88%)

Number of rules (9.76%)

Productivity performance metrics (12.20%)

(10.64%) Neural networks

  
   
 
 
 
 
  
 
 
   

(8.51%) Naive Bayes \

(17.02%) Model agnostic ensitivity to input perturbation (26.83%)

Sensitivity to model parameter
randomization (9.76%)

(10.64%) Learning systems Text quality metrics(4.88%)

   
   
 
 
 
 
  
  
   
       
 

Evaluation
(59 articles)

(12.77%) Decision t Trade off model’s accuracy - size (17.07%)
. ecision trees

(8.51%) Decision tables

(6.38%) Context-aware mT

(6.38%) Case-based reasoning
(6.38%) Bayesian networks

Autonomous agents (8.11%)
ontext-aware systems (16.22%)

Expert systems (10.81%)

(8.11%) Support vector machine Feature extraction and selection (8.11%)
(18.92%) Neural networks \ ‘isher vector classifiers (8.11%)
Model agnostic (10.81%) Learning systems (8.11%)

Figure 17: Classification of the approaches to evaluate methods for explainability (up) and distribution of the relative
scientific studies across categories (down).

users of these models when following these explanations instead of those produced by human
engineers, as done in [26, 2]. Because they involve humans, they are merged into the human-
grounded metrics.

7.1. Objective evaluations

Scholars proposed several metrics to evaluate, formally and objectively, the methods for ex-
plainability, listed in table 1. In the scientific literature, there is consensus that simpler learning
techniques, such as linear regression and DTs, can lead to more transparent inferences than
more complex techniques, such as neural network, as they are intrinsically self-interpretable
[26]. However, these simpler techniques usually do not lead to the construction of models with
the same level of accuracy than those induced by more complex learning techniques. The inter-
pretability of these models depends on many factors such as the learning algorithm, the learning
architecture and its configuration (hyper-parameters).

44
A few sale performance indicators were utilized as a formal metric to assess the increase
in productivity of the sales department when two methods of explainability, Explain and Ime
[150, 151], were applied to a complex real-world business problem of business-to-business sales
forecasting [321, 322]. The system was tested for a long period in a real-world company and the
sale performance indicators were monitored. The indicators showed that the forecasts based on
the Explain and Ime explanations outperformed initial sales forecasts, which supported the hy-
pothesis that data-driven explanations better facilitate unbiased decision-making than the mental
models of sale experts based upon their previous experience. Two quantitative evaluation met-
rics to assess the interpretability of methods generating visual explanations of a neural network
trained to classify images were presented in [30]. The first metric, filter interpretability, consid-
ers six types of semantics for CNN filters that must be annotated by humans on testing images
at the pixel level: objects, parts, scenes, textures, materials, and colours. The metric measures
the intersection areas between these annotations and the distribution of the activation values of a
filter over a heat-map. If there are overlapping areas, it can be said that the filter represents these
semantic concepts. The second metric, location instability, checks if a CNN locates the relevant
parts of the same object, shown in different images, at an almost constant relative distance, as the
distances between the parts of an object must be almost invariant. [256] proposed to use three
automated quantitative metrics, designed to assess the quality of text documents, to evaluate tex-
tual explanations automatically generated by methods for explainability: BiLingual Evaluation
Understudy (BLEU) that assesses the similarity of sentences based on the average percentage of
n-gram matches, Automatic NT Translation Metric (METEOR) that evaluates semantically the
similarity between words of sentences by using pre-trained word embeddings and Consensus-
based Image Description Evaluation (CIDEr) that compares sentences generated by neural net-
works to reference explanations written by humans by counting “Term Frequency—Inverse Doc-
ument Frequency’ weighted n-grams. A general evaluation metric for post-hoc methods for
explainability was presented in [323] based on the risk of generating unjustified ‘counterfactual
examples’ which are instances that do not depend on previous knowledge but are artefacts of the
classifier. This might happen when a model must predict an area not covered by the training set.
The algorithm that generates these examples applies the minimal perturbation that changes the
predicted classes of observation in such a way that it is still connected to the input data and avoid
the construction of examples representing situations that are neither feasible nor logical. The
explanations of the predictions made by an underlying model for these observations would not
make sense and would not help the understanding of the model’s logic.

Only a few scholars carried out formal comparisons, based on heuristic-based metrics, be-
tween different methods for explainability to evaluate their strengths and weaknesses. The
methodologies utilized for the comparisons are (see also table A.15):

® sensitivity to input perturbation - some features of the input dataset are re-
moved, masked or altered and the explanations generated by a method for explainability
from the model trained on both the original and modified inputs are compared;

® sensitivity to model parameter randomization - the outputs a method for ex-
plainability generated from a trained model and another model of the same architecture
with some or all parameters replaced by random values are compared;

e explanation completeness - these approaches check which method generates expla-
nations that describe the inferential process of the underlying model to the highest extent.
45
This consists of capturing the highest number of features of the input that affect the deci-
sion process of the model.

The vast majority of these scientific articles compared methods for explainability designed
to generate visual explanations of the logic followed by neural networks for the classification of
either images or texts. All these methods produce maps, like heat-maps or feature maps, and
the comparison is carried out by measuring the differences in these maps generated before and
after the input or the model’s parameters were perturbed. The complete list of the methods that
were compared, along with the type of input data that were analysed in these comparisons, is
shown in table A.16. [324, 325] compared the saliency maps generated by various methods for
visual explainability to either a randomly initialised untrained network or from a copy of the
dataset in which the labels were randomly permutated. The degree of correlation between the
saliency maps was measured by calculating Spearman Rank Correlation coefficients. Similarly,
[326] proposed to vary input images by occluding with zero-valued pixels their portions sharing
the same relevance level, according to the saliency maps generated by four gradient-based attri-
bution methods. The sensitivity of the four methods to this input perturbation was assessed with
a formal metric, Sensitivity-n, which quantifies the variation in the output caused when features
sharing the same relevance level are removed. The results of this analysis showed that Occlusion
Sensitivity (see Section 6) is the method that identifies the few most important features, in re-
spect with the other methods, because it suffers the faster variations in the output when the most
relevant pixels are removed. Layer-wise Relevance Propagation (LRP) and Sensitivity Analysis
were tested in [327, 328, 329, 330] by removing important words from input documents in text
classification problems [327, 328] or replacing the most relevant pixels (in case of images) by
randomly sampling new pixel values from a uniform distribution [329, 330]. The metric used in
this study assesses the differences in the model’s classification accuracy between the original and
the perturbed input points when fed into the model. Both studies showed that LRP qualitatively
and quantitatively provides a better explanation of what made a DNN reach a specific classifi-
cation prediction. [331] used the same evaluation approach of [329, 330] to compare LRP with
Occlusion Sensitivity and Sensitivity Analysis. LRP and Occlusion Sensitivity performed better
than Sensitivity Analysis, seconding the findings of [329, 330]. Input perturbation was also used
in [90, 332, 92] to test the robustness of several methods that generate visual explanations of
the inferential process of DNNs applied to image classification problems. Robustness concerns
variations of an explanation provided by a method with respect to changes in the input leading to
that eplanation. Intuitively, if the input being explained is modified slightly-subtly enough not to
change the prediction of the model then the explanation must not change much either [90]. On
the one hand, [90] applied a Gaussian noise to input images and measured the relative changes
in the output with respect to these perturbations with the Local Lipschitz Continuity metric. On
the other hand, [332, 92] added/subtracted a constant shift to the input images. Then, [332] used
two metrics to assess the similarity between the heat-maps generated from the original and the
perturbed images: Spearman Rank Correlation coefficients and Top-« intersection which mea-
sure the size of the intersection of the x most important features before and after perturbation.
[92] instead measure the differences in the model’s predictions by checking whether the methods
for explainability under analysis satisfy the requirement of ‘input invariance’ (see Section 5.1).
These studies show that all tested methods (table A.16) are vulnerable even to small perturba-
tions that do not affect the predictions of an underlying model but they significantly change the
heat and saliency maps produced by the explainers. These do not satisfy the ‘input invariance’
requirement [92], that means they do not reflect the sensitivity of a model with respect to input

46
perturbations. Lastly, [333] compared the completeness of the explanations generated by seven
methods for explainability that interpret the logic of DNNs by calculating the partial derivatives
of the output according to the inputs, their perturbation and analysing the network’s weights.

7.2. Human-centred evaluation

Explanations are effective when they help end-users to build a complete and correct mental
representation of the inferential process of a given model. Many scientific articles focused on
testing the degree of explainability of one or multiple methods, with a human-in-the-loop ap-
proach. These experiments involved human participants of two kinds. On the one hand, people
randomly selected from the lay public and without any prior technical/domain knowledge who
were asked to interact with one or more explanatory tools and give feedback by filling question-
naires. On the other hand, domain experts who were asked to give informed opinions on the
explanations produced by these methods and verify the consistency of the explanations with the
domain knowledge. Examined scientific articles can be clustered into two categories, depending
on the nature of questions administered to people (see table A.14). Qualitative studies are based
upon open-ended questions aimed at achieving deeper insights whereas quantitative studies make
use of close-ended questions that can be easily analyzed statistically. The first methods for ex-
plainability that were tested by human users are those generating textual explanations for Expert
Systems (ES) in the 90’s [334, 335]. These researches aimed at collecting pieces of evidence on
whether and how explanations could enhance the user’s confidence in the decisions proposed by
an ES. Scholars carried out several human-centred evaluations over the years to assess the effects
of textual explanations on end-users to other types of systems employing ML models, such as
learning systems [336]. The impact of explanations on the reliability, trust and reliance of end-
users on automated systems was explored in [54]. The participants were presented with photos of
the Fort Sill terrain where the presence of a camouflaged soldier was indicated by an automated
decision system. Initially, they considered the inference produced by the system to be trustwor-
thy and reliable but, after observing the system making errors, they distrusted even reliable aids,
unless an explanation was provided regarding why the system failed. In conclusion, explanations
of these errors increased the trust of the participants in the automated system who were asked to
estimate their perceived reliability on a 9-point Likert-format scale, ranging from ‘not very well’
to ‘very well’. The influence of explanation factors over the capacity of end-users to understand
and develop a mental representation of the internal reference process of a model was investigated
in [337, 338, 339, 340]. The explanations produced by the analysed methods for explainability
consisted of graphical representation of the most relevant features. [337, 338] showed that users
of simulation-based training systems with virtual players prefer short explanations to long ones
where length is defined by the number of elements in an explanation. An element can be a goal or
an event of the training program. This was tested by showing to the participants four explanation
alternatives of different length (they contained either one or two elements), in the form of DTs,
for each action of the virtual players and asked to indicate which alternative they considered the
most useful for increasing end-user understanding. The influence factors analysed in [339] were
the number of independent variables of a trained linear regression model and the values of these
variables for each instance. Some participants were randomly assigned to check either a model
that uses only two features or a model that uses eight features. The coefficients of the linear
regression model were also presented only to half of the participants. Then, the participants were
asked to estimate what would be the output of the model and to correct it in case it was not ac-
curate. As expected, users can easily simulate models with a small number of features whereas,

47
surprisingly, displaying model internal parameters can hamper their ability to notice unusual in-
puts and correct inaccurate predictions. The method for explainability tested in [340] listed the
two most relevant features and the prediction of the model. The prediction was coded with a solid
colour taken from a scale running from red, representing the most negative possible outcome, to
green, the most positive one. Participants were asked to interact with the system for two weeks
at the end of which they were interviewed to collect their feedback and to check whether they
gained some insight on the logic of the model to be explained by the explanatory system under
analysis. [103] studied the explainability of an interactive interface, called Prospector, contain-
ing a set of diagnostic tools that allows end-user, via visual and numerical representations, to
understand how the features of a dataset affect the prediction of a model overall. Users can also
inspect a specific instance to check its prediction and can weak feature values to see how the
model responds. A team of data-scientists was asked to interact with this tool to debug a set of
models designed to predict if a patient is at risk of developing diabetes by using a database of
electronic medical records. The human experiment consists of interviewing, at the end of the
experiment, the data scientists on whether they feel that it was beneficial for their work. Methods
for evaluating the interpretability of data-driven models with a human-in-the-loop approach were
proposed in [341, 342, 343, 344]. The approach proposed in [341] identifies some proxies which
consist of other, simpler models inherently more explainable. For example, a DT is inherently
more interpretable than DNNs and the former methods can be used to explain the latter. The
authors presented to participants a list of the coefficients of the features used by each proxy and
a graphical depiction of its structure (in the form of a DT) and they asked them to identify the
correct prediction. [344] proposed instead to assess the comprehensibility of DTs by asking the
participants to perform the following tasks: (I) classify a data-point according to the classification
tree, (IL) explain the classification of a data-point by pointing out which attributes’ values must
be changed or retained to modify the instance’s class, (IIT) validate a part of the classification tree
by asking the participant to check whether a statement about the domain is confirmed/rejected
by the tree, ([V) discover new knowledge by finding a property (attribute-value pair) that is un-
usual for instances from one class, (V) rate the classification tree by giving a subjective opinion
on the comprehensibility of the tree and, lastly, (VI) compare two classification tree by saying
which one is more comprehensible. The two studies presented in [342, 343] analysed the inter-
pretability of predictive models by asking participants to interact with them and fill self-reporting
questionnaires. The surveys carried out in [342] aimed at comparing six supervised learning al-
gorithms. These were ranked in order of preference based on the subjective quantification of
understandability obtained from the self-reporting questionnaires filled by participants. Pairs of
models trained on the same dataset were generated and participants were asked to rate them on a
scale where one extreme is ‘the first model is the most understandable’ to the other extreme ‘the
second model is the most understandable’ via increasingly positive grades. In the study carried
out in [343], participants were required to evaluate the explanations of a credit model, trained
to accept or reject loan applications, consisting of IF-THEN rules and displayed as a decision
tree. They were asked to predict the model’s outcome on a new loan application, answer a few
yes/no questions such as “Does the model accept all the people with an age above 60?” and rate,
for each question, the degree of confidence in the answer on a scale from 1 (Totally not con-
fident) to 5 (Very confident). The authors measured, besides the answer confidence, other two
variables about task performance: accuracy, quantified as the percentage of correct answers, and
the time in seconds spent to answer the questions. The effectiveness of why-oriented explanation
systems in debugging a naive Bayes learning model for text classification and in context-aware
applications were respectively tested in [345, 60] and [346, 77]. [345, 60] asked participants to
48
train a prototype system, based on a Multinomial naive Bayes classifier, that can learn from users
how to automatically classify emails by manually moving a few of them from the inbox into an
appropriate folder. The system was subsequently run over a new set of messages, some of which
were wrongly classified. The participants had to debug the system by asking ‘why’ questions via
an interactive explanation tool producing textual answers and by giving two types of feedback:
some participants could label the most relevant feature (words) whereas the others could only
provide more labelled instances (moving more messages to the appropriate folders). At the end
of the session, the participants filled a questionnaire to express their opinions on the prototype.
In the experiment run in [346, 77], participants were invited to interact with a model that predicts
whether a person is doing physical exercise or not, based on the body temperature and the pace.
They were shown with some examples of inputs and outputs accompanied by graphical (in the
form of decision trees) and textual explanations on the logic followed by the model. Half of
the participants were presented with why explanations, such as “Output classified as Not Exer-
cising, because Body Temperature < 37 and Pace < 3” whereas the other half were presented
with why not explanations, such as “Output not classified as Exercising, because Pace < 3, but
not Body Temperature > 37”. Subsequently, the participants had to fill two questionnaires to
check their understanding by asking questions how the system works and to give feedback on the
explanations in terms of understandability, trust and usefulness. Both questionnaires contained
a mix of open and close questions, where the close ones consisted of a 7-point Likert scale. A
qualitative evaluation of the interpretability of the Mind the Gap Model (MGM) method for ex-
plainability was gathered in [316]. MGM clustered the data of an input dataset according to the
most relevant features. The participants were presented with the raw data and the data clustered
with MGM and k-means and were asked to write a 2-3 sentence executive summary of each data
representation within 5 minutes. They all found impossible to summarise the raw data, not being
able to complete the task in the given amount of time, but they managed to do so on the data with
clustered MGM and k-means. To test Bayesian Case Model (BCM), [309] asked the participants
to assign sixteen recipes, described only by a set of ingredients, to the right category (so a recipe
of cookies had to be classified under ‘cookie’) and then they counted how many of them were
correctly classified. BCM was compared with Latent Dirichlet Allocation (LDA), a clustering
approach based on extracting similar characteristics in the data. The need for XAI in Intelligent
Tutoring Systems (ITS) was explored in [347]. The participants in the study were asked to use an
ITS that provided tools to explore and explain an algorithm solving constraint satisfaction prob-
lems in an interactive simulation. The participants were instructed by the exploration tool with a
textual hint message. They could select to have the hint explained by the explanatory tool which
was also designed to solicit their suggestions on the explanations they would like to see for each
hint by presenting them a checkbox list with the following options: ‘why the system gave this
hint’, ‘how the system chose this hint’, ‘some other explanation about this hint’ (followed by an
open-text field) and ‘I do not want an explanation for this hint’

 

Many scholars proposed human-centred evaluation approaches for methods for explainabil-
ity generating visual explanations. The participants selected in the study in [348] were presented
with whole images misclassified by a DNN and the visual explanations generated by LIME and
MMD-critic. For example, a photo of a Jeep with a red-cross was wrongly classified as an am-
bulance and the visual explanations show the red-cross with the rest of the image greyed out.
Participants were asked to say whether the class predicted by the model was nonetheless relevant
where the possible answers to questions like ‘Is the label Red-Cross relevant?’ were ‘yes’ and
‘no’. A similar experiment was carried out in [349] to test GAN Dissection. Participants were

49
presented with images reporting highlighted patches showing the most highly-activating regions
for each unit at each intermediate convolutional layer of a DNN. Each layer was aligned with
a semantic and were given labels across a range of objects, parts, scenes, textures, materials,
and colours. For example, if a DNN was trained to recognize a list of object in input images,
like flowers and cars, the semantic consists of this list and images showing flowers were labelled
‘flower’. The participants were asked to say if the highlighted patches were pertinent to the la-
bel by answering yes/no questions. The capacity of Anchors and LIME in helping end-users
forecasting the predictions of an image classifier was tested in [140]. Participants were asked
to predict the output class assigned by the classifier to ten random test instances before and ten
instances after seeing two rounds of explanations generated by either Anchors or LIME. A few
scholars conducted human-centred studies to test the interpretability of the heat-maps generated
with LRP. [350, 351] applied it respectively to test models built with Fisher vector classifiers for
object recognition in images and to SVMs, trained on videos, to understand and interpret action
recognition and to check whether LRP allows identifying in which point of the video the impor-
tant action happens. By visually inspecting the heat-maps, the authors of the two studies could
show a possible weakness of the underlying classifiers by looking at the regions highlighted in
the heat-maps and examining whether they were relevant for the recognition of an object (or at
least part of it) in images and of the areas of video frames showing the actions performed in
the video. Similarly, [352] employed LRP with DNNs for electroencephalography (EEG) data
analysis. The predictions of the trained DNNs are transformed with LRP, for every trial, into
heat-maps indicating the relevance of each data point. The relevance information can be plotted
as a scalp topography that can be visually inspected by experts to check if there are neurophysi-
ologically plausible patterns in the EEG data. [353] used LRP for computing the contribution of
contextual words to arbitrary hidden states in the attention-based encoder-decoder framework of
neural machine translation (NMT). As per the previous studies, the authors checked if the trans-
lation made by the NMT (Japanese-English) were right or wrong and what types of errors were
made more frequently. The participants in [133] were asked to interact with explAIner which
showed them explanations generated by LRP and Saliency Analysis of both a simple and a com-
plex network trained on the MNIST dataset and, in the meantime, to communicate their thoughts
and actions by ‘thinking aloud’. The sessions were audio-recorded and screen-captured. At the
end of the sessions, participants were also interviewed to provide qualitative feedback on the
overall experience. Another method for explainability producing visual explanations as maps,
GradCam, was applied to multivariate time series from photovoltaic power plants that were fed
into a neural network to forecast the energy production of these plants in different weather con-
ditions [354]. GradCam was used to explain which features, such as environmental temperature,
wind bearing or humidity, or any combinations of these features were responsible, at different
time intervals, for a given prediction. The results showed that GradCam was able to visualise
the network’s attention over the time dimension and the features of multivariate time series data.
[355] compared other three methods, namely Activation Maximization, Sampling Unit and Lin-
ear Combination, designed to produce explanations as heat-maps of the most relevant features of
the input images. Activation Maximization consists of selecting the input features that maximise
the activation of a single hidden neuron. Sampling Unit consists of setting the value of a neuron
to one and calculating the probability with which each sample is assigned to a class. Lastly,
Linear Combination consists of choosing the largest weights of the connections between neurons
of two adjacent layers. The authors did not use any objective measure to compare these methods
but a qualitative visual inspection of the heat-maps and the connections between them.

50
8. Final remarks and recommendations

Research on methods to explain the inner logic either of a learning algorithm, a model in-
duced from it, or a knowledge-based approach for inference is now generally recognized as a core
area of AI and is referred to as eXplainable Artificial Intelligence (XAI). Note that other common
terms exist, such as ‘Interpretable Machine Learning’, but with XAI we would like to emphasise
the wider applicability of this emerging and fascinating field of research. Several scientific stud-
ies are published every year, with many workshops and conferences organised around the world.
to propose novel methods and disseminate findings. Although this has led to the production of an
abundance of knowledge, unfortunately, this is very scattered. This systematic review attempted
to fill this gap by organising this vast knowledge in a structured and hierarchical way. Some
scholars already tried to classify scientific studies but, given a large amount of literature, they
decided to focus only on a specific aspect of explainability. To the best of our knowledge, this is
the first attempt to review a wider body of literature that has led to the definition of four clusters:
(D reviews focused on specific aspects of XAI, (ID) the theories and notions related to the concept
of explainability, (III) the methods aimed at explaining the inferential process of data-driven and
knowledge-based modelling approaches, and (IV) the ways to evaluate the methods for explain-
ability. Many studies within XAI have focused on improving the quality and widening the variety
of explanations for several types of learning approaches with data. Since the early ’80s and ’90s,
with research concerned only with textual explanations, to nowadays, scholars have been target-
ing new explanation formats such as visual aids, rules, numbers and different combinations of
these. For each of these formats, scholars designed, deployed and tested several solutions, such
as saliency masks, attention maps, heat-maps, feature maps, as well as graphs, rules sets, trees
and dialogues. These advances were aimed at meeting the needs of different types of end-users
operating in various fields, such as laypeople, doctors and lawyers, and adapting explanations
to their domains of application. Additionally, scholars widened their research horizons by in-
corporating the knowledge developed in other fields, like Psychology, Philosophy and Social
Sciences, into the design of the novel methods for explainability. The goal was to improve the
structure, efficiency and efficacy on people’s understanding of automatically generated explana-
tions. All this research has produced many definitions of explainability and identified several
notions related to it, such as interpretability, understandability, comprehensibility and justifiabil-
ity, just to mention a few. Coupled to these notions, different objective metrics have also been
produced for their measurement. Despite the large number and variety of methods and metrics
for explainability proposed so far, there are still important scientific issues that must be tackled.
Firstly, there is no agreement among scholars on what an explanation exactly is and which are
the salient properties that should be considered to make it effective and understandable for end-
users, in particular non-experts. Secondly, the construct of explainability is a concept borrowed
from Psychology, since it is strictly connected to humans, and it is also linked to other constructs
such as trust, transparency and privacy. Thirdly, this concept has been invoked in several fields,
such as Physics, Mathematics, Social Sciences and Medicine. All this make its formalisation
and operationalisation a non-trivial research task. This holds true for every explanation format,
either textual, visual, numerical. The same can be said for rule-based explanations, in particular
for those that are generated after a model has been induced by employing deep-learning neu-
ral networks. In accordance with [1], we believe that scholars have produced enough material
that can be used to construct a generally applicable framework for XAI. This would guide the
advancement of novel, end-to-end methods for explainability, rather than keep creating isolated
methods that remain only fragments of a broad solution which should also be flexible enough

31
to adapt to various contexts, fields of application and type of end-users. Additionally, the ulti-
mate scope of an explanation is to help end-users build a complete and correct mental model of
the inferential process of either a learning algorithm or a knowledge-based system and to pro-
mote trust for its outputs. An area for future research is the involvement of humans, as final
users of artificial explanations, since their role has not been sufficiently studied in the creation
and exploitation of existing explainability methods [1]. To support this research direction, we
recommend exploiting knowledge and experiences belonging to the field of Human-Computer
Interaction and its advances for the development of interactive explanatory interfaces [356]. This
should always take into consideration the existing trade-off between the dimensions of models
accuracy and their interpretability/explainability which are currently inversely correlated. One
possible suggestion is the use of methods that take advantage of modern learning techniques, to
maximise the former dimension, and reasoning approaches to optimise the latter dimension. The
assumption is that integrating connectionist and symbolic paradigms is the most efficient way to
produce meaningful and precise explanations. Advances on these two paradigms are immense,
however, their intersection is under exploration. For example, on one hand, a school of thought
suggests to firstly train accurate models from data and then wrap them with a reasoning layer
[165]. This layer, for instance, can be produced by exploiting advances in defeasible reasoning
and argumentation [143, 142, 279] making use of knowledge-bases constructed with a human-in-
the-loop approach. On the other hand, another direction is to promote the use of neuro-symbolic
learning and reasoning in parallel, each one informing the other at all stages of model construc-
tion and evaluation [45]. Eventually, another interesting, novel and under-explored direction for
future scholars concerns the development of structured formats of explanations. These formats
should consider all the elements and notions related to explainability, that can be trained with
connectionist paradigms from data.

Explainable Artificial Intelligence

 

 

   
  
  

 
  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

evaluation
evaluation modeling
objective |
human-centred attributes
numerical
| visual | textual explanators explanators
rule-based | multi-format TIP
ta
scope | stage XAI methods ‘Textual | pictorial | rules | dialogue | mix-format
problem type
oe oi . Comprehensibility, interestingness, persuasiveness...
Connectionist data-driven | ; modeling P! ity ig Pp
symbolic knowledge-based reasoning
Al Connectionist learning + Al symbolic reasoning
numerical | categorical data
| textual | pictorial | time series Human-in-the-loop + interface interaction
(a) (b)

Figure 18: State of the art (a) and envisioned (b) frameworks for eXplainable Artificial Intelligence.

52
In summary, an high-level structure of a the current state-of-the-are in XAI is depicted in
figure 18 (part a). On one hand, here, emphasis has been placed on the sequence of research ac-
tivities currently and often performed by several scholars, their dependencies and order. This
sequence usually starts from input data that is then used for modeling purposes, employing
connectionist data-driven learning or symbolic reasoning knowledge-based paradigms. After
a model has been formed, then an XAI methods is applied for its analysis, knowledge discovery,
supporting its interpretability. This phase provide the end-users of these models with one or more
explanators for the purpose of its explainability. Eventually, very few scholars have proposed ap-
proaches for evaluating such layer of explainability, either proposing formal, objective metrics
or involving human-centred evaluation with model designers and end-users. On the other end,
what we believe is an ideal framework for XAI is depicted in figure 18 (part b). Here, the main
focus should be on the explanators, which is what end-users will ultimately interact with. The
development of explanators should be designed by taking into account the multiple attributes that
are linked to the psychological construct of explainability. Subsequently, scholars can focus on
the modeling phase, preferrably using both connectionist and symbolic paradigms from Artificial
Intelligence. This will allow to develop models that are both robust in terms of accuracy but also
intrinsically interpretable during all the stages of development. Eventually, the last phase should
focus on the evaluation of explainability of such models with a human-in-the-loop approach, in-
volving both designers and end-users, and the development of interactive interface for supporting
model interpretability and inference explainability.

53
References

Q]
[2]

BB

[4

[5

[6

[7

[8

[9

[10]

Qi

[12]

[13]

[14]

[15]

[16]

117)

[18]

[19]

[20]

[21]

[22]

Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A survey on explainable artificial intelli-
gence (xai). IEEE Access, 6:52138-52160, 2018. doi: 10.1109/ACCESS.2018.2870052.

Alun Preece. Asking “why” in ai: Explainability of intelligent systems—perspectives and challenges. Intelligent
Systems tn Accounting, Finance and Management, 25(2):63-72, 2018. doi: 10.1002/isaf.1422.

Edward H Shortliffe, Randall Davis, Stanton G Axline, Bruce G Buchanan, C Cordell Green, and Stanley N
Cohen. Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the
mycin system. Computers and biomedical research, 8(4):303-320, 1975. doi: 10.1016/0010-4809(75)90009-9.
Vanya Van Belle and Paulo Lisboa. Research directions in interpretable machine learning models. In 2/st Euro-
pean Symposium on Artificial Neural Networks, ESANN, pages 533-541, Bruges, Belgium, 2013. iédoc.
Jonathan Dodge, Sean Penney, Andrew Anderson, and Margaret M Burnett. What should be in an xai explanation?
what ift reveals. In JUI Workshop 7: Explainable Smart Systems - ExSS, Tokyo, Japan, 2018. CEUR-WS.org.
Alfredo Vellido, José David Martin-Guerrero, and Paulo JG Lisboa. Making machine learning models inter-
pretable. In European Symposium on Artificial Neural Networks, ESANN, volume 12, pages 163-172, Bruges,
Belgium, 2012. i6doc.

Weiquan Wang and Izak Benbasat. Recommendation agents for electronic commerce: Effects of explanation
facilities on trusting beliefs. Journal of Management Information Systems, 23(4):217-246, 2007. doi: 10.2753/
mis0742- 1222230410.

Sebastian Lapuschkin, Stephan Waldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, and Klaus-
Robert Miiller. Unmasking clever hans predictors and assessing what machines really learn. Nature communica-
tions, 10(1):1096, 2019. doi: 10.1038/s41467-019-08987-4.

Cynthia Rudin. Algorithms for interpretable machine learning. In Proceedings of the 20th ACM SIGKDD inter-
national conference on Knowledge discovery and data mining, pages 1519-1519, New York, New York, USA,
2014. ACM. doi: 10.1145/2623330.2630823.

Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable
models instead. Nature Machine Intelligence, 1(5):206, 2019. doi: 10.1038/s42256-019-0048-x.

Jean-Marc Fellous, Guillermo Sapiro, Andrew Rossi, Helen S Mayberg, and Michele Ferrante. Explainable
artificial intelligence for neuroscience: Behavioral neurostimulation. Frontiers in Neuroscience, 13:1346, 2019.
doi: 10.3389/fnins.2019.01346.

Maria Fox, Derek Long, and Daniele Magazzeni. Explainable planning. In L/CAI 2017 Workshop on Explain-
able Artificial Intelligence (XAI), pages 24-30, Melbourne, Australia, 2017. International Joint Conferences on
Artificial Intelligence, Inc.

Filip Karlo DoSilovié, Mario Bréié, and Nikica Hlupié. Explainable artificial intelligence: A survey. In 4/st Inter-
national Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO),
pages 0210-0215, Opatija, Croatia, 2018. IEEE. doi: 10.23919/mipro.2018.8400040.

Eva Thelisson, Kirtan Padh, and L Elisa Celis. Regulatory mechanisms and algorithms towards trust in ai/ml. In
IICAI Workshop on Explainable Al (XAI), pages 53-57, Melbourne, Australia, 2017. International Joint Confer-
ences on Artificial Intelligence, Inc.

Eva Thelisson. Towards trust, transparency, and liability in ai/as systems. In Proceedings of the 26th International
Joint Conference on Artificial Intelligence, pages 5215-5216, Melbourne, Australia, 2017. International Joint
Conferences on Artificial Intelligence, Inc. doi: 10.24963/IJCAL2017/767.

Sandra Wachter, Brent Mittelstadt, and Luciano Floridi. Transparent, explainable, and accountable ai for robotics.
Science Robotics, 2(6), 2017. doi: 10.1126/scirobotics.aan6080.

Wojciech Samek and Klaus-Robert Miiller. Towards explainable artificial intelligence. In Explainable AI: In-
terpreting, Explaining and Visualizing Deep Learning, pages 5-22. Springer, Cham, Switzerland, 2019. doi:
10.1007/978-3-030-28954-6\_1.

Carmen Lacave and Francisco J Diez. A review of explanation methods for bayesian networks. The Knowledge
Engineering Review, 17(2):107-127, 2002. doi: 10.1017/s02698889020001 9x.

David Martens, Bart Baesens, Tony Van Gestel, and Jan Vanthienen. Comprehensible credit scoring models using
rule extraction from support vector machines. European journal of operational research, 183(3):1466-1476,
2007. doi: 10.1016/j.ejor.2006.04.051.

Jaegul Choo and Shixia Liu. Visual analytics for explainable deep learning. JEEE Computer Graphics and
Applications, 38(4):84—92, 2018. doi: 10.1109/mcg.2018.04273 1661.

Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. A
survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5):93:1-93:42, 2018.
doi: 10.1145/3236009.

Tim Miller. Explanation in artificial intelligence: insights from the social sciences. Artificial Intelligence, 267:
1-38, 2019. doi: 10.1016/j.artint.2018.07.007.

34
[23]

[24]

[25]

[26]

[27]

[28]

[29]
[30]
[31]
[32]
[33]

[34]

[35]

[36]

[37]

[38]

[39]
[40]
[41]

[42]

[43]

[44]

[45]

Tim Miller, Piers Howe, and Liz Sonenberg. Explainable ai: Beware of inmates running the asylum or: How i
learnt to stop worrying and love the social and behavioural sciences. In [CAI Workshop on Explainable Al (XA),
pages 36-42, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.

Henrik Jacobsson. Rule extraction from recurrent neural networks: A taxonomy and review. Neural Computation,
17(6): 1223-1263, 2005. doi: 10.1162/0899766053630350.

Leila Arras, Anmed Osman, Klaus-Robert Miiller, and Wojciech Samek. Evaluating recurrent neural network
explanations. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP, pages 113-126, Florence, Italy, 2019. Association for Computational Linguistics. doi:
10.18653/v1/W 19-4813.

Hoa Khanh Dam, Truyen Tran, and Aditya Ghose. Explainable software analytics. In Proceedings of the 40th
International Conference on Software Engineering: New Ideas and Emerging Results, pages 53-56, Gothenburg,
Sweden, 2018. ACM. doi: 10.1145/3183399.3183424.

Robert R Hoffman, Gary Klein, and Shane T Mueller. Explaining explanation for “explainable ai”. In Proceed-
ings of the Human Factors and Ergonomics Society Annual Meeting, volume 62, pages 197-201, Philadelphia,
Pennsylvania, USA, 2018. SAGE Publications Sage CA: Los Angeles, CA. doi: 10.1177/1541931218621047.
Sherin Mary Mathews. Explainable artificial intelligence applications in nlp, biomedical, and malware classifica-
tion: A literature review. In Intelligent Computing-Proceedings of the Computing Conference, pages 1269-1292,
London, United Kingdom, 2019. Springer. doi: 10.1007/978-3-030-22868-2\_90.

Grégoire Montavon, Wojciech Samek, and Klaus-Robert Miiller. Methods for interpreting and understanding deep
neural networks. Digital Signal Processing, 73:1—-15, 2017. doi: 10.1016/.dsp.2017.10.011.

Quan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of Information
Technology & Electronic Engineering, 19(1):27-39, 2018.

Andreas Backhaus and Udo Seiffert. Classification in high-dimensional spectral data: Accuracy vs. interpretability
vs. model size. Neurocomputing, 131:15-22, 2014. doi: 10.1016/j.neucom.2013.09.048.

William R Swartout and Johanna D Moore. Explanation in second generation expert systems. In Second genera-
tion expert systems, pages 543-585. Springer, Berlin, Germany, 1993. doi: 10.1007/978-3-642-77927-5\_24.
Shirley Gregor and Izak Benbasat. Explanations from intelligent systems: Theoretical foundations and implica-
tions for practice. MIS quarterly, 23(4):497-530, 1999. doi: 10.2307/249487.

Alexis Papadimitriou, Panagiotis Symeonidis, and Yannis Manolopoulos. A generalized taxonomy of explanations
styles for traditional and social recommender systems. Data Mining and Knowledge Discovery, 24(3):555-583,
2012. doi: 10.1007/s10618-011-0215-0.

Frode Sgrmo, Jorg Cassens, and Agnar Aamodt. Explanation in case-based reasoning—perspectives and goals.
Artificial Intelligence Review, 24(2): 109-143, 2005. doi: 10.1007/S10462-005-4607-7.

R Byrne. Counterfactuals in explainable artificial intelligence (xai): evidence from human reasoning. In Inter-
national Joint Conference on Al (LJCAD), pages 1438-1444, Macao, China, 2019. International Joint Conferences
on Artificial Intelligence, Inc. doi: 10.24963/ijcai.2019/199.

Corrado Mencar and José M Alonso. Paving the way to explainable artificial intelligence with fuzzy modeling. In
International Workshop on Fuzzy Logic and Applications, pages 215-227, Santa Margherita Ligure, Italy, 2018.
Springer. doi: 10.1007/978-3-030-12544-8\_17.

Vaishak Belle. Logic meets probability: towards explainable ai systems for uncertain worlds. In Proceedings
of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pages 19-25, Melbourne, Australia,
2017. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.24963/ijcai.2017/733.

Mark W Craven and Jude W Shavlik. Visualizing learning and computation in artificial neural networks. Inter-
national journal on artificial intelligence tools, 1(03):399-425, 1992. doi: 10.1142/s0218213092000260.

Paulo JG Lisboa. Interpretability in machine learning—principles and practice. In International Workshop on
Fuzzy Logic and Applications, pages 15-21, Genoa, Italy, 2013. Springer. doi: 10.1007/978-3-319-03200-9\_2.
Tameru Hailesilassie. Rule extraction algorithm for deep neural networks: A review. (//CSIS) International
Journal of Computer Science and Information Security, 14(7):376-381, 2016.

Alberto Fernandez, Francisco Herrera, Oscar Cordon, Maria Jose del Jesus, and Francesco Marcelloni. Evolution-
ary fuzzy systems for explainable artificial intelligence: Why, when, what for, and where to? IEEE Computational
Intelligence Magazine, 14(1):69-81, 2019.

Serge Guillaume. Designing fuzzy inference systems from data: An interpretability-oriented review. [EEE trans-
actions on fuzzy systems, 9(3):426443, 2001. doi: 10.1109/91.928739.

Tarek R Besold and Kai-Uwe Kiihnberger. Towards integrated neural-symbolic systems for human-level ai: Two
research programs helping to bridge the gaps. Biologically Inspired Cognitive Architectures, 14:97—-110, 2015.
doi: 10.1016/j.bica.2015.09.003.

Artur d’ Avila Garcez, Tarek R Besold, Luc De Raedt, Peter Féldiak, Pascal Hitzler, Thomas Icard, Kai-Uwe
Kiihnberger, Luis C Lamb, Risto Miikkulainen, and Daniel L Silver. Neural-symbolic learning and reasoning:
contributions and challenges. In AAAI Spring Symposium Series, pages 20-23, Palo Alto, California, USA, 2015.

55
[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54]

[55]

[56]
[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

AAAI Press.

Maria Paola Bonacina. Automated reasoning for explainable artificial intelligence. In The First International
ARCADE (Automated Reasoning: Challenges, Applications, Directions, Exemplary Achievements) Workshop (in
association with CADE-26), pages 24-28, Gothenburg, Sweden, 2017. ARCADE@ CADE. doi: 10.29007/4b7h.
Clemens Otte. Safe and interpretable machine learning: a methodological review. In Computational intelligence in
intelligent data analysis, pages 111-122. Springer, Berlin, Germany, 2013. doi: 10.1007/978-3-642-32378-2\_8.
Alex A Freitas, Daniela C Wieser, and Rolf Apweiler. On the importance of comprehensible classification models
for protein function prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),
7(1):172-182, 2010. doi: 10.1109/tcbb.2008.47.

David Martens, Jan Vanthienen, Wouter Verbeke, and Bart Baesens. Performance of classification models from a
user perspective. Decision Support Systems, 51(4):782—793, 2011. doi: 10.1016/j.dss.2011.01.013.

Or Biran and Courtenay Cotton. Explanation and justification in machine learning: A survey. In L/CAI 2017
Workshop on Explainable Artificial Intelligence (XAI), pages 8-13, Melbourne, Australia, 2017. International
Joint Conferences on Artificial Intelligence, Inc.

Krishna Gade, Sahin Cem Geyik, Krishnaram Kenthapadi, Varun Mithal, and Ankur Taly. Explainable ai in
industry. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pages 3203-3204, Anchorage, Alaska, USA, 2019. ACM.

Feiyu Xu, Hans Uszkoreit, Yangzhou Du, Wei Fan, Dongyan Zhao, and Jun Zhu. Explainable ai: a brief
survey on history, research areas, approaches and challenges. In CCF International Conference on Natu-
ral Language Processing and Chinese Computing, pages 563-574, Switzerland, 2019. Springer, Cham. doi:
10.1007/978-3-030-32236-6\_51.

Xiaocong Cui, Jung Min Lee, and J Hsieh. An integrative 3c evaluation framework for explainable artificial
intelligence. In A/ and semantic technologies for intelligent information systems (SIGODIS), pages 1-10, Canciin,
Mexico, 2019. AIS eLibrary.

Mary T Dzindolet, Scott A Peterson, Regina A Pomranky, Linda G Pierce, and Hall P Beck. The role of trust
in automation reliance. International journal of human-computer studies, 58(6):697—718, 2003. doi: 10.1016/
s1071-5819(03)00038-7.

Nava Tintarev and Judith Masthoff. A survey of explanations in recommender systems. In JEEE 23rd international
conference on data engineering workshop, pages 801-810, Istanbul, Turkey, 2007. IEEE. doi: 10.1109/icdew.
2007.4401070.

Zachary C Lipton. The mythos of model interpretability. Commun. ACM, 61(10):36-43, 2018.

Taehyun Ha, Sangwon Lee, and Sangyeon Kim. Designing explainability of an artificial intelligence system. In
Proceedings of the Technology, Mind, and Society, page 14:1, Washington, District of Columbia, USA, 2018.
ACM. doi: 10.1145/3183654.3 183683.

Urszula Chajewska and Joseph Y Halpern. Defining explanation in probabilistic systems. In Proceedings of the
Thirteenth conference on Uncertainty in artificial intelligence, pages 62-71, Providence, Rhode Island, USA,
1997. Morgan Kaufmann Publishers Inc.

Andreas Holzinger, Georg Langs, Helmut Denk, Kurt Zatloukal, and Heimo Miiller. Causability and explainability
of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9:
€1312, 2019. doi: 10.1002/widm.1312.

Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. Principles of explanatory debugging to
personalize interactive machine learning. In Proceedings of the 20th international conference on intelligent user
interfaces, pages 126-137, Atlanta, Georgia, USA, 2015. ACM. doi: 10.1145/2678025.2701399.

Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng-Keen Wong. Too much,
too little, or just right? ways explanations impact end users’ mental models. In JEEE Symposium on Visual
Languages and Human-Centric Computing (VL/HCC), pages 3-10, Raleigh, NC, USA, 2013. IEEE. doi: 10.
1109/vihec.2013.6645235.

Irit Askira-Gelman. Knowledge discovery: comprehensibility of the results. In Proceedings of the Thirty-First
Hawaii International Conference on System Sciences, volume 5, pages 247-255, Hawaii, 1998. IEEE. doi: 10.
1109/hicss. 1998.6483 19.

Jose M Alonso, Ciro Castiello, and Corrado Mencar. A bibliometric analysis of the explainable artificial intelli-
gence research field. In International Conference on Information Processing and Management of Uncertainty in
Knowledge-Based Systems, pages 3-15, Cadiz, Spain, 2018. Springer.

Adrien Bibal and Benoit Frénay. Interpretability of machine learning models and representations: an introduc-
tion. In Proceedings on the European Symposium on Artificial Neural Networks, ESANN, pages 77-82, Bruges,
Belgium, 2016. i6doc.

Ivan Bratko. Machine learning: Between accuracy and interpretability. In Learning, networks and statistics, pages
163-177. Springer, Vienna, Austria, 1997. doi: 10.1007/978-3-7091-2668-4\_10.

Derek Doran, Sarah Schulz, and Tarek R Besold. What does explainable ai really mean? a new conceptualiza-

56
[67]

[68]

[69]

[70]

(71)

[72]

[73]

[74]

[75]

[76]

[77]

[78]

[79]

[80]

[81]

[82]

[83]

[84]

tion of perspectives. In /6th International Conference of the italian Association of Artificial Intelligence, 2017.
Workshop on Comprehensibility and Explanation in AI and ML, pages 1-8, Bari, Italy, 2017. Cex, University of
Bremen, Germany.

Alex A Freitas. Are we really discovering interesting knowledge from data. Expert Update (the BCS-SGAI
magazine), 9(1):41-47, 2006.

Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, Simone Stumpf, Peter Kiese-
berg, and Andreas Holzinger. Explainable ai: the new 42? In International Cross-Domain Conference
for Machine Learning and Knowledge Extraction, pages 295-303, Hamburg, Germany, 2018. Springer. doi:
10.1007/978-3-319-99740-7\_21.

David S Watson, Jenny Krutzinna, Ian N Bruce, Christopher EM Griffiths, Iain B McInnes, Michael R Barnes,
and Luciano Floridi. Clinical applications of machine learning algorithms: beyond the black box. BMJ, 364:1886,
2019. doi: 10.1136/bmj.1886.

Nava Tintarev and Judith Masthoff. Designing and evaluating explanations for recommender systems. In Rec-
ommender systems handbook, pages 479-510. Springer, Boston, Massachusetts, USA, 2011. doi: 10.1007/
978-0-387-85820-3\_15.

Nava Tintarev and Judith Masthoff. Explaining recommendations: Design and evaluation. In Recommender sys-
tems handbook, pages 353-382. Springer, Boston, Massachusetts, USA, 2015. doi: 10.1007/978- 1-4899-7637-6\
-10.

Ajay Chander and Ramya Srinivasan. Evaluating explanations by cognitive value. In International Cross-
Domain Conference for Machine Learning and Knowledge Extraction, pages 314-328, Hamburg, Germany, 2018.
Springer.

Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, Hankz Hankui Zhuo, and Subbarao
Kambhampati. Plan explicability and predictability for robot task planning. In JEEE International Conference on
Robotics and Automation (ICRA), pages 1313-1320, Singapore, 2017. IEEE. doi: 10.1 109/icra.2017.7989155.
David Alvarez-Melis and Tommi S. Jaakkola. Towards robust interpretability with self-explaining neural net-
works. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems (NeurIPS), pages 7786-7795, Montréal, Canada, 2018. Neural Information Processing Sys-
tems Foundation, Inc.

Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y Lim, and Mohan Kankanhalli. Trends and trajectories for
explainable, accountable and intelligible systems: An hci research agenda. In Proceedings of the CHI Conference
on Human Factors in Computing Systems, pages 582-599, Montréal, Canada, 2018. ACM. doi: 10.1145/3173574.
3174156.

Michael Chromik, Malin Eiband, Sarah Theres Vélkel, and Daniel Buschek. Dark patterns of explainability,
transparency, and user control for intelligent systems. In Joint Proceedings of the ACM IUI 2019 Workshops
co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles,
California, USA, 2019. CEUR-WS.org.

Brian Y Lim, Anind K Dey, and Daniel Avrahami. Why and why not explanations improve the intelligibility of
context-aware intelligent systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, pages 2119-2128, Boston, Massachusetts, USA, 2009. ACM. doi: 10.1145/1518701.1519023.

Brian Y Lim, Qian Yang, Ashraf M Abdul, and Danding Wang. Why these explanations? selecting intelligibility
types for explanation goals. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th
ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019.
CEUR-WS.org.

Johanna D Moore and Cécile L Paris. Planning text for advisory dialogues: Capturing intentional and rhetorical
information. Computational linguistics, 19(4):651—694, 1993.

Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. A grounded interaction protocol for explainable
artificial intelligence. In Proceedings of the 18th International Conference on Autonomous Agents and MultiA-
gent Systems, pages 1033-1041, Montréal, Canada, 2019. International Foundation for Autonomous Agents and
Multiagent Systems.

Alex A Freitas. On rule interestingness measures. In Research and Development in Expert Systems XV, pages
147-158. Springer, United Kingdom, 1999. doi: 10.1016/s0950-7051(99)00019-2.

Pedro Sequeira, Eric Yeh, and Melinda T Gervasio. Interestingness elements for explainable reinforcement learn-
ing through introspection. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM
Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-
WS.org.

André Carrington, Paul Fieguth, and Helen Chen. Measures of model interpretability for model selection. In Inter-
national Cross-Domain Conference for Machine Learning and Knowledge Extraction, pages 329-349, Hamburg,
Germany, 2018. Springer.

Isabel Sassoon, Nadin K6kciyan, Elizabeth Sklar, and Simon Parsons. Explainable argumentation for wellness

57
[85]

[86]

[87]

[88]
[89]

[90]

[91]

[92]

[93]

[94]
[95]
[96]
[97]
[98]

[99]

[100]

[101]

[102]

[103]

[104]

[105]

consultation. In /nternational Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Sys-
tems, pages 186-202, Switzerland, 2019. Springer, Cham.

Mukund Sundararajan, Jinhua Xu, Ankur Taly, Rory Sayres, and Amir Najmi. Exploring principled visualizations
for deep network attributions. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th
ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019.
CEUR-WS.org.

Shang-Ming Zhou and John Q Gan. Low-level interpretability and high-level interpretability: a unified view
of data-driven interpretable fuzzy system modelling. Fuzzy Sets and Systems, 159(23):3091-3131, 2008. doi:
10.1016/j.fss.2008.05.016.

Claus Weihs and UM Sondhauss. Combining mental fit and data fit for classification rule selection. In
Exploratory Data Analysis in Empirical Research, pages 188-203. Springer, Munich, Germany, 2003. doi:
10.1007/978-3-642-55721-7\_21.

Alex A Freitas. Comprehensible classification models: a position paper. ACM SIGKDD explorations newsletter,
15(1):1-10, 2014. doi: 10.1145/2594473.2594475.

Shixia Liu, Xiting Wang, Mengchen Liu, and Jun Zhu. Towards better analysis of machine learning models: A
visual analytics perspective. Visual Informatics, 1(1):48-56, 2017. doi: 10.1016/j.visinf.2017.01.006.

David Alvarez-Melis and Tommi S. Jaakkola. On the robustness of interpretability methods. In Proceedings of the
2018 ICML Workshop in Human Interpretability in Machine Learning, pages 66-71, Stockholm, Sweden, 2018.
ICML.

Rowan McAllister, Yarin Gal, Alex Kendall, Mark Van Der Wilk, Amar Shah, Roberto Cipolla, and Adrian Vivian
Weller. Concrete problems for autonomous vehicle safety: Advantages of bayesian deep learning. In Proceed-
ings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pages 4745-4753, Melbourne,
Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.24963/ijcai.2017/661.
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schiitt, Sven Daéhne, Dumitru
Erhan, and Been Kim. The (un) reliability of saliency methods. In NIPS workshop on Explaining and Visualizing
Deep Learning, pages 93-101, Long Beach, California, USA, 2017. NIPS.

Mukund Sundararajan, Ankur Taly, and Qigi Yan. Axiomatic attribution for deep networks. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pages 3319-3328, Sydney, Australia, 2017.
JMLR.org.

Fabian Offert. “i know it when i see it”. visualization and intuitive interpretability. In N/PS Symposium on
interpretable Machine Learning, pages 43-46, Long Beach, California, USA, 2017. NIPS.

Joseph B Lyons. Being transparent about transparency. In AAAI Spring Symposium, pages 48-53, Palo Alto,
California, USA, 2013. AAAI Press.

Adrian Weller. Challenges for transparency. In Proceedings of the ICML Workshop on Human Interpretability in
Machine Learning, pages 55-62, Sydney, Australia, 2017. ICML.

Andrés Paez. The pragmatic turn in explainable artificial intelligence (xai). Minds and Machines, 29:1—-19, 2019.
doi: 10.1007/s11023-019-09502-w.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising
image classification models and saliency maps. In Proceedings of ICLR Workshop, Banff, Canada, 2014. ICLR.
Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In Proceedings
of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150-158,
Beijing, China, 2012. ACM. doi: 10.1145/2339530.2339556.

Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating
activation differences. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 3145-3153, Sydney, Australia, 2017. JMLR.org.

Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Miiller, and Wojciech
Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS
one, 10(7):e0130140, 2015.

Jonathan L Herlocker, Joseph A Konstan, and John Riedl. Explaining collaborative filtering recommendations. In
Proceedings of the 2000 ACM conference on Computer supported cooperative work, pages 241-250, Philadelphia,
Pennsylvania, USA, 2000. ACM. doi: 10.1145/358916.358995.

Josua Krause, Adam Perer, and Kenney Ng. Interacting with predictions: Visual inspection of black-box machine
learning models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages
5686-5697, San Jose, California, USA, 2016. ACM. doi: 10.1145/2858036.2858529.

Mireia Ribera and Agata Lapedriza. Can we do better explanations? a proposal of user-centered explainable ai.
In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent
User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-WS.org.

MM de Graaf and Bertram F Malle. How people explain action (and autonomous intelligent systems should
too). In AAAI Fall Symposium on Artificial Intelligence for Human-Robot Interaction, pages 19-26, Arlington,

58
[106]

[107]

[108]
[109]

[110]

(111)

[112]

[113]

[114]
[115]

[116]
(117)

[118]

[119]

[120]

[121]

[122]

[123]

[124]

[125]

[126]

[127]

Virginia, USA, 2017. AAAI Press.

Maaike Harbers, Karel van den Bosch, and John-Jules Ch Meyer. A study into preferred explanations of virtual
agent behavior. In International Workshop on Intelligent Virtual Agents, pages 132-145, Amsterdam, Netherlands,
2009. Springer. doi: 10.1007/978-3-642-04380-2\_17.

Michael R Wick and William B Thompson. Reconstructive explanation: Explanation as complex problem solving.
In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pages 135-140, Detroit,
Michigan, USA, 1989. International Joint Conferences on Artificial Intelligence, Inc.

Michael R Wick. Second generation expert system explanation. In Second Generation Expert Systems, pages
614-640. Springer, Berlin, Germany, 1993. doi: 10.1007/978-3-642-77927-5\_26.

Steven R Haynes, Mark A Cohen, and Frank E Ritter. Designs for explaining intelligent agents. International
Journal of Human-Computer Studies, 67(1):90-110, 2009. doi: 10.1016/..ijhcs.2008.09.008.

Raymond Sheh and Isaac Monteath. Introspectively assessing failures through explainable artificial intelligence.
In JROS Workshop on Introspective Methods for Reliable Autonomy, pages 40-47, Vancouver, Canada, 2017.
iliad-project.eu.

Regina Barzilay, Daryl McCullough, Owen Rambow, Jonathan DeCristofaro, Tanya Korelsky, and Benoit Lavoie.
A new approach to expert system explanations. In Proceedings of the Ninth International Workshop on Natural
Language Generation, pages 78-87, Niagara-on-the-Lake, Ontario, Canada, 1998. Association for Computational
Linguistics.

Tania Lombrozo. The structure and function of explanations. Trends in cognitive sciences, 10(10):464—-470, 2006.
doi: 10.1016/j.tics.2006.08.004.

JL Weiner. Blah, a system which explains its reasoning. Artificial intelligence, 15(1-2):19-48, 1980. doi: 10.
1016/0004-3702(80)9002 1-1.

Douglas Walton. A dialogue system specification for explanation. Synthese, 182(3):349-374, 2011.

Alison Cawsey. Generating interactive explanations. In Proceedings of the 9th National Conference on Artificial
Intelligence, volume 1, pages 86-91, Anaheim, California, USA, 1991. Citeseer.

Alison Cawsey. Planning interactive explanations. International Journal of Man-Machine Studies, 38(2):169-199,
1993. doi: 10.1006/imms.1993.1009.

Alison Cawsey. User modelling in interactive explanations. User Modeling and User-Adapted Interaction, 3(3):
221-247, 1993. doi: 10.1007/bf01257890.

Martha E Pollack, Julia Hirschberg, and Bonnie Webber. User participation in the reasoning processes of expert
systems. In Proceedings of Second National Conference Artificial Intelligence, pages 358-361, Menlo Park,
California, USA, 1982. MIT Press, Cambridge, Massachusetts.

Hilary Johnson and Peter Johnson. Explanation facilities and interactive systems. In Proceedings of the Ist
international conference on Intelligent user interfaces, pages 159-166, Orlando, Florida, USA, 1993. ACM. doi:
10.1145/169891.169951.

Johanna D Moore and Cecile L Paris. Planning text for advisory dialogues. In Proceedings of the 27th annual
meeting on Association for Computational Linguistics, pages 203-211, Vancouver, British Columbia, Canada,
1989. Association for Computational Linguistics. doi: 10.3115/981623.981648.

Johanna D Moore and William R Swartout. A reactive approach to explanation. In Z/CA/, pages 1504-1510,
Detroit, Michigan, USA, 1989. International Joint Conferences on Artificial Intelligence, Inc.

Johanna D Moore and William R Swartout. A reactive approach to explanation: Taking the user’s feedback into
account. In Natural language generation in artificial intelligence and computational linguistics, pages 3-48.
Springer, USA, 1991.

Mark G Core, H Chad Lane, Michael Van Lent, Dave Gomboc, Steve Solomon, and Milton Rosenberg. Building
explainable artificial intelligence systems. In Proceedings of the 21st national conference on Artificial intelligence,
pages 1766-1773, Boston, Massachusetts, USA, 2006. AAAI Press. doi: 10.21236/ada459166.

Dave Gomboc, Steve Solomon, Mark G Core, H Chad Lane, and Michael Van Lent. Design recommendations
to support automated explanation and tutoring. In Proceedings of the Fourteenth Conference on Behavior Rep-
resentation in Modelling and Simulation, pages 331-340, Universal City, California, USA, 2005. Simulation
Interoperability Standards Organization (SISO).

H Chad Lane, Mark G Core, Michael Van Lent, Steve Solomon, and Dave Gomboc. Explainable artificial intelli-
gence for training and tutoring. In Proceedings of the 12th International Conference on Artificial Intelligence in
Education, AIED, pages 762-764, Amsterdam, The Netherlands, 2005. IOS Press.

Michael Van Lent, William Fisher, and Michael Mancuso. An explainable artificial intelligence system for small-
unit tactical behavior. In Proceedings of the National Conference on Artificial Intelligence, pages 900-907, San
Jose, California, USA, 2004. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.

Arthur C Graesser, Patrick Chipman, Brian C Haynes, and Andrew Olney. Autotutor: An intelligent tutoring
system with mixed-initiative dialogue. /EEE Transactions on Education, 48(4):612-618, 2005. doi: 10.1109/te.
2005.856149.

59
[128]

[129]

[130]

[131]

[132]

[133]

[134]

[135]

[136]

[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

[146]

[147]

Pat Langley, Ben Meadows, Mohan Sridharan, and Dongkyu Choi. Explainable agency for intelligent autonomous
systems. In Proceedings of the Thirty-First Conference on Artificial Intelligence, pages 4762-4764, San Francisco,
California, USA, 2017. AAAI Press.

Shirin Sohrabi, Jorge A Baier, and Sheila A Mcllraith. Preferred explanations: Theory and generation via plan-
ning. In Proceedings of the Twenty-Fifth Conference on Artificial Intelligence, pages 261-267, San Francisco,
California, USA, 2011. AAAI Press.

Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013. doi: 10.1109/tpami.
2013.50.

Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with pairwise inter-
actions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 623-631, Chicago, Illinois, USA, 2013. ACM. doi: 10.1145/2487575.2487579.

Guido Bologna and Yoichi Hayashi. A comparison study on rule extraction from neural network ensembles,
boosted shallow trees, and svms. Applied Computational Intelligence and Soft Computing, 2018, 2018. doi:
10.1155/2018/4084850.

Thilo Spinner, Udo Schlegel, Hanna Schafer, and Mennatallah El-Assady. explainer: A visual analytics framework
for interactive and explainable machine learning. [EEE transactions on visualization and computer graphics, 26:
1064-1074, 2019. doi: 10.1109/TVCG.2019.2934629.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of
any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and
data mining, pages 1135-1144, San Francisco, CA, USA, 2016. ACM. doi: 10.1145/2939672.2939778.

Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M Rush. Lstmvis: A tool for visual
analysis of hidden state dynamics in recurrent neural networks. [EEE transactions on visualization and computer
graphics, 24(1):667—-676, 2018. doi: 10.1109/tveg.2017.2744158.

Kanit Wongsuphasawat, Daniel Smilkov, James Wexler, Jimbo Wilson, Dandelion Mané, Doug Fritz, Dilip Kr
ishnan, Fernanda B Viégas, and Martin Wattenberg. Visualizing dataflow graphs of deep learning models in
tensorflow. JEEE transactions on visualization and computer graphics, 24(1):1-12, 2018. doi: 10.1109/tvcg.
2017.2744878.

Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. Grounding visual explanations. In Com-
puter Vision - ECCV 2018 - 15th European Conference, Proceedings, Part H, pages 269-286, Munich, Germany,
2018. Springer. doi: 10.1007/978-3-030-01216-8\_17.

Glenn Fung, Sathyakama Sandilya, and R Bharat Rao. Rule extraction from linear support vector machines. In
Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,
pages 3240, Chicago, Illinois, USA, 2005. ACM. doi: 10.1145/1081870.1081878.

Guido Bologna and Yoichi Hayashi. Characterization of symbolic rules embedded in deep dimlp networks: a
challenge to transparency of deep learning. Journal of Artificial Intelligence and Soft Computing Research, 7(4):
265-286, 2017. doi: 10.1515/jaiscr-2017-0019.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations.
In Thirty-Second AAAI Conference on Artificial Intelligence, pages 1527-1535, New Orleans, Louisiana, USA,
2018. AAAI Press.

Vasile Palade, Daniel-Ciprian Neagu, and Ron J Patton. Interpretation of trained neural networks by rule extrac-
tion. In International Conference on Computational Intelligence, pages 152-161, Dortmund, Germany, 2001.
Springer. doi: 10.1007/3-540-45493-4\_20.

Lucas Rizzo and Luca Longo. Inferential models of mental workload with defeasible argumentation and non-
monotonic fuzzy reasoning: a comparative study. In 2nd Workshop on Advances In Argumentation In Artificial
Intelligence, pages 11-26, Trento, Italy, 2018. CEUR-WS.org.

Lucas Rizzo and Luca Longo. A qualitative investigation of the explainability of defeasible argumentation and
non-monotonic fuzzy reasoning. In Proceedings for the 26th AIAI Irish Conference on Artificial Intelligence and
Cognitive Science Trinity College Dublin, pages 138-149, Dublin, Ireland, 2018. CEUR-WS.org.

Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. In 5th
International Conference on Learning Representations, Workshop Track Proceedings, page 68, Toulon, France,
2017. ICLR.

Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability
beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International Conference
on Machine Learning, pages 2673-2682, Stockholm, Sweden, 2018. ICML.

David Barbella, Sami Benzaid, Janara M Christensen, Bret Jackson, X Victor Qin, and David R Musicant. Un-
derstanding support vector machine classifications via a recommender system-like approach. In DMIN, pages
305-311, Las Vegas, Nevada, USA, 2009. CSREA Press.

K Xu, J Ba, R Kiros, A Courville, R Salakhutdinov, R Zemel, and Y Bengio. Show, attend and tell: Neural image

60
[148]

[149]

[150]
[151]

[152]

[153]
[154]
[155]

[156]

[157]

[158]

[159]

[160]

[161]

[162]

[163]

[164]

[165]

[166]

[167]

[168]

[169]

caption generation with visual attention. Proceedings of the International Conference on Machine Learning,
2048:2057—-2088, 2015.

Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-compare: Auditing black-box models using
transparent model distillation. In Proceedings of the AAAYACM Conference on Al, Ethics, and Society, pages
303-310, New Orleans, Louisiana, USA, 2018. ACM. doi: 10.1145/3278721.3278725.

Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural
Information Processing Systems, pages 4765-4774, Long Beach, California, USA, 2017. Neural Information
Processing Systems Foundation, Inc.

Marko Robnik-Sikonja and Igor Kononenko. Explaining classifications for individual instances. IEEE Transac-
tions on Knowledge and Data Engineering, 20(5):589-600, 2008. doi: 10.1109/tkde.2007.190734.

Marko Robnik-Sikonja. Explanation of prediction models with explain prediction. Informatica, 42(1):13-22,
2018.

Paulo Cortez and Mark J Embrechts. Opening black box data mining models using sensitivity analysis. In IEEE
Symposium on Computational Intelligence and Data Mining (CIDM), pages 341-348, Paris, France, 2011. IEEE.
doi: 10.1109/cidm.2011.5949423.

Paulo Cortez and Mark J Embrechts. Using sensitivity analysis and visualization techniques to open black box
data mining models. Information Sciences, 225:1-17, 2013. doi: 10.1016/j.ins.2012.10.039.

Erik Strumbelj and Igor Kononenko. An efficient explanation of individual classifications using game theory.
Journal of Machine Learning Research, 11:1—-18, March 2010. ISSN 1532-4435.

Igor Kononenko, Erik Strumbelj, Zoran Bosnié, Darko Pevec, Matja% Kukar, and Marko Robnik-Sikonja. Expla-
nation and reliability of individual predictions. Informatica, 37(1):41-48, 2013.

Erik Strumbelj, Igor Kononenko, and M Robnik Sikonja. Explaining instance classifications with interactions of
subsets of feature values. Data & Knowledge Engineering, 68(10):886-904, 2009. doi: 10.1016/j.datak.2009.01.
004.

Erik Strumbelj and Igor Kononenko. Towards a model independent method for explaining classification for
individual instances. In International Conference on Data Warehousing and Knowledge Discovery, pages 273—
282, Turin, Italy, 2008. Springer. doi: 10.1007/978-3-540-85836-2\_26.

Erik Strumbelj, Zoran Bosnic¢, Igor Kononenko, Branko Zakotnik, and Cvetka GraSié Kuhar. Explanation and
reliability of prediction models: the case of breast cancer recurrence. Knowledge and information systems, 24(2):
305-324, 2010. doi: 10.1007/s10115-009-0244-9.

Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influence: Theory and
experiments with learning systems. In JEEE symposium on security and privacy (SP), pages 598-617, San Jose,
California, USA, 2016. IEEE. doi: 10.1109/sp.2016.42.

Philip Adler, Casey Falk, Sorelle A Friedler, Tionney Nix, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith,
and Suresh Venkatasubramanian. Auditing black-box models for indirect influence. Knowledge and Information
Systems, 54(1):95—122, 2018. doi: 10.1007/s10115-017-1116-3.

Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pages 1885-1894, Sydney, Australia, 2017.
JMLR.org.

Jakub Sliwinski, Martin Strobel, and Yair Zick. A characterization of monotone influence measures for data classi-
fication. In JJCAL-17 Workshop on Explainable Al (XAI), pages 48-52, Melbourne, Australia, 2017. International
Joint Conferences on Artificial Intelligence, Inc.

Andreas Henelius, Kai Puolamaki, Henrik Bostrém, Lars Asker, and Panagiotis Papapetrou. A peek into the black
box: exploring classifiers by randomization. Data mining and knowledge discovery, 28(5-6):1503-1529, 2014.
doi: 10.1007/s10618-014-0368-8.

Erik Strumbelj and Igor Kononenko. Explaining prediction models and individual predictions with feature con-
tributions. Knowledge and information systems, 41(3):647—-665, 2014. doi: 10.1007/s10115-013-0679-x.
Hadrien Bride, Jie Dong, Jin Song Dong, and Zhé Héu. Towards dependable and explainable machine learning
using automated reasoning. In International Conference on Formal Engineering Methods, pages 412-416, Gold
Coast, Australia, 2018. Springer. doi: 10.1007/978-3-030-02450-5\_25.

Ulf Johansson, Lars Niklasson, and Rikard Konig. Accuracy vs. comprehensibility in data mining models. In
Proceedings of the seventh international conference on information fusion, volume 1, pages 295-300, Stockholm,
Sweden, 2004. Elsevier.

Ulf Johansson, Rikard K6nig, and Lars Niklasson. The truth is in there-rule extraction from opaque models using
genetic programming. In FLAIRS Conference, pages 658-663, Miami Beach, Florida, USA, 2004. AAAI Press.
Osbert Bastani, Carolyn Kim, and Hamsa Bastani. Interpretability via model extraction. In Fairness, Account-
ability, and Transparency in Machine Learning Workshop, pages 57-61, Halifax, Nova Scotia, Canada, 2017.
FAT/ML.

Sanjay Krishnan and Eugene Wu. Palm: Machine learning explanations for iterative debugging. In Proceedings

61
[170]

(171)

[172]

[173]

[174]

[175]

[176]

[177]

[178]

[179]

[180]

[181]

[182]

[183]

[184]

[185]

[186]

[187]

[188]

of the 2nd Workshop on Human-In-the-Loop Data Analytics, page 4, Chicago, Illinois, USA, 2017. ACM. doi:
10.1145/3077257.3077271.

Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In
Proceedings of the IEEE International Conference on Computer Vision, pages 3429-3437, Venice, Italy, 2017.
IEEE. doi: 10.1109/iccv.2017.371.

Lingqiao Liu and Lei Wang. What has my classifier learned? visualizing the classification rules of bag-of-feature
model by support region detection. In 20/2 IEEE Conference on Computer Vision and Pattern Recognition, pages
3586-3593, Providence, Rhode Island, USA, 2012. IEEE. doi: 10.1109/cvpr.2012.6248103.

Jaegul Choo, Hanseung Lee, Jaeyeon Kihm, and Haesun Park. ivisclassifier: An interactive visual analytics system
for classification based on supervised dimension reduction. In JEEE Symposium on visual analytics; Science and
Technology, pages 27-34, Salt Lake City, Utah, USA, 2010. IEEE. doi: 10.1109/vast.2010.5652443.

Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in Neural
Information Processing Systems, pages 6967-6976, Long Beach, California, USA, 2017. Neural Information
Processing Systems Foundation, Inc.

David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert
MAiller. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun):
1803-1831, 2010.

Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. Peeking inside the black box: Visualizing
statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical
Statistics, 24(1):44-65, 2015. doi: 10.1080/10618600.2014.907095.

Giuseppe Casalicchio, Christoph Molnar, and Bernd Bischl. Visualizing the feature importance for black box
models. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages
655-670, Dublin, Ireland, 2018. Springer. doi: 10.1007/978-3-030- 10925-7\_40.

David Alvarez-Melis and Tommi § Jaakkola. A causal framework for explaining the predictions of black-box
sequence-to-sequence models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, pages 412-421, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.
18653/v1/d17-1042.

Jan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harmessing adversarial examples. In
Proceedings of the 3rd International Conference on Learning Representations, San Diego, California, USA, 2015.
ICLR.

Josua Krause, Adam Perer, and Enrico Bertini. Using visual analytics to interpret predictive machine learning
models. In JCML Workshop on Human Interpretability in Machine Learning, pages 106-110, New York City,
New York, USA, 2016. ICML.

Brett Poulin, Roman Eisner, Duane Szafron, Paul Lu, Russell Greiner, David S Wishart, Alona Fyshe, Brandon
Pearcy, Cam MacDonell, and John Anvik. Visual explanation of evidence with additive classifiers. In Proceedings
Of The National Conference On Artificial Intelligence, volume 21, pages 1822—1829, Boston, Massachusetts,
USA, 2006. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.

Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, and David S Ebert. Manifold: A model-agnostic framework
for interpretation and diagnosis of machine learning models. [EEE transactions on visualization and computer
graphics, 25(1):364-373, 2019.

Minsuk Kahng, Dezhi Fang, and Duen Horng Polo Chau. Visual exploration of machine learning results using
data cube analysis. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics, page 1, San Francisco,
California, USA, 2016. ACM. doi: 10.1145/2939502.2939503.

Giles Hooker. Discovering additive structure in black box functions. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data mining, pages 575-580, Seattle, Washington, USA,
2004. ACM. doi: 10.1145/1014052.1014122.

Or Biran and Kathleen McKeown. Justification narratives for individual classifications. In Proceedings of the
AutoML workshop at ICML, volume 2014, pages 1-7, Beijing, China, 2014. ICML.

Paolo Tamagnini, Josua Krause, Aritra Dasgupta, and Enrico Bertini. Interpreting black-box classifiers using
instance-level visual explanations. In Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics,
pages 6:1-6:6, San Francisco, California, USA, 2017. ACM. doi: 10.1145/3077257.3077260.

Scott Cheng-Hsin Yang and Patrick Shafto. Explainable artificial intelligence via bayesian teaching. In NIPS
2017 workshop on Teaching Machines, Robots, and Humans, pages 127-137, Long Beach, California, USA,
2017. NIPS.

Rajiv Khanna, Been Kim, Joydeep Ghosh, and Sanmi Koyejo. Interpreting black box predictions using fisher
kernels. In Kamalika Chaudhuri and Masashi Sugiyama, editors, The 22nd International Conference on Artificial
Intelligence and Statistics, volume 89, pages 3382-3390, Naha, Okinawa, Japan, 16-18 Apr 2019. Proceedings
of Machine Learning Research.

Jacob Bien, Robert Tibshirani, et al. Prototype selection for interpretable classification. The Annals of Applied

62
[189]

[190]

[191]

[192]

[193]

[194]

[195]

[196]

[197]

[198]

[199]

[200]

[201]

[202]

[203]

[204]

[205]

[206]

[207]

Statistics, 5(4):2403-2424, 2011. doi: 10.1214/11-aoas495.

Rich Caruana, Hooshang Kangarloo, JD Dionisio, Usha Sinha, and David Johnson. Case-based explanation of
non-case-based learning methods. In Proceedings of the AMIA Symposium, page 212, Washington, District of
Columbia, USA, 1999. American Medical Informatics Association.

Ninghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1803-1811,
London, United Kingdom, 2018. ACM. doi: 10.1145/3219819.3220027.

Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism
for interpretability. In Advances in Neural Information Processing Systems, pages 2280-2288, Long Beach,
California, USA, 2016. Neural Information Processing Systems Foundation, Inc.

Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel
Das. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. In Advances
in Neural Information Processing Systems 3] (NIPS), pages 592-603, Montréal, Canada, 2018. Curran Associates,
Inc.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability of machine learning. In
Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining, pages 1135-1144,
San Francisco, California, USA, 2016. ACM.

Devinder Kumar, Alexander Wong, and Graham W Taylor. Explaining the unexplained: A class-enhanced atten-
tive response (clear) approach to understanding deep neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops, pages 36-44, Honolulu, Hawaii, USA, 2017. IEEE. doi:
10.1109/evprw.2017.215.

Ge Liu and David Gifford. Visualizing feature maps in deep neural networks using deepresolve. a genomics case
study. In International Conference on Machine Learning 2017 - Workshop on Visualization for Deep Learning,
pages 32-41, Sydney, Australia, 2017. ICML.

Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv
Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the
IEEE International Conference on Computer Vision, pages 618-626, Venice, Italy, 2017. IEEE. doi: 10.1109/
ICCV.2017.74.

Nazneen Fatema Rajani and Raymond J Mooney. Using explanations to improve ensembling of visual question
answering systems. Training, 82:248-349, 2017.

Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra. Towards transparent ai systems: Interpreting visual
question answering models. In JCML Workshop on Visualization for Deep Learning, New York City, New York,
USA, 2016. ICML.

Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European confer-
ence on computer vision, pages 818-833, Zurich, Switzerland, 2014. Springer.

Ruth Fong and Andrea Vedaldi. Net2vec: Quantifying and explaining how concepts are encoded by filters in deep
neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
8730-8738, Salt Lake City, Utah, USA, 2018. IEEE. doi: 10.1109/CVPR.2018.00910.

Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5188-5196, Boston,
Massachusetts, USA, 2015. IEEE. doi: 10.1109/cvpr.2015.7299155.

Mengnan Du, Ninghao Liu, Qingquan Song, and Xia Hu. Towards explanation of dnn-based prediction with
guided feature inversion. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 1358-1367, London, United Kingdom, 2018. ACM. doi: 10.1145/3219819.
3220099.

Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad: removing
noise by adding noise. In /nternational Conference on Machine Learning 2017 - Workshop on Visualization for
Deep Learning, pages 15-24, Sydney, Australia, 2017. ICML.

Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolutional neural networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8827-8836, Salt Lake City, Utah,
USA, 2018. IEEE. doi: 10.1109/cvpr.2018.00920.

Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions:
Prediction difference analysis. In Sth International Conference on Learning Representations, Toulon, France,
2017. ICLR.

Leila Arras, Grégoire Montavon, Klaus-Robert Miller, and Wojciech Samek. Explaining recurrent neural net-
work predictions in sentiment analysis. In Proceedings of the 8th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis, pages 159-168, Copenhagen, Denmark, 2017. Association for
Computational Linguistics. doi: 10.18653/v1/w17-5221.

Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, Klaus-Robert Miiller, and Wojciech Samek. Layer-

63
[208]

[209]

[210]

[211]

[212]

[213]

[214]

[215]

[216]

[217]

[218]

[219]

[220]

[221]

[222]

[223]

[224]

[225]

wise relevance propagation for neural networks with local renormalization layers. In Jnternational Conference on
Artificial Neural Networks, pages 63—71, Barcelona, Spain, 2016. Springer. doi: 10.1007/978-3-319-44781-0\_8.
Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Miiller. Ex-
plaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition, 65(May):211-
222, 2017.

Sen He and Nicolas Pugeault. Deep saliency: What is learnt by a deep network about saliency? In International
Conference on Machine Learning - Workshop on Visualization for Deep Learning, pages 1-5, Sydney, Australia,
2017. ICML.

Pieter-Jan Kindermans, Kristof T. Schiitt, Maximilian Alber, Klaus-Robert Miiller, Dumitru Erhan, Been Kim,
and Sven Dahne. Learning how to explain neural networks: Patternnet and patternattribution. In 6th International
Conference on Learning Representations, Vancouver, Canada, 2018. ICLR.

Oramas Mogrovejo, José Antonio, Kaili Wang, and Tinne Tuytelaars. Visual explanation by interpretation: Im-
proving visual feedback capabilities of deep neural networks. In 7th International Conference on Learning Rep-
resentations, New Orleans, Louisiana, USA, 2019. ICLR.

Eoin M Kenny and Mark T Keane. Twin-systems to explain artificial neural networks using case-based reason-
ing: comparative tests of feature-weighting methods in ann-cbr twins for xai. In Twenty-Eighth International
Joint Conferences on Artifical Intelligence (1/CAI), pages 2708-2715, Macao, China, 2019. International Joint
Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2019/376.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp. In
The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 681-691, San Diego California, USA, 2016. The Association for Computational
Linguistics. doi: 10.18653/v1/n16- 1082.

Lingyang Chu, Xia Hu, Juhua Hu, Lanjun Wang, and Jian Pei. Exact and consistent interpretation for piecewise
linear neural networks: A closed form solution. In Proceedings of the 24th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining, pages 1244-1253, London, United Kingdom, 2018. ACM. doi:
10.1145/3219819.3220063.

Xuan Liu, Xiaoguang Wang, and Stan Matwin. Interpretable deep convolutional neural networks via meta-
learning. In International Joint Conference on Neural Networks (1JCNN), pages 1-9, Rio de Janeiro, Brazil,
2018. IEEE. doi: 10.1 109/ijenn.2018.8489172.

Mathieu Aubry and Bryan C Russell. Understanding deep features with computer-generated imagery. In Proceed-
ings of the IEEE International Conference on Computer Vision, pages 2875-2883, Santiago, Chile, 2015. IEEE.
doi: 10.1109/iccv.2015.329.

Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dgns. In International
Conference on Machine Learning, pages 1899-1908, New York City, New York, USA, 2016. PMLR.

Paulo E Rauber, Samuel G Fadel, Alexandre X Falcao, and Alexandru C Telea. Visualizing the hidden activity of
artificial neural networks. [EEE transactions on visualization and computer graphics, 23(1):101—-110, 2017. doi:
10.1109/tveg.2016.2598838.

Jayaraman J Thiagarajan, Bhavya Kailkhura, Prasanna Sattigeri, and Karthikeyan Natesan Ramamurthy. Tree-
view: Peeking into deep neural networks via feature-space partitioning. In NIPS Interpretability Workshop,
Barcelona, Spain, 2016. NIPS.

David Bau, Jun-Yan Zhu, Hendrik Strobelt, Zhou Bolei, Joshua B. Tenenbaum, William T. Freeman, and Antonio
Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In Proceedings of the
International Conference on Learning Representation, New Orleans, Louisiana, USA, 2019. ICLR.

Benjamin J Lengerich, Sandeep Konam, Eric P Xing, Stephanie Rosenthal, and Manuela Veloso. Towards visual
explanations for convolutional neural networks via input resampling. In International Conference on Machine
Learning 2017 - Workshop on Visualization for Deep Learning, pages 25-31, Sydney, Australia, 2017. ICML.
Dumitru Erhan, Aaron Courville, and Yoshua Bengio. Understanding representations learned in deep architec-
tures. Department d’ Informatique et Recherche Operationnelle, University of Montreal, QC, Canada, Tech. Rep,
1355:1, 2010.

Anh Nguyen, Jason Yosinski, and Jeff Clune. Multifaceted feature visualization: Uncovering the different types
of features learned by each neuron in deep neural networks. In Visualization for Deep Learning workshop. Inter-
national Conference on Machine Learning, New York City, New York, USA, 2016. ICML.

Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the preferred
inputs for neurons in neural networks via deep generator networks. In Advances in Neural Information Processing
Systems, pages 3387-3395, Barcelona, Spain, 2016. Neural Information Processing Systems Foundation, Inc.
Mandana Hamidi-Haines, Zhongang Qi, Alan Fern, Fuxin Li, and Prasad Tadepalli. Interactive naming for ex-
plaining deep neural networks: A formative study. In Joint Proceedings of the ACM IUI 2019 Workshops co-
located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles,
California, USA, 2019. CEUR-WS.org.

64
[226]

[227]

[228]

[229]

[230]

[231]

[232]

[233]

[234]

[235]

[236]

[237]

[238]

[239]
[240]

[241]

[242]

[243]

[244]

[245]

[246]

[247]

Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. In JCLR
Workshops, San Juan, Puerto Rico, 2016. ICLR.

Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and Song-Chun Zhu. Interpreting cnn knowledge via an
explanatory graph. In Thirty-Second AAAI Conference on Artificial Intelligence, pages 2124-2132, New Orleans,
Louisiana, USA, 2018. AAAI Press.

Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, and Eric P Xing. Symbolic graph reasoning meets convolu-
tions. In Advances in Neural Information Processing Systems, pages 1853-1863, Montréal, Canada, 2018. Neural
Information Processing Systems Foundation, Inc.

Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu. Growing interpretable part graphs on con-
vnets via multi-shot leaming. In Thirty-First AAAI Conference on Artificial Intelligence, pages 2898-2906, San
Francisco, California, USA, 2017. AAAI Press.

Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander
Mordvintsev. The building blocks of interpretability. Distil/, 3(3):e10, 2018. doi: 10.23915/distill.00010.
Minsuk Kahng, Pierre Y Andrews, Aditya Kalro, and Duen Horng Polo Chau. A cti v is: Visual exploration of
industry-scale deep neural network models. /EEE transactions on visualization and computer graphics, 24(1):
88-97, 2018.

Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through
deep visualization. In In JCML Workshop on Deep Learning, poster presentation, Lille, France, 2015. ICML.
Wen Zhong, Cong Xie, Yuan Zhong, Yang Wang, Wei Xu, Shenghui Cheng, and Klaus Mueller. Evolutionary
visual analysis of deep neural networks. In International Conference on Machine Learning 2017 - Workshop on
Visualization for Deep Learning, pages 6-14, Sydney, Australia, 2017. ICML.

Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam Hagele, Kristof T. Schiitt, Grégoire Mon-
tavon, Wojciech Samek, Klaus-Robert Miiller, Sven Dahne, and Pieter-Jan Kindermans. innvestigate neural net-
works. Journal of Machine Learning Research, 20:1—8, 2019.

Hendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister, and Alexander M Rush.
Seq2seq-vis: A visual debugging tool for sequence-to-sequence models. JEEE transactions on visualization and
computer graphics, 25(1):353-363, 2018. doi: 10.1109/TVCG.2018.2865044.

Matthew J Streeter, Matthew O Ward, and Sergio A Alvarez. Nvis: An interactive visualization tool for neural
networks. In Visual Data Exploration and Analysis VII, volume 4302, pages 234-242, San Jose, California,
USA, 2001. International Society for Optics and Photonics.

Rudy Setiono and Huan Liu. Understanding neural networks via rule extraction. In //CA/, volume 1, pages
480-485, Montréal, Québec, 1995. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.1142/
$0129065797000380.

Andrey Bondarenko, Ludmila Aleksejeva, Vilen Jumutc, and Arkady Borisov. Classification tree extraction from
trained artificial neural networks. Procedia Computer Science, 104:556-563, 2017. doi: 10.1016/j.procs.2017.
01.172.

Sebastian Thrun. Extracting rules from artificial neural networks with distributed representations. In Advances in
neural information processing systems, pages 505-512, Denver, Colorado, USA, 1995. MIT Press.

Guido Bologna. Symbolic rule extraction from the dimlp neural network. In International Workshop on Hybrid
Neural Systems, pages 240-254, Denver, Colorado, USA, 1998. Springer. doi: 10.1007/10719871\_17.

Guido Bologna. A rule extraction study based on a convolutional neural network. In International Cross-
Domain Conference for Machine Learning and Knowledge Extraction, pages 304-313, Hamburg, Germany, 2018.
Springer. doi: 10.1007/978-3-319-99740-7\_22.

M Gethsiyal Augasta and Thangairulappan Kathirvalavakumar. Reverse engineering the neural networks for
rule extraction in classification problems. Neural processing letters, 35(2):131-150, 2012. doi: 10.1007/
s11063-011-9207-8.

Saroj Kumar Biswas, Manomita Chakraborty, Biswajit Purkayastha, Pinki Roy, and Dalton Meitei Thounaojam.
Rule extraction from training data using neural network. International Journal on Artificial Intelligence Tools, 26
(03):1750006, 2017. doi: 10.1142/S0218213017500063.

Arthur d’ Avila Garcez, Krysia Broda, and Dov M Gabbay. Symbolic knowledge extraction from trained neural
networks: A sound approach. Artificial Intelligence, 125(1-2):155—207, 2001. doi: 10.1016/s0004-3702(00)
00077-1.

Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. In 16th Interna-
tional Conference of the italian Association of Artificial Intelligence, 2017. Workshop on Comprehensibility and
Explanation in Al and ML, pages 1-8, Bari, Italy, 2017. Cex, University of Bremen, Germany.

Quanshi Zhang, Yu Yang, Haotian Ma, and Ying Nian Wu. Interpreting cnns via decision trees. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6261-6270, Long Beach, California,
USA, 2019. IEEE.

Zhi-Hua Zhou, Yuan Jiang, and Shi-Fu Chen. Extracting symbolic rules from trained neural network ensembles.

65
[248]

[249]

[250]

[251]

[252]

[253]

[254]

[255]

[256]

[257]

[258]

[259]

[260]

[261]

[262]

[263]

[264]

[265]

[266]

[267]

[268]

Al Communications, 16(1):3-15, 2003.

Zhi-Hua Zhou and Yuan Jiang. Medical diagnosis with c4. 5 rule preceded by artificial neural network ensemble.
IEEE Transactions on information Technology in Biomedicine, 7(1):37-42, 2003. doi: 10.1109/titb.2003.808498.
Olcay Boz. Extracting decision trees from trained neural networks. In Proceedings of the eighth ACM SIGKDD
international conference on Knowledge discovery and data mining, pages 456-461, Edmonton, Alberta, Canada,
2002. ACM. doi: 10.1145/775047.775113.

Mark W Craven and Jude W Shavlik. Using sampling and queries to extract rules from trained neural networks.
In Machine Learning Proceedings, pages 37-45. Elsevier, New Brunswick, New Jersey, USA, 1994. doi: 10.
1016/b978- 1-55860-335-6.50013-1.

Mark Craven and Jude W Shavlik. Extracting tree-structured representations of trained networks. In Advances in
neural information processing systems, pages 24-30, Denver, Colorado, USA, 1996. MIT Press.

Mike Wu, Michael C Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. Beyond
sparsity: Tree regularization of deep models for interpretability. In Thirty-Second AAAI Conference on Artificial
Intelligence, pages 1670-1678, New Orleans, Louisiana, USA, 2018. AAAI Press.

W James Murdoch and Arthur Szlam. Automatic rule extraction from long short term memory networks. In
Sth International Conference on Learning Representations, Conference Track Proceedings, Toulon, France, 2017.
ICLR.

Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with
logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL,
Volume 1: Long Papers, pages 2410-2420, Berlin, Germany, 2016. Association for Computational Linguistics.
doi: 10.18653/v1/p16- 1228.

Son N Tran. Unsupervised neural-symbolic integration. In JJCAL-17 Workshop on Explainable AI (XAl), pages
58-62, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.

Shane Barratt. Interpnet: Neural introspection for interpretable deep learning. In NIPS Symposium on Inter-
pretable Machine Learning, pages 47-53, Long Beach, California, USA, 2017. NIPS.

Ivan Garcfa-Magarifio, Rajarajan Muttukrishnan, and Jaime Lloret. Human-centric ai for trustworthy iot systems
with explainable multilayer perceptrons. [EEE Access, 7:125562—125574, 2019. doi: 10.1109/ACCESS.2019.
2937521.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of the 2016
Conference on Empirical Methods in Natural Language Processing, pages 107-117, Austin, Texas, USA, 2016.
Association for Computational Linguistics.

Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell. Generat-
ing visual explanations. In European Conference on Computer Vision, pages 3-19, Amsterdam, The Netherlands,
2016. Springer. doi: 10.1007/978-3-319-46493-0\_1.

Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical cor-
relation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing
Systems, pages 6076-6085, Long Beach, California, USA, 2017. Neural Information Processing Systems Foun-
dation, Inc.

Raphael Féraud and Fabrice Clérot. A methodology to explain neural network classification. Neural Networks,
15(2):237-246, 2002. doi: 10.1016/s0893-6080(01)00127-7.

Kary Fraémling. Explaining results of neural networks by contextual importance and utility. In Proceedings of Rule
Extraction From Trained Artificial Neural Networks Workshop, pages 43-56, Brighton, England, 1996. Citeseer.
Jérémie Clos, Nirmalie Wiratunga, and Stewart Massie. Towards explainable text classification by jointly learning
lexicon and modifier terms. In JJ/CAI-17 Workshop on Explainable Al (XAI), pages 19-23, Melbourne, Australia,
2017. International Joint Conferences on Artificial Intelligence, Inc.

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving
vehicles. In Proceedings of the European conference on computer vision (ECCV), pages 563-578, Munich,
Germany, 2018. ECCV.

Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Mul-
timodal explanations: Justifying decisions and pointing to the evidence. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 8779-8788, Salt Lake City, Utah, USA, 2018. IEEE.

Franz Mayr and Sergio Yovine. Regular inference on artificial neural networks. In /nternational Cross-
Domain Conference for Machine Learning and Knowledge Extraction, pages 350-369, Hamburg, Germany, 2018.
Springer. doi: 10.1007/978-3-319-99740-7\_25.

Christian W Omlin and C Lee Giles. Extraction of rules from discrete-time recurrent neural networks. Neural
networks, 9(1):41-52, 1996. doi: 10.1016/0893-6080(95)00086-0.

Martin Tamajka, Wanda Benesova, and Matej Kompanek. Transforming convolutional neural network to an
interpretable classifier. In International Conference on Systems, Signals and Image Processing (IWSSIP), pages
255-259, Osijek, Croatia, 2019. IEEE.

66
[269]

[270]

[271]

[272]

[273]

[274]

[275]

[276]

[277]

[278]

[279]

[280]

[281]

[282]

[283]
[284]

[285]

[286]

[287]

[288]

Fernando EB Otero and Alex A Freitas. Improving the interpretability of classification rules discovered by an
ant colony algorithm: Extended results. Evolutionary computation, 24(3):385—409, 2016. doi: 10.1162/evco\_a\
00155.

Wouter Verbeke, David Martens, Christophe Mues, and Bart Baesens. Building comprehensible customer churn
prediction models with advanced rule induction techniques. Expert systems with applications, 38(3):2354—-2364,
2011. doi: 10.1016/j.eswa.2010.08.023.

Brian R. Gaines. Transforming rules and trees into comprehensible knowledge structures. In Usama M. Fayyad,
Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors, Advances in Knowledge Dis-
covery and Data Mining, pages 205-226. American Association for Artificial Intelligence, Menlo Park, CA, USA,
1996. ISBN 0-262-56097-6.

Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. Interpretable decision sets: A joint frame-
work for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on
knowledge discovery and data mining, pages 1675-1684, San Francisco, California, USA, 2016. ACM. doi:
10.1145/2939672.2939874.

Benjamin Letham, Cynthia Rudin, Tyler H McCormick, and David Madigan. Building interpretable classifiers
with rules using bayesian analysis. Department of Statistics Technical Report tr609, University of Washington, 9
(3):1350-1371, 2012.

Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. An interpretable stroke prediction
model using rules and bayesian analysis. In Proceedings of the 17th AAAI Conference on Late-Breaking Devel-
opments in the Field of Artificial Intelligence, AAAIWS’ 13-17, pages 65-67, Palo Alto, California, USA, 2013.
AAAI Press.

Benjamin Letham, Cynthia Rudin, Tyler H McCormick, David Madigan, et al. Interpretable classifiers using
rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3):
1350-1371, 2015. doi: 10.1214/15-aoas848.

Tong Wang, Cynthia Rudin, Finale Velez-Doshi, Yimin Liu, Erica Klampfl, and Perry MacNeille. Bayesian
rule sets for interpretable classification. In [EEE 16th International Conference on Data Mining (ICDM), pages
1269-1274, Barcelona, Spain, 2016. IEEE. doi: 10.1109/icdm.2016.0171.

Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. A bayesian
framework for learning rule sets for interpretable classification. The Journal of Machine Learning Research, 18
(1):2357-2393, 2017.

M Pazzani. Comprehensible knowledge discovery: gaining insight from data. In First Federal Data Mining
Conference and Exposition, pages 73-82, London, United Kingdom, 1997. ACM.

Zhiwei Zeng, Chunyan Miao, Cyril Leung, and Jing Jih Chin. Building more explainable artificial intelligence
with argumentation. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-
18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th Symposium on Educational
Advances in Artificial Intelligence (EAAI-18), pages 8044-8046, New Orleans, Louisiana, USA, 2018. AAAI
Press.

Hisao Ishibuchi and Yusuke Nojima. Analysis of interpretability-accuracy tradeoff of fuzzy systems by mullti-
objective fuzzy genetics-based machine learning. International Journal of Approximate Reasoning, 44(1):4-31,
2007. doi: 10.1016/j.ijar.2006.01.004.

Yaochu Jin. Fuzzy modeling of high-dimensional systems: complexity reduction and interpretability improve-
ment. IEEE Transactions on Fuzzy Systems, 8(2):212—221, 2000. doi: 10.1109/91.842154.

Régis Pierrard, Jean-Philippe Poli, and Céline Hudelot. Learning fuzzy relations and properties for explainable
artificial intelligence. In JEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pages 1-8, Rio de
Janeiro, Brazil, 2018. IEEE. doi: 10.1109/FUZZ-IEEE.2018.8491538.

Zhenyu Wang and Vasile Palade. Building interpretable fuzzy models for high dimensional data analysis in cancer
diagnosis. BMC genomics, 12(2):S5:1—-S5:11, 2011. doi: 10.1186/1471-2164-12-s2-s5.

Alberto Cano, Amelia Zafra, and SebastidN Ventura. An interpretable classification rule mining algorithm. /nfor-
mation Sciences, 240:1—20, 2013. doi: 10.1016/j.ins.2013.03.038.

Dmitry M Malioutov, Kush R Varshney, Amin Emad, and Sanjeeb Dash. Learning interpretable classification
rules with boolean compressed sensing. In Transparent Data Mining for Big and Small Data, pages 95-121.
Springer, Cham, Switzerland, 2017. doi: 10.1007/978-3-319-54024-5\_5.

Guolong Su, Dennis Wei, Kush R Varshney, and Dmitry M Malioutov. Interpretable two-level boolean rule
learning for classification. In Proceedings of ICML Workshop Human Interpretability in Machine Learning,
pages 66-70, New York City, New York, USA, 2016. ICML.

Blen M Keneni, Devinder Kaur, Ali Al Bataineh, Vijaya K Devabhaktuni, Ahmad Y Javaid, Jack D Zaientz, and
Robert P Marinier. Evolving rule-based explainable artificial intelligence for unmanned aerial vehicles. [EEE
Access, 7:17001—-17016, 2019. doi: 10.1109/ACCESS.2019.2893141.

David P Pancho, Jose M Alonso, Oscar Cordén, Arnaud Quirin, and Luis Magdalena. Fingrams: visual repre-

67
[289]

[290]

[291]

[292]

[293]

[294]

[295]

[296]

[297]

[298]
[299]

[300]

[301]

[302]

[303]

[304]

[305]

[306]

[307]

[308]

sentations of fuzzy rule-based inference for expert analysis of comprehensibility. [EEE Transactions on Fuzzy
Systems, 21(6):1133-1149, 2013. doi: 10.1109/tfuzz.2013.2245130.

Jose M Alonso. Explainable artificial intelligence for kids. In Conference of the International Fuzzy Systems
Association and the European Society for Fuzzy Logic and Technology (EUSFLAT), Prague, Czech Republic,
2019. Atlantis Press. doi: 10.2991 /eusflat-19.2019.21.

Gabriele Tolomei, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. Interpretable predictions of tree-based
ensembles via actionable feature tweaking. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages 465-474, Halifax, Nova Scotia, Canada, 2017. ACM. doi:
10.1145/3097983.3098039.

José M Alonso, Alejandro Ramos-Soto, Ciro Castiello, and Corrado Mencar. Explainable ai beer style classifier. In
Proceedings of the SICSA Workshop on Reasoning, Learning and Explainability, pages 1-5, Aberdeen, Scotland,
UK, 2018. CEUR-WS.

César Ferri, José Hernandez-Orallo, and M José Ramirez-Quintana. From ensemble methods to comprehensible
models. In International Conference on Discovery Science, pages 165-177, Liibeck, Germany, 2002. Springer.
doi: 10.1007/3-540-36182-0\_16.

Amneleen Van Assche and Hendrik Blockeel. Seeing the forest through the trees: Learning a comprehensible
model from an ensemble. In European Conference on Machine Learning, pages 418-429, Warsaw, Poland, 2007.
Springer. doi: 10.1007/978-3-540-74958-5\_39.

Artur Andrzejak, Felix Langner, and Silvestre Zabala. Interpretable models from distributed data via merging
of decision trees. In JEEE Symposium on Computational Intelligence and Data Mining (CIDM), pages 1-9,
Singapore, 2013. IEEE. doi: 10.1109/cidm.2013.6597210.

Satoshi Hara and Kohei Hayashi. Making tree ensembles interpretable: A bayesian model selection approach. In
International Conference on Artificial Intelligence and Statistics, AISTATS, pages 77-85, Playa Blanca, Lanzarote,
Canary Islands, Spain, 2018. PMLR.

Houtao Deng. Interpreting tree ensembles with intrees. International Journal of Data Science and Analytics, 7
(4):1-11, 2018. doi: 10.1007/s41060-018-0144-8.

Jun Gao, Ninghao Liu, Mark Lawley, and Xia Hu. An interpretable classification framework for information
extraction from online healthcare forums. Journal of healthcare engineering, 2017:798-809, 2017. doi: 10.1155/
2017/2460174.

Hui Fen Tan, Giles Hooker, and Martin T Wells. Tree space prototypes: Another look at making tree ensembles
interpretable. In NIPS Interpretability Workshop, Barcelona, Spain, 2016. NIPS.

Haydemar Nujiez, Cecilio Angulo, and Andreu Catala. Rule extraction from support vector machines. In Euro-
pean Symposium on Artificial Neural Networks, ESANN, pages 107-112, Bruges, Belgium, 2002. iédoc.

Doina Caragea, Dianne Cook, and Vasant Honavar. Towards simple, easy-to-understand, yet accurate classifiers.
In Third IEEE International Conference on Data Mining, pages 497-500, San Francisco, California, USA, 2003.
IEEE. doi: 10.1109/icdm.2003.1250961.

Lutz Hamel. Visualization of support vector machines with unsupervised leaming. In IEEE Symposium on
Computational Intelligence and Bioinformatics and Computational Biology, pages 1-8, Toronto, Ontario, Canada,
2006. IEEE. doi: 10.1109/cibcb.2006.330984.

Aleks Jakulin, Martin MoZina, Janez DemSar, Ivan Bratko, and Blaz Zupan. Nomograms for visualizing support
vector machines. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery
in data mining, pages 108-117, Chicago, Illinois, USA, 2005. ACM. doi: 10.1145/1081870.1081886.

Baek Hwan Cho, Hwanjo Yu, Jongshill Lee, Young Joon Chee, In Young Kim, and Sun I Kim. Nonlinear support
vector machine visualization for risk factor analysis using nomograms and localized radial basis function kernels.
IEEE Transactions on Information Technology in Biomedicine, 12(2):247-256, 2008. doi: 10.1109/titb.2007.
902300.

Martin MozZina, Janez DemSar, Michael Kattan, and Blaz Zupan. Nomograms for visualization of naive bayesian
classifier. In European Conference on Principles of Data Mining and Knowledge Discovery, pages 337-348, Pisa,
Italy, 2004. Springer. doi: 10.1007/978-3-540-30116-5\32.

Ghim-Eng Yap, Ah-Hwee Tan, and Hwee-Hwa Pang. Explaining inferences in bayesian networks. Applied
Intelligence, 29(3):263-278, 2008. doi: 10.1007/s10489-007-0093-8.

Charlotte S Vlek, Henry Prakken, Silja Renooij, and Bart Verheij. A method for explaining bayesian net-
works for legal evidence with scenarios. Artificial Intelligence and Law, 24(3):285-324, 2016. doi: 10.1007/
s10506-016-9183-4.

Sjoerd T Timmer, John-Jules Ch Meyer, Henry Prakken, Silja Renooij, and Bart Verheij. A two-phase method for
extracting explanatory arguments from bayesian networks. International Journal of Approximate Reasoning, 80:
475494, 2017. doi: 10.1016/j.ijar.2016.09.002.

Will Landecker, Michael D. Thomure, Luis M. A. Bettencourt, Melanie Mitchell, Garrett T Kenyon, and Steven P.
Brumby. Interpreting individual classifications of hierarchical networks. In JEEE Symposium on Computational

68
[309]

[310]

[311]

[312]

[313]

[314]

[315]

[316]

[317]

[318]

[319]

[320]

[321]

[322]

[323]

[324]

[325]

[326]

[327]

Intelligence and Data Mining (CIDM), volume 165, pages 32-38, Singapore, apr 2013. IEEE. ISBN 978-1-4673-
5895-8. doi: 10.1109/CIDM.2013.6597214.

Been Kim, Cynthia Rudin, and Julie A Shah. The bayesian case model: A generative approach for case-based.
reasoning and prototype classification. In Advances in Neural Information Processing Systems, pages 1952-1960,
Montréal, Québec, Canada, 2014. Neural Information Processing Systems Foundation, Inc.

Matthew S Caywood, Daniel M Roberts, Jeffrey B Colombe, Hal S Greenwald, and Monica Z Weiland. Gaussian
process regression for predictive but interpretable machine learning models: An example of predicting mental
workload across tasks. Frontiers in human neuroscience, 10:647-665, 2017. doi: 10.3389/fnhum.2016.00647.
Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for
healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 1721-1730, Sydney, Australia, 2015.
ACM. doi: 10.1145/2783258.2788613.

Jialei Wang, Ryohei Fujimaki, and Yosuke Motohashi. Trading interpretability for accuracy: Oblique treed sparse
additive models. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1245-1254, Sydney, Australia, 2015. ACM. doi: 10.1145/2783258.2783407.

Gerald Fahner. Developing transparent credit risk scorecards more effectively: An explainable artificial intelli-
gence approach. In The Seventh International Conference on Data Analytics, pages 17-24, Athens, Greece, 2018.
Data Analytics.

Daniel Howard and Mark A Edwards. Explainable ai: The promise of genetic programming multi-run subtree
encapsulation. In International Conference on Machine Learning and Data Engineering (i(CMLDE), pages 158—
159, Dallas, Texas, USA, 2018. IEEE. doi: 10.1109/iCMLDE.2018.00037.

Yitao Liang and Guy Van den Broeck. Towards compact interpretable models: Shrinking of learned probabilistic
sentential decision diagrams. In LICAI-17 Workshop on Explainable Al (XAI), pages 31-35, Melbourne, Australia,
2017. International Joint Conferences on Artificial Intelligence, Inc.

Been Kim, Julie A Shah, and Finale Doshi-Velez. Mind the gap: A generative approach to interpretable feature
selection and extraction. In Advances in Neural Information Processing Systems, pages 2260-2268, Montréal,
Québec, Canada, 2015. Neural Information Processing Systems Foundation, Inc.

Berk Ustun, Stefano Traca, and Cynthia Rudin. Supersparse linear integer models for interpretable classification.
Stat, 1050:11-47, 2014.

Alexander Panchenko, Eugen Ruppert, Stefano Faralli, Simone Paolo Ponzetto, and Chris Biemann. Unsupervised
does not mean uninterpretable: The case for word sense induction and disambiguation. In Proceedings of the 15th
Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers,
volume 1, pages 86-98, Valencia, Spain, 2017. Association for Computational Linguistics.

Maria Jose Gacto, Rafael Alcala, and Francisco Herrera. Interpretability of linguistic fuzzy rule-based systems:
An overview of interpretability measures. Information Sciences, 181(20):4340-4360, 2011. doi: 10.1016/j.ins.
2011.02.021.

Salvador Garcia, Alberto Fernandez, Julidn Luengo, and Francisco Herrera. A study of statistical techniques and
performance measures for genetics-based machine learning: accuracy and interpretability. Soft Computing, 13
(10):959, 2009.

Marko Bohanec, Marko Robnik-Sikonja, and Mirjana K]jajié BorStar. Decision-making framework with double-
loop learning through interpretable black-box machine learning models. Industrial Management & Data Systems,
117(7):1389-1406, 2017. doi: 10.1108/imds-09-2016-0409.

Marko Bohanec, Mirjana Kljajié BorSmar, and Marko Robnik-Sikonja. Explaining machine learning models in
sales predictions. Expert Systems with Applications, 71:416428, 2017. doi: 10.1016/j.eswa.2016.11.010.
Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. The dangers
of post-hoc interpretability: Unjustified counterfactual explanations. In Proceedings of the Twenty-Eighth Inter-
national Joint Conference on Artificial Intelligence, (LICAI), pages 2801-2807, Macao, China, 2019. International
Joint Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2019/388.

Julius Adebayo, Justin Gilmer, Ian Goodfellow, and Been Kim. Local explanation methods for deep neural
networks lack sensitivity to parameter values. In 6th International Conference on Learning Representations,
Vancouver, Canada, 2018. ICLR.

Julius Adebayo, Justin Gilmer, Michael Muelly, lan Goodfellow, Moritz Hardt, and Been Kim. Sanity checks
for saliency maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS, pages 9505-9515, Montréal, Canada, 2018. Neural Information
Processing Systems Foundation, Inc.

Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding of gradient-based
attribution methods for deep neural networks. In 6th International Conference on Learning Representations,
Vancouver, Canada, 2018. ICLR.

Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Miller, and Wojciech Samek. Explaining predic-

69
[328]

[329]

[330]

[331]

[332]

[333]

[334]
[335]

[336]

[337]

[338]

[339]

[340]

[341]

[342]

[343]

[344]

[345]

[346]

[347]

tions of non-linear classifiers in nlp. In Proceedings of the Ist Workshop on Representation Learning for NLP,
pages 1-7, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/w16- 1601.
Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Miller, and Wojciech Samek. “what is relevant
in a text document?”: An interpretable machine learning approach. PloS one, 12(8):e0181142, 2017. doi: 10.
137 1/journal.pone.0181142.

Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Miiller. Eval-
uating the visualization of what a deep neural network has learned. [EEE transactions on neural networks and
learning systems, 28(11):2660-2673, 2017. doi: 10.1109/tnnls.2016.2599820.

Wojciech Samek, Thomas Wiegand, and Klaus-Robert Miiller. Explainable artificial intelligence: Understanding,
visualizing and interpreting deep learning models. /TU Journal: ICT Discoveries, 1:1—-10, 2017.

Alexander Binder, Wojciech Samek, Grégoire Montavon, Sebastian Bach, and Klaus-Robert Miiller. Analyzing
and validating neural networks predictions. In Proceedings of the ICML Workshop on Visualization for Deep
Learning, pages 118-121, New York City, New York, USA, 2016. ICML.

Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In NIPS workshop
on Machine Deception, Long Beach, California, USA, 2017. NIPS.

Muriel Gevrey, loannis Dimopoulos, and Sovan Lek. Review and comparison of methods to study the contribution
of variables in artificial neural network models. Ecological modelling, 160(3):249-264, 2003. doi: 10.1016/
s0304-3800(02)00257-0.

Henri J Suermondt and Gregory F Cooper. An evaluation of explanations of probabilistic inference. Computers
and Biomedical Research, 26(3):242-254, 1993. doi: 10.1006/cbmr.1993.1017.

L Richard Ye and Paul E Johnson. The impact of explanation facilities on user acceptance of expert systems
advice. Mis Quarterly, 19(2):157—-172, 1995. doi: 10.2307/249686.

Vincent AWMM Aleven and Kenneth R Koedinger. An effective metacognitive strategy: Learning by doing
and explaining with a computer-based cognitive tutor. Cognitive science, 26(2):147-179, 2002. doi: 10.1016/
s0364-0213(02)00061-7.

Maaike Harbers, Joost Broekens, Karel Van Den Bosch, and John-Jules Meyer. Guidelines for developing ex-
plainable cognitive models. In Proceedings of ICCM, pages 85-90, Berlin, Germany, 2010. Citeseer.

Maaike Harbers, Karel van den Bosch, and John-Jules Meyer. Design and evaluation of explainable bdi agents. In
2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology, volume 2,
pages 125-132, Toronto, Canada, 2010. IEEE. doi: 10.1109/wi-iat.2010.115.

Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, and Hanna Wal-
lach. Manipulating and measuring model interpretability. In NJ/PS Women in Machine Learning Workshop, Long
Beach, California, USA, 2017. NIPS.

Joe Tullio, Anind K Dey, Jason Chalecki, and James Fogarty. How it works: a field study of non-technical users
interacting with an intelligent system. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, pages 31-40, San Jose, California, USA, 2007. ACM. doi: 10.1145/1240624.1240630.

Isaac Lage, Andrew Ross, Samuel J Gershman, Been Kim, and Finale Doshi-Velez. Human-in-the-loop inter-
pretability prior. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In-
formation Processing Systems 2018, NeurIPS, pages 10180-10189, Montréal, Canada, 2018. Neural Information
Processing Systems Foundation, Inc.

Hiva Allahyari and Niklas Lavesson. User-oriented assessment of classification model understandability. In
I] th scandinavian conference on Artificial intelligence, pages 11-19, Trondheim, Norway, 2011. IOS Press. doi:
10.3233/978- 1-60750-754-3-11.

Johan Huysmans, Karel Dejaeger, Christophe Mues, Jan Vanthienen, and Bart Baesens. An empirical evaluation
of the comprehensibility of decision table, tree and rule based predictive models. Decision Support Systems, 51
(1):141-154, 2011. doi: 10.1016/j.dss.2010.12.003.

Mitja LuStrek, Matjaz Gams, Sanda Martinéié-Ipsié, et al. Comprehensibility of classification trees—survey design
validation. In 6th International Conference on Information Technologies and Information Society-ITIS2014, pages
46-61, Smarje’ke toplice, Slovenia, 2014. ITIS.

Todd Kulesza, Simone Stumpf, Weng-Keen Wong, Margaret M Burnett, Stephen Perona, Andrew Ko, and Jan
Oberst. Why-oriented end-user debugging of naive bayes text classification. ACM Transactions on Interactive
Intelligent Systems (TiS), 1(1):2:1—-2:31, 2011. doi: 10.1145/2030365.2030367.

Brian Y Lim and Anind K Dey. Assessing demand for intelligibility in context-aware applications. In Proceedings
of the 11th international conference on Ubiquitous computing, pages 195-204, Orlando, Florida, USA, 2009.
ACM. doi: 10.1145/1620545.1620576.

Vanessa Putnam and Cristina Conati. Exploring the need for explainable artificial intelligence (xai) in intelligent
tutoring systems (its). In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM
Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-
WS.org.

70
[348]

[349]

[350]

[351]

[352]

[353]

[354]

[355]
[356]
[357]
[358]

[359]

[360]

[361]

Pierre Stock and Moustapha Cisse. Convnets and imagenet beyond accuracy: Understanding mistakes and uncov-
ering biases. In Proceedings of the European Conference on Computer Vision (ECCV), pages 498-512, Munich,
Germany, 2018. Springer. doi: 10.1007/978-3-030-01231-1\31.

David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying
interpretability of deep visual representations. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6541-6549, Honolulu, Hawaii, USA, 2017. IEEE. doi: 10.1109/cvpr.2017.354.
Sebastian Lapuschkin, Alexander Binder, Grégoire Montavon, Klaus-Robert Miiller, and Wojciech Samek. An-
alyzing classifiers: Fisher vectors and deep neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2912-2920, Las Vegas, Nevada, USA, 2016. IEEE. doi:
10.1109/cvpr.2016.318.

Vignesh Srinivasan, Sebastian Lapuschkin, Comelius Hellge, Klaus-Robert Muller, and Wojciech Samek. Inter-
pretable human action recognition in compressed domain. In 20/7 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 1692-1696, New Orleans, Louisiana, USA, 2017. IEEE. doi:
10.1 LO9/ICASSP.2017.7952445.

Irene Sturm, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert Miiller. Interpretable deep neu-
ral networks for single-trial eeg classification. Journal of neuroscience methods, 274:141-145, 2016. doi:
10.1016/j.jneumeth.2016.10.008.

Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. Visualizing and understanding neural machine trans-
lation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1150-1159, Vancouver, Canada, 2017. Association for Computational Linguistics.
doi: 10.18653/v1/p17- 1106.

Roy Assaf and Anika Schumann. Explainable deep neural networks for multivariate time series predictions.
In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 6488-6490, Macao,
China, 2019. International Joint Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2019/
932.

Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep
network. University of Montreal, 1341(3):1, 2009.

William F Lawless, Ranjeev Mittu, Donald Sofge, and Laura Hiatt. Artificial intelligence, autonomy, and human-
machine teams: Interdependence, context, and explainable ai. AJ Magazine, 40(3):5—13, 2019.

Lauren Gordon, Teodor Grantcharov, and Frank Rudzicz. Explainable artificial intelligence for safe intraoperative
decision support. JAMA surgery, 154(11):1064—-1065, 2019. doi: 10.1001/jamasurg.2019.2821.

Andreas Holzinger. Interactive machine learning for health informatics: when do we need the human-in-the-loop?
Brain Informatics, 3(2):119-131, 2016. doi: 10.1007/s40708-016-0042-6.

S Ram. The importance of ensuring artificial intelligence and machine learning can be understood at the human
level: Sudha ram. European Journal of Public Health, 29(Supplement_4):ckz185-259, 2019. doi: 10.1093/
eurpub/ckz185.259.

Robert Andrews, Joachim Diederich, and Alan B Tickle. Survey and critique of techniques for extracting
rules from trained artificial neural networks. Knowledge-based systems, 8(6):373-389, 1995. doi: 10.1016/
0950-705 1(96)8 1920-4.

Katja Hansen, David Baehrens, Timon Schroeter, Matthias Rupp, and Klaus-Robert Miiller. Visual interpretation
of kernel-based prediction models. Molecular Informatics, 30(9):817-826, 2011. doi: 10.1002/minf.201 100059.

71
Appendix A. Appendix

Table A.2: Classification of the systematic and literature review articles on explainable artificial intelligence and machine

learning interpretability

 

 

Category Subcategory Reference
Application fields Finance, military and transportation [1]
Application fields Law [1, 9, 10]
Application fields Healthcare [11, 48, 357, 358, 359, 9, 10]
Application fields Human-computer interaction [356]
Approaches - data-driven Bayesian networks [18]
\ . [360, 25, 130, 20, 39, 26, 41, 27, 24,
Approaches - data-driven Neural networks 28, 29, 17, 30]
Approaches - data-driven Support vector machines [31, 19]
Approaches - knowledge-driven Expert systems [32]
Approaches - knowledge-driven Intelligent systems [33]
Approaches - knowledge-driven Recommender systems [34]

Theories and concepts
Output formats
Output formats
Output formats
Problem Types
Problem Types
Generic reviews

See expanded subcategories in Sec. 5
Rule

Textual

Visual

Classification

Regression

[75, 64, 36, 66, 13, 56, 23, 97, 35]
[360, 44, 46, 42, 45, 43, 41, 24, 40, 19]
[38, 37, 32]

[20, 39, 40, 30]

[31, 20, 48, 43, 41, 49, 47, 30]

[47]

[1, 50, 53, 51, 21, 52]

 

Table A.3: Model agnostic methods for explainability generating numerical explanations, classified according to the
output format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G:
Global, L: Local) and input data (NC: Numerical / Categorical, P: Pictorials, T: Textual, TS: Time Series) of the model.

 

 

Method for explainability Authors Ref Year Stage Scope Problem Input
Distill-and-Compare Tan et al. [148] 2018 PH G C/R NC
Robnik-Sikonja and
Explain and Ime Kononenko, Robnik- [150, 2008, PH L Cc NC
Sa: 151] 2018
Sikonja
Strumbelj and
Kononenko, [154, 2010,
Feature contribution Kononenko 155, 2013, PH L C/R NC
et al,  Strumbelj 156] 2009
etal.
Strumbel} and
Feature contribution Kononenko, [157, 2008, PH G C/R NC
x : 158] 2010
Strumbe]j et al.
Feature Importance Henelius et al. [163] 2014 PH G Cc NC
Feature perturbation Strumbelj and [164] 2014 PH G C/R NC
Kononenko
Cortez and = Em- (152, 2011
Global Sensitivity Analysis (GSA) brechts, Cortez and > > PH G C/R NC
153] 2013
Embrechts
Gradient Feature Auditing (GFA) Adler et al. [160] 2018 PH G C/R NC
Influence functions Koh and Liang [161] 2017 PH G Cc P
Monotone Influence Measures Sliwinski et al. [162] 2017 PH L Cc P
Quantitative Input Influence (QW) pata et al. [159] 2016 PH G c NC
functions
SHapley Additive exPlanations
(SHAP) Lundberg and Lee [149] 2017 PH G Cc P

 

72
Table A.4: Model agnostic methods for explainability generating mixed explanations, classified according to the output
format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L:
Local) and input data (NC: Numerical / Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.

 

 

Method for explainability Authors Ref Year Stage Scope Problem Input
Bayesian Teaching Yang and Shafto [186] 2017 PH L C/R NC
Evasion-Prone Samples Selection Liuet al. [190] 2018 PH G Cc T
ExplAlner Spinner et al. [133] 2019 PH Go cyR RANG
Functional ANOVA decomposition,

Variable Interaction Network graph Hooker [183] 2004 PH G C/R NC
Justification Narratives Biran and McKeown [184] 2014 PH G Cc NC
Local Interpretable Model-Agnostic ss [193, .
Explanations (LIME) Ribeiro et al. 134] 2016 PH L Cc P;T
Maximum Mean Discrepancy :

(MMD)-critic Kim et al. [191] 2016 PH L Cc P
Neighborhood-Based Explanations Caruana et al. [189] 1999 PH L Cc NC
Pertinent negatives Dhurandhar et al. [192] 2018 PH L Cc P; NC
Rivelo Tamagnini et al. [185] 2017 PH L Cc T
Sequential Bayesian Quadrature Khanna et al. [187] 2019 PH L Cc P; NC
Set Cover Optimization (SCO) Bien et al. [188] 2011 PH L Cc P; NC

 

Table A.5: Model agnostic methods for explainability generating rule-based explanations, classified according to the
output format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), the scope (G:
Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the model.

 

 

Method for explainability Authors Ref Year Stage Scope Problem Input
Anchors Ribeiro et al. [140] 2018 PH G/L C T
Automated Reasoning Bride et al. [165] 2018 PH G Cc NC
Genetic Rule EXtraction (G-REX) Johansson et al. ‘on 2004 PH G C/R NC
Model Extraction Bastani et al. [168] 2017 PH G C/R NC
Partition Aware Local Model (PALM) _ Krishnan and Wu [169] 2017 PH G C/R NC

 

Table A.6: Model agnostic methods for explainability generating visual explanations, classified according to output
format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), the scope (G: Global,
L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.

 

 

Method for explainability Authors Ref Year Stage Scope Problem Input
Class Signatures Krause et al. [179] 2016 PH G C/R NC
ExplainD Poulin et al. [180] 2006 PH G Cc NC
Explanation Graph based on perturbed  Alvarez-Melis and [177] 2017 PH L c T
input element order Jaakkola

Image Perturbation Fong and Vedaldi [170] 2017 PH L Cc P
Individual Conditional Expectation Goldstein et al. [175] 2015 PH G C/R NC
iVisClassifier Choo et al. [172] 2010 PH G Cc NC
Layer-wise Relevance Propagation Bach et al. [101] 2015 PH L Cc P
Manifold Zhang et al. [181] 2019 PH G C/R NC
MLCube Explorer Kahng et al. [182] 2016 PH G Cc NC
Partial Importance and Individual Con-

ditional Importance plots based on  Casalicchio et al. [176] 2018 PH G C/R NC
Shapley feature importance

Restricted Support Region Set (RSRS) Liu and Wang [171] 2012 PH L Cc T
Detection

Saliency Detection Dabkowskiand Gal [173] 2017 PH L Cc P
Sensitivity analysis Baehrens et al. [174] 2010 PH L Cc P; NC
Spectral Relevance Analysis (SpRAy) Lapuschkin et al. [8] 2019 PH G Cc P
Worst-case perturbations Goodfellow et al. [178] 2015 PH L Cc P

 

73
Table A.7: Methods for explainability for neural networks generating visual explanations, classified according to the
stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local)
and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series).

 

 

Method for explainability Authors Ref Year Stage Scope Problem Input
Activation maps Hamidi-Haines et al. [225] 2019 PH L Cc P

[222,
Activation Maximization Erhan et al., Nguyen 223, 2010, PH L Cc P

etal. 2016

224]
ActiVis Kahng et al. [231] 2018 PH G C/R NC
And-Or Graph (AOG) Zhang et al. [229] 2017. PH G Cc P
Cell Activation Values Karpathy et al. [226] 2016 PH G/L Cc T
sone (CLEAR) Attentive Re mar et al. [194] 2017 PHL c NC
Cnn-Inte Liu et al. [215] 2018 PH G Cc P
Compositionality Liet al. [213] 2016 PH L Cc T
Data-flow graphs Wongsuphasawat et al. [136] 2018 PH G C/R P; NC; T
ne (DepED) Fea Shrikumar et al. [100] 2017 PH OL c P:NC
Deep View (DV) Zhong et al. [233] 2017 PH G C/R P
Deep Visualization Toolbox Yosinski et al. [232] 2015 PH G Cc P
Deep-Taylor Decomposition Montavon et al. [208] 2017 PH G Cc P
DeepResolve Liu and Gifford [195] 2017 PH G Cc NC
Explanatory Graph Zhang et al. [227] 2018 PH G Cc P
Feature maps Zhang et al. [204] 2018 AH L Cc P
(GAN) Das Adversarial Network pau etal. (220) 2019 PHL c P
GradCam Selvaraju et al. [196] 2017 PH L Cc P
Guided BackProp and Occlusion Goyal et al. [198] 2016 PH L Cc P
Guided Feature Inversion Duet al. [202] 2018 PH L Cc P
Hidden Activity Visualization Rauber et al. [218] 2017 PH G Cc P
Important Neurons and Patches Lengerich et al. [221] 2017 PH G Cc P
iNNvestigate Alber et al. [234] 2019 PH L Cc P
Integrated Gradients Sundararajan et al. [93] 2017 PH L Cc P
Inverting Representations Mahendran and Vedaldi [201] 2015 PH L Cc P
LRP w/ Relevance Conservation Arras et al. [206] 2017 PH L Cc T
ERP w/ Local Renormalization pi vder et al, 207] 2016 PH LC P
Layers
LSTMVis Strobelt et al. [135] 2018 PH G/L Cc T
N¢VIS Streeter et al. [236] 2001 PH G C/R NC
Net2Vec Fong and Vedaldi [200] 2018 PH G Cc P
Neural Network and CBR Twine ony and Keane [212] 2019 PH OL c P
systems
OcclusionSensitivity Zeiler and Fergus [199] 2014 PH G Cc P
OpenBox Chu et al. [214] 2018 PH G Cc P; NC
PatternNet, PatternAttribution Kindermans et al. [210] 2018 PH L Cc P
Prediction Difference Analysis Zintgraf et al. [205] 2017 PH L Cc P
Principal Component Analysis Aubry and Russell [216] 2015 PH G Cc P
Receptive Fields He and Pugeault [209] 2017. PH G Cc P
Relevant Features Selection Mogrovejo et al. [211] 2019 PH L Cc P
Saliency maps Olah et al. [230] 2018 PH G/L Cc P
Saliency maps Simonyan et al. [98] 2014 PH L Cc P
Seq2seq-Vis Strobelt et al. [235] 2018 PH L Cc T
SmoothGrad Smilkov et al. [203] 2017 PH L Cc P
iSwar w/ Auxiliary Features pani and Mooney [197] 2017 PH OL c P
Symbolic Graph Reasoning (SGR) Liang et al. [228] 2018 AH G C/R P
t-SNE maps Zahavy et al. [217] 2016 PH G Cc NC
Tree View Thiagarajan et al. [219] 2016 PH G Cc P

 

74
Table A.8: Methods for explainability for neural networks generating rule-based explanations, classified according to the
stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local)
and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.

 

 

Method for explainability Authors Ref Year Stage Scope Problem Input
C4.5Rule-PANE Zhou and Jiang [248] 2003 PH L C/R NC
DecText Boz [249] 2002 PH G Cc NC
1139, 2017,
Discretized Interpretable Multi Layer Bologna and =. 240, , oN.
Perceptrons (DIMLP) Hayashi, Bologna 241, 1998, PH GiL C¢ P|NC;T
132] 2018

Discretizing Hidden Unit Activation

Values by Clustering Setiono and Liu [237] 1995 PH G Cc NC
as Frosst and Hin- [245, 2017,
DT extraction ton, Zhang et al. 246) 2019 PH G c P
Interval Propagation Palade et al. [141] 2001 PH G Cc NC
Iterative Rule Knowledge Distillation Hu et al. [254] 2016 AH G Cc T
Neural Network Knowledge eXtrac-
tion (NNKX) Bondarenko et al. [238] 2017 PH G Cc NC
Rule Extraction From Neural net-
work Ensemble (REFNE) Zhou et al. [247] 2003 PH G C/R NC
Rule Extraction from Neural Net-
work using Classified and Misclassi- Biswas et al. [243] 2017 PH G Cc NC
fied data (RxNCM)
as : Augasta and
Rule Extraction by Reverse Engineer: sthirvalavaku- [242] 2012 AH G = C/R— NC
ing (RxREN) mar
Symbolic logic integration Tran [255] 2017 AH G C/R NC
Symbolic rules Garcez et al. [244] 2001 PH G C/R NC
Tree Regularization Wuetal. [252] 2018 AH G Cc NC
Craven and Shav-
Trepan lik, Craven and [250, 1994, PH G C/R NC
: 251] 1996
Shavlik
Validity Interval Analysis (VIA) Thrun [239] 1995 PH G C/R TS
Word Importance Scores Murdoch and [253] 2017 PH G Cc T

Szlam

 

75
Table A.9: Methods for explainability for neural networks generating textual and numerical explanations, classified
according to the output format (N: Numerical, T: Textual), stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C:
Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T:
Textual, TS: Time Series) of the underlying model.

 

 

Method for explainability Authors Ref Year ue vt Stage Scope Problem Input
Causal Importance Féraud and Clérot [261] 2002 N PH G Cc NC
Concept Activation Vectors Kim et al. [145] 2018 N PH G Cc P
Contextual Importance, Utility Framling [262] 1996 N PH G/L Cc NC
InterpNET Barratt [256] 2017 T PH L Cc P
Most-Weighted-Path, Most- Garefa-Magarifio
Weighted-Combination, 8 [257] 2019 T PH L Cc TS

: : etal.
Maximum-Frequency-Difference
Probes Alain and Bengio [144] 2017 N PH G Cc P
Rationales Lei et al. [258] 2016 T PH L Cc T
REcurrent LEXicon NETwork
(RELEXNET) Clos et al. [263] 2017 N AH G Cc

ce Teamisinatt . [259, 2016,

Relevance, Discriminative Loss Hendricks et al. 137] 2018 T PH L Cc P
Singular Vector Canonical Corre- Raghu et al. 1260] 2017 N PH G C/R P

lation Analysis (SVCCA)

 

Table A.10: Methods for explainability for neural networks generating mixed explanations, classified according to the
stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local)
and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.

 

 

Method for explainability Authors Ref Year Stage Scope Problem Input
Activation values of hidden neurons Tamajka et al. [268] 2019 AH L Cc P
Attention Alignment Kim et al. [264] 2018 PH L Cc P
Deterministic Finite Automaton Mayr and Yovine [266] 2018 PH L Cc NC
Deterministic Finite-state Automata Omlin and Giles 1267] 1996 PH G Cc NC
(DFAs)

Image Caption Generation with Atten- Xu etal. [147] 2015 PH L Cc

tion Mechanism

Pointing and Justification Model (PJ-X) _ Park et al. [265] 2018 AH L Cc

 

76
Table A.11: Methods for explainability for rule-based construction approaches, classified according to the output format
(M: Mixed, N: Numerical, R: Rules, T: Textual, N: Numerical), stage (AH: Ante-hoc, PH: Post-hoc), type of problems
(C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials,
T: Textual, TS: Time Series) of the underlying model.

 

 

Method for explainability Authors Ref Year nue vt Stage Scope Problem Input

ac oe Optimization Goro and Freitas [269] 2016 R AH G c NC

AntMiner+ and ALBA Verbeke et al. [270] 2011 R AH G Cc NC

Argumentation Rizzo and Longo 2018 R AH G Cc NC

Argumentation Zeng et al. [279] 2018 R AH G C/R P
[273, 2012,

Bayesian Rule Lists (BRL) Letham et al. 274, 2013, R AH G Cc NC
275] 2015

: [276, 2016,

Bayesian Rule Sets (BRS) Wang et al. 277) 2017 R AH G Cc NC

Interpretable Decision Set Lakkaraju et al. [272] 2016 R AH G Cc NC

Exception Directed Acyclic :

Graphs (EDAGs) Gaines [271] 1996 M AH G Cc NC

ExpliClas Alonso [289] 2019 M PH L Cc NC

First Order Combined Learner :

(FOCL) Pazzani [278] 1997 R AH G Cc NC

Fuzzy logic Pierrard et al. [282] 2018 R AH L Cc NC

Fuzzy system Jin [281] 2000 R AH G Cc NC

Fuzzy Inference-Grams (Fine pancho et al. [288] 2013. Vv PH OG c NC

grams)

Fuzzy Inference Systems Keneni et al. [287] 2019 T PH L Cc TS

Genetics-Based Machine —Ishibuchi and No-

Learning (GBML) jima [280] 2007 R AH G Cc NC

Interpretable Classification

Rule Mining (CRM) Cano et al. [284] 2013 R AH G Cc NC

Linear Programming Relax- Malioutov et al.,  [285, 2017,

ation Suet al. 286] 2016 * AH Gs NC

Multi-Objective Evolutionary

Algorithms based Interpretable Wang and Palade [283] 2011 R AH G Cc NC

Fuzzy (MOEAIF)

Mycin Shortliffe et al. [3] 1975 T PH L Cc NC

 

77
Table A.12: Methods for explainability for data-driven approaches, classified according to the output format (M: Mixed,
N: Numerical, R: Rules, T: Textual, N: Numerical), stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classifi-
cation, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorial, T: Textual,

TS: Time Series).

 

 

Method for Authors Ref Year Construction Output Stage Scope Problem Input
explainability approach Format
Contribution Propa-  Landecker [308] 2013 Hierarchical Vv PH L Cc P
gation etal. networks
DT extraction Andrzejak 994) 2013 Distributed og PH OG c/R NC
etal. DTs
Ferri et al.,
as Van Assche [292, 2002,
DT extraction and Block. 293] 2007 Ensembles R PH G Cc NC
eel
DT extraction Alonsoetal. [291] 2018 Ensembles PH L NC
Piscriminative Pat- Gao et al. [297] 2017 Ensembles PH T
Explaining
Bayesian Met-  vapetal. [305] 2008 Bayesian R re c NC
work Inferences networks
(EBI)
Hyperplane-
ExtractRule Fung et al. [138] 2005 Based Linear R PH G Cc P; NC
Classifiers
Factorized Asymp- H d
totic Bayesian Havashi an [295] 2018 Ensembles R PH G Cc NC
(FAB) inference y
Feature Tweaking Tolomel [290] 2017 Ensembles N PH L Cc NC
Important Support
Vectors and Border B&bella [146] 2009 SVM N PHL c NC
. : etal.
Classification
inTrees Deng [296] 2018 Ensembles R PH G C/R NC
Nomograms Jakulinetal. [302] 2005 SVM Vv PH G Cc NC
Nomograms Moana [304] 2004 Naive Bayes Vv PH G Cc NC
Probabilistically Ti B .
Supported Argue mer 307] 2017 7ayesian M PH OG Cc NC
etal. networks
ments
Scenarios Viek et al. [306] 2016 Bayesian T PH OL Cc NC
networks
Ms’ Hamel [301] 2006 SVM Vv PH OG Cc NC
SVM-+Prototypes Nijiezetal. [299] 2002 SVM M PH G NC
tyes Space Proto; tan et al. [298] 2016 Ensembles  M PH OL Cc NC
Visualization for
Risk Factor Analy- Cho et al. [803] 2008 SVM Vv PH G Cc NC
sis (VRIFA)
Weighted — Linear Caragea [300] 2003 SVM N re c NC
Classifier etal.

 

78
Table A.13: Methods for explainability generating white-box models, classified according to the output format (M:
Mixed, N: Numerical, R: Rules, T: Textual, N: Numerical), stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C:
Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorial, T:
Textual, TS: Time Series) of the underlying model.

 

 

Method for Authors Ref Year Construction Output Stage Scope Problem Input
explainability approach Format
Bayesian Case : Bayesian .
Model (BCM) Kim et al. [309] 2014 case-based M AH G Cc P;T
reasoning
Gaussian — Pro- Caywood Gaussian
cess Regression etal [3810] 2017 Process N AH G R TS
(GPR) . Regression
General Additive Lou et al,  [99, Additive
Models (GAMs) Lou et al. 131] 2012 models M AH G CIR NC
GAMs with pair- Caruana Additive
wise interactions et al. Lou [811] 2015 M AH G C/R NC
(GA2Ms) etal. models
: Mind the P;
ee (wom Kim etal. [316] 2015 Gap Model M AH G Cc NC;
(MGM) T
Multi-Run Howard and Tree-based
Subtree Encapsu- [3814] 2018 Genetic Proe M AH G Cc NC
. Edwards :
lation gramming
Oblique Treed
Sparse Add Wanget al. [312] 2015. Additive N AH GC NC
(OT-SpAMs)
Probabilistic : Probabilistic
Sentential Deci- Liang and sentential
. . Van den [315] 2017 a R AH G C/R NC
sion Diagrams Broeck decision
(PSDD) diagrams
Supersparse Lin- Scoring sys-
ear Integer Model —_ Ustun et al. [817] 2014 tems N AH G Cc NC
(SLIM)
Unsupervised.
Interpretable Panchenko [318] 2017 UIWSD Vv AH G c T
Word. Sense etal.
Disambiguation
Transparent Gen-
calized Addie Fahner [313] 201g Adahive R AH GC NC
(TGAMT)

 

79
Table A.14: List and classification of the scientific articles proposing human-centered approaches to evaluate methods
for explainability, classified according to the construction approach, the type of measurement employed (qualitative or

quantitative), and the format of their output explanation.

 

 

Measure type Construction approach Output format Reference
Qualitative Autonomous agents Textual [54]
Qualitative Context-aware systems Textual [346, 77]
Qualitative Context-aware systems Visual [346]
Qualitative Data clustering Mixed [316]
Qualitative Expert Systems Textual [334, 335]
Qualitative Learning Systems Textual [347]
Qualitative ML (Model agnostic) Visual [103, 340]
Qualitative Fisher vector classifiers Visual [350]
Qualitative Support Vector Machine Visual [351]
Qualitative Neural networks Visual [354, 355, 353, 352]
Quantitative Case-based Reasoning Mixed [309]
Quantitative Context-aware systems Textual [77]
Quantitative Neural networks Visual [348]
Quantitative Neural networks Visual [349]
Quantitative Decision trees Mixed [344]
Quantitative Kernel-based neural networks Visual [361]
Quantitative Learning Systems Textual [336, 337, 338]
Quantitative ML (Model agnostic) Mixed [341]
Quantitative ML (Model agnostic) Rules [140]
Quantitative ML (Model agnostic) Mixed [339]
Quantitative ML (Model agnostic) Visual [133]
Quantitative Naive Bayes Textual [345, 60]
Quantitative Rule-based classifier Mixed [842, 343]
Quantitative Decision trees Mixed [842, 343]
Quantitative Decision tables Mixed [343]

 

Table A.15: Classification of the scientific articles proposing comparative approaches to evaluate methods for explain-

ability, classified according to the methodology followed to carry out the comparison task.

 

Comparison approach

Reference

 

Sensitivity to input perturbation

Sensitivity to model parameter randomization

Explanation completeness

[324, 325, 90, 326, 327, 331, 332, 92, 330, 329]

[324, 325, 355]
[333]

 

80
Table A.16: List of the methods for explainability generating visual explanations, such as heat-maps, whose degree of
explainability is evaluated by comparison (Listed in the fourth column). These comparisons were carried out over different

types of input data (listed in the third column).

 

Method for explainability (references)

Acronym Input type

Compared with (references)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Deep-Taylor Decomposition [208] DTD Pictorial GBP, IG, SA, PM in [92]
ar IG, LRP in [326]
DeepLIFT [100] DLT Pictorial IG. SA in [332]
GC, GBP and SG [324]
Gradient* Input [100] GI Pictorial GC, GBP, IG, SG [325]
IG, LIME, OS, SM, SHAP in [90]
ar GI, GBP, SG [324]
GrandCam [196] GC Pictorial GL GBP. IG, SG [325]
GI, GC, SG [324]
Guided BackProp [198] GBP Pictorial GI, GC, IG, SG [325]
DTD, IG, SA, PM in [92]
GI, GC, GBP, SG [325]
LRP, LIME, OS, SM, SHAP [90]
Integrated Gradients [93] IG Pictorial DLT and LRP in [326]
DLT, SA in [332]
DTD, GBP, SA, PM in [92]
IG, LIME, OS, SM, SHAP in [90]
Layer-wise Relevance Propagation . . DLT, IG in [326]
[101] ERP Pictorial SA in [331, 329, 330]
OS in [331]
: Pictorial
Local Interpretable Model-Agnostic | ty4z = Numerical / GI, IG, OS, SM, SHAP in [90]
Explanations [134] :
Categorical
: sas ok GI, IG, LIME, SM, SHAP in [90]
Occlusion Sensitivity [199] Os Pictorial LRP in [331]
LRP in [331]
sas : ok DLT, IG in [332]
Sensitivity Analysis [174] SA Pictorial DTD, GBP, IG, PM in [92]
LRP in [329, 330]
PatternNet [210] PM Pictorial DTD, GBP, IG, SA in [92]
Saliency Maps [98] SM Pictorial GI, IG, LIME, OS, SHAP in [90]
SHapley Additive exPlanations [149] SHAP Pictorial GI, IG, LIME, OS, SM in [90]
ar GI, GC, GBP [324]
SmoothGrad [203] SG Pictorial GL GC. GBP and IG [325]
hol Relevance Propagation LRP Textual SA in [327, 330]
Sensitivity Analysis methods [174] SA Textual LRP in [327, 330]

 

81
NISTIR 8312

Four Principles of Explainable Artificial
Intelligence

P. Jonathon Phillips
Carina A. Hahn
Peter C. Fontana
Amy N. Yates
Kristen Greene

David A. Broniatowski
Mark A. Przybocki

This publication is available free of charge from:
https://doi.org/10.6028/NIST.IR.83 12

NIST

National Institute of
Standards and Technology
U.S. Department of Commerce
NISTIR 8312

Four Principles of Explainable Artificial
Intelligence

P. Jonathon Phillips

Carina A. Hahn

Peter C. Fontana

Amy N. Yates

Kristen Greene

Information Access Division
Information Technology Laboratory

David A. Broniatowski
Information Technology Laboratory

Mark A. Przybocki
Information Access Division
Information Technology Laboratory

This publication is available free of charge from:
https://doi.org/10.6028/NIST.IR.83 12

September 2021

 

U.S. Department of Commerce
Gina M. Raimondo, Secretary

National Institute of Standards and Technology
James K. Olthoff, Performing the Non-Exclusive Functions and Duties of the Under Secretary of Commerce
for Standards and Technology & Director, National Institute of Standards and Technology
Certain commercial entities, equipment, or materials may be identified in this document in order to describe
an experimental procedure or concept adequately. Such identification is not intended to imply
recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to
imply that the entities, materials, or equipment are necessarily the best available for the purpose.

National Institute of Standards and Technology
Interagency or Internal Report 8312
Natl. Inst. Stand. Technol. Interag. Intern. Rep. 8312, 43 pages (September 2021)

This publication is available free of charge from:
https://doi.org/10.6028/NIST.IR.8312
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

Abstract

We introduce four principles for explainable artificial intelligence (AD that comprise fun-
damental properties for explainable AI systems. We propose that explainable AI systems
deliver accompanying evidence or reasons for outcomes and processes; provide explana-
tions that are understandable to individual users; provide explanations that correctly reflect
the system’s process for generating the output; and that a system only operates under condi-
tions for which it was designed and when it reaches sufficient confidence in its output. We
have termed these four principles as explanation, meaningful, explanation accuracy, and
knowledge limits, respectively. Through significant stakeholder engagement, these four
principles were developed to encompass the multidisciplinary nature of explainable AI,
including the fields of computer science, engineering, and psychology. Because one-size-
fits-all explanations do not exist, different users will require different types of explanations.
We present five categories of explanation and summarize theories of explainable AI. We
give an overview of the algorithms in the field that cover the major classes of explainable
algorithms. As a baseline comparison, we assess how well explanations provided by people
follow our four principles. This assessment provides insights to the challenges of designing
explainable AI systems.

Key words

Artificial Intelligence (AD; explainable AI; explainability; trustworthy AI.
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

Executive Summary

The AI space is vast, complicated, and continually evolving. With advances in computing
power and ever-larger datasets, AI algorithms are being explored and developed for use
in a wide variety of application spaces, with a variety of potential users and associated
risks. The AI community is pursuing explainability as one of many desirable character-
istics for trustworthy AI systems. Working with the AI community, NIST has identified
additional technical characteristics to cultivate trust in AI. In addition to explainability and
interpretability, among other AI system characteristics proposed to support system trust-
worthiness are accuracy, privacy, reliability, robustness, safety, security (resilience), miti-
gation of harmful bias, transparency, fairness, and accountability. Explainability and other
AI system characteristics interact at various stages in the AI lifecycle. While all are criti-
cally important, this work focuses solely on principles of explainable AI systems.

In this paper, we introduce four principles that we believe comprise fundamental prop-
erties for explainable AI systems. These principles of explainable AI were informed by
engagement with the larger AI community through a NIST public workshop and public
comment period. We recognize that not all AI systems may require explanations. How-
ever, for those AI systems that are intended or required to be explainable, we propose that
those systems adhere to the following four principles:

Explanation: A system delivers or contains accompanying evidence or reason(s) for out-
puts and/or processes.

Meaningful: A system provides explanations that are understandable to the intended con-
sumer(s).

Explanation Accuracy: An explanation correctly reflects the reason for generating the
output and/or accurately reflects the system’s process.

Knowledge Limits: A system only operates under conditions for which it was designed
and when it reaches sufficient confidence in its output.

In this work, we recognize the importance of both process-based and outcome-based
explanations, as well as the importance of explanation purpose and style. For example, AI
developers and designers may have very different explanation needs than policy makers and
end users. Therefore, why an explanation is requested and how it is delivered may differ
depending on the AI users. These four principles are heavily influenced by considering
the AI system’s interaction with the human recipient of the information. The requirements
of the given situation, the task at hand, and the consumer will all influence the type of
explanation deemed appropriate for the situation. These situations can include, but are not
limited to, regulator and legal requirements, quality control of an AI system, and customer
relations. Our four principles of explainable AI systems are intended to capture a broad set
of motivations, reasons, and perspectives. The principles allow for defining the contextual
factors to consider for an explanation, and pave the way forward to measuring explanation
quality.

ii
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

We imagine that given the complexity of the AI space, these principles will benefit
from additional refinement and community input over time. We fully acknowledge that
there are numerous other socio-technical factors that influence AI trustworthiness beyond
explainability. This work on principles of explainable AI systems is part of the much
larger NIST AI portfolio! around trustworthy AI data, standards, evaluation, validation,
and verification—all necessary for AI measurements. NIST is a metrology institute and
as such, defining initial principles of explainable AI systems acts as a roadmap for future
measurement and evaluation activities. The agency’s AI goals and activities are prioritized
and informed by its statutory mandates, White House directions, and the needs expressed
by U.S. industry, other federal agencies, and the global AI research community. The current
work is but one step in this much larger space, and we imagine this work will continue to
evolve and progress over time, much like the larger AI field.

 

'NIST AI Program Fact Sheet: https://www-nist.gov/system/files/documents/2021/08/10/AI%20Fact%
20Sheet%200615%20FINAL.pdf

iii
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

HR wm B&B we

9

Table of Contents

Introduction

Four Principles of Explainable AI
2.1 Explanation

2.2 Meaningful

2.3. Explanation Accuracy

2.4 Knowledge Limits

2.5 Summary

Purposes and styles of explanations
Risk Management of Explainable AI
Overview of Principles in the Literature

Overview of Explainable AI Algorithms
6.1 Self-Interpretable Models
6.2 Post-Hoc Explanations
6.2.1 Local Explanations
6.2.2. Global Explanations
6.3. Adversarial Attacks on Explainability

Evaluating Explainable AI Algorithms
7.1 Evaluating Meaningfulness
7.2 Evaluating Explanation Accuracy

Humans as a Comparison Group for Explainable AI
8.1 Explanation

8.2 Meaningful

8.3. Explanation Accuracy

8.4 Knowledge Limits

Discussion and Conclusions

References

Fig. 1 Illustration of the four principles of explainable artificial intelligence. Ar-
rows indicate that for a system to be explainable, it must provide an explana-
tion. The remaining three principles are the fundamental properties of those

List of Figures

explanations.

Fig. 2 Illustration of our elements of explanation styles.

iv

PND UMP WWN —

—
>

SRR ot
Ap WWYN WN

—
Don uw

NN ee iM
CoO OO

nn WN
oem
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

1. Introduction

When the father of one of the authors was diagnosed with cancer, they went to speak with
his oncologist. The oncologist described the state of his cancer and went through strategies
and options for treatment. The oncologist answered the father’s questions and explained his
role in his treatment. The author’s father felt he was a partner and had some control. The
father trusted the treatment because he received a meaningful and understandable explana-
tion about the process. The doctor’s bedside manner won over the father. The medical arts
have changed, and possessing a good bedside manner has become de rigueur. When arti-
ficial intelligence (AD systems contribute to a diagnosis, they could support good bedside
manners by explaining their recommendations to physicians.

Medical diagnoses are just one example where AI systems contribute to decisions that
impact a person’s life. Other examples are systems which evaluate loan applications and
recommend jail sentences. The nature of these decisions has spurred a drive to create algo-
rithms, methods, and techniques to accompany outputs from AI systems with explanations.
This drive is motivated in part by laws and regulations which state that decisions, including
those from automated systems, must provide information about the reasoning behind those
decisions”. It is also motivated by the desire to create trustworthy AI [49, 109, 131].

Explainable AI is one of several properties that characterize trust in AI systems [121,
127, 134]. Other properties include resiliency, reliability, bias, and accountability. Usually,
these terms are not defined in isolation, but as a part or set of principles or pillars. The
definitions vary by author, and they focus on the norms that society expects AI systems to
follow. Based on the calls for explainable systems [59], it can be assumed that the failure
to articulate the rationale for an answer can affect the level of trust users will grant that
system. Suspicions that the system is biased or unfair can raise concerns about harm to
individuals and to society [119, 146]. This may slow societal acceptance and adoption of
the technology.

With the increased call for explanations, the field needs a principled method that char-
acterizes a good explanation from an AI system. First, the characterization needs to be
human-centered, because humans consume them. Second, they need to be understandable
to people. Third, explanations should correctly reflect the system’s process for generating
the output. To foster confidence in explanations, the system should indicate when it is op-
erating outside its designed conditions. These core concepts of a good explanation are the
basis for our four principles of explainable AI.

Although these principles may affect the methods in which algorithms operate to meet
explainable AI goals, the focus of the concepts is not algorithmic methods or computations
themselves. Also, the principles do not pertain to the system’s usage during deployment.
Rather, we present four principles organized around the humans that consume the expla-
nations. They provide a structure to begin measuring components of explanations: their
quality, goodness, accuracy, and limitations. To measure explanations in a structured way

 

The Fair Credit Reporting Act (FCRA) and the European Union (E.U.) General Data Protection Regulation
(GDPR) Article 13.
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

is essential for the field to make progress toward concrete definitions by which explana-
tion quality can be measured. They serve as a guide for future research directions for the
field. The four principles support the foundation of explainable AI measurement, policy
considerations, safety, acceptance by society, and other aspects of AI technology.

We present and discuss the principles in Section 2. We adopt an expansive view of
explanations and characterize the space in Section 3. We outline risks introduced by ex-
plainable AI — especially those introduced if the principles are not met (Section 4). To
put current work into context, we provide a review of current explainable AI methods and
evaluation metrics, and other existing principles for explainable AI (Sections 5, 6, and 7).
Finally, we review existing literature to assess the extent to which humans meet the same
principles we introduce for AI (Section 8). Performance expectations can vary for humans
and machines. Although in some contexts, these differing expectations may or may not be
appropriate, a baseline on which they could be compared is needed.

2. Four Principles of Explainable AI

We present four fundamental principles for explainable AI systems. These principles are
heavily influenced by considering the AI system’s interaction with the human recipient of
the information. The requirements of the given situation, the task at hand, and the consumer
will all influence the type of explanation deemed appropriate for the situation. These situa-
tions can include, but are not limited to, regulator and legal requirements, quality control of
an AI system, and customer relations. Our four principles are intended to capture a broad
set of motivations, reasons, and perspectives. Our principles apply to systems that produce
explanations, and they support the full range of AI techniques, not only machine learning
ones.

Before we delve into the principles, for this document, we operationally define three
key terms: explanation, output, and process. An explanation is the evidence, support, or
reasoning related to a system’s output or process. We define the output of a system as 1) the
outcome from or ii) the action taken by a machine or system performing a task. The output
of a system differs by task. For a loan application, the output is a decision: approved or
denied. For a recommendation system, the output could be a list of recommended movies.
For a grammar checking system, the output is grammatical errors and recommended cor-
rections. For a classification system, it could be an object identifier or a spam detector.
For automated driving, it could be the navigation itself. The process refers to the proce-
dures, design, and system workflow which underlie the system (c.f. [50]). This includes
documentation about the system, information on data used for system development or data
stored, and related knowledge about the system.

Briefly, our four principles of explainable AI are:

Explanation: A system delivers or contains accompanying evidence or reason(s) for out-
puts and/or processes.
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

Meaningful: A system provides explanations that are understandable to the intended con-
sumer(s).

Explanation Accuracy: An explanation correctly reflects the reason for generating the
output and/or accurately reflects the system’s process.

Knowledge Limits: A system only operates under conditions for which it was designed
and when it reaches sufficient confidence in its output.

These are defined and put into context in more detail below. Figure 1 shows the prin-
ciples and indicates that for a system to be considered explainable, it must first have an
explanation or contain accompanying evidence which can be accessed.

 

Delivers or contains accompanying evidence or reason(s) for outputs and/or processes

tS

| Explanation:

 

 

a M i . ~ fo 1 . » / 1 T .
eaningful: \ { Explanation Accuracy: { Knowledge Limits:
System provides Explanation correctly System only operates
explanations that reflects the reason for under conditions for
are understandable generating the output which it was designed
to the intended and/or accurately and when it reaches
consumer(s) reflects the system's sufficient confidence in
\ \ process J \ its output.
\ J \

Fig. 1. [stration of the four principles of explainable artificial intelligence. Arrows indicate that
for a system to be explainable, it must provide an explanation. The remaining three principles are
the fundamental properties of those explanations.

2.1 Explanation

The Explanation principle states that for a system to be considered explainable it supplies
evidence, support, or reasoning related to an outcome from or a process of an AI system.
By itself, the explanation principle is independent of whether the explanation is correct,
informative, or intelligible. This principle does not impose any metric of quality on those
explanations. These factors are components of the meaningful and explanation accuracy
principles. Explanations in practice will vary, and should, according to the given system
and scenario. This means there will be a large range of ways an explanation can be executed
or embedded into a system. To accommodate a large range of applications we adopt a
deliberately broad definition of an explanation.

2.2 Meaningful

A system fulfills the Meaningful principle if the intended recipient understands the system’s
explanation(s). There are commonalities across explanations which can make them more
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

meaningful [84]. For example, stating why the system did behave a certain way can be
more understandable than describing why it did not behave a certain way [76]. Many fac-
tors contribute to what individual people will consider a “good” explanation [55, 84, 139].
Therefore, developers need to consider the intended audience [44]. Several factors influ-
ence what information people will find important, relevant, or useful. These include a
person’s prior knowledge and experiences and the overall psychological differences be-
tween people [18, 64, 90]. Moreover, what they consider meaningful will change over time
as they gain experience with a task or system [18]. Different groups of people will also
have different desires from a system’s explanations [13, 44, 50]. Groups may be defined
broadly according to their role or relationship to the system. For example: developers of a
system are likely to have different desires from an explanation compared to an end-user.

In addition to its audience, what is considered meaningful will vary according to the ex-
planation’s purpose. Different scenarios and needs will drive what is important and useful
in a given context. Meeting the Meaningful principle will be accomplished by understand-
ing the audience’s needs, level of expertise, and relevancy to the question or query at hand.
We provide a more detailed discussion of these purposes in Section 3.

Measuring the meaningful principle is an area of ongoing work (Section 7.1). The
challenge is to develop measurement protocols that adapt to different audiences. Rather
than viewing this as a burden, we argue that both the awareness and appreciation of an
explanation’s context support the ability to measure the quality of AI explanations. Scoping
these factors will therefore bound the possibilities for how to execute the explanation in a
goal-oriented and meaningful way.

2.3 Explanation Accuracy

Together, the Explanation and Meaningful principles only call for a system to produce
explanations that are intelligible to the intended audience. These two principles do not
require that an explanation correctly reflects a system’s process for generating its output.
The Explanation Accuracy principle imposes veracity on a system’s explanations.

Explanation accuracy is a distinct concept from decision accuracy. Decision accuracy
refers to whether the system’s judgment is correct or incorrect. Regardless of the system’s
decision accuracy, the corresponding explanation may or may not accurately describe how
the system came to its conclusion or action. Researchers in AI have developed standard
measures of algorithm and system accuracy [23, 29, 52, 96, 97, 99, 103, 114]. While these
established decision accuracy metrics exist, researchers are in the process of developing
performance metrics for explanation accuracy. In Section 7.2, we review current work on
this subject.

Additionally, explanation accuracy needs to account for the level of detail in the expla-
nation. For some audiences and/or purposes, simple explanations will suffice. The given
reasoning might succinctly focus on the critical point(s) or provide a high level reasoning
without extensive detail. These simple explanations could lack nuances that are necessary
to completely characterize the algorithm’s process for generating its output. However, these
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

nuances may only be meaningful to certain audiences, such as experts of the system. This
is similar to how humans approach explaining complex topics. A professor of neuroscience
may explain a new finding with extensive and technical details to a colleague. That same
finding will likely be distilled and changed for presenting to an undergraduate student in
order to present the pertinent and higher level details. That same professor may explain the
finding very differently to their untrained friends and parents.

Together, this highlights the point that explanation accuracy and meaningfulness inter-
act. A detailed explanation may accurately reflect the system’s processing, but sacrifice
how useful and accessible it is to certain audiences. Likewise, a brief, simple explanation
may be highly understandable but would not fully characterize the system. Given these
considerations, this principle allows for flexibility in explanation accuracy metrics.

2.4 Knowledge Limits

The previous principles implicitly assume that a system is operating within the scope of
its design and knowledge boundaries. The Knowledge Limits principle states that systems
identify cases in which they were not designed or approved to operate, or in cases for which
their answers are not reliable. By identifying and declaring knowledge limits, this practice
safeguards answers so that a judgment is not provided when it may be inappropriate to do
so. This principle can increase trust in a system by preventing misleading, dangerous, or
unjust outputs.

There are two ways a system can reach or exceed its knowledge limits. In one way, the
operation or query to the system can be outside its domain. For example, in a system built
to classify bird species, a user may input an image of an apple. The system could return an
answer to indicate that it could not find any birds in the input image; therefore, the system
cannot provide an answer. This is both an answer and an explanation. In a second way, the
confidence of the most likely answer may be too low, depending on an internal confidence
threshold. To revisit an example of the bird classification system, the input image of a bird
may be too blurry to determine its species. In this case, the system may recognize that the
image is of a bird but that the image is of low quality. An example output may be: “I found
a bird in the image, but the image quality is too low to identify it.”

2.5 Summary

Given the wide range of needs and applications of explainable AI systems, a system may
be considered more explainable, or better able to meet the principles, if it can generate
more than one type of explanation. Further, the metrics used to evaluate the accuracy of an
explanation may not be universal or absolute. A body of ongoing work currently seeks to
develop and validate explainable AI methods. An overview of these efforts is provided in
Sections 6 and 7. The four principles serve as a guidance for how to consider whether the
explanation itself meets user needs.

The field of explainable AI is an area of active research. Our understanding of these
systems and their use will vary as the field grows with new knowledge and data. Therefore,
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

these principles serve as a way to guide how we think about the needs of the system. These
principles provide a basis for approaching new challenges and questions.

3. Purposes and styles of explanations

To illustrate the broad range of explanations, we characterize explanations by two proper-
ties: purpose and style. Purpose is the reason why a person requests an explanation or what
question the explanation intends to answer. Style describes how an explanation is delivered.

The audience will strongly influence the purpose of the explanation and the information
it provides. This information will vary according to different groups of people and their role
in the system [13, 44, 50]. A system builder may want explanations related to debugging
AI models or evaluating training data. Regulators may inquire if a system meets stated
regulatory requirements [44].

The explanation’s purpose will in turn influence its style. In Figure 2, we visualize
our three elements of style: level of detail, degree of interaction between the human and
machine, and its format. These attributes are not exhaustive — explanations can take many
forms. However, we highlight these elements as ones closely related to meeting the four
principles. Therefore, considering these will lay the groundwork for producing explana-
tions. We expound upon these in more detail below.

The level of detail is depicted as a range, from sparse to extensive. By sparse, we mean
that the amount of information provided is brief, limited, and/or at a high-level, lacking in
detail. An example of a sparse explanation might be an explanation for a decision made
by an alert system (e.g., “system processes slowed because of overheating.”). An extensive
explanation may contain detailed information about a system and/or provide a large amount
of information (e.g., a report with relevant system information to understand its process).

We place the degree of human-machine interaction into three categories: declarative
explanations, one-way interaction, and two-way interaction. In a declarative explanation,
the systems provides an explanation, and there is no further interaction. This describes
most current explainable AI methods (Section 6). For example, a loan application system
may always output the rationale for an acceptance or rejection. An object classifier may
output a saliency map [120]. A model card [86] may contain pre-determined information
about the system. A declarative explanation is based on a default query, such as “why did
the object classifier produce this decision?”’. The human cannot alter the question being
asked (barring a change in the system itself to produce something different).

A higher degree of interaction is a one-way interaction. For this, the explanation is
determined based on a query or question input to the system [20, 35, 142]. For example,
this may be a graphical output depending on the factors a person wishes to visualize. This
may allow the explanation’s consumer the ability to probe further or to submit different
queries.

We define the category with the deepest interaction level as the two-way interaction.
This models a conversation between people. The person can probe further, and the ma-
chine can probe back, ask clarifying questions, or provide new avenues of exploration. For
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

Level of detail
Sparse Extensive

—_—_————_

 

Degree of interaction

Declarative One-way Two-way
Cis = :
Pd _

Explanation(s) Cia Ca
{

; | \

aa Explanation(s) Explanation(s)

 

 

 

Format

  
   

   
     
 
       
 

Verbal
(written or
auditory)

 
   

Graphical/
visual

auditory alerts
(e.g., siren
system)

 

 

Fig. 2. [lustration of our elements of explanation styles.
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

example, a system may probe the user for additional details or propose alternate questions.
To our knowledge, two-way interactions do not yet exist. Developing them is a future
research direction.

The explanation’s format includes visual and graphical, verbal, and auditory or visual
alerts. Examples of graphical formats include outputs from data analyses or saliency maps.
Verbal formats can include written outputs and reports as well as auditory outputs, such as
speech. These visual and verbal formats carry the assumption that the audience is expect-
ing and attending to the explanation. Another form of explanation can capture an unaware
audience’s attention. A siren or light system can produce different alarms, light flashing
patterns, and/or light colors as an explanation that alert the audience. For example, a spe-
cific siren pitch or pattern could indicate something about a system’s status which may need
attention.

Each of these style elements will need to be considered to produce an explanation for
its purpose and to meet the four principles. Some cases may call for a simple, declarative
explanation as the most appropriate style to optimize how meaningful it is. This is some-
times the case in a weather emergency, such as when a tornado is in the area. A current
weather alert from the National Weather Service, “Tornado Warning: Take Action!”?, op-
erates as both an alert and a simple explanation. The alert is to “Take Action” with the
simple explanation of “Tornado Warning.” Depending on the metric, this explanation may
not be considered highly accurate because the minimal level of detail: it does not include
why a tornado warning is declared in its explanation. However, in this example, brevity
is appropriate to ensure it is understood by a wide audience and to enable swift action.
Although minimal, additional information with an alert may be helpful to address non-
compliance in responding to weather alerts (e.g., “cry-wolf” effects) [73]. In a different
scenario, such as when debugging a system, the explanation could include information on
the internal steps of a system. This could be lengthy and contain field-specific language.
The audience may need time and more effort to examine the explanation and to decide on
their next actions. Here, more details in the user’s preferred format would be helpful, and
two-way interactions could become important.

These different purposes, styles, and considerations illustrate the range and types of
explanations. This points to the need for flexibility in addressing the scope of systems that
require explanations. Because the circumstances under which an explanation is provided
will differ, the four principles encourage adapting to different styles as appropriate. Some
explanations will be easier to achieve than others, and designers will need to consider trade-
offs between accomplishing different goals.

4. Risk Management of Explainable AI

Risk is defined as “the effect of uncertainty of objectives” [22, pg. 6] and includes both neg-
ative outcomes (threats) as well as positive outcomes (opportunities). Risk management is
a process that can be used to define, assess, and mitigate risk. Explainable AI can mitigate

 

2https://www.weather.gow/safety/tornado-ww
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

the risks of artificial intelligence by assessing, measuring, or predicting the risk in a model
or system. Explanations can be used to test for vulnerabilities [41]. Alternatively, explain-
able AI can introduce risks of its own, e.g., adversarial attacks discussed in Section 6.3.
This section focuses on the latter, managing the potential risks introduced by explainable
AL

Any explainable AI system will contain potential risks, both threats and opportunities.
The degree to which stakeholders are prepared to accept the trade-off of general risk and
goals is called the stakeholders’ risk appetite [22]. Many risk management strategies share
the common components: identifying, analyzing, responding, monitoring, and reviewing.
For explainable AI, a risk management strategy will need to factor the four principles.

An explanation, the first principle, is necessary for explainable AI, but an explanation
itself introduces risks, both positive and negative. A potential negative outcome of having
an explanation is the exposure of proprietary details [85]. A single explanation may not
expose the inner workings of the system. However, multiple explanations, either from
multiple independent queries or through a two-way interaction, could expose intellectual
property when connected to each other. How many explanations must an end user have
access to before they have an understanding of the system? The scope of each explanation
may impact the number. These include explanations that describe the limit of the system’s
knowledge.

However, explanations also have the potential for positive outcomes. A user can better
understand the system. This could lead to improvements, such as increased trust in the
system. Explanations may also be necessary for compliance with regulations, e.g., The
Fair Credit Reporting Act (FCRA) and Article 13 of the European Union General Data
Protection Regulation (GDPR).

Explanations need to be meaningful for the audience. This is the second principle
and introduces its own risks. A meaningful explanation can give deeper insight into the
system, but it may expose intellectual property or system vulnerabilities by exposing its
inner workings. An explanation that is not meaningful, on the other hand, is in jeopardy of
being ignored or not recognized as an explanation.

In order to be useful, an explanation not only needs to be meaningful, it must also be
accurate, our third principle. A relevant potential risk is commonly known as model risk,
the potential negative outcomes derived from an invalid or misapplied model. As stated
in the Federal Reserve System [31], one of the two main sources for this type of risk is
underlying errors in the model causing erroneous outputs. An inaccurate explanation can
lead to a misinterpretation or misunderstanding of how the system works or arrived at an
outcome. This is a negative risk for an end user, but AI systems are also used as one part
of a larger system where the AI might bias a human.

In face recognition, a human face examiner could receive information from an AI algo-
rithm on which parts of the face are useful. An accurate explanation can help the examiner
more accurately assess the pair of faces while an inaccurate explanation could lead to a
wrong decision. In the judicial system, AI algorithms have been used in decision, such as
if a defendant may be arrested again [8]. An inaccurate explanation of how the algorithm
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

arrived at its outcome could result in a miscarriage of justice. An accurate explanation can
help create a more just society.

The other main source of model risk is using the model incorrectly or beyond its knowl-
edge limits [31]. Explanations that describe the knowledge limits of the system, the fourth
principle, can provide assurance the model is not operating out of scope and nurture confi-
dence. Describing the limits of a system can potentially expose the inner workings of the
system, especially if combined with information collected from other explanations.

Examining the potential risks of software exposure, there are different contexts, cate-
gories, and levels of risk. Who are the end users? If they are only internal to the organi-
zations such as developers, the management strategy will be different than if the end users
include external customers.

Explainable AI introduces new threats to a system. However, it also introduces new op-
portunities as well. Whether the outcome is a threat or an opportunity sometimes depends
on the audience. Risk management considers the trade-offs and possibilities of these and
other factors. When assessing risk, two components often assessed are the likelihood of the
risk and the impact of the outcome [37].

In general for AI, there is a need to develop a risk management framework; a request for
information by NIST* occurred on 2021-07-29. For more information on risk management,
see [22, 37, 128].

5. Overview of Principles in the Literature

Theories and properties of explainable AI have been discussed from different perspectives,
with commonalities and differences across these points of view [9, 26, 34, 50, 79, 111, 112,
141].

Lipton [79] divides explainable techniques into two broad categories: transparent and
post-hoc interpretability. Lipton [79] defines a transparent explanation as reflecting to some
degree how a system came to its output. A subclass is simulatability, which requires that
a person can grasp the entire model. This implies that explanations will reflect the inner
workings of a system. Their post-hoc explanations “often do not elucidate precisely how
a model works, they may nonetheless confer useful information for practitioners and end
users of machine learning.” For example, the bird is a cardinal because it is similar to
cardinals in the training set.

Rudin [111] argues that it should not be assumed that interpretability must be sacrificed
for state-of-the-art accuracy. They recommend that for high stakes decisions, one should
avoid a black-box model, unless one can prove that an interpretable model does not exist
with the same level of accuracy. Note that we will we refer to black-box as closed-box for
the remainder of this document [94]. Rudin et al. [113] builds on their previous work by
presenting five principles and ten grand challenges of interpretable machine learning.

Mueller et al. [91] reviews some of the basic concepts for user-centered explainable
AI systems. Based on these concepts, they describe the Self-Explanation Scorecard, and

 

*https:/Avww federalregister.gov/d/2021-16176; Date Accessed: 2021-08-31

10
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

present a set of user-centered design principles.

From a psychological perspective, Broniatowski [17] makes the case that interpretabil-
ity and explainability are distinct requirements for machine learning systems. The resulting
analysis implies that system output should be tailored to different types of users.

Wachter et al. [140] argue that explanations do need to meet the explanation accuracy
property. They claim that counterfactual explanations are sufficient. “A counterfactual ex-
planation of a prediction describes the smallest change to the feature values that changes the
prediction to a predefined output [88];” e.g., if you had arrived to the platform 15 minutes
earlier, you would have caught the train. Counterfactual explanations do not necessarily
reveal the inner workings of a system. This property allows counterfactual explanations to
protect intellectual property.

Gilpin et al. [34] defines a set of concepts for explainable AI. Similar to the meaningful
and explanation accuracy principles in our current work, Gilpin et al. [34] propose that
explanations should allow for a trade-off between their interpretability and completeness.
In addition, they state that trade-offs must not obscure key limitations of a system.

Doshi-Velez and Kim [26] address the critical question: measuring if explanations are
meaningful for users or consumers. They present a framework for a science to measure
the efficiency of explanations. This paper discusses factors that are required to begin test-
ing interpretability of explainable systems. The paper highlights the gap between these
principles as a concept and creating metrics and evaluation methods.

Information Commissioner’s Office and The Alan Turing Institute [50] lays out prin-
ciples to follow for explainable AI. These principles are: be transparent, be accountable,
consider the context you are operating in, and reflect on the impact of your AI system on the
individuals affected as well as the wider society [50]. In addition to discussing principles,
they discuss different things that go into an explanation, including process-based explana-
tions vs. outcome-based explanations, the rationale, the responsibility of who made what
decisions, an explanation of the data, and design steps that maximize fairmess, safety, and
impact of the use of the system.

Barredo Arrieta et al. [9] discuss these various terms used in different sources to de-
scribe explainability or interpretability: understandability, comprehensibility, interpretabil-
ity, explainability, and transparency. They discuss how these terms are all different yet tied
together.

Weller [141] discusses types of transparency and how they address different classes
of users or consumers of explanations. Similar to the explanation accuracy principle, the
paper introduces faithfulness of an explanation as

..broadly beneficial for society provided that explanations given are faithful,
in the sense that they accurately convey a true understanding without hiding
important details. This notion of faithful can be hard to characterize precisely.
It is similar in spirit to the instructions sometimes given in courts to tell “the
truth, the whole truth, and nothing but the truth.” [141]

Across these viewpoints, there exist both commonalities and disagreement. Similar to

11
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

our four principles, commonalities include concepts which distinguish between the exis-
tence of an explanation, how meaningful it is, and how accurate or complete it is. Although
disagreements remain, these perspectives provide guidance for development of explainable
systems. A key disagreement between philosophies is the relative importance of explana-
tion meaningfulness and accuracy. These disagreements highlight the difficulty in balanc-
ing multiple principles simultaneously. Context of the application, community and user
requirements, and the specific task will drive the importance of each principle.

6. Overview of Explainable AI Algorithms

Researchers have developed different algorithms to explain AI systems. Following other
sources [9, 38, 68], we organize the explanations into two broad categories: self-interpretable
models and post-hoc explanations. Self-interpretable models are the algorithm model (or a
representation of the algorithm itself) that can be directly read and interpreted by a human.
In this case the model itself is the explanation. Post-hoc explanations are explanations, of-
ten generated by other software tools, that describe, explain, or model the algorithm to give
an idea of how the algorithm works. Post-hoc explanations often can be used on algorithms
without any inner knowledge of how the algorithm works, provided that it can be queried
for outputs on chosen inputs.

Rather than mention all of the different explanation subtypes and all of the different
explanations available, we highlight a few widely-used examples, some categorizations,
and then refer the reader to various surveys on explainable AI [2, 9, 38, 68, 78, 89].

6.1 Self-Interpretable Models

Self-interpretable models are models that are themselves the explanations. Not only do they
explain the entire model globally, but by walking through each input through the model,
the simulation of the input on the self-interpretable model can provide a local explanation
for each decision.

Some of the most common self-interpretable models include decision trees and regres-
sion models (including logistic regression). There is work in producing many more inter-
pretable models that improve in accuracy over basic decision trees and basic regression
models in many cases. These models include decision lists [70], decision sets [71], pro-
totypes (representative samples of each class) Kim et al. [56], feature combination rules
that completely classify sets of inputs Kuhn et al. [61], Bayesian Rule Lists [74], additive
decision trees [81] and improved variants of decision trees [4, 11, 77].

With self-interpretable models, some sources state an accuracy-interpretability trade-
off (19, 27, 78]: self-interpretable models are less accurate than post-hoc models because
there is a trade-off between making the model more exact or more meaningful to humans.
However, Rudin [111], Rudin and Radin [112] disagree, arguing that there is not necessarily
an accuracy-interpretability trade-off and in many cases interpretable models can be used
without a loss of decision accuracy.

12
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

6.2 Post-Hoc Explanations

Post-hoc explanations are grouped into two kinds: local explanations and global explana-
tions. A local explanation explains a subset of decisions or is a per-decision explanation.
A global explanation produces a model that approximates the non-interpretable model. In
some cases, a global explanation can also provide local explanations by simulating them
on specific inputs to provide local explanations for those individual inputs. As simple ex-
amples, consider a logistic regression (which could either be a self-interpretable model or a
post-hoc approximation to an opaque model). The regression coefficients provide a global
explanation that explain all inputs. However, one can plug the input in with the weights
and then use those weights to explain the output of the algorithm.

We discuss each of these explanations in the following subsections, describing local
explanations in Section 6.2.1 and global explanations in Section 6.2.2.

6.2.1 Local Explanations

Local explanations explain a subset of inputs. The most common type of local explanation
is a per-decision or single-decision explanation, which provides an explanation for the
algorithm output or decision on a single input point.

One commonly-used local explanation algorithm is LIME (Local Interpretable Model-
Agnostic Explainer) [107]. LIME takes a decision, and by querying nearby points, builds
an interpretable model that represents the local decision, and then uses that model to pro-
vide per-feature explanations. The default model chosen is logistic regression. For images,
LIME breaks each image into superpixels, and then queries the model with a random search
space where it varies which superpixels are omitted and replaced with all black (or a color
of the user’s choice).

Another commonly-used local explanation algorithm is SHAP (SHapley Additive ex-
Planations) [82]. SHAP provides a per-feature importance for an input on a regression
problem by converting the scenario to a coalitional game from game theory and then pro-
ducing the Shapley values from that game. SHAP treats the features as the players, the
features value vs. a default value as the strategies, and the system output as the payoff,
forming a coalitional game from the input. See Ferguson [32] for more information on
Shapley values and coalitional games.

Another common local explanation is a counterfactual. A counterfactual is an expla-
nation saying “if the input were this new input instead, the system would have made a
different decision.” [140] In these explanations, although there are often multiple widely-
differing instances that all are counterfactuals, a counterfactual explanation often provides
a single instance. The hope is that the instance is as similar as possible to the input with
the exception that the system makes a different decision. However, some systems can pro-
duce multiple counterfactual instances as a single explanation. Ustun et al. [138] develop
a counterfactual explanation of logistic (or linear) regression models. Counterfactuals are
represented as the amounts of specific features to change.

Another popular type of local explanations for problems on image data are saliency

13
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

pixels. Saliency pixels color each pixel depending on how much that pixel contributes to
the classification decision. One of the first saliency algorithms is Class Activation Maps
(CAM) [150]. A popular saliency pixel algorithm that enhanced CAM is GRAD-CAM
[120]. GRAD-CAM generalized CAM so that it can explain any convolutional network.

An additional local explanation in Koh and Liang [58] takes a decision and produces
an estimate of the influence of each training data point on that particular decision. Another
additional local explanation is an Individual Conditional Expectation (ICE) [89, 149]. An
ICE curve shows the marginal effect of the change in one feature for an instance of the
data.

6.2.2 Global Explanations

Global explanations produce post-hoc explanations on the entire algorithm. Often, this
involves producing a global model for an algorithm or a system.

One Global explanation is Partial Dependence Plots (PDPs) [89, 149]. A Partial Depen-
dence Plots shows the marginal change of the predicted response when the feature (value
of that specific data column or component) changes. PDPs are useful for determining if a
relationship between a feature and the response is linear or more complex [89].

In deep neural networks, one such global algorithm is TCAV (Testing with Concept Ac-
tivation Vectors) [153]. TCAV wishes to explain a neural network in a more user-friendly
way by representing the neural network state as a linear weighting of human-friendly con-
cepts, called Concept Activation Vectors (CAVs). TCAV was applied to explain image
classification algorithms through learning CAVs including color, to see how colors influ-
enced the image classifier’s decisions.

Two visualizations used to provide global explanations are Partial Dependence Plots
(PDPs) and Individual Conditional Expectation (ICE) [89, 149]. The partial dependence
plot shows the marginal change of the predicted response when the feature (value of that
specific data column or component) changes. PDPs are useful for determining if a relation-
ship between a feature and the response is linear or more complex [89]. The ICE curves are
finer-grained and show the marginal effect of the change in one feature for each instance
of the data. ICE curves are useful to check if the relationship visualized in the PCP is the
same across all ICE curves, and can help identify potential interactions.

Prototypes [75], representative samples of each class, are also sometimes used as a
global explanation for a neural network in addition to sometimes being a self-interpretable
model as mentioned in Section 6.1.

Another way to produce global explanations is to summarize local explanations taken
on a variety of inputs. A variant of LIME, SP-LIME [107], uses a submodular pick to
choose the most relevant local LIME explanations as summary explanations. Another way
is to try to approximate the post-hoc model by learning a global model on a system such as
a decision set [72] or a summary of counterfactual rules [106].

14
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

6.3. Adversarial Attacks on Explainability

Explanation accuracy (Principle 3) is an important component of explanations. Sometimes,
if an explanation does not have 100 percent explanation accuracy, it can be exploited by
adversaries who manipulate a classifier’s output on small perturbations of an input to hide
the biases of a system. First, Lakkaraju and Bastani [69] observes that even if an expla-
nation can mimic the predictions of the closed-box that this is insufficient for explanation
accuracy and such systems can produce explanations that may mislead users. An approach
to generate misleading explanations is demonstrated in Slack et al. [124]. They do this
by producing a scaffolding around a given classifier that matches the classification on all
input data instances but changes outputs for small perturbations of input points, which can
obfuscate global system behavior when only queried locally. This means that if the sys-
tem is anticipating being explained by a tool such as LIME that gives similar instances to
training set instances as inputs, the system will develop an alternative protocol to decide
those instances that differ from how they will classify trials in the training and test sets.
This can mislead the explainer by anticipating which trials the system might be asked to
classify. Another similar approach is demonstrated in Aivodji et al. [5]. They fairwash a
model by taking a closed-box model and produce an ensemble of interpretable models that
approximate the original model but are much fairer, which then hide the unfairness of the
original model. Another example of slightly perturbing a model to manipulate explanations
is demonstrated in Dimanov et al. [24]. The ability for developers to cover up unfairness in
closed-box models is one of the several vulnerabilities of explainable AI discussed in Hall
et al. [41]. Kindermans et al. [57] shows that many saliency pixel explanations lack input
invariance, meaning that a small change to the input can greatly change the output and the
attribution to relevant pixels.

7. Evaluating Explainable AI Algorithms

This sections summarizes the state-of-the art of evaluating explainable AI algorithms (cf.,
[151]). In this paper, we separate the evaluation of explainable AI algorithms according to
which principle is being evaluated. The Explanation principle (Principle 1) is covered under
the section, Overview of Explainable AI Algorithms (Section 6), which reviews current
explanation methods. In this section, we review current methods for measuring explanation
meaningfulness (Principle 2) and explanation accuracy (Principle 3). To our knowledge
there is limited work on developing and evaluating algorithms’ knowledge limits (Principle
4). As a result, we do not discuss evaluating knowledge limits in this section.

7.1 Evaluating Meaningfulness

One way to measure the meaningfulness of an explanation involves measuring human sim-
ulatability. This is essentially the ability for a person to understand a machine learning
model to the extent that they would be able to take the same input data as the model, and
understand the parameters of the model such that they would be able to produce a prediction

15
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

from the model themselves in a reasonable amount of time [79]. The ability to simulate the
model themselves would reflect a high degree of understanding. This is typically measured
for self-interpretable models as a way to measure the complexity of the model.

Several studies have put human simulatability to the test. Lage et al. [65] and Lage et al.
[66] measured the accuracy of the humans’ results, the response time taken, and a human
poll of the subjective difficulty of simulating the model. Hase and Bansal [43] discusses
two kinds of human simulatability: forward simulation, which is when a human predicts a
system’s output for a given input; and counterfactual simulation, where a human is given
an input and an output. They must predict what output the system would give if the input
were changed in a particular way. When evaluating explanations, they evaluated forward
and counterfactual simulation by measuring the change in user accuracy relative to different
explanations. Poursabzi-Sangdeh et al. [101] measured the accuracy of humans simulating
different logistic regression models on housing prices. Slack et al. [123] conducted a “what-
if’ simulatability evaluation: the user receives an input with an explanation. The user is
then asked to simulate the model on a new input that is slightly perturbed from the given
input (the new input mirrors a what-if or counterfactual).

Another strategy to evaluate meaningfulness is to ask humans to complete a task using
the provided system’s output as input, then measuring the human’s time taken and deci-
sion accuracy on the task. Poursabzi-Sangdeh et al. [101] does this by also asking humans
to predict what they believe house prices should be, in addition to asking what the model
will predict the house price will be (humans can disagree with the models in this step).
Kim et al. [56] harnessed the power of examples. Their model, the Bayesian Case Model
(BCM), learned prototypes of different cooking recipes. Humans were provided only the
ingredients of the prototype and were measured on how well they were able to classify
each recipe. Lai and Tan [67] tested meaningfulness in a deception detection task. The task
was to determine if hotel reviews were genuine or deceptive. Human accuracy of deception
detection was compared when they were only provided the review itself and when they
were presented with explanations from a machine. This comparison enables comparing
human decision accuracy with and without machine assistance/explanations. Lakkaraju
et al. [72] evaluated the interpretability of different complexity decision sets by asking
humans to view explanations and make decisions, measuring their accuracy and response
time. Mac Aodha et al. [83] evaluated an explanation by comparing human accuracy when
humans are trained with systems that provide explanations compared to being trained with
systems that do not provide an explanation. Schmidt and Biessmann [116] recruited users
to complete tasks given with and without system explanations and measures each user’s
total time taken and decision accuracy. Anderson et al. [7] studied two techniques for
explaining the actions of reinforcement learning agents to people not trained in AI. They
tested multiple explanation conditions: no explanation, each of the two explanations sepa-
rately, and both explanations. Overall, humans were most accurate when combining both
techniques, saliency maps and reward-decomposition bars.

Meaningfulness has been measured with subjective ratings as well. Hoffman et al. [45]
discussed a variety of criteria for good explanations and provide an Explanation Satisfac-

16
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

tion Scale. Holzinger et al. [46] developed the System Causability Scale (SCS) to compare
explanations. As part of their evaluation of human simulatability, Lage et al. [65] also
asked humans to subjectively rate the difficulty of simulating a model. Rajagopal et al.
[105] conducted experiments asking users to evaluate different properties of explanations.

Metrics on the size or complexity of a model are sometimes used as measures for a
model’s interpretability. Lakkaraju et al. [71] measured the interpretability of a model by
asking users if the information provided was sufficient to make conclusions. Poursabzi-
Sangdeh et al. [101] compared two types of models to test which enabled participants more
closely simulate the model’s actual predictions. They found that less information (a less
transparent model) could enable this better than a more transparent model perhaps due to
“information overload”). Lage et al. [66] measured the effect of complexity on human sim-
ulatability. The idea is that different levels, and types, of complexity can affect transparency
more or less than other types. Lakkaraju et al. [72] asked humans to made decisions, pro-
vided them the explanation as help, and measured how quickly and how accurately they
made decisions. Narayanan et al. [92] compared different types of output complexity for
how they affected human performance. Bhatt et al. [12] designed a complexity metric to
quantify “feature importance” explanations.

7.2 Evaluating Explanation Accuracy

Explanation accuracy is closely related to work on “fidelity”. Several studies have eval-
uated explanation fidelity [87, 110]. One way this has been tested is to simulate models
by using the system output as the ground truth and evaluating the post-hoc explanations
using a machine learning metric [87, 107]. Lakkaraju et al. [72] followed this strategy
but also checked that each instance had at most one explanation and that every instance
is explained by the post-hoc explanation model. The second method Mohseni et al. [87]
proposed is having humans evaluate the explanations and apply “sanity checks” to evalu-
ate the explanation accuracy. The third method asked the system to explain a variety of
inputs. In many cases, the inputs are adaptive. New inputs are slightly changed versions
of the previous inputs, based on the provided explanation. Experiments then measure the
change in output relative to the change in input and the importance of the changed features
from the explanation. Samek et al. [115] evaluated the quality of the explanation accu-
racy with saliency pixels. They gradually deleted the most important pixels and measured
how much the classification score changes. The idea is that if pixels which are impor-
tant have more influence on decision accuracy, and as they are deleted, the system is less
likely to classify the image as the original class. Hooker et al. [47] tested whether systems
performed worse when important features are removed. They applied a strategy in which
they removed important pixels, then retrained systems and measured decision accuracy of
the retrained systems. Yeh et al. [147] developed an “infidelity measure” to evaluate ex-
planation accuracy. Alvarez Melis and Jaakkola [6] evaluated the explanation accuracy, or
faithfulness, by removing the model’s higher order features and measuring the drop in clas-
sification probability. They also measured explanation accuracy by adding white noise to

17
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

the inputs and measuring how much the explanation changes. Adebayo et al. [3] evaluated
explanation accuracy of saliency pixel explanations for deep neural networks by measur-
ing the amount the explanation changed relative to how the trained models differed. Sixt
et al. [122] evaluated the quality of saliency pixels by randomizing middle convolutional
layers and comparing saliency pixels. They also compared the saliency pixels when the
labels are the actual labels vs. random. Qi et al. [104] evaluated explanation accuracy by
adding or deleting image pixels deemed relevant by the explanation. They then compared
the system’s scores on the new images. Bhatt et al. [12] evaluated the explanation accuracy
of “feature importance” explanations by both checking sensitivity, meaning similar inputs
have similar feature importance explanations, and faithfulness, meaning the change in the
explanations should correlate to the change in inputs.

The quality of counterfactual explanations were tested in Wachter et al. [140]. A coun-
terfactual explanation should answer, “what is the minimum amount an input would need
to change for the system to change its decision on that input?” Therefore, they tested how
far away the counterfactual was from the original data point.

8. Humans as a Comparison Group for Explainable AI

When considering the performance of humans and AI systems, there are fairly significant
differences of opinion regarding performance expectations. Some argue that we should
hold machines to a much higher standard than humans, while others believe it is suffi-
cient for machines to simply be as good as humans. A cascade of interesting and difficult
questions arise from this overarching philosophical divide, such as how much better do
machines have to be than humans? In what way(s) must they be better? How do we mea-
sure “as good as”? Regardless of where one falls on this particular philosophical debate,
it is nonetheless helpful to consider human performance as a baseline. In this section, we
describe human decision-making with respect to the extent humans explanations line up
with our four principles.

Independent of AI, humans operating alone also make high stakes decisions with the
expectation that they be explainable. For example, physicians, judges, lawyers, and foren-
Sic scientists are often expected to provide a rationale for their judgments. How do these
proffered explanations adhere to our four principles? We focused strictly on human expla-
nations of their own judgments and decisions (e.g.,“why did you arrive at this conclusion
or choice?”), not of external events (e.g., “why is the sky blue?” or “why did an event oc-
cur?”). External events accompanied by explanations can be helpful for human reasoning
and formulating predictions [80]. This is consistent with a desire for explainable AI. How-
ever, as outlined in what follows, human-produced explanations for their own judgments,
decisions, and conclusions are largely unreliable. Humans as a comparison group for ex-
plainable AI can inform the development of benchmark metrics for explainable AI systems;
and lead to a better understanding of the dynamics of human-machine collaboration.

18
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

8.1 Explanation

This principle states only that for a system to be considered explainable, it provides an
explanation. In this section, we will focus on whether humans produce explanations of their
own judgments and decisions and whether doing so is beneficial for the decision makers
themselves. In Section 8.2, we will discuss whether human explanations are meaningful,
and in Section 8.3, we will discuss the accuracy of those explanations.

Humans are able to produce a variety of explanation types [55, 79, 84]. However,
producing verbal explanations can interfere with decision and reasoning processes [117,
118, 144]. It is thought that as one gains expertise, the underlying processes become more
automatic, outside of conscious awareness, and therefore, more difficult to explain verbally
[28, 30, 63, 117]. This produces a similar tension which exists for AI itself: the desire for
high accuracy are often thought to come with reductions in explainability (however, c.f.,
[79]).

More generally, processes which occur with limited conscious awareness can be harmed
by requiring the decision itself to be expressed explicitly. An example of this comes from
lie detection. Lie detection based on explicitly judging whether or not a person is telling
the truth or a lie is typically inaccurate [16, 130]. However, when judgments are provided
via implicit categorization tasks, therefore bypassing an explicit judgment, lie detection ac-
curacy can be improved [129, 130]. This suggests that lie detection may be a nonconscious
process which is interrupted when forced to be made a conscious one.

Together these findings suggest that some assessments from humans may be more ac-
curate when left automatic and implicit, compared to requiring an explicit judgment or
explanation. Human judgments and decision making can oftentimes operate as a closed-
box [79], and interfering with this closed-box process can be deleterious to the accuracy of
a decision.

8.2 Meaningful

To meet this principle, the system provides explanations that are intelligible and under-
standable to the intended audience. For this, we focused on the ability of humans to inter-
pret how another human arrived at a conclusion. Here, consider this to mean: 1) whether
the audience reaches the same conclusion as intended by the person providing the explana-
tion, and 2) whether the audience agrees with each other on what the conclusion is, based
on an explanation.

One analogous case to explainable AI for human-to-human interaction is that of a foren-
sic scientist explaining forensic evidence to laypeople (e.g., members of a jury). Currently,
there are gaps between the ways forensic scientists report results and the understanding of
those results by laypeople (see Edmond et al. [28], Jackson et al. [51] for reviews). Jack-
son et al. [51] extensively studied the types of evidence presented to juries and the ability
for juries to understand that evidence. They found that most types of explanations from
forensic scientists are misleading or prone to confusion. Therefore, they do not meet our
internal criteria for being “meaningful.” A challenge for the field is learning how to improve

19
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

explanations, and the proposed solutions do not always have consistent outcomes [51].

Complications for producing meaningful explanations for others include people expect-
ing different explanation types, depending on the question at hand [84], context driving the
formation of opinions [51], and individual differences in what is considered to be a satis-
factory explanation [90]. Therefore, what is considered meaningful varies by context and
across people.

8.3. Explanation Accuracy

This principle states that a system’s explanation correctly reflects its reasons for generating
a certain output and/or accurately reflects its process. For humans, this is analogous to an
explanation of one’s decision processes truly reflecting the mental processes behind that
decision. In this section, we focused on this aspect only. An evaluation of the quality or
coherence of the explanation falls outside of the scope of this principle.

For the type of introspection related to explanation accuracy, it is well-documented that
although people often report their reasoning for decisions, this does not reliably reflect ac-
curate or meaningful introspection [93, 102, 143]. This has been coined the “introspection
illusion”: a term to indicate that information gained by looking inward to one’s mental con-
tents is based on mistaken notions that doing so has value [102]. People fabricate reasons
for their decisions, even those thought to be immutable, such as personally held opinions
[40, 53, 143]. In fact, people’s conscious reasoning that is able to be verbalized does not
seem to always occur before the expressed decision. Instead, evidence suggests that people
make their decision and then apply reasons for those decisions after the fact [137]. From a
neuroscience perspective, neural markers of a decision can occur up to 10 seconds before a
person’s conscious awareness [125]. This finding suggests that decision making processes
begin long before our conscious awareness.

People are largely unaware of their inability to introspect accurately. This is docu-
mented through studies of “choice blindness” in which people do not accurately recall their
prior decisions. Despite this inaccurate recollection, participants will provide reasons for
making selections they never, in fact, made [39, 40, 53]. For studies that do not involve
long-term memory, participants have also been shown to be unaware of the ways they eval-
uate perceptual judgments. For example, people are inaccurate when reporting which facial
features they use to determine someone’s identity [108, 135].

Based on our definition of explanation accuracy, these findings do not support the idea
that humans reliably meet this criteria. As is the case with algorithms, human decision
accuracy and explanation accuracy are distinct. For numerous tasks, humans can be highly
accurate but cannot verbalize their decision process.

8.4 Knowledge Limits

This principle states that the system only operates 1) under the conditions it was designed
and 2) when it reaches a sufficient confidence in its output or actions. For this principle,
we narrowed down the broad field of metacognition, or thinking about one’s own thinking.

20
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

Here, we focused on whether humans correctly assess their own ability and accuracy, and
whether they know when to report that they do not know an answer. There are several
ways to test whether people can evaluate their own knowledge limits. One method is to
ask participants to predict how well they believe they performed or will perform on a task,
relative to others (e.g., in what percentile will their scores fall relative to other task-takers).
Another way to test the awareness of knowledge limits is to obtain a measure of their
response confidence, with higher confidence indicating that a person believes with greater
likelihood that they are correct.

As demonstrated by the well known Dunning-Kruger Effect [60], most people inac-
curately estimate their own ability relative to others. A similar finding is that people, in-
cluding experts, generally do not predict their own accuracy and ability well when asked
to explicitly estimate performance [14, 15, 21, 42, 95]. However, a recent replication of
the Dunning-Kruger Effect for face perception showed that, although people did not reli-
ably predict their accuracy, their ability estimates varied accordingly with the task difficulty
[152]. This suggests that although the exact value (e.g., predicted performance percentile
relative to others, or predicted percent correct) may be erroneous, people can modulate the
direction of their predicted performance appropriately (e.g., knowing a task was more or
less difficult for them).

For a variety of judgments and decisions, people often know when they have made
errors, even in the absence of feedback [148]. To use eyewitness testimony as a relevant
example: although confidence and accuracy have repeatedly shown to be weakly related
[126], a person’s confidence does predict their accuracy in the absence of “contamination”
through the interrogation process and extended time between the event and the time of
recollection [145]. Therefore, human shortcomings in assessing their knowledge limits are
similar to those of producing explanations themselves. When asked explicitly to produce
an explanation, these explanations can interfere with more automatic processes gained by
expertise; they often do not accurately reflect the true cognitive processes. Likewise, as
outlined in this section, when people are asked to explicitly predict or estimate their ability
level relative to others, they are often inaccurate. However, when asked to assess their
confidence for a given decision vs. this explicit judgment, people can gauge their accuracy
at levels above chance. This suggests people do have insight into their own knowledge
limits, although this insight can be limited or weak in some cases.

9. Discussion and Conclusions

We introduced four principles to encapsulate the fundamental elements for explainable AI
systems. The principles provide a framework with which to address different components
of an explainable system. These four principles are that the system produce an explanation,
that the explanation be meaningful to humans, that the explanation reflects the system’s
processes accurately, and that the system expresses its knowledge limits. The principles
derive their strength when a system follows all four. A system that provides an explanation
but is not understandable, not accurate, or outside knowledge limits has reduced value. In

21
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

fact, it may impact users’ acceptance of a system’s outcomes.

There are different approaches and philosophies for developing and evaluating explain-
able AI. Computer science approaches tackle the problem of explainable AI from a variety
of computational and graphical techniques and perspectives, which may lead to promising
breakthroughs. A blossoming field puts humans at the forefront when considering the ef-
fectiveness of AI explanations and the human factors behind their effectiveness. Our four
principles provide a multidisciplinary framework with which to explore this type of human-
machine interaction. The practical needs of the system will influence how these principles
are addressed (or dismissed). With these needs in mind, the community will ultimately
adapt and apply the four principles to capture a wide scope of applications.

The focus of explainable AI has been to advance the capability of the systems to pro-
duce a quality explanation. Here, we addressed whether humans can meet the same set of
principles we set forth for AI. We showed that humans demonstrate only limited ability to
meet the principles outlined here. This provides a benchmark with which to compare AI
systems. In reflection of societal expectations, recent regulations have imposed a degree
of accountability on AI systems via the requirement for explainable AI [1]. As advances
are made in explainable AI, we may find that certain parts of AI systems are better able
to meet societal expectations and goals compared to humans. By understanding the ex-
plainability of both the AI system and the human in the human-machine collaboration, this
opens the door to pursue implementations which incorporate the strengths of each, poten-
tially improving explainability beyond the capability of either the human or AI system in
isolation.

In this paper, we focused on a limited set of human factors related to explainable de-
cisions. Much is to be learned and studied regarding the interaction between humans and
explainable machines. Although beyond the scope of the current paper, in considering the
interface between AI and humans, understanding general principles that drive human rea-
soning and decision making may prove to be highly informative for the field of explainable
AI [36]. For humans, there are general tendencies for preferring simpler and more general
explanations [84]. However, as described earlier, there are individual differences in which
explanations are considered high quality. The context of the decision and the type of de-
cision being made can influence this as well. Humans do not make decisions in isolation
of other factors [64]. Without conscious awareness, people incorporate irrelevant infor-
mation into a variety of decisions such as first impressions, personality trait judgments,
and jury decisions [33, 48, 132, 133]. Even when provided identical information, the con-
text, a person’s biases, and the way in which information is presented influences decisions
[10, 25, 28, 36, 54, 62, 100, 136]. Considering these human factors within the context of
explainable AI has only just begun.

To succeed in explainable AI, the community needs to study the interface between hu-
mans and AI systems. Human-machine collaborations have shown to be highly effective
in terms of accuracy [98]. There may be similar breakthroughs for AI explainability in
human-machine collaborations. The principles defined here provide guidance and a phi-
losophy for driving explainable AI toward a safer world by giving users a deeper under-

22
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

standing into a system’s output. Meaningful and accurate explanations empower users to
apply this information to adapt their behavior and/or appeal decisions. For developers and
auditors, explanations equip them with the ability to improve, maintain, and deploy sys-
tems as appropriate. Explainable AI contributes to the safe operation and trust of multiple
facets of complex AI systems. The common framework and definitions under the four prin-
ciples facilitate the evolution of explainable AI methods necessary for complex, real-world
systems.

Acknowledgments

The authors thank Harold Booth, John Libert, Reva Schwartz, Thurston Sexton, Brian
Stanton, Craig Watson, and Jesse Zhang for their insightful comments and discussions. We
thank everyone who responded to our call and submitted comments to the draft version of
this paper. We thank the panelists and participants of the NIST Explainable AI Workshop
for their discussions, insight, and thought provoking commentary. These were all essential
for shaping and improving the paper and the future directions of this work.

References

[1] General Data Protection Regulation (GDPR), 2018.

[2] Amina Adadi and Mohammed Berrada. Peeking Inside the Black-Box: A Survey
on Explainable Artificial Intelligence (XAD. IEEE Access, 6:52138-52160, 2018.
ISSN 2169-3536. doi: 10.1109/ACCESS .2018.2870052.

[3] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt,
and Been Kim. Sanity Checks for Saliency Maps. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances
in Neural Information Processing Systems 31, pages 9505-9515. Curran Associates,
Inc., 2018. URL http://papers.nips.cc/paper/8 160-sanity-checks-for-saliency-maps.
pdf.

[4] Gael Aglin, Siegfried Nijssen, and Pierre Schaus. Learning Optimal Decision Trees
Using Caching Branch-and-Bound Search. 2020. URL https://dial-uclouvain.be/pr/
boreal/object/boreal:223390.

[5] Ulrich Aivodji, Hiromi Arai, Olivier Fortineau, Sébastien Gambs, Satoshi Hara, and
Alain Tapp. Fairwashing: the risk of rationalization. In International Conference on
Machine Learning, pages 161-170, May 2019. URL http://proceedings.mlr.press/
v97/aivodjil9a.html. ISSN: 1938-7228 Section: Machine Learning.

[6] David Alvarez Melis and Tommi Jaakkola. Towards Robust Interpretabil-
ity with Self-Explaining Neural Networks. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, edi-
tors, Advances in Neural Information Processing Systems 31, pages 7775-—
7784. Curran Associates, Inc., 2018. URL _http://papers.nips.cc/paper/
8003-towards-robust-interpretability- with-self-explaining-neural-networks.pdf.

23
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[7]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

Andrew Anderson, Jonathan Dodge, Amrita Sadarangani, Zoe Juozapaitis, Evan
Newman, Jed Irvine, Souti Chattopadhyay, Matthew Olson, Alan Fern, and Mar-
garet Burnett. Mental models of mere mortals with explanations of reinforcement
learning. ACM Trans. Interact. Intell. Syst., 10(2), May 2020. ISSN 2160-6455. doi:
10.1145/3366485. URL https://doi.org/10.1145/3366485.

Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias:
There’s software used across the country to predict future criminals. and it’s biased
against blacks. ProPublica, May 2016. URL https://www.propublica.org/article/
machine-bias-risk-assessments-in-criminal-sentencing.

Alejandro Barredo Arrieta, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Ben-
netot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel
Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. Explainable Arti-
ficial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges to-
ward responsible AI. Information Fusion, 58:82-115, June 2020. ISSN 1566-
2535. doi: 10.1016/j.inffus.2019.12.012. URL http://www.sciencedirect.com/
science/article/pii/S 15662535 19308103.

Marianne Bertrand and Sendhil Mullainathan. Are Emily and Greg More Em-
ployable Than Lakisha and Jamal?: A Field Experiment on Labor Market Dis-
crimination. American Economic Review, 94(4):991-1013, 2004. doi: 10.4324/
9780429499821-53.

Dimitris Bertsimas and Jack Dunn. Optimal classification trees. Machine Learning,
106(7): 1039-1082, July 2017. ISSN 1573-0565. doi: 10.1007/s 10994-017-5633-9.
URL https://doi.org/10.1007/s 10994-017-5633-9.

Umang Bhatt, José M. F. Moura, and Adrian Weller. Evaluating and Aggregating
Feature-based Model Explanations. volume 3, pages 3016-3022, July 2020. doi:
10.24963/ijcai.2020/417. URL https://www.ijcai.org/proceedings/2020/417. ISSN:
1045-0823.

Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan
Jia, Joydeep Ghosh, Ruchir Puri, José MF Moura, and Peter Eckersley. Explain-
able machine learning in deployment. In Proceedings of the 2020 Conference on
Fairness, Accountability, and Transparency, pages 648-657, 2020.

Markus Bindemann, Janice Attard, and Robert A. Johnston. Perceived ability and
actual recognition accuracy for unfamiliar and famous faces. Cogent Psychology,
1(1), 2014. ISSN 23311908. doi: 10.1080/23311908.2014.986903. URL http:
//dx.doi.org/10.1080/23311908.2014.986903.

Anna K. Bobak, Viktoria R Mileva, and Peter JB Hancock. Facing the facts:
Naive participants have only moderate insight into their face recognition and
face perception abilities. Quarterly Journal of Experimental Psychology, page
174702181877614, 2018. ISSN 1747-0218. doi: 10.1177/17470218 18776145.
Charles F Bond and Bella M DePaulo. Accuracy of Deception Judgments Character-
izations of Deception. Personality and Social Psychology Review, 10(3):214—234,
2006.

24
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

David A. Broniatowski. Psychological foundations of explainability and inter-
pretability in artificial intelligence. NISTIR 8367, National Institute of Standards
and Technology, 2021.

David A. Broniatowski and Valerie F. Reyna. A formal model of fuzzy-trace theroy:
Variations on framing effects and the Allais paradox. Decision (Wash D C), 5(A):
205-252, 2018. doi: 10.1037/dec0000083.

Rich Caruana, Scott Lundberg, Marco Tulio Ribeiro, Harsha Nori, and Samuel Jenk-
ins. Intelligible and Explainable Machine Learning: Best Practices and Practical
Challenges. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pages 3511-3512. Association for Comput-
ing Machinery, New York, NY, USA, August 2020. ISBN 978-1-4503-7998-4. URL
https://doi.org/10.1145/3394486.3406707.

C. Chen, K. Lin, Cynthia Rudin, Y. Shaposhnik, S. Wang, and T. Wang. An
explainable model for credit risk performance. Technical report, 2018. URL
https://users.cs.duke.edu/{~}cynthia/docs/DukeFICO201 8Documentation.pdf.

M. Chi. Two approaches to the study of experts’ characteristics. In K. Ericsson,
N. Charness, P. Feltovich, and R. Hoffman, editors, The Cambridge Handbook of
Expertise and Expert Performance, chapter 2, pages 21-30. Cambridge University
Press, Cambridge, 2006. doi: 10.1017/CBO97805 11816796.002.

Chief Finalcial Officers Council and Performance Improvement Council. Play-
book: Enterprise Risk Management for the U.S. Federal Government. Tech-
nical report, July 2016. URL https://www.cfo.gov/wp-content/uploads/2016/07/
FINAL-ERM-Playbook. pdf.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-
Scale Hierarchical Image Database. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2009.

Botty Dimanov, Umang Bhatt, Mateja Jamnik, and Adrian Weller. You shouldn’t
trust me: Learning models which conceal unfairness from multiple explanation
methods. In European Conference on Artificial Intelligence, 2020.

Jennifer L. Doleac and Luke C.D. Stein. The visible hand: Race and online market
outcomes. The Economic Journal, 123(572):F469-F492, 2013. doi: 10.111 1/ecoj.
12082. URL http://www.jstor.com/stable/429 19259.

Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable ma-
chine learning. arXiv preprint arXiv: 1702.08608, 2017.

F. K. Dogilovié, M. Bréié, and N. Hlupi¢. Explainable artificial intelligence: A
survey. In 2018 41st International Convention on Information and Communica-
tion Technology, Electronics and Microelectronics (MIPRO), pages 0210-0215, May
2018. doi: 10.23919/MIPRO.2018.8400040.

Gary Edmond, Alice Towler, Bethany Growns, Gianni Ribeiro, Bryan Found, David
White, Kaye Ballantyne, Rachel A. Searston, Matthew B. Thompson, Jason M. Tan-
gen, Richard I. Kemp, and Kristy Martire. Thinking forensics: Cognitive science for
forensic practitioners. Science and Justice, 57(2):144—-154, 2017. ISSN 18764452.

25
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]
[37]

[38]

[39]

[40]

[41]

[42]

doi: 10.1016/j.scijus.2016.11.005. URL http://dx.doi.org/10.1016/).scijus.2016.11.
005.

M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes challenge: A retrospective. Interna-
tional Journal of Computer Vision, 111(1):98—136, January 2015.

Marte Fallshore and Jonathan W. Schooler. Verbal Vulnerability of Perceptual Ex-
pertise. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21
(6):1608—-1623, 1995. ISSN 02787393. doi: 10.1037/0278-7393.21.6.1608.

Board of Governors Federal Reserve System. SR 11-7: Guidance on Model Risk
Management. Supervision and Regulation Letters SR 11-7, The Federal Reserve
System, April 2011. URL https://www.federalreserve.gov/supervisionreg/srletters/
sr1107.htm.

Tom Ferguson. Game Theory. Second edition, 2014. URL https://www.math.ucla.
edu/~tom/Game_Theory/Contents html.

Heather D. Flowe and Joyce E. Humphries. An examination of criminal face bias in
a random sample of police lineups. Applied Cognitive Psychology, 25(2):265-273,
2011. ISSN 08884080. doi: 10.1002/acp. 1673.

Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, and
Lalana Kagal. Explaining explanations: An overview of interpretability of machine
learning. Proceedings - 2018 IEEE 5th International Conference on Data Science
and Advanced Analytics, DSAA 2018, pages 80-89, 2018. doi: 10.1109/DSAA.
2018.00018.

Google Inc. Facets, 2019. URL https://pair-code.github.io/facets/.

Google LLC. AI Explanations Whitepaper. pages 1-28, 2019.

United States Government Accountability Office. Report to the Committee on Over-
sight and Government Reform, House of Representatives. Technical Report GAO-
17-63, December 2016. URL https://www.gao.gov/products/gao- 17-63.

Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Gian-
notti, and Dino Pedreschi. A Survey of Methods for Explaining Black Box Models.
ACM Computing Surveys (CSUR), 51(5):93:1-93:42, August 2018. ISSN 0360-
0300. doi: 10.1145/3236009. URL https://doi.org/10.1145/3236009.

Lars Hall, Petter Johansson, Betty Tarning, Sverker Sikstrém, and Thérése Deutgen.
Magic at the marketplace: Choice blindness for the taste of jam and the smell of tea.
Cognition, 117(1):54-61, 2010. ISSN 00100277. doi: 10.1016/j.cognition.2010.06.
010. URL http://dx.doi.org/10.1016/j.cognition.2010.06.010.

Lars Hall, Petter Johansson, and Thomas Strandberg. Lifting the Veil of Moral-
ity: Choice Blindness and Attitude Reversals on a Self-Transforming Survey. PLoS
ONE, 7(9), 2012. ISSN 19326203. doi: 10.1371/journal.pone.0045457.

Patrick Hall, Navdeep Gill, and Nicholas Schmidt. Proposed guidelines for the re-
sponsible use of explainable machine learning, 2019.

Nigel Harvey. Confidence in judgment. Trends in Cognitive Sciences, 1(2):78-82,
1997. ISSN 13646613. doi: 10.1016/S1364-6613(97)01014-0.

26
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54]

Peter Hase and Mohit Bansal. Evaluating Explainable AI: Which Algorithmic Ex-
planations Help Users Predict Model Behavior? In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pages 5540-5552, On-
line, July 2020. Association for Computational Linguistics. doi: 10.18653/v 1/2020.
acl-main.491. URL https://www.aclweb.org/anthology/2020.acl-main.491.

Michael Hind. Explaining Explainable AI. XRDS, 25(3):16-19, April 2019. ISSN
1528-4972. doi: 10.1145/3313096. URL http://doi.acm.org/10.1145/3313096.
Robert R. Hoffman, Shane T. Mueller, Gary Klein, and Jordan Litman. Metrics for
Explainable AI: Challenges and Prospects. arXiv: 1812.04608 [cs], February 2019.
URL http://arxiv.org/abs/1812.04608. arXiv: 1812.04608.

Andreas Holzinger, André Carrington, and Heimo Miiller. Measuring the Quality
of Explanations: The System Causability Scale (SCS). KI - Kiinstliche Intelligenz,
34(2):193-198, June 2020. ISSN 1610-1987. doi: 10.1007/s13218-020-00636-z.
URL https://doi.org/10.1007/s 132 18-020-00636-z.

Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A Benchmark
for Interpretability Methods in Deep Neural Networks. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems, volume 32, pages 9737-9748.
Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf.

Ying Hu, Connor J. Parde, Matthew Q. Hill, Naureen Mahmood, and Alice J.
O’Toole. First Impressions of Personality Traits From Body Shapes.  Psy-
chological Science, 29(12):1969-1983, 2018. ISSN 14679280. doi: 10.1177/
09567976 18799300.

IBM Research. Trusting AI, Accessed July 8, 2020. URL https://www.research.ibm.
com/artificial-intelligence/trusted-ai/.

Information Commissioner’s Office and The Alan Turing Institute.
Explaining decisions made with AI, 2020. URL _https://ico.org.uk/
for-organisations/guide-to-data-protection/key-data-protection-themes/
explaining-decisions-made-with-ai/.

G. Jackson, D. H. Kaye, C. Neumann, A. Ranadive, and V. F Reyna. Communicat-
ing the Results of Forensic Science Examinations. Technical report, 2015.

Natalie Japkowicz and Mohak Shah. Evaluating Learning  Al-
gorithms A Classification Perspective. Cambridge —_ University
Press, June 2014. URL _ http:/Awww.cambridge.org/us/academic/

subjects/computer-science/pattern-recognition-and-machine-learning/
evaluating-learning-algorithms-classification-perspective.

Petter Johansson, Lars Hall, Sverker Sikstr6m, and Andreas Olsson. Failure to detect
mismatches between intention and outcome in a simple decision task. Science, 310
(5745):116-119, 2005. ISSN 00368075. doi: 10.1126/science.1111709.

Saul M. Kassin, Itiel E. Dror, and Jeff Kukucka. The forensic confirmation bias:
Problems, perspectives, and proposed solutions. Journal of Applied Research in

27
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

Memory and Cognition, 2(1):42—52, 2013. ISSN 22113681. doi: 10.1016/,.jarmac.
2013.01.001. URL http://dx.doi.org/10.1016/j.jarmac.2013.01.001.

[55] Frank C. Keil. Explanation and understanding. Annual Review of Psychology, 57:
227-254, 2006. doi: 10.1146/annurev.psych.57.102904. 190100. URL https://www.
ncebi.nlm.nih.gov/pmce/articles/PMC3624763/pdf/nihms4 12728 pdf.

[56] Been Kim, Cynthia Rudin, and Julie A Shah. The Bayesian Case Model:
A Generative Approach for Case-Based Reasoning and Prototype Classifica-
tion. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems 27,
pages 1952-1960. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
53 13-the-bayesian-case-model-a- generative-approach-for-case-based-reasoning-and-prototype-classi
pdf.

[57] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T.
Schiitt, Sven Dahne, Dumitru Erhan, and Been Kim. The (Un)reliability of Saliency
Methods. In Wojciech Samek, Grégoire Montavon, Andrea Vedaldi, Lars Kai
Hansen, and Klaus-Robert Miiller, editors, Explainable AI: Interpreting, Explaining
and Visualizing Deep Learning, Lecture Notes in Computer Science, pages 267-280.
Springer International Publishing, Cham, 2019. ISBN 978-3-030-28954-6. doi: 10.
1007/978-3-030-28954-6_14. URL https://doi.org/10.1007/978-3-030-28954-6_14.

[58] Pang Wei Koh and Percy Liang. Understanding Black-Box Predictions via Influ-
ence Functions. In Proceedings of the 34th International Conference on Machine
Learning - Volume 70, ICML’ 17, pages 1885-1894. JMLR.org, 2017. event-place:
Sydney, NSW, Australia.

[59] Joshua A. Kroll, Joanna Huey, Solon Barocas, Edward W. Felton, Joel R. Reiden-
berg, David G. Robinson, and Harlan Yu. Accountable Algorithms. University of
Pennsylvania Law Review, pages 633-705, 2017.

[60] Justin Kruger and David Dunning. Unskilled and unaware of it: How difficulties in
recognizing one’s own incompetence lead to inflated self-assessments. Journal of
Personality and Social Psychology, 77(6):1121-1134, 1999.

[61] D. Richard Kuhn, Raghu Kacker, Yu Lei, and Dimitris E. Simos. Com-
binatorial Methods for Explainable AT. In IWCT 2020, March 2020.
URL https://conf.researchr.org/details/iwet-2020/iwct-2020-papers/20/
Combinatorial-Methods-for-Explainable- AI. Library Catalog: conf.researchr.org.

[62] Jeff Kukucka, Saul M. Kassin, Patricia A. Zapf, and Itiel E. Dror. Cognitive Bias
and Blindness: A Global Survey of Forensic Science Examiners. Journal of Applied
Research in Memory and Cognition, 6(4):452-459, 2017. ISSN 22113681. doi: 10.
1016/,.jarmac.2017.09.001. URL http://dx.doi.org/10.1016/j.jarmac.2017.09.001.

[63] Chan Kulatunga-Moruzi, Lee R. Brooks, and Geoffrey R. Norman. Using compre-
hensive feature lists to bias medical diagnosis. Journal of Experimental Psychol-
ogy: Learning Memory and Cognition, 30(3):563—-572, 2004. ISSN 02787393. doi:
10.1037/0278-7393.30.3.563.

[64] Kestutis Kveraga, Avniel S$. Ghuman, and Moshe Bar. Top-down prediction in the

28
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[65]

[66]

[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

cognitive brain. Brain and cognition, 65(2):145-168, 2007. doi: 10.1016/j.bandc.
2007.06.007.

Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman,
and Finale Doshi-Velez. An Evaluation of the Human-Interpretability of Explana-
tion. arXiv:1902.00006 [cs, stat], August 2019. URL http://arxiv.org/abs/1902.
00006. arXiv: 1902.00006.

Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel J. Ger-
shman, and Finale Doshi-Velez. Human Evaluation of Models Built for Interpretabil-
ity. Proceedings of the AAAI Conference on Human Computation and Crowdsourc-
ing, 7(1):59-67, October 2019. URL https://aaai.org/ojs/index.php/HCOMPYarticle/
view/5280.

Vivian Lai and Chenhao Tan. On Human Predictions with Explanations and Predic-
tions of Machine Learning Models: A Case Study on Deception Detection. Novem-
ber 2018. doi: 10.1145/3287560.3287590. URL https://arxiv.org/abs/1811.07901 v4.
Hima Lakkaraju, Julius Adebayo, and Sameer Singh. Explaining Maching Learning
Predictions: State-of-the-art, Challenges, and Opportunities, December 2020. URL
https://explainm1-tutorial.github.io/neurips20.

Himabindu Lakkaraju and Osbert Bastani. “how do i fool you?”: Manipulating
user trust via misleading black box explanations. In Proceedings of the AAAI/ACM
Conference on Al, Ethics, and Society, ATES ’20, page 79-85, New York, NY, USA,
2020. Association for Computing Machinery. ISBN 9781450371100. doi: 10.1145/
3375627.3375833. URL https://doi.org/10.1145/3375627.3375833.

Himabindu Lakkaraju and Cynthia Rudin. Learning Cost-Effective and Interpretable
Treatment Regimes. In Artificial Intelligence and Statistics, pages 166-175, April
2017. URL http://proceedings.mlr.press/v54/lakkarajul7a.html.

Himabindu Lakkaraju, Stephen H. Bach, and Jure Leskovec. Interpretable Deci-
sion Sets: A Joint Framework for Description and Prediction. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD 16, pages 1675-1684, New York, NY, USA, 2016. Association for
Computing Machinery. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2939874.
URL https://doi.org/10.1145/2939672.2939874. event-place: San Francisco, Cali-
fornia, USA.

Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. Faithful
and Customizable Explanations of Black Box Models. In Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society, ATES °19, pages 131-138, New
York, NY, USA, 2019. Association for Computing Machinery. ISBN 978-1-4503-
6324-2. doi: 10.1145/3306618.3314229. URL https://doi.org/10.1145/3306618.
3314229. event-place: Honolulu, HI, USA.

Jared LeClere and Susan Joslyn. The cry wolf effect and weather-related decision
making. Risk analysis, 35(3):385—-395, 2015.

Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. In-
terpretable classifiers using rules and Bayesian analysis: Building a better stroke

29
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[75]

[76]

[77]

[78]

[79]

[80]

[81]

[82]

[83]

[84]

prediction model. The Annals of Applied Statistics, 9(3):1350—1371, September
2015. ISSN 1932-6157, 1941-7330. doi: 10.1214/15-AOAS848. URL https:
/fprojecteuclid.org/euclid.aoas/1446488742.

Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep Learning for Case-
Based Reasoning Through Prototypes: A Neural Network That Explains Its Predic-
tions. In Thirty-Second AAAI Conference on Artificial Intelligence, April 2018. URL
https://www.aaai.org/ocs/index.php/AAAT/AAAT18/paper/view/17082.

Brian Y. Lim, Anind K. Dey, and Daniel Avrahami. Why and why not explanations
improve the intelligibility of context-aware intelligent systems. Conference on Hu-
man Factors in Computing Systems - Proceedings, pages 2119-2128, 2009. doi:
10.1145/1518701.1519023.

Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. Generalized
and Scalable Optimal Sparse Decision Trees. In Proceedings of the International
Conference on Machine Learning, volume 1, 2020. URL https://proceedings.icml.
cc/paper/2020/hash/8alee9f2b7abe6e88d 1a479ab6a42c5e-Abstract.html.

Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable
AI: A Review of Machine Learning Interpretability Methods. Entropy, 23(1):18,
January 2021. doi: 10.3390/e23010018. URL https://www.mdpi.com/1099-4300/
23/1/18. Number: | Publisher: Multidisciplinary Digital Publishing Institute.
Zachary C Lipton. The mythos of model interpretability. Communications of the
ACM, 61(10):36-43, 2018.

Tania Lombrozo. The structure and function of explanations. Trends in
Cognitive Sciences, 10(10):464-470, October 2006. ISSN 1364-6613. — doi:
10.1016/).tics.2006.08.004. URL http://www.sciencedirect.com/science/article/pii/
$1364661306002117.

José Marcio Luna, Efstathios D. Gennatas, Lyle H. Ungar, Eric Eaton, Eric S. Dif-
fenderfer, Shane T. Jensen, Charles B. Simone, Jerome H. Friedman, Timothy D.
Solberg, and Gilmer Valdes. Building more accurate decision trees with the addi-
tive tree. Proceedings of the National Academy of Sciences, 116(40):19887-19893,
October 2019.

Scott M Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Pre-
dictions. In I. Guyon, U. V. Luxburg, S$. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Information Processing Sys-
tems 30, pages 4765-4774. Curran Associates, Inc., 2017. URL http://papers.nips.
cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf.

Ojisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue. Teaching
Categories to Human Learners with Visual Explanations. February 2018. URL
https://arxiv.org/abs/1802.06924v 1.

Tim Miller. Explanation in artificial intelligence: Insights from the social sci-
ences. Artificial Intelligence, 267:1-38, February 2019. ISSN 0004-3702. doi:
10.1016/).artint.2018.07.007. URL http://www.sciencedirect.com/science/article/
pii/S0004370218305988.

30
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[85]

[86]

[87]

[88]
[89]

[90]

[91]

[92]

[93]

[94]
[95]
[96]

[97]

[98]

Smitha Milli, Ludwig Schmidt, Anca D. Dragan, and Moritz Hardt. Model recon-
struction from model explanations, 2018. URL https://arxiv.org/abs/1807.05185v1.
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model
Cards for Model Reporting. In Proceedings of the Conference on Fairness, Ac-
countability, and Transparency, FAT* °19, pages 220-229, New York, NY, USA,
January 2019. Association for Computing Machinery. ISBN 978-1-4503-6125-5.
doi: 10.1145/3287560.3287596. URL https://doi.org/10.1145/3287560.3287596.
Sina Mohseni, Niloofar Zarei, and Eric D. Ragan. A Multidisciplinary Sur-
vey and Framework for Design and Evaluation of Explainable AI Systems.
arXiv: 1811.11839 [cs], August 2020. URL http://arxiv.org/abs/1811.11839. arXiv:
1811.11839.

Christoph Molnar. Interpretable Machine Learning, 2018.

Christoph Molnar. Interpretable Machine Learning. @ChristophMolnar, online edi-
tion edition, April 2019. URL https://christophm. github.io/interpretable-ml-book/.
Shane T. Mueller, Robert R. Hoffman, William Clancey, Abigail Emrey, and Gary
Klein. Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of
Key Ideas and Publications, and Bibliography for Explainable AI. arXiv: 1902.01876
[cs], February 2019. URL http://arxiv.org/abs/1902.01876. arXiv: 1902.01876.
Shane T. Mueller, Elizabeth S. Veinott, Robert R. Hoffman, Gary Klein, Lamia
Alam, Tauseef Mamun, and William J. Clancey. Principles of explanation in human-
ai systems. arXiv preprint arXiv:2102.04972, 2021. URL https://arxiv.org/abs/2102.
04972.

Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, and
Finale Doshi-Velez. How do Humans Understand Explanations from Machine
Learning Systems? An Evaluation of the Human-Interpretability of Explana-
tion. arXiv: 1802.00682 [cs], February 2018. URL http://arxiv.org/abs/1802.00682.
arXiv: 1802.00682.

Richard E Nisbett, Timothy Decamp Wilson, Michael Kruger, Lee Ross, Amos In-
deed, Nancy Bellows, Dorwin Cartwright, Alvin Goldman, Sharon Gurwitz, Ronald
Lemley, Harvey London, and Hazel Markus. Telling more than we can know: Verbal
reports on mental processes. Psychological Review, 84(3), 1977.

NIST Standards Inclusivity Effort Team. Guidance for NIST staff on using inclusive
language in documentary standards, NIST Interagency or Internal report 8366, 2021.
Stuart Oskamp. Overconfidence in case-study judgments. Journal of Consulting
Psychology, 29(3):261-265, 1965. ISSN 00958891. doi: 10.1037/h0022125.

P. J. Phillips, H. Moon, S. Rizvi, and P. Rauss. The FERET evaluation methodology
for face-recognition algorithms. [EEE Trans. PAMI, 22:1090-1104, October 2000.
P. J. Phillips, W. T. Scruggs, A. J. O’ Toole, P. J. Flynn, K. W. Bowyer, C. L. Schott,
and M. Sharpe. FRVT 2006 and ICE 2006 large-scale results. IEEE Trans. PAMI,
32(5):83 1-846, 2010.

P Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey

31
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[99]

[100]

[101]

[102]

[103]

[104]

[105]

[106]

[107]

[108]

[109]

Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankara-
narayanan, et al. Face recognition accuracy of forensic examiners, superrecognizers,
and face recognition algorithms. Proceedings of the National Academy of Sciences,
115(24):6171-6176, 2018.

PJ. Phillips, K. W. Bowyer, P. J. Flynn, X. Liu, and W. T. Scruggs. The Iris Challenge
Evaluation 2005. In Second IEEE International Conference on Biometrics: Theory,
Applications, and Systems, 2008.

Rtidiger F. Pohl, editor. Cognitive illusions: A handbook on fallacies and biases in
thinking, judgement and memory. Psychology Press, 2004.

Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wort-
man Vaughan, and Hanna Wallach. Manipulating and Measuring Model Inter-
pretability. arXiv: 1802.07810 [cs], November 2019. URL http://arxiv.org/abs/1802.
07810. arXiv: 1802.07810.

Emily Pronin. The introspection illusion. In Advances in experimental social psy-
chology, pages 1-67. Elsevier, 2009.

Mark A Przybocki, Alvin F Martin, and Audrey N Le. Nist speaker recognition
evaluations utilizing the mixer corpora—2004, 2005, 2006. IEEE Transactions on
Audio, Speech, and Language Processing, 15(7):1951-1959, 2007.

Zhongang Qi, Saeed Khorram, and Li Fuxin. Visualizing Deep Networks by Op-
timizing with Integrated Gradients. Proceedings of the AAAI Conference on Ar-
tificial Intelligence, 34(07):11890-11898, April 2020. ISSN 2374-3468, 2159-
5399. doi: 10.1609/aaai.v34i07.6863. URL http://arxiv.org/abs/1905.00954. arXiv:
1905.00954.

Dheeraj Rajagopal, Vidhisha Balachandran, Eduard Hovy, and Yulia Tsvetkov.
SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers.
arXiv:2103.12279 [cs], March 2021. URL http://arxiv.org/abs/2103.12279. arXiv:
2103.12279.

Kaivalya Rawal and Himabindu Lakkaraju. Beyond Individualized Recourse: In-
terpretable and Interactive Summaries of Actionable Recourses. In Advances in
Neural Information Processing Systems, volume 33, pages 12187-12198. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
8ee7730e97c67473a424ccfeff49ab20- Abstract. html.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why
Should I Trust you?’ Explaining the Predictions of Any Classifier. In
KDD 2016: — Proceedings of the 22nd ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, San Francisco, CA, USA, Au-
gust 2016. ACM. URL _ https://www.kdd.org/kdd2016/subtopic/view/
why-should-i-trust-you-explaining-the-predictions-of-any-classifier.

Allyson Rice, P. Jonathon Phillips, and Alice J. O’Toole. The role of the face and
body in unfamiliar person identification. Applied Cognitive Psychology, 27:761-
768, 2013.

John Roach. Microsoft responsible machine learning capabilities build trust in AI

32
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[110]

[111]

[112]

[113]

[114]

[115]

[116]

[117]

[118]

[119]

[120]

systems, developers say, Accessed July 29, 2020. URL https://blogs.microsoft.com/
ai/azure-responsible-machine-learning/.

Marko Robnik-Sikonja and Marko Bohanec. Perturbation-Based Explanations of
Prediction Models. In Jianlong Zhou and Fang Chen, editors, Human and Machine
Learning: Visible, Explainable, Trustworthy and Transparent, Human—Computer
Interaction Series, pages 159-175. Springer International Publishing, Cham, 2018.
ISBN 978-3-319-90403-0. doi: 10.1007/978-3-319-90403-0_9. URL https://doi.
org/10.1007/978-3-3 19-90403-0_9.

Cynthia Rudin. Stop explaining black box machine learning models for high stakes
decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):
206-215, 2019.

Cynthia Rudin and Joanna Radin. Why are we using black box models in AI when
we don’t need to? A lesson from an explainable AI competition. Harvard Data
Science Review, 1(2), 2019.

Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and
Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand
challenges. arXiv preprint arXiv:2103.11251, 2021. URL https://arxiv.org/abs/2103.
11251.

Seyed Omid Sadjadi, Timothée Kheyrkhah, Audrey Tong, Craig S Greenberg, Dou-
glas A Reynolds, Elliot Singer, Lisa P Mason, and Jaime Hernandez-Cordero. The
2016 nist speaker recognition evaluation. In Interspeech, pages 1353-1357, 2017.
Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and
Klaus-Robert Miiller. Evaluating the Visualization of What a Deep Neural Network
Has Learned. [EEE Transactions on Neural Networks and Learning Systems, 28
(11):2660-2673, November 2017. ISSN 2162-2388. doi: 10.1109/TNNLS.2016.
2599820. Conference Name: IEEE Transactions on Neural Networks and Learning
Systems.

Philipp Schmidt and Felix Biessmann. Quantifying Interpretability and Trust in
Machine Learning Systems. arXiv:1901.08558 [cs, stat], January 2019. URL http:
/larxiv.org/abs/1901.08558. arXiv: 1901.08558.

Jonathan W. Schooler and Tonya Y. Engstler-Schooler. Verbal overshadowing of
visual memories: Some things are better left unsaid. Cognitive Psychology, 22(1):
36-71, 1990. ISSN 00100285. doi: 10.1016/0010-0285(90)90003-M.

Jonathan W. Schooler, Stellan Ohlsson, and Kevin Brooks. Thoughts Beyond Words:
When Language Overshadows Insight. Journal of Experimental Psychology: Gen-
eral, 122(2):166—-183, 1993. ISSN 00963445. doi: 10.1037/0096-3445.122.2.166.
Reva Schwartz, Leann Down, Adam Jonas, and Elham Tabassi. A proposal for iden-
tifying and managing bias in artificial intelligence. Draft NIST Special Publication
1270, National Institute of Standards and Technology, 2021.

Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep net-
works via gradient-based localization. In Proceedings of the IEEE International

33
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[121]

[122]

[123]

[124]

[125]

[126]

[127]

[128]

[129]

[130]

[131]

[132]

[133]

Conference on Computer Vision, pages 618-626, 2017.

Keng Siau and Weiyu Wang. Building trust in artificial intelligence, machine learn-
ing, and robotics. Cutter Business Technology Journal, 31(2):47-53, 2018. ISSN
24753742.

Leon Sixt, Maximilian Granz, and Tim Landgraf. When Explanations Lie: Why
Many Modified BP Attributions Fail. December 2019. URL https://arxiv.org/abs/
1912.098 1 8v6.

Dylan Slack, Sorelle A. Friedler, Carlos Scheidegger, and Chitradeep Dutta Roy. As-
sessing the Local Interpretability of Machine Learning Models. arXiv: 1902.03501
[cs, stat], August 2019. URL http://arxiv.org/abs/1902.03501. arXiv: 1902.03501.
Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In
Proceedings of the AAAYACM Conference on AI, Ethics, and Society, ATES °20,
page 180-186, New York, NY, USA, 2020. Association for Computing Machinery.
ISBN 9781450371100. doi: 10.1145/3375627.3375830. URL https://doi.org/10.
1145/3375627.3375830.

Chun Siong Soon, Marcel Brass, Hans Jochen Heinze, and John Dylan Haynes. Un-
conscious determinants of free decisions in the human brain. Nature Neuroscience,
11(5):543-545, 2008. ISSN 10976256. doi: 10.1038/nn.2112.

Siegfried Ludwig Sporer, Steven Penrod, Don Read, and Brian Cutler. Choosing,
Confidence, and Accuracy: A Meta-Analysis of the Confidence-Accuracy Relation
in Eyewitness Identification Studies. Psychological Bulletin, 118(3):3 15-327, 1995.
ISSN 00332909. doi: 10.1037/0033-2909.118.3.315.

Brian Stanton and Theodore Jensen. Trust and artificial intelligence. Draft NISTIR
8332, National Institute of Standards and Technology, 2021.

Kevin Stine, Stephen Quinn, Gregory Witte, and Robert Gardner. Integrating Cyber-
security and Enterprise Risk Management (ERM). Technical Report NIST Internal
or Interagency Report (NISTIR) 8286, National Institute of Standards and Technol-
ogy, October 2020. URL https://csre.nist.gov/publications/detail/nistir/8286/final.
Leanne ten Brinke, Dayna Stimson, and Dana R. Carney. Some Evidence for Un-
conscious Lie Detection. Psychological Science, 2014.

Leanne ten Brinke, Kathleen D. Vohs, and Dana R. Carney. Can Ordinary People
Detect Deception After All? Trends in Cognitive Sciences, 20(8):579-588, 2016.
ISSN 13646613. doi: 10.1016/j.tics.2016.05.012. URL http://linkinghub.elsevier.
com/retrieve/pii/S 13646613 16300547.

The Royal Society. Explainable AI: the basics policy briefing,
2019. URL _https://royalsociety.org/-/media/policy/projects/explainable-ai/
Al-and-interpretability-policy-briefing.pdf.

Alexander Todorov. Face value: The irresistible influence of first impressions.
Princeton University Press, 2017.

Alexander Todorov, Anesu N Mandisodza, Amir Goren, and Crystal C Hall. In-
ferences of competence from faces predict election outcomes. Science (New York,

34
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[134]

[135]

[136]

[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

[146]

N.Y.), 308(5728): 1623-6, 6 2005. ISSN 1095-9203. doi: 10.1126/science. 1110589.
URL http://www.ncbi.nlm.nih.gov/pubmed/15947187.

Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliot, Carlos Gonzalez-
Zelaya, and Aad van Moorsel. The relationship between trust in AI and trustwor-
thy machine learning technologies. In Conference on Fairness, Accountability, and
Transparency (FAT* ’20), Barcelona, Spain, 2020. doi: 10.1145/3351095.3372834.
Alice Towler, David White, and Richard I Kemp. Evaluating the feature compari-
son strategy for forensic face identification. Journal of Experimental Psychology:
Applied, 23(1):47, 2017.

Amos Tversky and Daniel Kahneman. The framing of decisions and the psychology
of choice. Science, 211(4481):453-458, 1981. doi: 10.1126/science.7455683.
Amos Tversky and Eldar Shafir. The Disjunction Effect in Choice Under Un-
certainty. Psychological Science, 3(5):305—309, 1992. ISSN 14679280. doi:
10.1111/4.1467-9280.1992.tb00678.x.

Berk Ustun, Alexander Spangher, and Yang Liu. Actionable Recourse in Linear
Classification. In Proceedings of the Conference on Fairness, Accountability, and
Transparency, FAT* °19, pages 10-19, New York, NY, USA, 2019. Association for
Computing Machinery. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287566.
URL https://doi.org/10.1145/3287560.3287566. event-place: Atlanta, GA, USA.
Ellen M Voorhees. System Explanations : A Cautionary Tale. In ACM CHI Work-
shop on Operationalizing Human-Centered Perspectives in Explainable AI, 2021.
Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations
without opening the black box: Automated decisions and the GDPR. Harv. JL &
Tech., 31:841, 2017.

Adrian Weller. Transparency: Motivations and challenges. In Explainable AI: Inter-
preting, Explaining and Visualizing Deep Learning, pages 23-40. Springer, 2019.
James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda
Viégas, and Jimbo Wilson. The what-if tool: Interactive probing of machine learning
models. IEEE Transactions on Visualization and Computer Graphics, 26(1):56-65,
2020. doi: 10.1109/TVCG.2019.2934619.

Timothy D. Wilson and Yoav Bar-Anan. The unseen mind. Science, 321(5892):
1046-1047, 2008. ISSN 00368075. doi: 10.1126/science. 1163029.

Timothy D. Wilson and Johnathan Schooler. Thinking too much: Introspection can
reduce the quality of preferences and decisions. Journal of Personality and Social
Psychology, 60(2):181-192, 1991. ISSN 0003-066X. doi: 10.1037/h0021466.
John T. Wixted, Laura Mickes, and Ronald P. Fisher. Rethinking the Reliability of
Eyewitness Memory. Perspectives on Psychological Science, 13(3):324—335, 2018.
ISSN 17456924. doi: 10.1177/1745691617734878.

Allison Woodruff, Sarah E. Fox, Steven Rousso-Schindler, and Jeff Warshaw. A
qualitative exploration of perceptions of algorithmic faimess. Conference on Human
Factors in Computing Systems - Proceedings, 2018-April:1—14, 2018. doi: 10.1145/
3173574.3174230.

35
ZLESUl LSIN/8Z09'0L/B10 lopy/:sdyy :woy s6seyo jo aay ajqeiiene si Uoleoliqnd siyL

 

[147]

[148]

[149]

[150]

[151]

[152]

[153]

Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I. Inouye, and
Pradeep K. Ravikumar. On the (In)fidelity and Sensitivity of Expla-
nations. pages 10967-10978, 2019. URL http://papers.neurips.cc/paper/
9278-on-the-infidelity-and-sensitivity-of-explanations.

Nick Yeung and Christopher Summerfield. Metacognition in human decision-
making: Confidence and error monitoring. Philosophical Transactions of the Royal
Society B: Biological Sciences, 367(1594):1310-1321, 2012. ISSN 14712970. doi:
10.1098/rstb.2011.0416.

Qingyuan Zhao and Trevor Hastie. Causal Interpretations of Black-Box Mod-
els. Journal of Business & Economic Statistics, 0(0):1-10, June 2019. ISSN
0735-0015. doi: 10.1080/07350015.2019.1624293. URL https://doi.org/10.1080/
07350015.2019.1624293.

Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.
Learning deep features for discriminative localization. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 2921-2929, 2016.
Jianlong Zhou, Amir H. Gandomi, Fang Chen, and Andreas Holzinger. Evalu-
ating the Quality of Machine Learning Explanations: A Survey on Methods and
Metrics. Electronics, 10(5):593, January 2021. doi: 10.3390/electronics 10050593.
URL https://www.mdpi.com/2079-9292/10/5/593. Number: 5 Publisher: Multidis-
ciplinary Digital Publishing Institute.

Xingchen Zhou and Rob Jenkins. Dunning—Kruger effects in face perception. Cog-
nition, 203(January), 2020. ISSN 18737838. doi: 10.1016/j.cognition.2020.104345.
Luisa M. Zintgraf, Taco S. Cohen, Tameem Adel, and Max Welling. Visualizing
Deep Neural Network Decisions: Prediction Difference Analysis. February 2017.
URL https://arxiv.org/abs/1702.04595v1.

36
From Zadeh’s Computing with Words towards eXplainable Artificial Intelligence

Jose M. Alonso!

 

Key words: Fuzzy Logic, Explainable AI, Computing with Words, Computing with Perceptions, Cointension,
Interpretable Fuzzy Systems

 

Extended Abstract

The European Commission has identified Artificial Intelligence (AI) as the “‘most strategic technology of the 21st
century” [7]. AI is already part of our everyday life through many successful applications into real-world usage and
according to Accenture [16] the economic impact of the automation of knowledge work, robots and self-driving vehi-
cles could reach between 6.5 and 12 €trillion annually by 2025. People are used to buzzwords like smart watch, smart
phone, smart home, smart car, smart city, etc. In practice, we are surrounded by smart gadgets, i.e., devices connected
to Internet and endowed with some level of autonomy and intelligence thanks to AI systems. The cohabitation of
humans and smart gadgets makes society demand the development of a new generation of explainable AI systems,
Le., AI systems ready to explain naturally (as humans do) their automatic decisions.

Thus, the research field on explainable AI is flourishing and attracting more and more attention not only regarding
technical but also ethical and legal issues [8]. The ACM Code of Ethics [1] highlighted explanation as a basic principle
in the search for “Algorithmic Transparency and Accountability”. In addition, Floridi et al. defined the concept of
“explicability” in reference to both “intelligibility” and “explainability” and hence captured the need for transparency
and for accountability in an ethical framework for AI [10]. Moreover, the new European General Data Protection
Regulation (GDPR) [14] refers to the “right to explanation”, i.e., GDPR states that European citizens have the right to
ask for explanations of decisions affecting them, no matter who (or what AI system) makes such decision.

The term eXplainable Artificial Intelligence (XAI) was coined by the USA Defense Advanced Research Projects
Agency (DARPA) [11]. Assuming that “even though current Al systems offer many benefits in many applications,
their effectiveness is limited by a lack of explanation ability when interacting with humans” DARPA launched to
the research community (including both academy and industry) the challenge of designing new self-explanatory AI
systems from 2017 to 2021.

In Europe, there is not any initiative similar to the DARPA challenge on XAI yet. However, the European Com-
mission has already pointed out the convenience of launching a pilot in XAI [7]. In June 2018, the Confederation of
Laboratories for Artificial Intelligence Research in Europe (CLAIRE’), a novel initiative to create a network of excel-
lence in AI with the most well-recognized universities and R+D centres, emphasized in its European vision for AI the
need to search for transparent, explainable, fair and socially compatible intelligent systems. Moreover, The AI4EU®

 

Email addresses: josemaria.alonso.moral@usc.es (Jose M. Alonso)
'Centro Singular de Investigacion en Tecnoloxias da Informacion (CiTIUS), Universidade de Santiago de Compostela, Campus Vida, E-15782,
Santiago de Compostela, Galicia, Spain

Research Centre in Information Technologies (CiTIUS), University of Santiago de Compostela (USC) (May 2018).
Final version published in Proceedings of the 12th International Workshop on Fuzzy Logic and Applications (WILF2018).
Please cite this article in press as:
J.M. Alonso, “From Zadeh’s computing with words towards explainable Artificial Intelligence”, WILF20/8 - 12th International Workshop on Fuzzy
Logie and Applications, Springer, pp. 244-248, 2019, DOL: 10.1007/978-3-030-12544-8_21
Available online at http: //dx.doi.org/10.1007/978-3-030-12544-8 21
*“https://claire-ai.org/
bhttp://aideu.org/
H2020 Project is funded by call ICT-26 2018 (grant 825619) with the aim of: (1) to mobilize the entire European AI
community to make AI promises real for the European Society and Economy; and (2) to create a leading collaborative
AI European platform to nurture economic growth. Explainable Human-centered AI is highlighted as one of the five
key research areas to consider and it is present in 5 out of the 8 experimental pilots to be developed.

In the rest of this manuscript we briefly review a selection of outstanding Zadeh’s contributions which are likely to
have direct impact in the research field of XAI. The paradigm of Computing with Words (CWW) is especially relevant
because humans are used to explanations in natural language (NL).

From Prof. Zadeh’s seminal ideas on fuzzy sets and systems [21], many key concepts such as linguistic variables
and linguistic rules have turned up in the field of Fuzzy Logic (FL). Accordingly, FL has many successful applica-
tions [19]. In addition, as it is described in [4], about 30% of publications in XAI come from authors well recognized
in the field of FL. This is mainly due to the commitment of the fuzzy community to produce interpretable fuzzy
systems [3]. Actually, interpretability is deeply rooted in the fundamentals of FL. However, it is worthy to note that
interpretability is not guaranteed only because of applying FL. In practice, producing interpretable fuzzy systems is a
matter of careful design [17].

In XAI, interpretability is a key issue but understandability and comprehensibility which are not so deeply consid-
ered by the FL community also play a prominent role. Nowadays, a new generation of intelligent systems is expected
to provide users with natural explanations. Those explanations should be easy to understand no matter the user back-
ground. Since, humans think and compute naturally with words, explanations in NL are likely to be considered as
natural explanations. Prof. Zadeh was the first to talk about CWW [22] as an extension of fuzzy sets and systems.
Later, Prof. Kacprzyk gave some hints about how to implement CWW [12]. Moreover, he highlighted the need to
connect CWW with the paradigm of NL Generation (NLG) [9]. It is worth noting that NLG is a well-known area
within the Computational Linguistics and AI research fields. The connection between FL and NLG has been further
researched by other authors [2, 15].

In addition, Prof. Zadeh was pioneer to introduce a new generation of more natural intelligent systems, ready to
compute with perceptions and make approximate reasoning as humans naturally do. Thus, the Computational Theory
of Perceptions (CTP) [20, 23] was first introduced by Zadeh and later applied by Trivino and Sugeno to automatically
generate linguistic descriptions of complex phenomena [18]. The CTP has been successfully applied for example to
explain the energy consumption at home [5] or to automatically generate linguistic descriptions associated to the USA
census data [6].

In addition, Prof. Zadeh also coined the concept of cointension [24]. The semantic-cointension approach [13] is
already applied to assess interpretability of fuzzy systems. Likewise, it can be considered when evaluating the un-
derstandability of explanations in XAI. In short, two different concepts referring almost to the same entities are taken
as cointensive. Accordingly, an explanation in NL is deemed as comprehensible only when the explicit semantics
embedded in it is cointensive with the implicit semantics inferred by the user when reading and processing the given
explanation.

To sum up, Prof. Zadeh made many highly valuable contributions to the FL field and beyond. Many of these
contributions were pioneer ideas and/or challenging proposals with a lot of potential to be fully developed later by
other researchers. Nowadays, XAI is a prominent and fruitful research field where many of Zaden’s contributions can
become crucial if they are carefully considered and thoroughly developed. For example, two major open challenges
for XAI are: (1) how to build conversational agents able to provide humans with semantic grounding, persuasive
and trustworthy interactive explanations; and (2) how to measure the effectiveness and naturalness of automatically
generated explanations. CWW as well as fuzzy measures and Z-numbers [25] introduced by Zadeh are likely to
contribute to successfully address both challenges and achieve valuable results.

Acknowledgments

Jose M. Alonso is Ramén y Cajal Researcher (RYC-2016-19802). In addition, this research was funded by
the Spanish Ministry of Economy and Competitiveness (grants TIN2014-56633-C3-1-R, TIN2017-90773-REDT and
TIN2017-84796-C2-1-R) and the Galician Ministry of Education (grants ED431F 2018/02, GRC2014/030 and “ac-
creditation 2016-2019, ED431G/08”). All grants were co-funded by the European Regional Development Fund
(ERDF/FEDER program).
References

Q]

[2

BB

[4

[5]

[6

[7

[8

19]
[10]
Qi
[12]
[13]
[14]
[15]
[16]
(17)
[18]
[19]
[20]

[21]
[22]

[23]
[24]
[25]

ACM US Public Policy Council: Statement on Algorithmic Transparency and Accountability (2017), http://www.acm.org/
binaries/content/assets/public-policy/2017{\_}usaem{\_}statement{\_}algorithms.pdf

Alonso, J.M., Bugarin, A., Reiter, E.: Special issue on natural language generation with computational intelligence. IEEE Computational
Intelligence Magazine 12(3), 8-9 (2017). https://doi.org/10.1109/MCI.2017.2708919

Alonso, J.M., Castiello, C., Mencar, C.: Interpretability of fuzzy systems: Current research trends and prospects. In: Handbook of Computa-
tional Intelligence, pp. 219-237. Springer (2015)

Alonso, J.M., Castiello, C., Mencar, C.: A bibliometric analysis of the explainable artificial intelligence research field. In: 17th International
Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU). vol. CCIS853, pp. 2-15
(2018)

Conde-Clemente, P., Alonso, J.M., Trivino, G.: Towards automatic generation of linguistic advice for saving energy at home. Soft Computing
22(2), 345-359 (2018)

Conde-Clemente, P., Trivino, G., Alonso, J.M.: Generating Automatic Linguistic Descriptions with Big Data. Inf. Sciences 380(2), 12-30
(2017)

European Commission: Artificial Intelligence for Europe. Tech. Rep. (2018), https: //ec.europa.eu/
digital-single-market/en/news/communication-artificial—-intelligence-europe

EU Al HLEG: AI Ethics Guidelines. Tech. Rep. (2019), https://ec.europa.eu/digital-single-market/en/news/
draft-ethics-guidelines-trustworthy-ai

Gatt, A., Krahmer, E.: Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of
Artificial Intelligence Research 61, 65-170 (2018)

Floridi, L. et al.: Al4People - An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds
and Machines 28(4), 689-707 (2018)

Gunning, D.: Explainable Artificial Intelligence (XAI). Tech. rep., Defense Advanced Research Projects Agency (DARPA), Arlington, USA
(2016), DARPA-BAA-16-53, https: //www.darpa.mil/program/explainable-artificial-intelligence

Kacprzyk, J.: Computing with words is an implementable paradigm: Fuzzy queries, linguistic data summaries, and natural-language genera-
tion. IEEE Transactions on Fuzzy Systems 8, 451-472 (2010)

Mencar, C., Castiello, C., Cannone, R., Fanelli, A.M.: Interpretability assessment of fuzzy knowledge bases: A cointension based approach.
International Journal of Approximate Reasoning 52(4), 501-518 (2011)

Parliament and Council of the European Union: General data protection regulation (GDPR) (2016), http: //data.europa.eu/eli/
reg/2016/679/o4

Ramos-Soto, A., Bugarin, A., Barro, S.: On the role of linguistic descriptions of data in the building of natural language generation systems.
Fuzzy Sets and Systems 285, 31-51 (2016)

Schoeman, W.: Why AI is the future of growth. Accenture Tech. Rep. (2016), https://www.accenture.com/za-en/
company-news-release-why-artificial-intelligence-future-growth

Trillas, E., Eciolaza, L.: Fuzzy Logic: An Introductory Course for Engineering Students. Springer (2015)

Trivino, G., Sugeno, M.: Towards linguistic descriptions of phenomena. International Journal of Approximate Reasoning 54, 22-34 (2013)
Yager, R.R., Zadeh, L.A.: An Introduction to Fuzzy Logic Applications in Intelligent Systems. The Springer International Series in Engineer-
ing and Computer Science, Springer US (2012)

Zadeh, L.A.: Toward a perception-based theory of probabilistic reasoning with imprecise probabilities. Statistical Planning and Inference
105, 233-264 (2002)

Zadeh, L.A.: Fuzzy sets. Information and Control 8(3), 338-353 (1965)

Zadeh, L.A.: From computing with numbers to computing with words - From manipulation of measurements to manipulation of perceptions
46(1), 105-119 (1999)

Zadeh, L.A.: A new direction in AI: toward a computational theory of perceptions. Artificial Intelligence Magazine 22(1), 73-84 (2001)
Zadeh, L.A.: Is there a need for fuzzy logic? Inf. Sciences 178, 2751-2779 (2008)

Zadeh, L.A.: A note on z-numbers Inf. Sciences 181, 2923-2932 (2011)
THE JUDICIAL DEMAND FOR EXPLAINABLE ARTIFICIAL
INTELLIGENCE

Ashley Deeks*

A recurrent concern about machine learning algorithms is that
they operate as “black boxes,” making it difficult to identify how and
why the algorithms reach particular decisions, recommendations, or pre-
dictions. Yet judges are confronting machine learning algorithms with
increasing frequency, including in criminal, administrative, and civil
cases. This Essay argues that judges should demand explanations for
these algorithmic outcomes. One way to address the “black box” problem
is to design systems that explain how the algorithms reach their conclu-
sions or predictions. If and as judges demand these explanations, they
will play a seminal role in shaping the nature and form of “explainable
Al” (xAI). Using the tools of the common law, courts can develop what
xAI should mean in different legal contexts. There ave advantages to
having courts to play this role: Judicial reasoning that builds from the
bottom up, using case-by-case consideration of the facts to produce nu-
anced decisions, is a pragmatic way to develop rules for xAI. Further,
courts are likely to stimulate the production of different forms of xAI
that are responsive to distinct legal settings and audiences. More geney-
ally, we should favor the greater involvement of public actors in shap-
ing xAI, which to date has largely been left in private hands.

INTRODUCTION

A recurrent concern about machine learning algorithms is that they
operate as “black boxes.” Because these algorithms repeatedly adjust the
way that they weigh inputs to improve the accuracy of their predictions, it
can be difficult to identify how and why the algorithms reach the out-
comes they do. Yet humans—and the law—often desire or demand an-
swers to the questions “Why?” and “How do you know?” One way to ad-
dress the “black box” problem is to design systems that explain how the
algorithms reach their conclusions or predictions. Sometimes called
“explainable AI” (xAI), legal and computer science scholarship has iden-
tified various actors who could benefit from (or who should demand)
xAI. These include criminal defendants who receive long sentences based
on opaque predictive algorithms,! military commanders who are

 

* E. James Kelly Jn—Class of 1965 Research Professor of Law, University of Virginia
Law School. Thanks to Danielle Citron, John Duffy, Bert Huang, Leslie Kendrick, Rich
Schragger, and Rebecca Wexler for helpful comments, and to Scott Harman-Heath for
excellent research assistance.

1. See, e.g., Megan T. Stevenson & Christopher Slobogin, Algorithmic Risk Assessments
and the Double-Edged Sword of Youth, 36 Behav. Sci. & L. 638, 639 (2018).

1829
1830 COLUMBIA LAW REVIEW [Vol. 119:1829

considering whether to deploy autonomous weapons,” and doctors who
worry about legal liability for using “black box” algorithms to make diag-
noses.* At the same time, there is a robust—but largely theoretical—de-
bate about which algorithmic decisions require an explanation and
which forms these explanations should take.

Although these conversations are critically important, they ignore a
key set of actors who will interact with machine learning algorithms with
increasing frequency and whose lifeblood is real-world controversies:
judges.* This Essay argues that judges will confront a variety of cases in
which they should demand explanations for algorithmic decisions,
recommendations, or predictions. If and as they demand these explana-
tions, judges will play a seminal role in shaping the nature and form of
xAI. Using the tools of the common law, courts can develop what xAI
should mean in different legal contexts, including criminal, administra-
tive, and civil cases. Further, there are advantages to having courts play
this role: Judicial reasoning that builds from the bottom up, using case-
by-case consideration of the facts to produce nuanced decisions, is a
pragmatic way to develop rules for xAI.° In addition, courts are likely to
stimulate (directly or indirectly) the production of different forms of xAI
that are responsive to distinct legal settings and audiences. At a more
theoretical level, we should favor the greater involvement of public actors
in shaping xAJI, which to date has largely been left in private hands.

Part I of this Essay introduces the idea of xAI. It identifies the types
of concerns that machine learning raises and that xAI may assuage. It
then considers some forms of xAI that currently exist and discusses the
advantages to each form. Finally, it identifies some of the basic xAl-re-
lated choices judges will need to make when they need or wish to under-
stand how a given algorithm operates.

 

2. See Matt Turek, Explainable Artificial Intelligence (XAT), DARPA, https://www.darpa.
mil/ program /explainable-artificialintelligence [https://perma.cc/ZNL9-86CF] (last visited
Aug. 13, 2019).

3. W. Nicholson Price II, Medical Malpractice and Black-Box Medicine, in Big Data,
Health Law, and Bioethics 295, 295-96 (I. Glenn Cohen, Holly Fernandez Lynch, Effy
Vayena & Urs Gasser eds., 2018).

4. See, e.g., Lilian Edwards & Michael Veale, Slave to the Algorithm? Why a ‘Right
to an Explanation’ Is Probably Not the Remedy You Are Looking for, 16 Duke L. & Tech.
Rev. 18, 67 (2017) [hereinafter Edwards & Veale, Slave to the Algorithm] (questioning
whether xAI will be useful because “[i]ndividual data subjects are not empowered to make
use of the kind of algorithmic explanations they are likely to be offered” but ignoring the
possible role for courts as users of xAI).

5. Cf. Andrew Tutt, An FDA for Algorithms, 69 Admin. L. Rey. 83, 109 (2017) (pro-
posing a federal statutory standard for explainability and arguing that “[i]f explainability
can be built into algorithmic design, the presence of a federal standard could nudge com-
panies developing machine-learning algorithms into incorporating explainability from the
outset”). I share Andrew Tutt’s view that it is possible to provide incentives for designers to
incorporate xAI into their products, but I believe that there are advantages to developing
these rules using common law processes.
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1831

Against that background, the Essay then turns to two concrete areas
of law in which judges are likely to play a critical role in fleshing out
whether xAI is required and, if so, what forms it should take. Part IT con-
siders the use of machine learning in agency rulemaking and adjudica-
tion and argues that judges should insist on some level of xAI in evaluat-
ing the reasons an agency gives when it produces a rule or decision using
algorithmic processes.® Further, if agencies employ advanced algorithms
to help them sort through high volumes of comments on proposed rules,
judges should seek explanations about those algorithms’ parameters and
training.” In both cases, if juadges demand xAI as part of the agency’s rea-
son-giving process, agency heads themselves will presumably insist that
their agencies regularly employ xAI in anticipation of litigation.

Part III explores the use of predictive algorithms in criminal sen-
tencing. These algorithms predict the likelihood that a defendant will
commit additional crimes in the future. Here, the judge herself is the key
consumer of the algorithm’s recommendations, and has a variety of
incentives—including the need to give reasons for a sentence, concerns
about reversal on appeal, a desire to ensure due process, and an interest
in demonstrating institutional integrity—to demand explanations for
how the sentencing algorithm functions.

As courts employ and develop existing case law in the face of predic-
tive algorithms that arise in an array of litigation, they will create the
“common law of xAI,” law sensitive to the requirements of different audi-
ences (judges, juries, plaintiffs, or defendants) and different uses for the
explanations given (criminal, civil, or administrative law settings).* A nu-
anced common law of xAI will also provide important incentives and
feedback to algorithm developers as they seek to translate what are cur-
rently theoretical debates into concrete xAI tools.? Courts should focus
on the power of xAI to identify algorithmic error and bias and the need

 

6. For the argument that judicial review of agency rulemaking employs common law
methodologies, see Jack M. Beermann, Common Law and Statute Law in Administrative
Law, 63 Admin. L. Rev. 1, 3 (2011).

7. See Melissa Mortazavi, Rulemaking Ex Machina, 117 Colum. L. Rev. Online 202,
207-08 (2017), https://columbialawreview.org/wp-content/uploads/ 2017/09 /Mortavazi-
v5.0.pdf [https://perma.cc/SF8R-EG9C] (examining the possibility that agencies may
deploy automated notice-and-comment review).

8. See Finale Doshi-Velez & Mason Kortz, Berkman Klein Ctr. Working Grp. on
Explanation & the Law, Accountability of AI Under the Law: The Role of Explanation 12
(2017), https://arxiv.org/pdf£/1711.01134.pdf [https://perma.cc/LQB3-HG7L] (“As we
have little data to determine the actual costs of requiring AI systems to generate expla-
nations, the role of explanation in ensuring accountability must also be re-evaluated from
time to time, to adapt with the ever-changing technology landscape.”).

9. At least one scholarly piece has concluded that “there is some danger of research
and legislative efforts being devoted to creating rights to a form of transparency that may
not be feasible, and may not match user needs.” Edwards & Veale, Slave to the Algorithm,
supra note 4, at 22. A common law approach to xAI can help ensure that the solutions are
both feasible and match user needs in specific cases.
1832 COLUMBIA LAW REVIEW [Vol. 119:1829

for xAI to be comprehensible to the relevant audience. Further, they
should be attuned to dynamic developments in xAI decisions across cate-
gories of cases when looking for relevant precedent and guidance.

I. THE WHAT AND WHY OF EXPLAINABLE AI

Artificial intelligence is a notoriously capacious and slippery term.
Generally, it refers to “a set of techniques aimed at approximating some
aspect of human or animal cognition using machines.” More con-
cretely, scientists and scholars often use the term to encompass technolo-
gies that include machine learning, speech recognition, natural language
processing, and image recognition.'! Machine learning systems and algo-
rithms, the driving force behind many AI developments, are valuable
because of their ability to learn for themselves “how to detect useful pat-
terns in massive data sets and put together information in ways that yield
remarkably accurate predictions or estimations.”'? Many machine learn-
ing systems are trained on large amounts of data and adjust their own
parameters to improve the reliability of their predictions over time."
Machine learning tools hold out the possibility of making more accurate
decisions, faster, based on far larger quantities of data than humans can
process and manipulate.’ Importantly, though, because a machine learn-
ing system learns on its own and adjusts its parameters in ways its pro-
grammers do not specifically dictate, it often remains unclear precisely
how the system reaches its predictions or recommendations.'® This is
particularly true for “deep learning” systems that use “neural networks,”
which are intended to replicate neural processes in the human brain."
Deep learning systems use nodes, arranged in multiple layers, which
transfer information to each other and learn on their own how to weigh

 

10. Ryan Calo, Artificial Intelligence Policy: A Primer and Roadmap, 51 U.C. Davis L.
Rev. 399, 404 (2017).

11. Artificial Intelligence, Lexico, https://www.lexico.com/en/definition /artificial_
intelligence [https://perma.cc/ MNB4-ZENF] (last visited Oct. 15, 2019) (defining “artificial
intelligence” as “[t]he theory and development of computer systems able to perform tasks
normally requiring human intelligence, such as visual perception, speech recognition,
decision-making, and translation between languages”).

12. Cary Coglianese & David Lehr, Transparency and Algorithmic Governance, 71
Admin. L. Rev. 1, 6 (2019) [hereinafter Coglianese & Lehr, Governance]; see also id. at
14-16 (describing how machine learning differs from traditional statistical techniques).

13. See Ethem Alpaydin, Machine Learning: The New AI 24-25 (2016).

14. See Coglianese & Lehr, Governance, supra note 12, at 16.

15. See Alpaydin, supra note 13, at 155; Will Knight, The Dark Secret at the Heart of
AI, MIT Tech. Rev., May/June 2017, at 55, 56-57.

16. See James Farrant & Christopher M. Ford, Autonomous Weapons and Weapon
Reviews: The UK Second International Weapon Review Forum, 93 Int'l L. Stud. 389, 400
(2017); see also David Lehr & Paul Ohm, Playing with the Data: What Legal Scholars
Should Learn About Machine Learning, 51 U.C. Davis L. Rev. 653, 693 & n.135 (2017).
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1833,

connections between nodes to correctly interpret objects in, say, a video
image.!”

Notwithstanding its potential benefits, the use of machine learning
has prompted a number of concerns, especially when the systems make
predictions that affect people’s liberty, safety, or privacy. One strand of
criticism focuses on the ways in which these algorithms can replicate and
exacerbate societal biases in light of the data on which scientists train
them. Another line of critiques questions the accuracy of various machine
learning predictions, with objectors claiming that tools such as criminal
justice algorithms predict recidivism less accurately than humans."*

A third concern, and the one most salient to this Essay, centers on
the lack of information about how the algorithm arrives at its results—
the “black box” problem.'® The inability to parse the reasons behind the
algorithm’s recommendations can harm those affected by the recom-
mendations. Opaque algorithms can undercut people’s sense of fairness
and trust—particularly when used by the government—and in the crim-
inal justice setting can undercut a defendant’s right to present a defense.
This Essay focuses on algorithms’ lack of transparency and interpretabil-
ity for two related reasons. First, shedding light on how an algorithm pro-
duces its recommendations can help address the other two critiques, by
allowing observers to identify biases and errors in the algorithm.”
Second, computer scientists have begun to make promising inroads into
the problem by developing what is often referred to as “explainable AI.”*!

 

17. See Farrant & Ford, supra note 16, at 400-01.

18. See Julia Dressel & Hany Farid, The Accuracy, Fairness, and Limits of Predicting
Recidivism, Sci. Advances, Jan. 2018, at 1, 3, https:/ /advances.sciencemag.org/content/
4/1/eaa05580/tab-pdf (on file with the Columbia Law Review).

19. See, e.g., Frank Pasquale, The Black Box Society: The Secret Algorithms that
Control Money and Information 3-4 (2015); Danielle Keats Citron, Technological Due
Process, 85 Wash. U. L. Rey. 1249, 1254 (2008) (expressing concern about the “opacity of
automated systems” used to inform administrative rulemaking).

20. Finale Doshi-Velez & Been Kim, Towards a Rigorous Science of Interpretable Machine
Learning 1, 3 (2017), https://arxivorg/pdf/1702.08608.pdf [https:// perma.cc/ ALR4-DM7J]
([I]f the system can explain its reasoning, we then can verify whether that reasoning is sound
with respect to... other desiderata—such as fairness, privacy, reliability, robustness, causality,
usability and trust... .”).

21. For a recent survey of developments in xAI, see Leilani H. Gilpin, David Bau, Ben
Z. Yuan, Ayesha Bajwa, Michael Specter & Lalana Kagal, Explaining Explanations: An
Overview of Interpretability of Machine Learning, arXiv (May 31, 2018), https://arxiv.org/pdtf/
1806.00069.pdf [https://perma.cc/3SG4-G5GA] (last updated Feb. 3, 2019). One reason
for recent progress in this area is the entry into force of the European Union’s General
Data Protection Regulation, which contains provisions that arguably give individuals affect-
ed by purely algorithmic decisions a “right to an explanation.” See Regulation (EU)
2016/679 of the European Parliament and of the Council of 27 April 2016 on the
Protection of Natural Persons with Regard to the Processing of Personal Data and on the
Free Movement of Such Data, and Repealing Directive 95/46/EC, arts. 13-15, 2016 O.J.
(L 119) 41-43 (providing rights to “meaningful information about the logic involved” in
certain automated decisions). The existence of these provisions, coupled with a lack of
detail about what form those explanations must take, has triggered extensive discussions in
1834 COLUMBIA LAW REVIEW [Vol. 119:1829

xAI encompasses a range of efforts to explain—or help humans
interpret—how a particular machine learning model reached its conclu-
sion. The concept of an explanation here “has come to refer to provid-
ing insight into the internal state of an algorithm, or to human-under-
standable approximations of the algorithm.”* xAI provides a variety of
benefits: It can foster trust between humans and the system,” identify cases
in which the system appears to be biased or unfair, and bolster our own
knowledge of how the world works.*4 As discussed below, in legal settings
xAI can benefit judges who wish to rely on the algorithms for decisional
support, litigants who seek to persuade judges that their use of
algorithms is defensible, and defendants who wish to challenge predic-
tions about their dangerousness.*° xAI is not without costs, however. Most
significantly, making an algorithm explainable may result in a decrease in
its accuracy.” xAI may also stifle innovation, force developers to reveal
trade secrets, and impose high monetary costs because xAI can be expen-
sive to build.?”

Fortunately, a variety of xAI currently exists, and computer scientists
continue to develop new forms of it.**7 Some machine learning models
are built to be intrinsically explainable, yet these models are often less

 

the legal and machine learning communities about how and in what form to explain the
results of highly complex algorithms to experts and nonexperts.

22. Sandra Wachter, Brent Mittelstadt & Chris Russell, Counterfactual Explanations
Without Opening the Black Box: Automated Decisions and the GDPR, 31 Harv. J.L. &
Tech. 841, 850 (2018); see also Doshi-Velez & Kim, supra note 20, at 2 (defining interpret-
ability as the “ability to explain or to present in understandable terms to a human”).

23. See Knight, supra note 15, at 61 (describing “explainability as the core of the
evolving relationship between humans and intelligent machines”); Turek, supra note 2
(“Explainable Al—especially explainable machine learning—will be essential if future
warfighters are to understand, appropriately trust, and effectively manage an emerging
generation of artificially intelligent machine partners.”).

24. See Doshi-Velez & Kim, supra note 20, at 3.

25. See, e.g., Robin A. Smith, Opening the Lid on Criminal Sentencing Software,
Duke Today (July 19, 2017), https://today.duke.edu/2017/07/opening-lid-criminal-sen-
tencing-software [https://perma.cc/F63A-VWLQ] (“Using... machine learning, Rudin
and colleagues are training computers to build statistical models to predict future criminal
behavior . . . that are just as accurate as black-box models, but more transparent and easier
to interpret.”).

26. See Doshi-Velez & Kortz, supra note 8, at 2 (“[E]xplanation would come at the price
of system accuracy or other performance objective[s].”).

27. See id. at 2, 12 (“Requiring every AI system to explain every decision could result
in less efficient systems, forced design choices, and a bias towards explainable but sub-
optimal outcomes.”).

28. This Essay’s discussion of categories of xAI is necessarily simplified, because there
are a wide range of approaches to categorizing xAI and the nomenclature is unsettled. For
a survey of the literature on types of xAI and a detailed taxonomy thereof, see Riccardo
Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Dino Pedreschi & Fosca
Giannotti, A Survey of Methods for Explaining Black Box Models 6-8 (2018),
https: / /arxiv.org/pdf/1802.01933.pdf [https:// perma.cc/P8PH-Z5V7].
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1835

complex as a result and tend to be less accurate in their predictions.”
Another set of models is not intrinsically explainable. For these models,
computer scientists have taken two basic approaches.*® One type (which
this Essay terms an “exogenous approach”) does not attempt to actually
explain the inner workings of (that is, the reasoning of) the machine
learning algorithm. Instead, it attempts to provide relevant information
to the algorithm’s user or subject about how the model works using ex-
trinsic, orthogonal methods.*' A second type of approach actually at-
tempts to explain or replicate the model’s reasoning, and sometimes is
referred to as a “decompositional approach.”**

Exogenous xAI approaches can either be model-centric or subject-cen-
tric? A model-centric approach, also referred to as global interpretabil-
ity, might involve, for instance, explaining the creator’s intentions be-
hind the modelling process, the family of model the system uses, the
parameters the creators specified before training the system, qualitative
descriptions of the input data the creator used to train the model, how
the model performed on new data, and how the creators tested the data
for undesirable properties.*® In other words, this constitutes a thick
description of the parts of the model that are knowable. A different type
of model-centric approach might audit the outcomes of the machine
learning system.*® This approach would scour the system’s decisions or
recommendations for appearances of bias or error. Model-centric ap-
proaches attempt to explain the whole model, rather than its performance

 

29. These include linear, parametric, and tree-based models. Dipanjan Sarkar, The
Importance of Human Interpretable Machine Learning, Towards Data Sci. (May 24, 2018),
https:/ /towardsdatascience.com /human-nterpretable+machine-learning-part-1-the-need-
and-importance-ofmodel-interpretation-2ed75815f476 [https://perma.cc/4XD8-F7CD]. For
an argument that society should use only intrinsically interpretable models for high-stakes
decisions, see generally Cynthia Rudin, Please Stop Explaining Black Box Models for High-
Stakes Decisions (2018), https://arxiv.org/pdf/1811.10154.pdf  [https:/ /perma.cc/Q7SF-
6DYN].

30. See Guidotti et al., supra note 28, at 2 (characterizing one category of xAI as fo-
cused on describing how black boxes work and another on explaining decisions without
understanding how the decision systems work); Edwards & Veale, Slave to the Algorithm,
supra note 4, at 64-65 (describing two styles of algorithmic explanation: one that “opens”
the black box and one that does not).

31. See Edwards & Veale, Slave to the Algorithm, supra note 4, at 65 (“[P]edagogical
systems... can get the information they need by simply querying it, like an oracle.”
(emphasis omitted) ).

32. Id. at 64.

33. Id. at 22.

34. See Sarkar, supra note 29.

35. See Edwards & Veale, Slave to the Algorithm, supra note 4, at 55-56.

36. Joshua A. Kroll, Solon Barocas, Edward W. Felton, Joel R. Reidenberg, David G.
Robinson & Harlan Yu, Accountable Algorithms, 165 U. Pa. L. Rev. 633, 660-61 (2017)
(explaining that auditing may test for discrimination in bargaining processes such as retail
car negotiations).
1836 COLUMBIA LAW REVIEW [Vol. 119:1829

in a particular case, and can help ensure that decisions are being made
in a procedurally regular way.”

A subject-centric approach, also referred to as local interpretability,**
in contrast, might provide the subject of a recommendation or decision
with information about the characteristics of individuals who received
similar decisions.** Another subject-centric approach involves the use of
counterfactuals.** Here, people seeking to understand which factors may
have most affected the algorithm’s recommendation about them may,
using that same algorithm, tweak the input factors to test how much a
given factor mattered in the original recommendation.*! For example, an
algorithm that deems someone convicted of an offense to be at high risk
of reoffending could be tested with counterfactuals to see whether the
recommendation would have been different if the person were ten years
older, or had one fewer arrest. The counterfactual approach could take
different forms: It might present several “close possible worlds” or one
“closest possible world,” and it might alter one factor or several different
factors. One advantage of an exogenous approach is that it does “not
require the data subject to understand any of the internal logic of a
model in order to make use of it.” Subject-centric approaches can be
particularly useful for individuals who are seeking to understand “if and
how they might achieve a different outcome”; they empower an individual

 

37. See Edwards & Veale, Slave to the Algorithm, supra note 4, at 55-56.

38. See Sarkar, supra note 29 (defining local interpretability as trying to understand
why the model made a particular decision in a single instance).

39. See Edwards & Veale, Slave to the Algorithm, supra note 4, at 58.

40. See Wachter et al., supra note 22, at 845 (“In the existing literature, ‘explanation’
typically refers to an attempt to convey the internal state or logic of an algorithm that leads
to a decision. In contrast, counterfactuals describe a dependency on the external facts that
led to that decision.”).

41. See id. at 854, 881-82 (discussing implementation options); see also Danielle
Keats Citron & Frank Pasquale, The Scored Society: Due Process for Automated Predictions,
89 Wash. L. Rev. 1, 28-29 (2014) (proposing a system to allow consumers to enter “hypothetical
alterations” to their credit histories to see how the alterations affect their score).

42. See Wachter et al., supra note 22, at 848 (“Such considerations [relevant to which
type of counterfactual you produce] may include the capabilities of the individual con-
cerned, sensitivity, mutability of the variables involved in a decision, and ethical or legal
requirements for disclosure.”); id. at 851 (noting that one could offer “multiple diverse
counterfactual explanations to the data subject”); see also Edwards & Veale, Slave to the
Algorithm, supra note 4, at 63 (describing how counterfactual models can allow individ-
uals to view and reflect upon the decisions about other users).

43. Wachter et al., supra note 22, at 851; id. at 860 (“[C]ounterfactuals bypass the
substantial challenge of explaining the internal workings of complex machine learning
systems,” providing information that “is both easily digestible and practically useful for
understanding the reasons for a decision, challenging them, and altering future behaviour
for a better result.”).
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1837

to more effectively navigate and challenge the process in a particular
44
case.

An alternative to these exogenous approaches is a category of xAI
that attempts to explain (or “decompose”) the model’s reasoning. The
most obvious way to do so is to reveal the source code for the machine
learning model, but that approach will often prove unsatisfactory (be-
cause of the way machine learning works and because most people will
not be able to understand the code).*° More nuanced alternatives exist,
however. One approach is to create a second system alongside the origi-
nal “black box” model, sometimes called a “surrogate model.”* A surro-
gate model works by analyzing featured input and output pairs but does
not have access to the internal weights of the model itself.*” For instance,
scholars constructed a decision tree that effectively mirrored the compu-
tations of a black box model that predicted patients’ risk for diabetes.
The decision tree allowed computer scientists to track which factors
(such as cholesterol level, nicotine dependence, and edema) the black
box model weighed in making its risk assessments. In a legal setting,
this approach might entail creating a decision tree that accurately recon-
structs the decisions of a self-driving car’s black box algorithms in a prod-
uct liability case, for example. These systems closely approximate the
predictions made by an underlying model, while being interpretable.”

There are a host of ways in which machine learning algorithms will
find their way into court in coming years. As a result, the courts them-
selves will be important actors in the machine learning ecosystem that is
working to decide when, how, and in what form to develop xAI for algo-
rithms. In specific cases, courts will need to consider a range of ques-
tions: Who is the audience for the explanation, and how simple or com-
plex should the explanation be? How long should it take the user to
understand the explanation?®® What structure or form should the xAI
take: lines of code, visual presentations, manipulable programs?*! What

 

44. See Andrew D. Selbst & Solon Barocas, The Intuitive Appeal of Explainable
Machines, 87 Fordham L. Rev. 1085, 1120 (2018).

45. Kroll et al., supra note 36, at 638-39 (arguing that revealing source code is a mis-
guided way of creating algorithmic accountability).

46. W. Andrew Pruett & Robert L. Hester, The Creation of Surrogate Models for Fast
Estimation of Complex Model Outcomes, PLOS One June 3, 2016), https://doi.org/10.
1371/journal.pone.0156574 [https:/ /perma.cc/GZ33-78MC].

47. Marco Tulio Ribeiro, Sameer Singh & Carlos Guestrin, “Why Should I Trust You?”:
Explaining the Predictions of Any Classifier, arXiv (Aug. 9, 2016), https://arxiv.org/
pdé£/1602.04938. pdf [https://perma.cc/8WN8-WQJF].

48. Osbert Bastani, Carolyn Kim & Hamsa Bastani, Interpreting Blackbox Models via
Model Extraction, arXiv (Jan. 24, 2019), https://arxivorg/pdf/1705.08504.pdf [https://
perma.cc/L2K4-ZPVU].

49. See id.

50. See Doshi-Velez & Kim, supra note 20, at 7-8.

51. See Wachter et al., supra note 22, at 872 (noting that one could disclose the algo-
rithm’s source code, formula, weights, and full set of variables).
1838 COLUMBIA LAW REVIEW [Vol. 119:1829

factors should the explanation focus on? When should xAI be model-
centric and when should it be subject-centric? If there are trade secrets at
issue, should the court review the algorithm in camera or request an
independent peer review under a nondisclosure agreement? More
generally, what will constitute a “meaningful explanation”?** Judges are
well positioned in this ecosystem to develop pragmatic approaches to
xAI, even though they are not—indeed, because they are not—experts in
machine learning technology.

To understand how these questions may arise concretely in practice,
the next Parts identify and analyze two legal settings in which courts soon
will need to make decisions about the types of xAI that are helpful—or
that may even be legally required.

II. ALGORITHMS IN AGENCY RULEMAKING AND ADJUDICATION

Scholars have begun to consider the ways in which machine learning
algorithms could advance the work of administrative agencies.** Cary
Coglianese and David Lehr write, “[N]ational security and law enforce-
ment agencies are starting to rely on machine learning .... [O]ther gov-
ernment agencies have also begun to explore uses of machine learning,
revealing growing recognition of its promise across a variety of policy set-
tings and at all levels of government.”® Machine learning algorithms

 

52. See Coglianese & Lehr, Governance, supra note 12, at 49 (suggesting these two
methods of review as ways to balance the need for transparency in administrative decision-
making with the need to protect trade secrets).

53. In the national security context, judges frequently have to decide what types of
classified explanations by the executive branch are sufficient. See Ashley S$. Deeks, Secret
Reason-Giving, 129 Yale L.J. (forthcoming 2019) (manuscript at 22-24) (on file with the
Columbia Law Review) (discussing secret reason-giving in the context of foreign surveil-
lance, asset freezes, state secrets, and the Freedom of Information Act).

54. See generally, e.g., Citron, supra note 19 (expressing concern that automated
decisionmaking will undermine procedural safeguards and displace expert reasoning);
Cary Coglianese & David Lehr, Regulating by Robot: Administrative Decision Making in
the Machine-Learning Era, 105 Geo. LJ. 1147 (2017) [hereinafter Coglianese & Lehr,
Regulating by Robot] (arguing that the use of machine learning in administrative actions
does not violate the nondelegation doctrine, due process, equal protection, or the rea-
soned explanation requirements of the Administrative Procedure Act); Mariano-
Florentino Cuéllar, Cyberdelegation and the Administrative State, in Administrative Law
from the Inside Out: Essays on Themes in the Work of Jerry L. Mashaw 134 (Nicholas R.
Parillo ed., 2017) (highlighting the potential tradeoff between the increased precision of
artificial intelligence decisionmaking and the risk of displacing agency deliberation about
social welfare); Benjamin Alarie, Anthony Niblett & Albert Yoon, Regulation by Machine
(Dec. 1, 2016) (unpublished manuscript), https://ssrn.com/abstract =2878950 (on file
with the Columbia Law Review) (envisioning that agencies may deploy algorithms to predict
how courts will decide administrative law cases).

55. Coglianese & Lehr, Regulating by Robot, supra note 54, at 1161; see also
Coglianese & Lehr, Governance, supra note 12, at 3 (“Scholars and policy officials alike
see increasing promise for the use of machine-learning algorithms by administrative agen-
cies in a range of domestic policy areas.”).
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1839

offer the potential to support agency rulemaking and also perhaps adjudi-
cations.°® Virtually all of the scholars who have studied the issue anticipate
that agencies’ use of algorithms will only increase in coming years.°”

Consider how agencies might deploy machine learning algorithms
to facilitate rulemaking. Justice Mariano-Florentino Cuéllar writes, “Over
time, neural networks and genetic algorithms will almost certainly inform
judgments about the proper scope of a rule... .”°° Coglianese and Lehr
go further, envisioning truly autonomous rulemaking in areas such as
SEC regulation of high-speed electronic trading or Treasury Department
regulations that respond to real-time market changes suggestive of sys-
temic risk.°? They even envision multiagent systems, where machine
learning algorithms would model different forecasts for different values
to be traded off, and a separate machine learning system representing
the agency would pick the model (and hence the rule) that maximizes
the objective selected by humans.

Another opportunity for the use of machine learning algorithms in
the agency setting might be to parse and summarize voluminous public
comments provided as part of notice and comment rulemaking.®! Fur-
ther, as noted above, agencies might turn to machine learning to help
them conduct adjudications.” This could include using algorithms to
predict pilot competence and grant pilot’s licenses, forecast the effects of
a proposed merger on competition, or decide disability claims.®* None of
these processes will exclude the human role entirely—at the very least,
computer scientists must code agency “values” into the algorithms in the
form of ones and zeros—but machine learning—driven rulemaking and
adjudication may embody a host of decisional steps that are nontrans-
parent and difficult to trace.

Courts are likely to confront all of these agency uses of algorithms.
Under the Administrative Procedure Act (APA), courts generally may

 

56. Coglianese & Lehr, Regulating by Robot, supra note 54, at 1167 (discussing pos-
sible applications of machine learning in administrative rulemaking and adjudications).

57. See, e.g., Cuéllar, supra note 54, at 135 (“Reliance on computer programs to
make administrative decisions — whether designed as conventional expert systems, more
elaborate genetic or otherwise selfmodifying algorithms, neural or ‘deep learning’ net-
works, or other machine learning mechanisms — will likely accelerate.” ).

58. Id. at 144.

59. Coglianese & Lehr, Regulating by Robot, supra note 54, at 1171-72.

60. Id. at 1174; Coglianese & Lehr, Governance, supra note 12, at 9-10; see also
Cuéllar, supra note 54, at 17.

61. Mortazavi, supra note 7, at 207-08.

62. See Coglianese & Lehr, Governance, supra note 12, at 9 (noting that “the statis
tical tools that will facilitate adjudicating by algorithm already exist and are already being
employed in analogous endeavors”); Cuéllar, supra note 54, at 137 (envisioning “sleek
black boxes” administering “bureaucratic justice”).

63. Coglianese & Lehr, Regulating by Robot, supra note 54, at 1170-71; Cuéllar,
supra note 54, at 136-37.
1840 COLUMBIA LAW REVIEW [Vol. 119:1829

review final agency actions.*! For example, in the informal rulemaking
context, courts may review agency factual determinations and discre-
tionary decisions and set aside those actions that are arbitrary, capricious,
or an abuse of discretion.” In that context, the Supreme Court requires
an agency to “examine the relevant data and articulate a satisfactory
explanation for its action including a ‘rational connection between the
facts found and the choice made.” More recently, the Court confirmed
that the courts’ role involves “examining the reasons for agency deci-
sions—or, as the case may be, the absence of such reasons.”°” Agencies
also are expected to address salient points raised in public comments.®
That said, courts will give an agency particular deference when the
agency is making predictions within its area of expertise that involve tech-
nical matters (“at the frontiers of science”).®°

Agency reason-giving thus plays an important role in defending the
rules that agencies produce. Yet reason-giving can be complicated, if not
confounded, by machine learning algorithms. An agency that has relied
heavily on a machine learning algorithm prediction about the impact of
a particular chemical on human health or about the population trajec-
tory of a threatened species may need to share with the court the types of
data it used, the type of machine learning model it used, the algorithm’s
error rate, and—possibly—the way the algorithm functioned to produce
its prediction.” It is not yet clear precisely what courts will demand of
agencies in this setting, or how agencies will respond.

Some scholars are relatively sanguine about the ease with which
courts will adjust to the growing use of algorithms by agencies. For exam-
ple, Coglianese and Lehr argue that current legal standards in admin-
istrative law do not demand anything close to transparency, that courts
apply a deferential standard to agency rulemaking that relies on complex
modelling, and that agencies will generally be able to meet that standard
if they can show that the algorithm has performed as intended and
achieves a justified objective.”! Other scholars are more skeptical.
Danielle Citron, for instance, worries that opaque algorithms impair
meaningful judicial review because courts cannot see the rules that are

 

64. 5 U.S.C. § 702 (2012); Block y. Cmty. Nutrition Inst., 467 U.S. 340, 345 (1984).

65. 5 U.S.C. § 706(2) (A); see also id. § 553.

66. Motor Vehicle Mfrs. Ass’n v. State Farm Mut. Auto. Ins. Co., 463 U.S. 29, 43
(1983) (quoting Burlington Truck Lines, Inc. v. United States, 371 U.S. 156, 168 (1962)).

67. Judulang v. Holder, 565 U.S. 42, 53 (2011) (emphasis added).

68. Perez y. Mortg. Bankers Ass’n, 135 S. Ct. 1199, 1203 (2015) (“An agency must
consider and respond to significant comments received during the period for public com-
ment.”).

69. Balt. Gas & Elec. Co. v. NRDC, 462 U.S. 87, 103 (1983).

70. See Cuéllar, supra note 54, at 151-52 (noting that courts may want to understand
how that process occurred and how users tested the system to ensure those values were
fairly captured in the output).

71. See Coglianese & Lehr, Governance, supra note 12, at 35-36, 39, 47-49.
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1841

actually applied in a given case.” One possibility is that a court could re-
duce its level of deference to an agency decision when the agency de-
ploys a black-box algorithm purchased from the private sector, because
the court concludes that the agency is making a prediction based on pri-
vate sector expertise, not its own.

Whether optimistic or pessimistic about the way courts will address
these challenges, many scholars take comfort in xAI’s possibilities. Justice
Cuéllar contemplates that machine learning algorithms may help agen-
cies withstand judicial scrutiny, because he assumes that their use could
“conceivably yield greater transparency by making it easier to follow what
precise considerations were used in driving a particular outcome.”” This
is only true, of course, if some form of xAI accompanies the algorithm.
Likewise, Coglianese and Lehr admit that xAI will make it easier to de-
fend an extensive use of machine learning algorithms by agencies. They
highlight the “widening panoply of techniques that data scientists are
developing to make learning algorithms more explainable” and note that
even when the government uses algorithms to make individual-level
predictions, “government agencies will likely have strategies available to
them to provide individual-level explanations.””4 In short, xAI is likely to
serve as an important linchpin in agencies’ transition from human-
dominated decisionmaking to machine-dominated decisionmaking. Yet

none of these scholars focus on the direct role that the courts will play in
affecting xAI itself.

As courts work through administrative law cases involving machine
learning algorithms, they will play a significant role in shaping the xAI
ecosystem. The extent to which courts seek information about the inputs,
outputs, and reliability of agency algorithms or express interest in testing
counterfactuals will give concrete form to current xAI discussions, which
are happening largely in the abstract. Courts’ approaches to agency algo-
rithms in rulemaking settings might prompt developers to pursue exo-
genous xAI approaches, using model-centric explanations to defend the
overall workings and reliability of the algorithm. Courts’ approaches to
agency algorithms in adjudication, in contrast, might lead developers to
pursue decompositional approaches, using subject-centric explanations
to defend the specific adjudicatory choices made. The healthy and grow-
ing set of xAI tools means that there is a range of choices from which to
draw—and, as of now, no statutory guidance about xAI.

The prospect of courts being able to select the proper xAI tool for a
given situation is a good thing, for all of the reasons that we celebrate the

 

72. Citron, supra note 19, at 1298.

73. Cuéllar, supra note 54, at 142, 153. Cuéllar seems less sanguine about situations
in which xAJ is not available, noting with concern that decisions could “be made on a basis
phenomenologically different from what could easily be understood or even explained by
human participants.” Id. at 157.

74. Coglianese & Lehr, Governance, supra note 12, at 6, 55.
1842 COLUMBIA LAW REVIEW [Vol. 119:1829

strengths of the common law.” Courts can move “cautiously and incre-
mentally” as they sort out what types of xAI will be effective and realisti-
cally achievable in explaining different types of agency algorithms.”° The
courts will confront a set of concrete facts, and can, as a result, produce
context-sensitive holdings that do not attempt to impose broad policies
on xAI developments. Further, the courts here will build on existing case
law that fleshes out the requirements of the APA, modestly adjusting that
case law for situations in which the use of this new technology raises un-
answered questions.”

xAI may also mitigate changes in the law that otherwise could result
from the technological disruptions wrought by machine learning. For
example, if courts become concerned about continuing to accord defer-
ence to agency decisionmakers who rely heavily on algorithms or worry
about granting opaque algorithmic decisionmaking a “presumption of
regularity,” xAI may help assuage these concerns. Agencies may per-
ceive the advantages of adopting xAI as a means to address judicial con-
cerns ex ante and thus to minimize disadvantageous doctrinal changes.”
Although common law xAI will, at least initially, offer less predictability
than a federal xAI statute would, it can more easily take into account
technological developments in xAI, and it can be more sensitive to what
is both necessary and possible in a given setting.

TI]. CRIMINAL SENTENCING ALGORITHMS

In the administrative law setting, judges will sit as neutral reviewers
of an agency’s use of machine learning algorithms. In the criminal justice
setting, judges themselves may be the ones using those algorithms.*

 

75. For a general discussion of the advantages of developing rules through the com-
mon law rather than by statute, see generally Jeffrey J. Rachlinski, Bottom-Up Versus Top-
Down Lawmaking, 73 U. Chi. L. Rev. 933 (2006).

76. Neal Devins & David Klein, The Vanishing Common Law Judge?, 165 U. Pa. L.
Rev. 595, 630 (2017) (“[A] series of such decisions will yield a refined principle or rule,
resulting in fewer injustices and inefficiencies than would result if the first court’s app-
roach were followed religiously in all similar cases.”).

77. See Aharon Barak, The Judge in a Democracy 156 (2006) (noting that expansive
judicial case law hangs on narrow statutory hooks and that judges develop common law
within the frameworks of statutes).

78. Cuéllar, supra note 54, at 154-56 (discussing varying levels of deference depen-
ding on the seniority of an agency decisionmaker); id. at 158 (asking whether courts should
revisit the presumption of regularity to “ensure that decisionmakers recognize the risks of
relying on automated analytical techniques they do not entirely understand”).

79. David A. Strauss, Common Law Constitutional Interpretation, 63 U. Chi. L. Rev.
877, 895 (1996) (“Everyone recognizes that law... is in substantial part about following
precedent and otherwise maintaining continuity with the past.”).

80. Many describe these tools as employing machine learning, though the companies
developing the algorithms often invoke “trade secrets,” which prevents both defendants
and judges from knowing precisely how the algorithms function. See, e.g., Ric Simmons,
Quantifying Criminal Procedure: How to Unlock the Potential of Big Data in Our Criminal
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1843,

Here, too, they may—and should—demand certain explanations for how
those algorithms work, to ensure that the algorithms are trustworthy and
fair. Defense counsel also are likely to press prosecutors and algorithm
developers for explanations, which in turn may stimulate judges to do
the same.

Officials in the criminal justice system often need to predict how
likely a person is to commit a dangerous act.*! In the bail context, for
example, judges must assess whether individuals are likely to return to
court for trial and whether they are likely to engage in criminal acts if
they are not kept in detention before trial.** When sentencing a defend-
ant, the judge considers in part how likely it is that the person will
reoffend if released after a particular period.” These data-driven algo-
rithms have the potential to help decisionmakers avoid relying on intui-
tion and personal biases and to allow governments to reduce jail popula-
tions without affecting public safety. As a result, the criminal justice sys-
tem has seen a widespread and growing use of predictive algorithms in
the bail, sentencing, and parole contexts.™

Notwithstanding their potential, these algorithms have come under
intense criticism. Some critiques focus on the idea that the data on which

 

Justice System, 2016 Mich. St. L. Rev. 947, 997 (discussing machine learning criminal justice
algorithms and noting that judges need to understand the factors that the algorithm used
and the historical accuracy of the algorithm’s results). Even if criminal justice algorithms
currently do not use advanced machine learning, scholars have argued that they soon will.
See, e.g., Richard Berk, Criminal Justice Forecasts of Risk: A Machine Learning Approach
110-11 (2012) (“Actuarial methods are changing rapidly. Forecasts increasingly exploit
enormous datasets that are routinely available in real time.”); Richard Berk & Jordan
Hyatt, Machine Learning Forecasts of Risk to Inform Sentencing Decisions, 27 Fed. Sent’g
Rep. 222, 222 (2015) (arguing that machine learning forecasting methods will produce
more accurate forecasts than more traditional regression analyses).

81. See Ashley Deeks, Predicting Enemies, 104 Va. L. Rev. 1529, 1538 (2018).

82. Samuel R. Wiseman, Fixing Bail, 84 Geo. Wash. L. Rev. 417, 420-21 (2016) (discuss-
ing dangerousness and flight risk as two key considerations in bail decisionmaking).

83. See Sonja B. Starr, Evidence-Based Sentencing and the Scientific Rationalization
of Discrimination, 66 Stan. L. Rev. 803, 809 (2014) (noting that evidence-based sentencing
is designed to assist judges in pursuit of sentencing objectives that are centered on reduc-
ing the defendant’s future crime risk).

84. Sam Corbett-Davies, Sharad Goel & Sandra GonzdlezBailén, Even Imperfect
Algorithms Can Improve the Criminal Justice System, NY. Times (Dec. 20, 2017),
https:/ /www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.
huml [https://perma.cc/E7BQ-EDCK].

85. See Algorithms in the Criminal Justice System, Elec. Privacy Info. Ctr,
https: //epic.org/algorithmic-transparency/crim-justice/ [https://perma.cc/3BES-5P2L]
(last visited Aug. 3, 2019) (listing different states’ uses of algorithmic tools for sentencing,
probation, and parole decisions). For a recent example of a state’s decision to require the
use of algorithms in the bail setting, see Dave Gershgorn, California Just Replaced Cash
Bail with Algorithms, Quartz (Sept. 4, 2018), https://qz.com/1375820/california-just-
replaced-cash-bail-with-algorithms/ [https://perma.cc/JRM4-RX57Z].
1844 COLUMBIA LAW REVIEW [Vol. 119:1829

computer scientists train the algorithms are racially biased. Others ar-
gue that the algorithms are no better at predicting recidivism than are
humans who lack criminal justice expertise.*” Finally, many object to the
fact that the algorithms’ structure, contents, and testing are opaque.®*
This latter concern came to a head in Siate v. Loomis, a case in which a
defendant challenged the judge’s use of a sentencing algorithm called
Correctional Offender Management Profiling for Alternative Sanctions
(COMPAS) that had categorized him as posing a “high risk of recidi-
vism.”®° The defendant argued that the court’s use of the risk assessment

violated his due process rights, in part because he was not able to assess
COMPAS’s accuracy.”

The Wisconsin Supreme Court upheld his sentence.*! Nevertheless,
the majority and a concurring Justice expressed caution about the use of
opaque sentencing algorithms. The majority required future presentence
investigation reports to contain warnings about the limitations of
COMBPAS, in order to avoid potential due process violations.” In concur-
rence, Justice Shirley Abrahamson stated that “this court’s lack of under-
standing of COMPAS was a significant problem in the instant case.” She
noted that “making a record, including a record explaining considera-
tion of the evidence-based tools and the limitations and strengths
thereof, is part of the long-standing, basic requirement that a circuit
court explain its exercise of discretion at sentencing.”®* Even the U.S.
government brief, filed to oppose the defendant’s petition for writ of
certiorari in the U.S. Supreme Court, conceded that “[s]ome uses of an
undisclosed risk-assessment algorithm might raise due process con-
cerns—if, for example, a defendant is denied access to the factual inputs

 

86. See, e.g., Andrew Guthrie Ferguson, The Rise of Big Data Policing 131-32 (2017)
(“Police data remains colored by explicit and implicit bias.”).

87. See, e.g., State v. Loomis, 881 N:.W.2d 749, 775 n.3 (Wis. 2016) (Abrahamson, J.,
concurring) (acknowledging that studies differ on the accuracy of the recidivism scores of
Correctional Offender Management Profiling for Alternative Sanctions (COMPAS));
Dressel & Farid, supra note 18, at 1 (concluding that nonexperts are “as accurate and fair
as COMPAS at predicting recidivism” and noting the inefficacy of its more sophisticated
features).

88. See, e.g., Andrea Roth, Trial by Machine, 104 Geo. LJ. 1245, 1270 (2016) (noting
concerns about obscuring hidden subjectivities and errors in criminal justice algorithms);
Rebecca Wexler, Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal
Justice System, 70 Stan. L. Rev. 1343, 1349-50 (2018) [hereinafter Wexler, Life, Liberty]
(“Developers often assert that details about how their tools function are trade secrets.”).

89. Loomis, 881 N.W.2d at 753, 755.

90. Id. at 757.

91. Id. at 772.

92. See id. at 769-70.

93. Id. at 774 (Abrahamson, J., concurring) (noting that, “[a]t oral argument, the
court repeatedly questioned both the State’s and defendant’s counsel about how COMPAS
works” but that “[f]ew answers were available”).

94. Id. at 775.
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1845

about his criminal and personal history, or if his risk scores form part of a
sentencing ‘matrix’ or establish a ‘presumptive’ term of imprisonment.”®

Perhaps not surprisingly, some jurisdictions are shifting away from
opaque commercial algorithms such as the one used in Loomis and to-
ward algorithms that use public data and publicly available source
codes. Even those jurisdictions may retain an interest in xAI, because
source codes alone are typically not selfexplanatory. In particular,
though, it is the courts in jurisdictions that continue to rely on opaque
predictive algorithms that may—and should—become more aggressive in
demanding xAI. There are a host of reasons why they might do so. First,
both federal and state courts often face statutory requirements to justify
the sentences they impose.” This allows the public to evaluate the
reasonableness of the sentence and see what factual findings the judge
made; it also permits review by appellate courts. Judges who rely in part
on sentencing algorithms might believe that they need to understand the
parameters of the algorithms to articulate the reasons for using them.
Second, judges serve as a bulwark to ensure accuracy and fairness in sen-
tencing; demanding xAI will help judges evaluate whether the algorithms
meet that standard or contain significant errors.” Third, judges might
demand xAI to ensure the institutional integrity of the courts, which is
undercut if courts use unreliable sources of guidance. Fourth, judges

 

95. Brief for the United States as Amicus Curiae at 18, Loomis v. Wisconsin, 137 S. Ct.
2290 (2017) (No. 16-6387), 2017 WL 2333897.

96. See, e.g., Elaine Angelino, Nicholas LarusStone, Daniel Alibi, Margo Seltzer &
Cynthia Rudin, Learning Certifiably Optimal Rule Lists for Categorical Data, J. Machine
Learning Res., June 2018, at 1, 2, http://www.jmlrorg/papers/volume18/17-716/17-
716.pdf [https://perma.cc/LA46-XA6R] (developing an open-source machine learning
model that accurately predicts a person’s likelihood of rearrest); Creating a Fairer Pretrial
System, Arnold Ventures (Dec. 1, 2017), https://www.arnoldventures.org/stories/creating-
a-fairer-pretrial-system [https://perma.cc/TP87-G2L6] (stating that roughly forty juris-
dictions have adopted or are in the process of implementing a pretrial risk assessment tool
called the Public Safety Assessment); Public Safety Assessment: Risk Factors and Formula, Pub.
Safety Assessment, https://www.psapretriaLorg/about/factors [https://perma.cc/W3P3-4UJN]
(last visited Aug. 3, 2019) (disclosing the risk factors and formula undergirding the Public
Safety Assessment).

97. See, e.g., 18 U.S.C. § 3553(c) (2012) (requiring that the “court, at the time of
sentencing, shall state in open court the reasons for its imposition of the particular sen-
tence,” including specific reasons for sentencing outside the range); Heather Young Keagle,
Appellate Div., N.J. Superior Court, Manual on New Jersey Sentencing Law 11 (rev. 2019),
hittps://njcourts.gov/attorneys/assets /attyresources/manualsentencinglaw.pdf [https://perma.
cc/UQ77-7U32] (“At the time of sentencing, the court must ‘state reasons for imposing such
sentence including... the factual basis supporting a finding of particular aggravating or
mitigating factors affecting sentence.” (quoting State v. Fuentes, 85 A.3d 923, 932 (N.J. 2014))).

98. Toby D. Slawsky, The Importance of Statements of Reasons in Guideline
Sentencing, 28 Fed. Sent’g Rep. 174, 174 (1990).

99. See Rebecca Wexler, Code of Silence, Wash. Monthly (June/July/Aug. 2017),
https:/ /washingtonmonthly.com/ magazine /junejulyaugust-2017/code-ofsilence/ [https://
perma.cc/5PHM-6SPV] (describing how opacity can conceal flaws in criminal justice
algorithms).
1846 COLUMBIA LAW REVIEW [Vol. 119:1829

might insist on some form of xAI because they are worried about being
reversed on appeal for relying on a flawed or poorly understood tool.
Finally, judges may demand xAlI at the behest of defense counsel, to facil-
itate adversarial challenges and promote procedural fairness.

What form is xAI likely to take here? In the administrative law con-
text, the audiences for the xAI (executive agencies, judges, and corpo-
rate or interest-group plaintiffs) are likely to be sophisticated actors. In
the criminal justice setting, there are three main audiences: (1) judges,
(2) defendants, and (3) their lawyers. Some judges and defense counsel
will be sophisticated repeat players, but the defendants themselves are
likely to have little experience with algorithms—and indeed judges them-
selves will have different levels of experience with tools such as regression
analyses." Judges might be more interested in model-centric explana-
tions, while recognizing that defendants may need subject-centric xAI.
Both audiences might benefit from being able to run counterfactuals
through the system as well. Judges will have to decide whether to demand
one or the other forms of xAI—or both.

Judges will encounter pushback from the producers of proprietary
algorithms, who have resisted revealing information about the workings
of their algorithms on the basis of trade secrets claims.'"! There are ways
to protect such secrets, however, including by issuing protective orders.’
Further, it might be possible to build a surrogate model of the sentencing
algorithm that sheds light on its functioning without forcing the pro-
ducer to reveal trade secrets. In those cases, xAI may play an important
role in counterbalancing trade secrets claims such as those in play in
Loomis.

There is another, less obvious advantage to judges’ use of xAI in the
criminal justice setting. A persistent concern about machine learning
algorithms is that they produce “automation bias”—a tendency to unduly
accept a machine’s recommendation.’ Putting xAI in front of judges

 

100. This level of experience presumably will increase over time as more machine
learning tools find their way into the practice of law.

101. See Wexler, Life, Liberty, supra note 88, at 1349-50.

102. See id. at 1409-10.

103. Kate Goddard, Abdul Roudsari & Jeremy C. Wyatt, Automation Bias: A Systematic
Review of Frequency, Effect Mediators, and Mitigators, 19 J. Am. Med. Informatics Ass’n
121, 121 (2012); Raja Parasuraman & Dietrich H. Manzey, Complacency and Bias in
Human Use of Automation: An Attentional Integration, 52 Hum. Factors 381, 397 (2010)
(concluding that both expert and inexpert participants suffer from complacency and bias
in their interactions with automated systems).
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1847

may lead them to question an algorithm’s conclusions in a way that helps
them avoid succumbing to automation bias.'™

In light of the various benefits of xAI and a growing number of xAI
tools in the toolbox, one puzzle is why courts have not already begun to
insist on xAI when confronted with machine learning algorithms in
criminal justice settings. Is it because the idea of xAI is nascent? Because
the use of algorithms in the criminal justice context is only now starting
to receive widespread scrutiny and criticism? Because of trade secrets
hurdles? Or because the courts themselves currently lack the confidence
to understand and use xAI?!® It is likely a combination of all of these
factors. However, as the use of machine learning and, concomitantly, xAI
spreads, the courtroom is a fertile ground in which to connect xAI to
real-world challenges.

CONCLUSION

Agency rulemaking and criminal justice are hardly the only areas of
law in which courts will confront machine learning algorithms. Other
possible legal contexts include product liability litigation involving self-
driving cars or the internet of things,'® litigation challenging school dis-
tricts’ use of algorithms for teacher evaluations,!°’ malpractice litigation
against doctors who rely on medical algorithms for diagnoses,'* individ-
ual challenges to governmental decisions to freeze people’s assets based
on algorithmic recommendations,'” defendants’ challenges to police

 

104. See Matt O’Brien & Dake Kang, AI in the Court: When Algorithms Rule on Jail
Time, Phys.org (Jan. 31, 2018), https://phys.org/news/2018-01-ai-courtalgorithms.html
[https://perma.cc/84R8-B2X4] (discussing automation bias in judges); see also id. (quot-
ing a Northwestern University computer scientist as arguing that judges need “boxes that
give [them] answers and explanations and ask [them] if there’s anything [they] want to
change”).

105. Lilian Edwards & Michael Veale, Enslaving the Algorithm: From a “Right to an
Explanation” to a “Right to Better Decisions”?, 16 IEEE Security & Privacy 46, 53 (2018)
[hereinafter Edwards & Veale, Enslaving the Algorithm] (“It seems quite likely that courts
will be reluctant to become activists about disclosures of source code, let alone algorithmic
training sets and models, until they feel more confident of their ability to comprehend
and use such evidence—which may take some time.”).

106. See Ian Bogost, Can You Sue a Robocar?, Atlantic (Mar. 20, 2018), https://www.
theatlantic.com/technology/archive/2018/03/can-you-sue-a-robocar/556007/ — [https://
perma.cc/84LK-AQ9V] (discussing the legal implications of accidents caused by selfdriv-
ing cars)

107. See Coglianese & Lehr, Governance, supra note 12, at 37-38 (discussing litigation
by teachers over a school district’s use of algorithms to rate teachers’ performance).

108. See Shailin Thomas, Artificial Intelligence, Medical Malpractice, and the End of
Defensive Medicine, Bill of Health (Jan. 26, 2017), http://blog.petrieflom.law.harvard.edu/
2017/01/26/artificial-intelligence-medical-malpractice-and-the-end-of-defensive-medicine/
[https://perma.cc/7AU6-P3QJ] (discussing the interaction between malpractice litigation
and use of machine learning algorithms).

109. See Cuéllar, supra note 54, at 144 (describing the potential use of algorithms to
include decisions to freeze individuals’ assets).
1848 COLUMBIA LAW REVIEW [Vol. 119:1829

stops based on the use of “automated suspicion” algorithms,’ govern-
ment requests for Foreign Intelligence Surveillance Act orders based on
algorithmic predictions about who is a foreign agent,''' or challenges to
algorithm-driven forensic testing.'!’? These cases might implicate ques-
tions of substantive or procedural due process,''* require “arbitrary and
capricious” review, or force courts to decide whether to allow expert testi-
mony about how a given algorithm functions.''4 Some scholars have pro-
posed the kinds of explanations courts should seek in certain types of
cases,'!® but the rubber will hit the road when the courts themselves de-
cide what is needed. Using the tools of the common law, judges can and
will productively drive the advancement and fine-tuning of xAI. When
deciding xAl-related questions, courts should focus on two principles
that can further public law values: maximizing xAI’s ability to help iden-
tify errors and biases within the algorithm, and aligning the form of xAI
in a given case with the needs of the relevant audiences.

The interest in xAI is not simply a U.S. phenomenon. The European
Union’s General Data Protection Regulation (GDPR), which applies to

 

110. See Michael Rich, Automated Suspicion Algorithms and the Fourth Amendment,
164 U. Pa. L. Rev. 871, 875-76 (2016) (“Machine learning provides a way to go one step
further and use data to identify likely criminals among the general population.”).

111. See 50 U.S.C. § 1805(a) (2012) (outlining the findings necessary for a judge to
enter an ex parte order approving electronic surveillance); Jim Baker, Counterintelligence
Implications of Artificial Intelligence—Part III, Lawfare (Oct. 10, 2018), https://www.
lawfareblog.com/counterintelligence-implications-artificial-intelligence-partiii  [https://
perma.cc/M6D3-KNW9] (discussing the use of AI in counterintelligence).

112. See Symposium on Forensic Expert Testimony, Daubert, and Rule 702, 86
Fordham L. Rev. 1463, 1513-15 (2018) (presenting discussion of the application of a
Daubertstyle test to algorithm-driven DNA testing by Professor Erin Murphy of the
Advisory Committee on Evidence Rules); Drew Harwell, Oregon Became a Testing
Ground for Amazon’s Facial-Recognition Policing. But What if Rekognition Gets It
Wrong?, Wash. Post (Apr. 30, 2019), https://www.washingtonpost.com/technology/
2019 /04/30/amazonsfacial-recognition-technology-issupercharging-local-police/ (on file
with the Columbia Law Review) (discussing how lawyers are preparing to litigate the admis-
sibility of facial-recognition system evidence in court).

113. See Coglianese & Lehr, Governance, supra note 12, at 38-43.

114. Courts already have confronted Daubert issues in the face of a juvenile sentencing
algorithm and algorithm-assisted discovery processes. See Moore v. Publicis Groupe, 287
ER.D. 182, 182-84, 188-89 (S.D.NY. 2012) (concluding that using sophisticated algorith-
mic tools to search for electronically stored information is acceptable and that Daubert did
not apply at the search stage of discovery because the documents were not yet being
introduced as evidence); AI Now Inst., Litigating Algorithms: Challenging Government
Use of Algorithmic Decision Systems 1, 13-14 (2018), https://ainowinstitute.org/
litigatingalgorithms.pdf [https:/ /perma.cc/Y5V4-NVRF] (noting that counsel persuaded the
judge that past studies had not sufficiently validated a juvenile sentencing algorithm).

115. See, e.g., Kroll et al., supra note 36, at 637 (describing verification methods to
ensure algorithms’ procedural regularity).
2019] EXPLAINABLE ARTIFICIAL INTELLIGENCE 1849

countries and companies in the European Union, contains provisions'!®
requiring what some have termed a “right to an explanation.”!!”7 Some
scholars have interpreted the GDPR to require data controllers who
make decisions about individuals based “solely on automated processing”
to provide those individuals with meaningful information about the logic
involved in that automated decisionmaking.''® But it remains unclear
precisely what the GDPR requires and what steps states and companies
must take to meet those requirements. Other countries have enacted
their own domestic “explainability” requirements. France, for instance,
in its Digital Republic Act, gives individuals a right to an explanation for
administrative algorithmic decisions made about those individuals.'!®
That law requires the administrative decisionmaker to provide a range of
information about the “degree and the mode of contribution of the algo-
rithmic processing to the decision making,” including what data were
processed, what the system’s parameters were, and how the algorithm
weighted factors.'*° Thus, U.S. common law decisions about xAI are likely
to be of interest not only to U.S. federal and state judges but to foreign
judges and administrative officials as well.

Nor are courts the only government actors that must navigate the
costs and benefits of xAI. Congress may demand and shape the use of
xAI across industries or within government via legislation, and it may also
demand the use of xAI in briefings by executive agencies, including the
intelligence community. Any statute regulating the use of xAI, however,
necessarily must be crafted at a high level of generality. That statute may
capture the basic values that Congress wants xAI to advance, but such a
statute may struggle to endure in this quickly shifting landscape. Further,
the likelihood that Congress will be able to act in this space is limited, if
its recent actions on complicated technology issues are any guide.”!

 

116. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27
April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal
Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General
Data Protection Regulation), arts. 15(1) (h), 22, 2016 O.J. (L119) 43, 46.

117. See, e.g., Bryce Goodman & Seth Flaxman, European Union Regulations on
Algorithmic Decision Making and a “Right to Explanation,” AI Mag., Fall 2017, at 50.

118. See, e.g., id. at 55. But see Sandra Wachter, Brett Mittelstadt & Luciano Floridi,
Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General
Data Protection Regulation, 7 Int’l Data Privacy L. 76, 77, 79-90 (2017) (arguing that the
GDPR implements a limited “right to be informed” rather than a “right to explanation”).

119. Edwards & Veale, Enslaving the Algorithm, supra note 105, at 48-49.

120. Id. at 48.

121. See Ashley Deeks, Facebook Unbound?, 105 Va. L. Rev. Online 1, 6-8 (2019),
http://wwwyirginialawreview.org /sites/virginialawreview.org /files/01.%20Final%20Decks.
pdf [https://perma.cc/QHE5-SJ7Q] (“[Congress] has failed in its efforts to legislate on
the use of encryption, election security... , ‘hacking back, and drone safety, and it has
not tried to regulate facial-recognition software. Efforts to impose federal data-privacy laws
on companies are just getting underway.”).
1850 COLUMBIA LAW REVIEW [Vol. 119:1829

Common law xAI thus offers real promise as we head deeper into
the age of algorithms. Courts will only be able to work xAI issues at the
edges, looking across legal categories to draw on xAI developments in
different doctrinal areas, but that work—and the response to that work
by the creators and users of machine learning algorithms—may get us
where we need to be.
2103.04244v2 [cs.AI] 8 Jun 2021

arXiv

Counterfactuals and Causability in Explainable Artificial Intelligence:
Theory, Algorithms, and Applications

Yu-Liang Chou!, Catarina Moreira!”, Peter Bruzat, Chun Ouyang!, Joaquim Jorge”
! School of Information Systems, Queensland University of Technology, Brisbane, Australia

2 INESC-ID Lisboa, Instituto Superior Técnico, ULisboa, Portugal

 

Abstract

Deep learning models have achieved high performances across different domains, such as medical
decision-making, autonomous vehicles, decision support systems, etc. Despite this success, the internal
mechanisms of these models are opaque because their internal representations are too complex for a human
to understand. This makes it hard to understand the how or the why of the predictions of deep learning
models.

There has been a growing interest in model-agnostic methods that can make deep learning models more
transparent and explainable to a user. Some researchers recently argued that for a machine to achieve a
certain degree of human-level explainability, this machine needs to provide human causally understandable
explanations, also known as causability. A specific class of algorithms that have the potential to provide
causability are counterfactuals.

This paper presents an in-depth systematic review of the diverse existing body of literature on counter-
factuals and causability for explainable artificial intelligence. We performed an LDA topic modeling analysis
under a PRISMA framework to find the most relevant literature articles. This analysis resulted in a novel
taxonomy that considers the grounding theories of the surveyed algorithms, together with their underlying
properties and applications in real-world data. This research suggests that current model-agnostic counter-
factual algorithms for explainable AI are not grounded on a causal theoretical formalism and, consequently,
cannot promote causability to a human decision-maker. Our findings suggest that the explanations derived
from major algorithms in the literature provide spurious correlations rather than cause/effects relationships,
leading to sub-optimal, erroneous, or even biased explanations. This paper also advances the literature
with new directions and challenges on promoting causability in model-agnostic approaches for explainable

artificial intelligence.

Keywords: Deep Learning, Explainable AI, Causability, Counterfactuals, Causality

 

Preprint submitted to Preprint June 9, 2021
1. Introduction

Artificial intelligence,in particular, deep learning, have made great strides in equaling, and even surpass-
ing human performance in many tasks such as categorization, recommendation, game playing or even in
medical decision-making [1]. Despite this success, the internal mechanisms of these technologies are an
enigma because humans cannot scrutinize how these intelligent systems do what they do. This is known
as the black-box problem [2]. Consequently, humans are reliant to blindly accept the answers produced by
machine intelligence without understanding how that outcome came to be. There is growing disquiet about
this state of affairs as intelligent technologies increasingly support human decision-makers in high-stakes

contexts such as the battlefield, law courts, operating theatres, etc.

1.1. The need for Explainability

Several factors motivated the rise of approaches that attempt to turn predictive black-boxes transparent
to the decision-maker [3, 4]. One of these factors is the recent European General Data Protection Regulation
(GDPR) [5], which made the audit and verifiability of decisions from intelligent autonomous systems
mandatory, increasing the demand for the ability to question and understand Machine Learning (ML)
systems. These regulations directly impact worldwide businesses because GDPR applies not only to data
being used by European organisations, but also to European data being used by other organisations. Another
important factor is concerned with discrimination (such as gender and racial bias) [6]. Studies suggest
that predictive algorithms widely used in healthcare, for instance, exhibited racial biases that prevented
minority societal groups from receiving extra care [7] or display cognitive biases associated with medical
decisions [8, 9]. In medical X-ray images, it was found that deep learning models have learned to detect a
metal token that technicians use to visualise the X-ray images, making this feature impacting the predictions
of the algorithm [10]. Other studies revealed gender and racial biases in automated facial analysis algorithms
made available by commercial companies [11], gender biases in textual predictive models [14, 15, 16] or
even more discriminatory topics such as facial features according to sexual orientation [17].

The black-box problem and the need for interpretability motivated an extensive novel body of liter-
ature in machine learning that focuses on developing new algorithms and approaches that not only can
interpret the complex internal mechanisms of machine learning predictions but also explain and provide
understanding to the decision-maker of the why these predictions [18, 19]. In this sense, interpretability
and explainability have become the main driving pillars of explainable AI (XAI) [20]. More specifically, we

define interpretability and explainability in the following way.

* Interpretability is defined as the extraction of relevant sub-symbolic information from a machine-

learning model concerning relationships either contained in data or learned by the model [21].
* Explainability, on the other hand, refers to the ability to translate this sub-symbolic information in a

comprehensible manner through human-understandable language expressions [22].

The overarching goal of XAI is to generate human-understandable explanations of the why and the how
of specific predictions from machine learning or deep learning (DL) systems. Paez [23] extends this goal
by adding that explainable algorithms should offer a pragmatic and naturalistic account of understanding
in AI predictive models and that explanatory strategies should offer well-defined goals when providing
explanations to its stakeholders.

Currently, there is an extensive body of literature reviewing different aspects of XAI

In Miller [24], the author portraits the missing link between the current research on explanations from
the fields of philosophy, psychology, and cognitive science. Miller [24] highlighted three main aspects
that an XAI system must have in order to achieve explainability: (1) people seek explanations of the for
why some event happened, instead of another?, which suggests a need for counterfactual explanations; (2)
recommendations can focus on a selective number of causes (not all of them), which suggests the need for
causality in XAI; and (3) explanations should consist in conversations and interactions with a user promoting
an explanation process where the user engages in and learns the explanations. In Guidotti et al. [25], the
authors survey black-box specific methods for explainability and propose a taxonomy for XAI systems based
on four features: (1) the type of problem faced based on their taxonomy; (2) the type of explainer adopted;
(3) the type of black-box model that the explainer can process; and (4) the type of data that the black-box
supports. On the other hand Das and Rad [26] proposed a taxonomy for categorizing XAI techniques based
on their scope of explanations, methodology behind the algorithms, and explanation level or usage. [27]
classified explainable methods according to (1) the interpretability of the model to be explained; (2) the
scope of the interpretability; and (3) if the black-box is dependent or not on any type of machine learning
model.

Barredo Arrieta et al. [28] propose and discuss a taxonomy related to explainability in different machine
learning and deep learning models. Additionally, the authors propose new methodologies towards responsi-
ble artificial intelligence and discuss several aspects regarding fairness, explainability, and accountability in
real-world organisations.

Some authors have reviewed evaluation methodologies for explainable systems and proposed a novel
categorisation of XAI design goals and evaluation measures according to stakeholders [29]. In contrast, other
authors identified objectives that evaluation metrics should achieve and demonstrated the subjectiveness
of evaluation measures regarding human-centered XAI systems [30]. Carvalho et al. [31] extensively
surveyed the XAI literature with a focus on both qualitative and quantitative evaluation metrics as well as
properties/axioms that explanations should have. For other examples of works that survey evaluation in

XAI, the reader can refer to Hoffman et al. [32], Alvarez-Melis and Jaakkola [33].
In terms of the generation of explanations, Chen et al. [34] surveyed the literature in terms of biases.
The authors identified seven different types of biases that were found in recommendations and proposed a

taxonomy in terms of current work on recommendations and potential ways to dibiase them.

1.2. From Explainability to the need of Causability

For a model to be interpretable, it must suggest explanations that make sense to the decision-maker
and ensure that those explanations accurately represent the true reasons for the model’s decisions [35].
Current XAI models that attempt to decipher a black-box that is already trained (also known as post-
hoc, model-agnostic models) build models around local interpretations, providing approximations to the
predictive black-box [36, 37], instead of reflecting the true underlying mechanisms of the black box (as
pointed out by [38]). In other words, these algorithms compute correlations between individual features
to approximate the predictions of the black-box. In this paper, we argue that the inability to disentangle
correlation from causation can deliver sub-optimal or even erroneous explanations to decision-makers [39].
Causal approaches should be emphasized in XAI to promote a higher degree of interpretability to its users
and avoid biases and discrimination in predictive black-boxes [40].

The ability to find causal relationships between the features and the predictions in observational data is
a challenging problem and constitutes a fundamental step towards explaining predictions [22]. Causation
is a ubiquitous notion in Humans’ conception of their environment [41]. Humans are extremely good at
constructing mental decision models from very few data samples because people excel at generalising data
and tend to think in cause/effect manners [42]. There has been a growing emphasis that AI systems should
be able to build causal models of the world that support explanation and understanding, rather than merely
solving pattern recognition problems [43]. For decision-support systems, whether in finance, law, or even
warfare, understanding the causality of learned representations is a crucial missing link [44, 45, 46]. However,
when considering machines, how can we make computer-generated explanations causally understandable

by humans? This notion was recently put forward by Holzinger et al. [22] in a term coined causability.

* Causability: the extent to which an explanation of a statement to a human expert achieves a specified
level of causal understanding with effectiveness, efficiency, and satisfaction in a specified context of
use [22]. In this sense, causability can be seen as a property of human intelligence, whereas explainability
as a property of artificial intelligence [47]. Figure 1 illustrates the notion of causability under the context

of XAI.

1.3. Counterfactuals as Means to Achieve Causability

Causality is a fundamental concept to gain intellectual understanding of the universe and its contents. It

is concerned with establishing cause-effect relationships [49]. Causal concepts are central to our practical
 

 

 

 

 

 

Black box model V(x)
Xy ———
x2 Connectionist Evolutionary
x= Models Models
x3 » TT = =
: Ressoning Probabilistic
XN | Models Models
XxX, AX i
input data model prediction caused 9, causal understanding

Figure 1: The notion of causability within this thesis: given a predictive black box model, the goal is to create interpretable and
explainable methods that will provide the user a causal understanding of why certain features contributed to a specific
prediction [22, 47, 48]

deliberations, health diagnosis, etc. [50, 22]. Even when one attempts to explain certain phenomena, the
explanation produced must acknowledge, to a certain degree, the causes of the effects being explained [51].
However, the nature and definition of causality is a topic that has promoted a lot of disagreement throughout
the centuries in Philosophical literature. While Bertrand Russel was known for being the most famous denier
of causality, arguing that it constituted an incoherent topic [52], it was mainly with the philosopher and
empiricist David Hume that the concept of causation started to be formally analyzed in terms of sufficient
and necessary conditions: an event c causes an event e if and only if there are event-types C and E such that
C is necessary and sufficient for E [52]. Hume was also one of the first philosophers to identify causation
through the notion of counterfactual: a cause to be an object followed by another in which if the first object
(the cause) had not occurred, then the second (the effect) would never exist [53]. This concept started to gain
more importance in the literature with the works of Lewis [54].

Counterfactuals are then defined as a conditional assertion whose antecedent is false and whose con-
sequent describes how the world would have been if the antecedent had occurred (a what-if question). In
the field of XAI, counterfactuals provide interpretations as a means to point out which changes would be
necessary to accomplish the desired goal (prediction), rather than supporting the understanding of why
the current situation had a certain predictive outcome [55]. While most XAI approaches tend to focus on
answering why a certain outcome was predicted by a black-box, counterfactuals attempt to answer this
question in another way by helping the user understand what features does the user need to change to achieve
a certain outcome [56]. For instance, in a scenario where a machine learning algorithm assesses whether a
person should be granted a loan or not, a counterfactual explanation of why a person did not have a loan
granted could be in a form of a scenario if your income was greater than $15,000 you would be granted a loan

[50].
1.4. Contributions

The hypothesis that we put forward in this paper is that the inability to disentangle correlation from
causation can deliver sub-optimal or even erroneous explanations to decision-makers [39]. Causal approaches
should be emphasized in XAI to promote a higher degree of interpretability to its users. In other words, to
achieve a certain degree of human-understandable explanations, causability should be a necessary condition.

Given that there is not a clear understanding of the current state of the art concerning causal and
causability approaches to XAI, it is the purpose of this paper to make a systematic review and critical
discussion of the diverse existing body of literature on these topics. This systematic review will introduce
researchers in the field of XAI that are interested in focusing their knowledge on the current state-of-the-art
approaches currently present in the literature. Recently two papers surveying counterfactuals in XAI were
proposed in the literature [57, 58, 59]. Our paper distinguishes from the current body of literature by
making a deep analysis on current counterfactual model-agnostic approaches, and how they could promote
causability.

In summary, this paper contributes to a literature review with discussions under three paradigms:

* Theory. Survey and formalize the most important theoretical approaches that ground current explain-

able AI models in the literature that are based on counterfactual theories for causality.

* Algorithms. Understand what are the main algorithms that have been proposed in the XAI literature
that use counterfactuals, and discuss which ones have are based on probabilistic approaches for

causality and which ones have the potential to achieve a certain degree of causability.

« Applications. A continuous use case analysis to understand the main domains and fields where XAI
algorithms that promote causability are emerging and the potential advantages and disadvantages of

such approaches in real-world problems, namely in the mitigation of biased predictions.

1.5. Paper Organisation

This paper is organised as follows. In Section 2, we present the taxonomy of the current state-of-the-art
algorithms in XAI. In Section 3 we present a systematic review on counterfactual and causability approaches
in XAI. In the following sections, we present the findings of our systematic literature review. Section 4,
we present properties that are used throughout the literature to assess what is a good counterfactual and
a discussion on the impacts of different distance functions on XAI algorithms. Section 5, we present our
first contribution where we analyse the theories that underpin the different algorithms of the literature
by introducing a novel taxonomy for model-agnostic counterfactuals in XAI. In Section 6, we analyse the
different algorithms of the literature based on the taxonomy that we proposed. In Section 7, we discuss

the main applications of XAI algorithms together with recent developments in causability. In Section 8,
we present the main characteristics that should be part of a causability system for XAI. Finally, Section 9

answers the proposed research questions, and Section 10 presents the main conclusions of the work.

2. A General Overview of Current Model-Agnostic Approaches in XAI

Various approaches have been proposed in the literature to address the problem of interpretability in
machine learning. Generally, this problem can be classified into two major models: interpretable models
(inherently transparent) and model-agnostic, also referred to as post-hoc models (which aim to extract
explanations out of opaque models). From our systematic literature review, these approaches can be

categorised within the taxonomy presented in Figure 2.

Taxonomy Models Algorithms Approach

Decision Tree

Bayesian network
Partial Dependence
See Dependency plots
Boosted Trees
Individual Conditional

Expectations (ICE) Dependency plots

Tree base model
zi Linear Regression
Nene a H-Statistic
a Partial Dependence ea
Models
H Regression model

Transparent Models H Generalised Linear

Models (GLM)

Rule-based models

Generalised Additive

Models (GAM)

Non-Gaussian
Outcomes Interaction Models

Logistic regression

Accumulated Local
Effects (ALE)

   
 
     

Model-Agnostic
madel

     
 
 
  

LIME Linear approximate

Surrogate (Local)
Models.

 

Anchors Fule-Based leaner

 

Explainable
Models

Counterfactual
instances

 

Counterfactual

 

 

 

Kernel SHAP

  

Game theory based

 
    

 
 

 

 

Post-Hoc

 

 

 

Support Vector
eae ee LH] Machines Explainability DesreuAr
TreeSHAP
Multi-layer Neural Model-Specitic
enon a) Feature relevance
explanation
Explanation by > inTress|

simplification

Figure 2: Taxonomy of explainable artificial intelligence based on the taxonomy proposed by Belle and Papantonis [60].

Interpretable models are by design already interpretable, providing the decision-maker with a trans-
parent white box approach for prediction [61]. Decision trees, logistic regression, and linear regression are
commonly used interpretable models. These models have been used to explain predictions of specific pre-
diction problems [62]. Model-agnostic approaches, on the other hand, refer to the derivation of explanations
from a black-box predictor by extracting information about the underlying mechanisms of the system [63].

Model-agnostic models (post-hoc) are divided into two major approaches: partial dependency plots and
surrogate models. The partial dependency plots can only provide pairwise interpretability by computing
the marginal effect that one or two features have on the prediction. On the other hand, surrogate models

consist of training a new local model that approximates the predictions of a black-box. Model-agnostic
post-hoc methods have the flexibility of being applied to any predictive model compared to model-specific
post-hoc approaches. The two most widely cited post-hoc models in the literature include LIME [36] and
Kernel SHAP [37]. Counterfactuals can be generated using a post-hoc approach, and they can either be
model-agnostic or model-specific. The main focus of this literature review is on model-agnostic post-hoc
counterfactuals due to their flexibility and ability to work in any pre-existing trained model. This is detailed

in Section 5.

2.1. LIME - Local Interpretable Model-Agnostic Explanations

Local Interpretable Model-agnostic Explanations (LIME) [36] explains the predictions of any classifier by
approximating it with a locally faithful interpretable model. Hence, LIME generates local interpretations by
perturbing a sample around the input vector within a neighborhood of a local decision boundary [64, 36].
Each feature is associated with a weight computed using a similarity function that measures the distances
between the original instance prediction and the predictions of the sampled points in the local decision
boundary. An interpretable model, such as linear regression or a decision tree, can learn the local importance

of each feature. This translates into a mathematical optimization problem expressed as

explanation(x) = argmingecLlf,g, Mx) + O(g), (1)

where £ is the loss function which measures the similarity of the explainable model in the boundary of a

perturbed data point z, g(z), to the original black-box prediction, f(z):

LF. g.%x)= J) mee(2) (F(z) gle)”. (2)

Z,2'EZ
In Equations 1 and 2, x is the instance to be explained and f corresponds to the original predictive
black-box model (such as a neural network). G is a set of interpretable models, where g is an instance of that
model (for instance, linear regression or a decision tree). The proximity measure 7, defines how large the
neighborhood around instance x is that we consider for the explanation. Finally, Q(g) corresponds to the
model complexity, that is, the number of features to be taken into account for the explanation (controlled by

the user) [61].

2.2. Approaches Based on LIME

LIME has been extensively applied in the literature. For instance, Stiffler et al. [65] used LIME to generate
salience maps of a certain region showing which parts of the image affect how the black-box model reaches a
classification for a given test image [66, 67]. Tan et al. [68] applied LIME to demonstrate the presence of
three sources of uncertainty: randomness in the sampling procedure, variation with sampling proximity,

and variation in the explained model across different data points.
In terms of image data, the explanations are produced by creating a set of perturbed instances by dividing
the input image into interpretable components (contiguous superpixels) and runs each perturbed instance
via the model to get a probability Badhrinarayan et al. [69]. After that, a simple linear model learns on this
data set, which is locally weighted. At the end of the process, LIME presents the superpixels with the highest
positive weights as an explanation. Preece [70] proposed a CNN-based classifier, a LIME-based saliency
map generator, and an R-CNN-based object detector to enable rapid prototyping and experimentation to
integrate multiple classifications for interpretation.

Other researchers propose extensions to LIME. Turner [71] derived a scoring system for searching the
best explanation based on formal requirements using Monte Carlo algorithms. They considered that the
explanations are simple logical statements, such as decision rules. [72] utilized a surrogate model to extract
a decision tree that represents the model behavior. [73] proposed an approach for building TreeView
visualizations using a surrogate model. LIME has also been used to investigate the quality of predictive
systems in predictive process analytics [74]. In Sindhgatta et al. [75] the authors found that predictive
process mining models suffered from different biases, including data leakage. They revealed that LIME
could be used as a tool to debug black box models.

Lastly, a rule-based approach extension for LIME is Anchor [76]. Anchor attempts to address some of the
limitations by maximizing the likelihood of how a certain feature might contribute to a prediction. Anchor
introduces IF-THEN rules as explanations and the notion of coverage, which allows the decision-maker to

understand the boundaries in which the generated explanations are valid.

2.3. SHAP - SHapley Additive exPlanations

The SHAP (SHapley Additive exPlanations) is an explanation method that uses Shapley values [77]
from coalitional game theory to fairly distribute the gain among players, where contributions of players are
unequal [37]. Shapley values are a concept in economics and game theory and consist of a method to fairly
distribute the payout of a game among a set of players. One can map these game-theoretic concepts directly
to an XAI approach: a game is the prediction task for a single instance; the players are the feature values of
the instance that collaborate to receive the gain. This gain consists of the difference between the Shapley
value of the prediction and the average of the Shapley values of the predictions among the feature values of
the instance to be explained [78].

In SHAP, an explanation model, g(z’) is given by a linear combination of Shapley values #; of a feature j

with a coalitional vector, Zi, of maximum size M,

M
glz’)= bot) jz}. (3)
jl

Strumbelj and Kononenko [78] claim that in a coalition game, it is usually assumed that n players form
a grand coalition that has a certain value. Given that we know how much each smaller (subset) coalition
would have been worth, the goal is to distribute the value of the grand coalition among players fairly (that is,
each player should receive a fair share, taking into account all sub-coalitions). Lundberg and Lee [37] on the
other hand, present an explanation using SHAP values and the differences between them to estimate the
gains of each feature.

To fairly distribute the payoff amongst players in a collaborative game, SHAP makes use of four fairness
properties: (1) Additivity, which states that amounts must sum up to the final game result, (2) Symmetry,
which states that if one player contributes more to the game, (s)he cannot get less reward, (3) Efficiency,
which states that the prediction must be fairly attributed to the feature values, and (4) Dummy, which says

that a feature that does not contribute to the outcome should have a Shapley value of zero.

2.4, Approaches Based on SHAP

In terms of related literature, Miller Janny Ariza-Garzén and Segovia-Vargas [79] adopted SHAP values
to assess the logistic regression model and several machine learning algorithms for granting scores in P2P
(peer-to-peer) lending; the authors point out SHAP values can reflect dispersion, nonlinearity and structural
breaks in the relationships between each feature and the target variable. They concluded that SHAP could
provide accurate and transparent results on the credit scoring model. Parsa et al. [80] also highlight that
SHAP could bring insightful meanings to interpret prediction outcomes. For instance, one of the techniques
in the model, XGBoost, not only is capable of evaluating the global importance of the impacts of features on
the output of a model, but it can also extract complex and non-linear joint impacts of local features.

Recently, Wang et al. [81] proposed to generalise the notion of Shapley value axioms for directed acyclic
graphs. The new algorithm is called Shapley flow and relies on causal graphs in order to be able to compute
the flow of Shapley values that describe the internal mechanisms of the black-box.

In the following sections, we will expand the analysis of model-agnostic approaches for XAI by conducting

a systematic literature review on counterfactual and causability approaches for XAI.

3. Systematic Literature Review Towards Counterfactuals and Causability in XAI

The purpose of this systematic review paper is to investigate the theories, algorithms, and applications
that underpin XAI approaches that have the potential to achieve causability. This means that this paper
will survey the approaches in the extensive body of literature that are primarily based on causality and
counterfactuals to help researchers identify knowledge gaps in the area of interest by extracting and
analysing the existing approaches.

Our systematic literature review follows the Preferred Reporting Items for Systematic Reviews and

Meta-Analyses (PRISMA) framework as a standardized way of extracting and synthesizing information from

10
existing studies concerning a set of research questions. More specifically, we followed the PRISMA checklist!
with the study search process presented in the PRISMA flow diagram’.

Based on PRISMA, the procedure of systematic review can be separated into several steps: (1) definition
of the research questions; (2) description of the literature search process and strategy. Inspired in the recent
work of Teh et al. [82], we conducted a topic modeling analysis to refine the search results using the Latent
Dirichlet Allocation (LDA) algorithm together with an inclusion and exclusion criteria to assist with the
selection of relevant literature; (3) extraction of publication data (title, abstract, author keywords and year),
systemisation, and analysis of the relevant literature on counterfactuals and causality in XAJ; (4) Lastly, we

conducted identification of biases and limitations in our review process.

3.1. Research Questions

To help researchers identify knowledge gaps in the area of causality, causability, and counterfactuals in

XAL, we proposed the following research questions:

* RQI1: What are the main theoretical approaches for counterfactuals in XAI (Theory)?

* RQ2: What are the main algorithms in XAI that use counterfactuals as a means to promote under-

standable causal explanations (Algorithms)?

* RQ3: What are the sufficient and necessary conditions for a system to promote causability (Applica-

tions)?

* RQ4: What are the pressing challenges and research opportunities in XAI systems that promote

Causability?

3.2. Search Process

To address the proposed research questions, in this paper, we used three well-known Computer Science
academic databases: (1) Scopus, (2) IEEE Xplore, and (3) Web of Science (WoS). We considered these
databases because they have good coverage of works on artificial intelligence, and they provide APIs to
retrieve the required data with few restrictions. We used the following search query to retrieve academic

papers in artificial intelligence related to explainability or interpretability and causality or counterfactuals.

artificial AND intelligence ) AND ( xai OR explai* OR interpretab* ) AND ( caus* OR counterf*
& P P

 

lnttp: //www. prisma- statement .org/documents /PRISMA%202009%20checklist . pdf
2http://prisma- statement .org/ documents /PRISMA%202009%20£ low%20diagram. pdf

11
This query allowed us to extract bibliometric information from different databases, such as publication
titles, abstracts, keywords, year, etc. The initial search returned the following articles: IEEE Xplore (6878),
Scopus (116), WoS (126). We removed duplicate entries in these results as well as results that had missing
entries. In the end, we reduced our search process to IEEE Xplore (4712), Scopus (709), WoS (124). Our

strategy is summarised in the PRISMA flow diagram illustrated in Figure 3.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

=
5
g Records identified through 3 databases by a searching query with output Record the numbe ol term in selected
= 8120 (IEEE= 6878, Scopus =1116, Web of Science=126) pl
3 Term IEEE Scopus WoS
Causability oO 2 2
= Record after duplicates removed Causal 82 83 32
= 5 5545 (IEEE= 4712, Seopus= 709, Web of Science= 124) ICounterfactual| = 34 10 5
3 Interpret 40 78 1
= Explain 561 487 68
Be : , : Reasoning 50 55 18
a3 jopic modeling (LDA) =
e Classify the topic base on the title, abstract and keywords Bayesian 34 33 4
= }

 

 

 

 

Record after selected topic
3187 (IEEE= 2456, Scopus= 632, Web of Science = 99)

 

Record after filter non-relalive article individually by
searching the term
125 (IEEE=50, Scopus= 85, Web of Science = 30)

Sereening phase

 

 

 

Figure 3: PRISMA flow diagram search results.

To guarantee that the initial query retrieved publications that match this review’s scope, we conducted a

topic modelling analysis based on Latent Dirichlet Allocation (LDA) to refine our search results.

3.3. Topic Modelling

Topic modelling is a natural language processing technique that consists of uncovering a document
collection’s underlying semantic structure based on a hierarchical Bayesian analysis. LDA is an example
of a topic model used to classify text in a document to a particular topic. It builds a topic per document
model and words per topic model, modelled as Dirichlet distributions. In our search strategy, LDA enabled
us to cluster words in publications with a high likelihood of term co-occurrence and allowed us to interpret
the topics in each cluster. This process guaranteed that the papers classified within a topic contain all the
relevant keywords to address our research questions.

In this paper, we used the title, abstract, and authors’ keywords retrieved from the proposed query,
and applied several text mining techniques, such as stop word removal, word tokenisation, stemming, and
lemmatisation. We then analysed the term co-occurrences with LDA for each database. The best-performing
model contained a total of 4 topics. The LDA model’s output is illustrated in Figure 4 with the inter-topic
distance showing the marginal topic distributions (left) and the top 10 most relevant terms for each topic.
Analysing Figure 4, Topic 1 contained all the words that are of interest to the research questions proposed
in this survey paper: explainability, causality, and artificial intelligence. Topic 2, on the other hand, has

captured words that are primarily related to data management and technology. Topic 3 has words related to

12
the human aspect of explainable AI, such as cognition, mental, and human. Finally, Topic 4 contains words
associated with XAI in healthcare. For this survey paper, we chose all the publications classified as either
Topic 1 or Topic 3. In the end, we were able to reduce our search results to IEEE Xplore (3187), Scopus (632),
WoS (99). After manually looking at these publication records and selecting articles about ’causability”,

”causal”, “counterfactual”, we obtained our final set of documents for analysis: IEEE Xplore (125), Scopus

(85), WoS (30).

 

 

 

 

 

 

 

 

   

 

 

 

 

Topic: 1 soso Topic:2 0x0
Intertopic Distance Map (via Multidimensional Scaling) Word Count Te Weigns Word Count Cr
ans § ans
3
Lt dd dda tak
xs Ze» > > £ 2 ® & > x
~ EF SEEFESEEE FE EE ES EEE SE
; “ “ x Oo SYS < oO eo
‘ SS FN FY PS g oe € SLY SF
< s&s * eg é 3s A s es SNE
F & eg
3500 Topic: 3 0.030 3500 Topic: 4 0.030
: 3000 | ors 2009 0.025
ql 8 0.015
0.010 1000 ome
500 0.005 | | ]
SS x x
° o.000 SSF ESN FSS LG
ee ~ f SF x Ca
~ SF EFSF SPELEESP SEF ES EEE SE ®
* SSE EF FEE ESSE SE g Ss & vs
SP 7 S S ¥ Ss
10% BS) < ¢ RS
3 Ss &
Cc

Figure 4: Best performing LDA topic model for Scopus database, using 709 titles, abstracts, and authors keywords found from the
proposed search query. Figure also shows the top 10 most relevant words for each Topic.

3.4. Word Co-Occurrence Analysis

In our survey, we are interested in understanding the necessary and sufficient conditions to achieve
causability and how current approaches can promote it. We started to analyse the keyword co-occurrence
in the returned documents from our search query to achieve this understanding. We collected the title,
abstract, and authors’ keywords from the search results in Scopus, and filtered the results from using three
different keywords of interest: explainable AI, counterfactuals, and causality. This resulted in three different
Scopus files with the keywords of interest.

To visualise the results, we used the graphical capabilities of VOS Viewer?, which is a software tool for

constructing and visualising bibliometric networks.

 

3https://www.vosviewer.com/
Figure 5 represents the co-occurrence of authors’ keywords regarding the field of XAI. The density
plot reveals a shift in research paradigms evolving from machine-centric topics to more human-centric
approaches involving intelligent systems and cognitive systems, to the need of explainability in autonomous

decision-making.

decision theory
crime

data privacy isi
clinical decisionssupport systems

econamics
data integpretation
tomography
economic andisocial effects face recognition
risk assessment

radiology

ethics
case base@reasonidack- modebhformasion use qualityscontrol

data@ring .
ee 1 + i> we
artificial ier syste: S decision support systems SA 4 reproducibility
th A iL Sona

decisi in |
/ ecis aKING -atncare
intelligefitagents bh
7 é ~
expl.

I) - \
sor Seorig@ Shee
lity" S tem NN >< = x
y AN SS -
wransprencaerees rt “A I - 9 Ke. oo pattern: #@gognition agnosye masing
s 5 natural _

    

    
    
 
 
 

atural Processing.
— image int@fpretation
-— SSR \ ge inter
po a ‘ _human intelligence i
Wi philosophigal aspects \ - .
explanation Harman compider intéraction J y computer-aided. diagnosis
j y fa
~/ cognitivesystems
\ f diseases Lies
trust knowledge répresentation . meas Mii
understandability
virtualseality
knowledge management
cognitive science visualization Compute vision

cognition image classification
object detection 2012 2014 2016 2018 2020
cognitive psychology

Figure 5: Network visualization of co-occurrence between keywords in articles about XAI.

It is interesting to note that machine-centric research interests (such as pattern recognition or computer-
aided diagnostic systems) started to change around 2016. The European Union Commission started to
put forward a long list of regulations for handling consumer data, the GDPR. In that year, publications
start shifting their focus from fully autonomous systems to a human-centric view of learning systems with
a need for interpretability in decision-making. Figure 5 also shows another shift of research paradigms
around 2018 towards explainable AI, which coincides with the year where GDPR was put into effect in the
European Union, imposing new privacy and security standards regarding data access and usage. One of
these standards is Article 22, which states that an individual has ’the right not to be subject to a decision
based solely on automated processing”*. In other words, an individual has the right for explainability
whenever a decision is computed from an autonomous intelligent system. Given that these systems are

highly opaque with complex internal mechanisms, there has been a recent growing need for transparent and

 

“https: //www.privacy-regulation.eu/en/article-22-automated- individual -decision-making-including-profiling-GDPR.
htm

14
interpretable systems that are able to secure ethics and promote user understandability and trust.

laws and KX“ n

machine le els - - /
Sess global burden of disease
- cl terfact explanation ~—
- ~~ gl ealth

/\\

major clinical study
NN

  
 
    
  

ble ai comparative study 4 life expectancy

Vf Ty - socioeconomic factors
A Tay yt
a I \ : “as
geen) I i arning 4 ee A
| outcome asse: a)

i iofraking y h S - mortality

stat . odel risk ass@ssment
x \; ==

decision tree: LA ; \} \ tisk @etors
Mint 8 TiL—Atit if \ fo. Way Zi ZE J
\e erie \ is Zz Z statistics andypumerical data.
leep g. — n\ ‘tual Z. <A Ly
L RSX s ] / ~ AN} J
t + _

   
   

. << eo ‘ Z A
predict sas J ‘ing bayes m
lackiboxe: f — oe \ Hix
artificial lligence _ » clinical-trial epidemiolagy
inference’engines causal inference
explafiation NN
causation
statisti€ab analysis
——=ELE___
2012-2014 = 2016. = 2018 2020 treatmentjoutcome

Figure 6: Network visualization of co-occurrence between keywords in articles about counterfactuals in XAI.

Some researchers argued that for a machine to achieve a certain degree of human intelligence, and
consequently, explainability, then counterfactuals need to be considered [22, 83]. Recently, Miller [24]
stated that explanations need to be counterfactuals (“contrary-to-fact”) [84], since they enable mental
representations of an event that happened and also representations of some other event alternative to
it [58]. Counterfactuals describe events or states of the world that did not occur and implicitly or explicitly
contradict factual world knowledge. For instance, in cognitive science, counterfactual reasoning is a crucial
tool for children to learn about the world [85]. The process of imagining a hypothetical scenario of an event
that is contrary to an event that happened and reasoning about its consequences is defined as counterfactual
reasoning [86]. We investigated the word co-occurrence in articles involving explainable AI and counterfactuals
to understand how the literature is progressing in this area. Figure 6 shows the obtained results.

In the density plot in Figure 6, one can see that counterfactual research in XAI is a topic that has gained
interest in the scientific community very recently, with most of the scientific papers dating from 2019
on-wards. This reflects the need for supporting explanations with contrastive effects: by asking ourselves
what would have been the effect of something if we had not taken action, or vice versa. Creating such
hypothetical worlds may increase the user’s understanding of how a system works. The figure seems to
be suggesting that the recent body of literature concerned with counterfactuals for XAI is motivated by
the medical decision-making domain since we can see relevant keywords such as patient treatment, domain
experts, and diagnosis. There is also a recent body of literature in clinical research supporting the usage of

counterfactuals and causality to provide interpretations and understandings for predictive systems [87].

15
philosophy

cognitivessystems

exelions

fo knowledgerepresentation

~ y TAS case basedireasoning
) P} . —
\ LY

J >< _\
NS —— fv _K j causal discovery causalifelations

= b. ee eG — interpretability
~ : Orie J =S<S ee
probability r ZS = ~ 7 ? i . SY
informations@rocessing PO a a VF sNS a alfmodel
; = - My Ah ; causa rence
F mac arning /

 
    

neural metworks

  
 

simulation

ra
uncertainty
observational data

visualization

2010 2012 2014 2016 2018

Figure 7: Network visualization of co-occurrence between keywords in articles about causality in XAI.

Some researchers also argued that for a machine to achieve a certain degree of human intelligence,
causality must be incorporated in the system [88]. Others support this idea in the context of XAI, where
they argue that one can only achieve a certain degree of explainability if there is a causal understanding of
the explanations, in order words, if the system promotes causability [22]. In this sense, we also analysed
co-occurrence between keywords in articles about causality in XAI. Figure 7 illustrates the results obtained.

In terms of causality, Figure 7, one can draw similar conclusions. Although the figure shows a clear
connection between Artificial Intelligence and causality (causal reasoning, causal graphs, causal relations),
the literature connecting causal relations to explainable Al is scarce. This opens new research opportunities
in the area, where we can see from Figure 7 a growing need for counterfactual research. Literature regarding
causability seems to be also very scarce and very recent. New approaches are needed in this direction, and it
is the purpose of this systematic review to understand which approaches for XAI are underpinned by causal

theories.

3.5. Inclusion and exclusion criteria

To select relevant literature from the obtained search results, we had to consider which papers should be
included in our analysis and which ones should be excluded in order to be able to address the proposed

research questions. Table 1 summarises the selected criteria.

16
 

Inclusion Criteria Exclusion Criteria

 

 

 

Papers about causality in XAI Papers about causal machine learning
Papers about counterfactuals in XAI | Papers about causality
Papers about causability Papers not in English

 

Papers about main algorithms in XAI | Papers without algorithms for XAI

 

 

 

 

Table 1: Inclusion and exclusion criteria to assess the eligibility of research papers to analyse in our systematic literature review.

3.6. Risk of bias

As with any human-driven task, the process of finding relevant research is affected by cognitive biases. In
this systematic review, we acknowledge that limiting our search to three databases (Scopes, Web of Science,
and IEEE) might have contributed to missing articles. Databases that could have complemented our search
could be Google Scholar, SpringerLink, and PubMed. Another consideration is that we did not extract the
references from the collected papers to enrich our search. The collection of retrieved documents was already
too big, and we found that doing this would exponentially increase the complexity of the LDA topic analysis
that we conducted. Finally, the search query was restricted to keywords relevant to collecting the papers
of interest. These keywords, however, might have limited our search, and we might have missed relevant

articles.

4. Counterfactual Approaches Explainable AI: Properties for Good Counterfactuals

The systematic review that we conducted allowed us to understand the different counterfactual ap-
proaches for XAI. As mentioned throughout this article, counterfactuals have been widely studied in
different domains, especially in philosophy, statistics, and cognitive science. Indeed, researchers are arguing
that counterfactuals are a crucial missing component that has the potential to provide a certain degree of
human intelligence and human-understandable explanations to the field of XAI [22]. Other researchers state
that counterfactuals are essential to elaborate predictions at the instance-level [89] and to make decisions
actionable [90]. Other researchers claim that counterfactuals can satisfy GDPR’s legal requirements for
explainability [55].

Most XAI algorithms attempt to achieve explainability by (1) perturbing a single data instance, generating
a set of perturbed data points around the decision boundary, (2) passing these perturbed instances through
the black box, generating labels to these data points, and by (3) fitting an interpretable model (such as
linear regression or a decision tree) to the perturbed data points [36]. Counterfactuals are classified as
example-based approaches for XAI [61]. They are based on approaches that compute which changes should
be made to the instance datapoint to flip its prediction to the desired outcome [55]. Figure 8 shows an
illustration of several counterfactual candidates for a data instance x according to different works in the

literature [56].

17
Change to desired

 

 

 

 

 

outcome
ae
Y 0.75
a
Black-Box 9.50 3
a
| 0.25
xX xX’
—
Min feature
change 0.00

 

Figure 8: Different counterfactual candidates for data instance x. According to many researchers, counterfactual a is the best candidate,
because it has the smallest Euclidean distance to x [55]. Other researchers argue that counterfactual instance y is the best choice since
it provides a feasible path from x to y [56]. Counterfactual f is another candidate of poor quality because it rests in a less defined
region of the decision boundary.

4.1. The Importance of Distance Functions in Counterfactual Approaches for XAI

The definition of counterfactual as the minimum distance (or change) between a data instance and a
counterfactual instance goes back to the theory proposed by Lewis [91]. Given a data point x, the closest
counterfactual x’ can be found by solving the problem where d(.,.) is a measurement for calculating the

distance from the initial point to the generated point.

argminy: d(x, x’) (4)

One important question that derives from Equation 4 is what kind of distance function should be used?
Different works in the literature address this optimization problem by exploring different distance functions
and L,-norms. This section will review different norms used as distance functions in the literature of XAI
and their properties.

In general, a norm measures the size of a vector, but it can also give rise to distance functions. The

L,-norm of a vector x is defined as:
l/p

n
lll =| )_ bal? (5)
i=l

Equation 5 shows that different values of p yields a different distance function with specific properties.
The systematic literature review revealed that most works in XAI used either the Lp-norm (which is not a
norm by definition), the L;-norm (also known as Manhattan distance), the L>-norm (known as the Euclidean
distance, and the L,,-norm. Figure 9 shows a graphical representation of the different norms and the

respective contours.

18
LO - Norm L1-Norm L2- Norm Leo - Norm

a2
wwe

Figure 9: Graphical visualisation of different Lp-norms: Lo-norms (which is not really a norm by definition), the L)-norm (also known
as Manhattan distance), the L7-norm (known as the Euclidean distance, and the L,,-norm.

    

 

* Lo-norm. The Lo-norm has been explored in the context of counterfactuals in XAI primarily by Dandl

et al. [92] and Karimi et al. [93]. Given a vector x, it is defined as

 

Intuitively, Lp-norm is the number of nonzero elements in a vector, and it is used to count the number
of features that change between the initial instance x and the counterfactual candidate x’, resulting in
sparse counterfactual candidates [93]. Figure 9 shows a visualisation of the Ly-norm where one can
see that the function is completely undifferentiable, making it very hard to find efficient solutions to
minimize it.

« L,-norm. The L)-norm (also known as the Manhattan distance) has been the most explored distance
function in the literature of counterfactuals in XAI. Sandra et al. [55] argued that the L; -norm provides

the best results for finding good counterfactuals, since it induces sparse solutions. Given a vector x,

Ikxlly = Debi. (7)

Intuitively, Lj-norm is used to restrict the average change in the distance between the initial instance x

the L,-norm is defined as

and the counterfactual candidate x’. Since the L;-norm gives an equal penalty to all parameters and
leads to solutions with more large residuals, it enforces sparsity. In Figure 9, one can see that the major

problem with L,-norms is its diamond shape, which makes it hard to differentiate.

« L>-norm. The L,-norm (also known as the Euclidean distance) has been one of the most explored

distance functions in the literature of counterfactuals in XAJ, although it does not provide sparse

19
solutions when compared with the L; or Lo-norm. Given a vector x, the L;-norm is defined as

 

Intuitively, the L2-norm measures the shortest distance between two points and can detect a much
larger error than the L,-norm, making it more sensitive to outliers. Although the L,-norm does not
lead to sparse vectors, it has the advantage that it is differentiable. Figure 9 shows that smoothness and
rotational invariance (a circle or a hyper-sphere in higher dimensions) are both desirable properties in

many optimization problems, making it computationally efficient.

* L,>norm. The L,,-norm has been explored in the context of counterfactuals in XAI primarily by

Karimi et al. [93]. Given a vector x, it is defined as

 

Intuitively, L,,-norm is used to restrict maximum change across features. The maximum change across
the features between the initial instance x and the counterfactual candidate x’ [93]. Computationally,
the L,,-norm is differentiable in every point, except when at least two features x; have the same absolute
values |x;|, which is illustrated in Figure 9. By minimizing the L-infinity norm, we are penalizing the

cost of the largest feature, leading to less sparse solutions compared to Lo-norm or L;-norm.

Distance functions in counterfactuals are associated with the sparsity of the vector, which is a highly
desirable property to have when looking for counterfactuals. The minimum the changes we can have in the
features, the better and more human interpretable counterfactuals we will find. The following section will

present the main properties that a theory for counterfactuals in XAI should satisfy.

4.2. Properties to Generate Good Counterfactuals

Literature suggests a set of properties that need to be satisfied in order to generate a good (interpretable)

counterfactual:

* Proximity. Proximity calculates the distance of a counterfactual from the input data point while
generating a counterfactual explanation, [57]. As mentioned in Section 4.1, many different distance
functions can be used to measure proximity, resulting in counterfactual candidates with different
properties. Other works in the literature consider another types of proximity measures such as Nearest

Neighbour Search [94], cosine similarity [95], or even learning to rank techniques [13, 12].

20
* Plausibility. This property is similar to the terms Actionability and Reasonability referred in [96, 57].
It emphasizes that the generated counterfactuals should be legitimate, and the search process should
ensure logically reasonable results. This means that a desirable counterfactual should never change
immutable features such as gender or race. When explaining a counterfactual, one cannot have
explanations like ”if you were a man, then you would be granted a loan”, since these would show an
inherent bias in the explanation. Mutable features, such as income, should be changed instead to find

good counterfactuals.

Sparsity. This property is related to the methods used to efficiently find the minimum features that

need to be changed to obtain a counterfactual [96].

In cognitive science, counterfactuals are used as a process of imagining a hypothetical scenario contrary
to an event that happened and reasoning about its consequences [86]. It is desired that counterfactuals
are sparse, i.e., with the fewest possible changes in their features. This property leads to more effective,
human-understandable, and interpretable counterfactuals. In Mothilal et al. [50], for instance, the
authors elaborate that sparsity is assessing how many features a user needs to change to transition
to the counterfactual class. On the other hand, Verma et al. [57] argues that sparsity can be seen
as a trade-off between the number of features and the total amount of change made to obtain the
counterfactual. Sandra et al. [55] also stands on this idea and asserts that pursuing the “closest possible
world”, or the smallest (minimum-sized) change to the world that can be made to obtain a desirable

outcome.

Recently, Pawelczyk et al. [97] proposed a theoretical framework that challenges the notion that
counterfactual recommendations should be sparse. The authors argue the problem of predictive
multiplicity can result in situations where there does not exist one superior solution to a prediction

problem with respect to a measure of interest (e.g. error rate).

Diversity. This property was introduced in the work of Russell [98] and also explored in Mothilal et al.
[50], Karimi et al. [59]. Finding the closest points of an instance x according to a distance function can
lead to very similar counterfactual candidates with small differences between them. Diversity was
introduced as the process of generating a set of diverse counterfactual explanations for the same data
instance x [93]. This leads to explanations that are more interpretable and more understandable to the

user.

Feasibility. This property was introduced by Poyiadzi et al. [56] as an answer to the argument that
finding the closest counterfactual to a data instance does not necessarily lead to a feasible change in the
features. In Figure 8, one can see different counterfactual candidates. The closest counterfactual to the

data instance x is a. However, this point falls in the decision boundary. Thus, the black-box is not very

21
certain about its class, which could lead to biased counterfactual explanations. To address this problem,
Poyiadzi et al. [56] argues that counterfactual y is a better one because it falls in a well-defined region
of the decision boundary and also corresponds to the point that has the shortest path to x. This way, it

is possible to generate human-interpretable counterfactuals with the least possible feature changes.

From the definitions of plausibility and feasibility, one can conclude that they are related to each other:
for a counterfactual to be feasible, it needs first to be plausible. Plausibility refers to a property that ensures
that generated counterfactuals are legitimate. This means that a legitimate counterfactual should never
change immutable features such as gender or race. On the other hand, feasibility is related to the search of a
counterfactual that does not lead to ”paradoxical interpretation”. Using the example from Poyiadzi et al.
[56], low-skilled unsuccessful mortgage applicants may be told to double their salary. This implies that they
need to increase their skill level first, which may lead to counterfactual explanations that are impractical
and, therefore, infeasible. Thus, satisfying feasibility automatically guarantees plausible counterfactuals,
promoting a higher level of interpretability of counterfactual explanations.

Given the above properties, in the following sections, we will classify the different algorithms found in

the literature by (1) their underlying theory (Section 5), and by (2) the above properties (Section 6).

5. Counterfactual Approaches in Explainable AI: The Theory

The systematic literature review contributed to developing a new taxonomy for the model-agnostic
counterfactual approaches for XAI. Throughout the review process, we noticed that many algorithms
derived from similar theoretical backgrounds. In total, we analysed 26 algorithms. We created a set
of six different categories representing the ”master theoretical algorithm”[99] from which each algorithm
derived. These categories are (1) instance-centric approaches, (2) constraint-centric approaches, (3) genetic-
centric approaches, (4) regression-centric approaches, (5) game theory-centric Approaches, (6) Case-Based
Reasoning Approaches, and (7) Network-Centric approaches. Figure 10 presents the proposed taxonomy as

well as the main algorithms that belong to each category.

* Instance-Centric. Correspond to all approaches that derive from the counterfactual formalism pro-
posed by Lewis [91] and Sandra et al. [55]. These approaches are based on random feature permutations
and consist of finding counterfactuals close to the original instance by some distance function. Instance-
centric algorithms seek novel loss functions and optimization algorithms to find counterfactuals. Thus,
they are more susceptible to fail the plausibility, the feasibility, and the diversity properties, although
some instance-centric algorithms incorporate mechanisms in their loss functions to overcome these

issues.

22
CBR for Good Counterfactuals Approaches

CRUDs Watcher CF

 
   
 
   
    

 
  
   
    

Prototype Counterfactuals
Instance-Centric
Approaches

PRINCE Probabilistic-Centric

Approaches Weighted Counterfactuals

    
   
   
 
  
    
  

RECOURSE

C-CHVAE FACE

CERTIFAI

     
   

Genetic-Centric
Approaches

Constraint-Centric
Approaches

   

Model-Agnostic
Counterfactuals in XAl

 
 

Coherent Counterfactuals MOCE

   
  
  
   
 
   
 

Regression-Centric
Approaches

  
 

Game Theory Centric
Approaches

  

Contrastive and
Counterfactual SHAP

Case Based Reasoning

Figure 10: Proposed Taxonomy for model-agnostic counterfactual approaches for XAI.

* Constraint-Centric. Corresponds to all approaches that are modeled as a constraint satisfaction prob-
lem. Algorithms that fall in this category use different strategies to model the constraint satisfaction
problem, such as satisfiability modulo theory solvers. The major advantage of these approaches is
that they are general and can easily satisfy different counterfactuals properties such as diversity and

plausibility.

* Genetic-Centric. Corresponds to all approaches that use genetic algorithms as an optimization method
to search for counterfactuals. Since genetic search allows feature vectors to crossover and mutate, these

approaches often satisfy properties such as diversity, plausibility, and feasibility.

* Regression-Centric. Corresponds to all approaches that generate explanations by using the weights of
a regression model. These approaches are very similar to LIME. The intuition is that an interpretable
model (in this case, linear regression) fits the newly generated data after permuting the features, and
the weights of each feature presented explanations. Counterfactuals based on these approaches have

difficulties satisfying several properties such as plausibility and diversity.

* Game Theory Centric. Corresponds to all approaches that generate explanations by using Shapley
values. These approaches are very similar to SHAP. Algorithms that fall in this approach mainly
extend the SHAP algorithm to take into consideration counterfactuals. Counterfactuals based on these

approaches have difficulties satisfying several properties such as plausibility and diversity.

23
* Case-Based Reasoning. Corresponds to all approaches inspired in the case-based reasoning paradigm
of artificial intelligence and cognitive science that models the reasoning process as primarily memory-
based. These approaches often solve new problems by retrieving stored cases describing similar prior
problem-solving episodes and adapting their solutions to fit new needs. In this case, the CBR system
stores good counterfactual explanations. The counterfactual search process consists of retrieving from
this database the closest counterfactuals to a given query. CBR approaches can easily satisfy different

counterfactual properties such as plausibility, feasibility, and diversity.

Probabilistic-Centric. Corresponds to approaches that model the counterfactual generation prob-
lem as a probabilistic problem. These approaches often consider random walks, Markov sampling,
variational autoencoders to learn efficient data codings in an unsupervised manner, or probabilistic
graphical models (PGMs). Approaches based on PGMs have the potential to satisfy the causality
framework proposed by Pearl [44].

6. Counterfactual Approaches to Explainable AI: The Algorithms

In this systematic review, we found 18 model-agnostic XAI counterfactual algorithms. We analyzed
each algorithm in-depth and classified them according to the different properties presented in Section 4.
We also classified them in terms of their applications, either for classification/regression problems and the
supporting data structures. We complemented the analysis with the information of whether the algorithm is
publicly available. Table 2 presents a classification of collected model-agnostic counterfactual algorithms
for XAI based on different properties, theoretical backgrounds, and applications.

In the following sections, each algorithm of Table 2 is analysed relatively to its grounding theoretical

master algorithm.

6.1. Instance-Centric Algorithms
In this section, we summarize the algorithms that we classified as instance-centric using the proposed
taxonomy. By definition, these algorithms are very similar, diverging primarily on the loss function

description with the corresponding optimization algorithm and the distance function specification.

* WatcherCF by Sandra et al. [55]. WatcherCF correspond to one of the first algorithms in model-
agnostic counterfactuals for XAI. They extend the notion of a minimum distance between datapoints
that was proposed initially by Lewis [91]. The goal is to find a counterfactual x’ as close as possible to

the original point x; as possible such that a new target y’ (the counterfactual) is found.

24
- Loss function. The loss function takes as input the data instance to be explained, x, the counter-
factual candidate, x’, and a parameter A, that balances the distance in the prediction (first term)
against the distance in feature values (second term) [61]. The higher the value of A, the closer the
counterfactual candidate, x’, is to the desired outcome, y’. Equation 10 presents the loss function
and respective optimization problem proposed by Sandra et al. [55]. The authors argue that the
type of optimiser is relatively unimportant since most optimisers used to train classifiers work in

this approach.

£(x,',9",A) = MF lx) —y'P + dlxx’) (10)
arg min maxL(x,x',y’,A)
x

- Distance function. Although the choice of optimiser does not impact the search for counterfactu-
als, the choice of the distance function does. Sandra et al. [55] argue that the L,-norm normalized
by the inverse of the median absolute deviation of feature j over the dataset is one of the best
performing distance functions because it ensures the sparsity of the counterfactual candidates.
Equation 11 presents the distance function used in their loss function.

P __y
Ix; x;

MAD,’ where

d(x,x’) =

f (11)

- Optimization algorithm: The Adam Gradient descent algorithm is used to minimize Equation 10.

* Prototype Counterfactuals by Looveren and Klaise [100]. Prototype Guided Explanations consist of
adding a prototype loss term in the objective result to generate more interpretable counterfactuals.
The authors performed experiments with two types of prototypes: an encoder or k-d trees, which

resulted in a significant speed-up in the counterfactual search ad generation process [100].

- Loss function: The loss function consists in two different steps: (1) guide the perturbations 6
towards an interpretable counterfactual x,¢ which falls in the distribution of counterfactual class

i, and (2) accelerate the counterfactual searching process. This is achieved through Equation 12,
Loss = C.Lyred + B-Ly +L + Lag + Lyroto- (12)

The Lyreq measures the divergence between the class prediction probabilities, L; and Lz cor-
respond to the elastic net regularizer, L4; represents an autoencoder loss term that penalizes

out-of-distribution counterfactual candidate instances (which can lead to uninterpretable coun-

25
terfactuals). Finally, Lyrozo is used to speed up the search process by guiding the counterfactual
candidate instances towards an interpretable solution.

— Distance function. Looveren and Klaise [100] use the L-norm to find the closest encoding of
perturbed instances, ENC(x + 6) of a data instance, x, to its prototype class, proto;. This is given

by Equation 13.

Lproto = 9 |IEN C(x + 5) — proto;|l5 (13)

— Optimization function: Looveren and Klaise [100] adopted a fast integrative threshold algorithm
(FISTA) which helps the perturbation parameter 6 to reach momentum for N optimization steps.

The L1 regularization has been used in the optimization function.

* Weighted Counterfactuals by Grath et al. [101]. Weighted counterfactuals extend WatcherCF ap-
proach in two dimensions by proposing: (1) the concepts of positive and weighted counterfactuals,
and (2) two weighting strategies to generate more interpretable counterfactual, one based on global

feature importance, the other based on nearest neighbours.
Traditional counterfactuals address the question why my load was not granted? through a hypothetical

what-if scenario. On the other hand, when the desired outcome is reached, positive counterfactuals

address the question how much was I accepted a loan by?

- Loss function. The weighted counterfactuals are computed in the same way as the WatcherCF [55]
as expressed in Equation 10.
— Distance function. The distance function used to compute weighted counterfactuals is the same

as in WatcherCF [55], with the addition of a weighting parameter 0;,

 

yy Pol
dxx)=) TaD, 0}. (14)
j=
— Optimization algorithm. While Sandra et al. [55] used gradient descent to minimize the loss
function, Grath et al. [101] used the Nelder-Mead algorithm, which was initially suggested in the
book of Molnar [61] and is used to find the minimum of a function in a multidimensional space.
The Nelder-Mead algorithm is a better algorithm to deal with the L,-norm since it works well

with nonlinear optimization problems for which derivatives may not be known.

Experiments conducted by Grath et al. [101] showed that weights generated from feature impor-
tance lead to more compact counterfactuals and consequently offered more human-understandable

interpretable features than the ones generated by nearest neighbours.

26
* Feasible and Actionable Counterfactual Explanations (FACE) by Poyiadzi et al. [56].

FACE aims to build coherent and feasible counterfactuals by using the shortest path distances defined
via density-weighted metrics. This approach allows the user to impose additional feasibility and
classifier confidence constraints naturally and intuitively. Moreover, FACE uses Dijkstra’s algorithm to

find the shortest path between existing training datapoints and the data instance to be explained [57].

Under this approach, feasibility refers to the search of a counterfactual that does not lead to paradoxical
interpretations. For instance, low-skilled unsuccessful mortgage applicants may be told to double their
salary, which may be hard without first increasing their skill level. This may render to counterfactual

explanations that are impractical and sometimes outright offensive [56].

- Main Function. The primary function of FACE’s algorithm is given by Equation 15, where f
corresponds to a positive scalar function and y is a function that connects the path between a

data instance x; and a counterfactual candidate instance x;.

ee 218) yi 4) yfwher
Dy y= [ro fy ‘(t)|dt.

(15)

When the partition Dry converges, Poyiadzi et al. [56] suggest, for a given threshold e, using

weights of the form

Xp Xj

 

w= f(b

when |x: — x;|| <e,

(16)

The f-distance function is used to quantify the trade-off between the path length and the density
in the path. This can subsequently be minimized using Dijkstra’s shortest path algorithm by

approximating the f-distance using a finite graph over the data set.

— Distance Function. Poyiadzi et al. [56] used the L2-norm in addition to Dijkstra’s algorithm to

generate the shortest path between a data instance x; and a counterfactual candidate instance xj.

- Optimization Function. Poyiadzi et al. [56] suggested three approaches that can be used to

27
estimate the weights in Equation 16:

Xp +X;

“Al 5) Ips
w= ball m= an
ol No

0 -f(aqh hl

The first equation requires using Kernel Density Estimators to allow convergence, the second

 

requires a k-NN graph construct, and the third equation requires e-graphs. In their experi-
ments, Poyiadzi et al. [56] found that the third weight equation together with e-graphs generated

the most feasible counterfactuals.

* Diverse Counterfactual Explanations (DiCE) by Mothilal et al. [50]. DICE is an extension and
improvement of the WatcherCF [55] throughout different properties: Diversity, proximity, and sparsity.
DiCE generates a set of diverse counterfactual explanations for the same data instance x, allowing the
user to choose counterfactuals that are more understandable and interpretable. Diversity is formalized
as a determinant point process, which is based on the determinant of the matrix containing information

about the distances between a counterfactual candidate instance and the data instance to be explained.

- Loss Function. In DiCE, the loss function is presented in Equation 18, and is given by a linear
combination of three components: (1) a hinge loss function that is a metric that minimizes the dis-
tance between the user prediction f(.)’ for c;s and an ideal outcome y, loss(f(c;), y), (2) a proximity

factor, which is given by a distance function, and (3) a diversity factor dpp_diversity(cy, ..., ¢,).

C(x x) =argming LY prosstf fej), ay ALY ai Cj,X (18)
i=l

—A, ddp_diversity(cy,...,¢x)

— Distance Function. DiCE uses the L;-norm normalized by the inverse of the median absolute

deviation of feature j over the dataset just like in b-counterfactual [55].

- Optimization Function. Gradient descent is used to minimize Equation 18.

* Unjustified Counterfactual Explanations (TRUCE) by [102, 103, 104] TRUCE consists in approach-
ing the problem of determining the minimal changes to alter a prediction by proposing an inverse
classification approach [102]. The authors present the Growing Spheres algorithm, which consists of
identifying a close neighbour classified differently through the specification of sparsity constraints

that define the notion of closeness.

28
— Loss Function. To simplify the process for reaching the closest desirable feature, [103] presented
a formalization for binary classification by finding an observation value e, then classified it into
a different class other than x. For instance, f(e) # f(x), indicates that the observation has been
classified into the same class as x, and a desirable feature has been found if it is classified to the
other class. For the next step, a function has been defined c : X x X > R* such that c(x, e) is the

cost of moving from observation x to enemy e.

e =argminic(x,e) | f(e)# f(x)} with
ecx (19)
e(x,e) = [|x — elly + 7 [Ix — ello

— Distance Function. Equation 19 consists in the minimisation of a cost function under the
constraint that the observation e¢ is classified into the same class as x. This cost function is defined
as a weighted linear equation consisting of L2-norm and the Lo-norm between the observation e
and the class x. The L;-norm computes the proximity between x and e, while the Lo-norm is used

as a weighted average to guarantee that the explanation is human-interpretable.

— Optimization Function. The authors used the proposed growing sphere algorithm to handle as
the optimiser. The algorithm applies a greedy method to find the closest feature in all possible
directions until the decision boundary is reached. This means that the L)-norm was successfully
minimized. The minimum feature change is also addressed through the minimisation of the
Lo-norm.

In a later work, the authors proposed to distinguish between justified and unjustified counter-
factual explanations [103, 104]. In this sense, unjustified explanations refer to counterfactuals
that result from artifacts learned by an interpretable post-hoc model and do not represent the
ground truth data. On the other hand, justified explanations refer to counterfactual explanations

according to the ground truth data.

6.2. Constraint-Centric Approaches

In this section, we summarize the algorithms that we classified as constraint centric using the proposed

taxonomy.

* Model-Agnostic Counterfactual Explanations for Consequential Decisions (MACE) by Karimi
et al. [93] MACE maps the problem of counterfactual explanation search into a satisfiability modulo
theory (SMT) model. MACE receives as input a sequence of satisfiability problems expressing the pre-

dictive model, f, the distance function, d, and the constraint functions. The algorithm’s goal is to map

29
these sequences into logical formulae and verify if there is a counterfactual explanation that satisfies a
distance smaller than some given threshold. The constraints that are taken into consideration in this
approach are plausibility and diversity. This is achieved in the following way. Given the counterfactual
logical formula PCF p(x) the distance formula #4,¢, constraints formula @,, and a threshold e, they are

combined into the counterfactual formula, $¢5(x), given by

z(x) = Oce (a(x) A baz A Pg x (20)

and used as input for a SMT solver, SAT (¢,.5(x)), which will find counterfactuals that will satisfy the
conditions with a distance smaller than e. MACE is a general algorithm that supports any L)-norm as

a distance function, as well as any number of constraints.

MACE was able to not only achieve high plausibility (what the authors define as coverage) but was
also able to generate counterfactuals at more favorable distances than existing optimization-based

approaches [93].

Coherent Counterfactuals by Russell [98]

This approach focuses on generating diverse counterfactuals based on ”mixed polytope” methods to
handle complex data with a contiguous range or an additional set of discrete states. Russell [98] created
a novel set of criteria for generating diverse counterfactuals to integrate them with the mixed polytope
method and to map them back into the original space. Before achieving the two targets (coherence
and diversity), Russell [98] firstly offers a solution on generating a counterfactual which is similar to
Sandra et al. [55]’s counterfactuals by finding the minimum change in the counterfactual candidate
that would lead to a change in the prediction (using the L; — norm). Then, the author proposes the
mixed polytope as a novel set of constraints. The program uses an integer programming solver and

receives a set of constraints in order to find coherent and diverse counterfactuals efficiently.

Generalized Inverse Classification by Lash et al. [105]

Inverse classification is similar to the sub-discipline of sensitivity analysis, which examines the impact
of predictive algorithm input on the output. [105] proposed Inverse Classification framework, which
mainly focuses on optimizing the generation of counterfactuals through a process of perturbing an
instance. This task is achieved by operating on features and tracking each change that leads to an
individual cost. Every change in perturbing an instance is subject to happen within a certain level of

cumulative change.

To assess the capability of GIC, [105] applied five methods, including three heuristic-based methods to

solve the generalized inverse classification problem, a hill-climbing + local search (HC+LS), a genetic

30
algorithm (GA), and a genetic algorithm + local search (GA+LS). The other two methods are sensitivity
analysis-based methods such as Local Variable Perturbation—Best Improvement (LVP-BI) and Local
Variable Perturbation—First Improvement (LVP-FI). These five algorithms were tested to examine the
average likelihood of test instances conforming to a non-ideal class over varying budget constraints.
The final result showed that LVP-FI outperforms all other methods, while LVP-BI is comparable to GA
and GA+LS. HC+LS has the worst performs.

6.3. Genetic-Centric Approaches

In this section, we summarize the algorithms that we classified as genetic-centric using the proposed

taxonomy.

* Local Rule-Based Explanations of Black Box Decision Systems by Guidotti et al. [106]

This approach attempts to provide interpretable and faithful explanations by learning a local inter-
pretable predictor on a synthetic neighborhood generated by a genetic algorithm. Explanations are

generated by decision rules that derive from the underlying logic of the local interpretable predictor.

LORE works as follows. Given a black-box predictor and a local counterfactual instance, x, with
outcome y, an interpretable predictor is created by generating a balanced set of neighbor instances of
the given instance x using an ad-hoc genetic algorithm. The interpretable model used to fit the data

corresponds to a decision-tree from which sets of counterfactual rules can be extracted as explanations.

The distance function used in this algorithm is given by Equation 21. The fitness function used
corresponds to the distance of x to a generated counterfactual candidate z, d(x,z). This algorithm
also considers the mixed types of features by a weighted sum of the simple matching coefficient for
categorical features, and by using the L,-norm to normalize the continuous features. Assuming h

corresponds to categorical features and m—h to continuous ones, then the distance function is given by

 

d(x,z) = » SimpleMatch(x,z) + ” NormEuclid(x,z). (21)
m

Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Arti-

ficial Intelligence models (CERTIFAI) by [107]

CERTIFAI is a custom genetic algorithm based explanation with several strengths, including the
capability of evaluating the robustness of a machine learning model (CERScore) and assessing fairness
with linear and non-linear models and any input form (from mixed tabular data to image data) without

any approximations to or assumptions for the model.

Establishing a CERTIFAI framework comes with several steps that start by creating an original genetic

framework, selecting a distance function, and improving counterfactuals with constraints. A custom

31
genetic algorithm was made in the first stage by considering f as a classifier for a black-box model,
and an instance x as an input. In this formalist, consider c as the counterfactual candidate instance
of x and d(x,c) the distance between them. The distance function used is the L;-norm normalized by
the median absolute deviation (MAD), as proposed by Sandra et al. [55]. The goal is to minimize the

distance between x and c by applying a genetic algorithm.

Given a variable W, we define the space from which individuals can be generated, to ensure feasible
solutions. By taking n dimensions as input, multiple constraints need to be created to match with
continuous, categorical, and immutable features. For instance, W,, W>..., W, is defined for continuous
feature constraints as W; € Wimin,Wengx» 2d W; € W,, Wo,..., W; is for categorical variables. Finally, a

feature i for an input x should be mutated by setting W; = X;.

The robustness and fairness of the population of the generated counterfactuals are given by
E *
CERScore(model) = —[d(x,c’*)]. (22)
x

Fairness ensures that the solutions generated contain different counterfactuals with multiple values of

an unchangeable feature (e.g., gender, race).

Multi-Objective Counterfactual Explanations (MOCE) by [92]. This approach consists of a multi-
objective counterfactual explanation algorithm which translates the counterfactual search into a
multi-objective optimization problem based on genetic algorithms. This approach brings the benefit of
providing a diverse set of counterfactuals with a variety of trade-offs between the proposed objectives

and maintains diversity in feature space at the same time.

Dand] et al. [92] proposed a four-objective loss equation to generate an explanation:
Lae, y' X) = (04 (FM, 9"), 02(0,2"}05(%,2),04(x7,X"*))) (23)

In the proposed equation, the four objectives 0,to04 represent one of the four criteria: Objective 1,
01, focuses on generating the closest possible result from a prediction of counterfactual x’ to the
desired prediction y’. It minimizes the distance between f(x’) and y’, and calculates it through the
L,1-norm. Objective 2 states that the ideal counterfactual should be as similar as possible to instance x.
It quantifies the distance between x’ and x using Grower’s distance. Objective 3, 03 is used to calculate
the sparse feature changes through Lp-norm. This norm is necessary because Grower distance can
handle numerical and categorical features but cannot count how many features were changed. Finally,
Objective 4 states that the ideal counterfactual should have similar feature value combinations as the

original data point. The solution is to measure how “likely” a data point uses the training data, Xobs.

32
Dand1] et al. [92] used the Nondominated Sorting Genetic Algorithm or short NSGA-II, which is a
nature-inspired algorithm and applies Darwin’s law of the survival of the fittest and denote the fitness
of a counterfactual by its vector of objectives values (01, 02, 03,04), this solution helps to produce a

fitter counterfactual result by showing the lower counterfactuals four objectives.

6.4. Regression-Centric Approaches

In this section, we summarize the algorithms that we classified as regression-centric using the proposed

taxonomy:

* Counterfactual Local Explanations via Regression (CLEAR) by White and d’Avila Garcez [108]:
This method aims to provide a counterfactual explained by regression coefficients, including interac-
tion terms, and significantly improve the fidelity of the regression. [108] firstly generates boundary
counterfactual explanations, which state minimum changes necessary to flip a prediction’s classifica-
tion, then builds local regression models using the boundary counterfactual to measure and improve

the fidelity of its regressions. .

CLEAR proposes a counterfactual fidelity error, CFE , which is based on the concept of b-perturbation.
It compares each b-perturbation with an estimation of that value, estimatedb-perturbation, which is

calculated by a local regression. CFE is given by

CFE = |estimated b— perturbation — b—perturbation|. (24)

The generation of counterfactuals in CLEAR has the following steps: (1) Determine x’s boundary
perturbations; (2) Generate labeled synthetic observations; (3) Create a balanced neighborhood dataset;
(4) Perform a stepwise regression on the neighborhood dataset, under the constraint that the regression
curve should go through x; (5) Estimate the b-perturbations; (6) Measure the fidelity of the regression

coefficients; (7) Iterate until the best explanation is found.

[108] compared the performance in terms of fidelity of CLEAR and LIME. The result showed that

regressions have significantly higher fidelity than LIME in five case studies.

Local Interpretable Model-Agnostic Explanations-Counterfactual (LIME)-C by Ramon et al. [109].
LIME-C is a hybrid solution which connects additive feature attribution explanations (like in LIME)
with counterfactuals. The motivation for this hybrid solution starts from an assumption which states
that if the importance-rankings of the features are sufficiently accurate, it may be possible to compute

counterfactuals from them.

Additive feature attribution methods use an explanation model g as an interpretable approximation of

33
the trained classification model C, which can be represented by a linear model [109]:
Ht
glx’)= hot) o)x) (25)
j=l

In Equation 25, x; € 0,1 is the binary representation of xj (where x; is 1, if x; is non-zero, else it equals
0), mis the number of features of instance x, and 9, @; € R. To generate a ranked list of counterfactual,

the authors used the linear algorithm for finding counterfactuals (SEDC) proposed by [95].

Ramon et al. [109] points out that this method is stable and effective for all data and models, and even
for very large data instances that require many features to be removed for a predicted class change,

LIME-C computes counterfactuals relatively fast.

Search for Explanations for Document Classification (SEDC) by Martens and Provost [95]

This approach was proposed in the domain of information retrieval back in 2014. It consists of
generating explanations for the user’s understanding of the system and also for model inspection.
SEDC was one of the first works that used Lewis [91] definition of counterfactual in an algorithm that
provides minimal sets of words as explanations, such as removing all words within this set from the

document that changes the predicted class from the class of interest.

SEDC outputs minimum-size explanations for linear models by ranking all words appearing in the
document through the product f;x,,, where f; is the linear model coefficient. The explanation with
the top-ranked words is an explanation of the smallest size, and therefore a counterfactual. Cosine-
similarity is used to measure the proximity between a document and the counterfactual document

candidate.

6.5. Game Theory Centric Approaches

In this section, we summarize the algorithms that we classified as Game Theory Centric using the

proposed taxonomy.

* SHAP Counterfactual (SHAP-C) by [109]
SHAP-C is a hybrid algorithm that combines Kernel SHAP [37] with SEDC [95].

SHAP-C works as follows. Given a data point and a black-box predictive model, first, we compute
the respective Shapley values using kernel SHAP. The algorithm ranks the most important features
by their respective SHAP values and adds these features to a set called the Evidence Counterfactual.
The algorithm then proceeds similarly as in SEDC: the most important features are perturbed so
that a minimum set of perturbations is found to flip the prediction of the datapoint. This evidence

counterfactual is returned as the explanation.

34
* Contrastive and Counterfactual SHAP (SHAP-CC) by [110].

SHAP-CC attempts to generate partial post-hoc contrastive explanations with a corresponding coun-
terfactual. Rathi [110] used a P-contrastive methodology for generating contrastive explanations that
would allow the user to understand why a particular feature is important, and why another specific

feature is not.

The main idea of this explanation is considered a P-contrast question which is equivalent to the format
"Why [predicted-class]” instead of [desired class ]?. To answer these questions, Rathi [110] explanation
computed the Shapely values for each of the possible target classes where a negative Shapely value
indicates the features that have negatively contributed to the specific class classification. Rathi [110]
generate a “Why P not Q” explanation by breaking it down into two questions: ’why P?” and ”Why not
Q.” The positive and negative Shapley values are given as an answer to these questions, respectively.
Furthermore, a contrastive and counterfactual explanation is given in terms of natural language to the

user.

6.6. Case-Based Reasoning Approaches
In this section, we summarize the algorithms that we classified as Case-Based Reasoning using the

proposed taxonomy.

* Case-based reasoning (CBR) for Good counterfactual by [96]

CBR for good counterfactuals uses a case-based system where examples of good counterfactuals are
stored. By good counterfactuals, Keane and Smyth [96] understands as counterfactuals that are sparse,
plausible, diverse, and feasible. The authors also introduce the explanatory coverage and counterfactual

potential as properties of CBR systems that can promote good counterfactuals.

In their algorithm, Keane and Smyth [96] refer to the pairing of a case and its corresponding good
counterfactual as an explanation case or XC. The goal is to generate good counterfactuals by retrieving,
reusing, and revising a nearby explanation case by taking the following steps: (1) identify the XC
that is most similar to the query datapoint p. In other words, XC corresponds to the explanatory
coverage between two data points, xc(x,x’); (2) for each of the features matched in xc(x, x’); we map
these features from p to the new generated counterfactual p’. In the same way, we add to p’ the
most different features in xc(x,x’). This procedure guarantees the diversity of the counterfactuals; (3)
through the definition of a counterfactual, p’ needs to provide a prediction contrary to the one by
p. This implies that p’ needs to go through the predictive black-box and returned to the user if the
prediction flips; (4) otherwise, an adaptation step to revise the values of the different features in p’ are

applied until there is a change of class.

35
6.7. Probabilistic-Centric Approaches

In this section, we summarize the algorithms that we classified as Probabilistic-Centric using the proposed
taxonomy:
* Provider-side Interpretability with Counterfactual Explanations in Recommender Systems (PRINCE)

proposed by Ghazimatin et al. [111]

This approach aims to detect the actual cause of a recommendation by a heterogeneous information
network (HIN) with users, items, reviews, and categories. It identifies a small set of a user’s actions by

removing actions that would lead to replacing the recommended item with a different item.

This approach provides user explanations by display what they can do to receive more relevant
recommendations to the users. Personalized PageRank(PPR) scores were chosen as the recommender
model to create a heterogeneous knowledge network. PRINCE is based on a polynomial-time algorithm
for searching the search space for subsets of user behaviors that could lead to a recommendation. By
adopting the reverse local push algorithm to a dynamic graph environment, the algorithm efficiently

computes PPR contributions for groups of actions about an object.

Experiments performed by Ghazimatin et al. [111] using data from Amazon and Goodreads showed
that simpler heuristics fail to find the best explanations. PRINCE, on the other hand, can guarantee

optimality, since it outperformed baselines in terms of interpretability in user studies.

Counterfactual Conditional Heterogeneous Autoencoder (C-CHVAE) by Pawelczyk et al. [112,
113]

C-CHVAE uses Variational Autoencoders (VAE) to search for faithful counterfactual, which consists
in counterfactuals that are not local outliers, and that are connected to regions with significant data
density (similar to the notion of feasibility introduced by Poyiadzi et al. [56]). Given the original
dataset, the data is converted into an encoding vector where the data is represented in a lower dimension,
and each dimension represents some learned probability distribution. It is this encoder that specifies
which low dimensional neighbourhood one should look for potential counterfactuals. The next steps
consist in perturbing the low dimensional data and pass it through a decoder, which will reconstruct
the lower dimensional potential counterfactuals into their original space. Finally, the new generated
counterfactuals are given to a pre-trained black-box in order to assess whether the prediction has been

altered.

Monte Carlo Bounds for Reasonable Predictions (MC-BRP) by Lucic et al. [114]. MC-BRP is an
algorithm that focuses on predictions with significant errors where Monte Carlo sampling methods
generate a set of permutations of an original data point instance that result in reasonable predictions.

Given a local instance, x;, a set of important features (i), a black-box model f,and an error threshold

36
e, MC-BRP uses Tuckey’s fence to determine outliers (predictions with high errors) for each feature of
the set of important features,

€ > Q3(E) + 1.5(Q3(E) — Q1(E)),

where Q;(E), Q3(E) are the first and third quartiles of the set of errors, E, of each feature, respectively.
Tukey’s fence will return a set of boundaries for which reasonable predictions would be expected for
each of the most important features. Using these boundaries, MC-BRP generates a set of permutations
using Monte Carlo sampling methods, which will be passed into the black-box f to obtain a new
prediction. Finally, a trend is computed based on Pearson correlation over the reasonable new
predictions. The reasonable bounds for each feature are recomputed and presented to the user ina

table.

Adversarial Black box Explainer generating Latent Exemplars (ABELE) by Guidotti et al. [115]

ABELE is a local model-agnostic explainer for image data that uses Aversarial AutoEncoderes (AAEs)

that aim at generating new counterfactuals that are highly similar to the training data.

ABELE generates counterfactuals in four steps: (1) by generating a neighborhood in the latent feature
space using the AAEs, (2) by learning a decision tree on the generated latent neighborhood by providing
local decision and counterfactuals rules, (3) by selecting and decoding exemplars and counter-examples

satisfying these rules, and (4) by extracting the saliency maps out of them.

Guidotti et al. [115] found that ABELE outperforms current state-of-the-art algorithms, such as LIME,

in terms of coherence, stability, and fidelity.

CRUDS: Counterfactual Recourse Using Disentangled Subspaces by Downs et al. [116]

CRUDs is a probabilistic model that uses conditional subspace variational auto encoders (CSVAEs)
that extracts latent features that are relevant for a prediction. CSVAE partitions the latent space into
two parts: one to learn representations that are predictive of the labels, and one to learn the remaining

latent representations that are required to generate data.

In CRUDs, counterfactuals that target desirable outcomes are generated using CSVAEs in five major
steps: (1) disentangling latent features relevant for classification from those that are not, (2) generating
counterfactuals by changing only relevant latent features, filtering cuonterfactuals given constraints,

and (4) summarise counterfactuals for interpretability.

Downs et al. [116] evaluated CRUDS on seven synthetically generated and three real datasets. The

result indicates that CRUDS counterfactuals preserve the true dependencies between the covariates in

37
the data in all datasets except one.

Recourse: Algorithmic Recourse Under Imperfect Causal Knowledge by Karimi et al. [117]

Traditional works on counterfactuals for XAI focus on finding the nearest counterfactual that promotes
a change in the prediction to a favourable outcome. On the other hand, algorithm recourse approaches
focus on generating a set of actions that an individual can perform to obtain a favourable outcome. This

implies a shift from minimising a distance function to optimising a personalised cost function [117].

To the best of our knowledge, recourse is the only model-agnostic algorithm in the literature that
attempts to use a causal probabilistic framework as proposed in Pearl [44] grounded on structural
causal models to generate counterfactuals, and where recourse actions are defined as interventions of

the form of do —calculus operations.

Karimi et al. [117] highlight that it is very challenging to extract a structured causal model (SCM) from
the observed data [118], and in their work, they assume an imperfect and partial SCM to generate
the recourse, which can lead to incorrect counterfactuals. To overcome this challenge, Karimi et al.
[117] proposed two probabilistic approaches that relax the strong assumptions of an SCM: the first one
consists of using additive Gaussian noise and Bayesian model averaging to estimate the counterfactual
distribution; the second removes any assumptions on the structural causal equations by computing
the average effect of recourse actions on individuals. Experiment results on synthetic data showed
that Recourse was able to make more reliable recommendations under a partial SCM than other

non-probabilistic approaches.

6.8. Summary

We conducted a thorough systematic literature review guided by the argument to measure the causability
of an XAI system. Then the system needs to be underpinned by a probabilistic causal framework such
as the one proposed in Pearl [44]. We believe that counterfactual reasoning could provide human causal
understandings of explanations, although some authors challenge this notion. Zheng et al. [119] conducted
studies to investigate whether presenting causal explanations to a user would lead to better decision-making.
Their results were mixed. They found that if the user has prior and existing domain knowledge, then
presenting causal information did not improve the decision-making quality of the user. On the other hand,
if the user did not have any prior knowledge or beliefs about a specific task, then causal information enabled
the user to make better decisions.

We also found that the majority of the counterfactual generation approaches are not grounded on a
formal and structured theory of causality as proposed by Pearl [44]. Current counterfactual explanation

generation approaches for XAI are based on spurious correlations rather than cause-effect relationships.

38
This inability to disentangle correlation from causation can deliver sub-optimal, erroneous, or even biased
explanations to decision-makers, as Richens et al. [39] highlighted in his work about medical decision
making.

Table 2 summarizes all the algorithms that we analysed in this section in therms of underlying theories,

algorithms, applications and properties.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Properties
Theory / Approach Algorithms Ref, Applications Code? Proximity Plausibility Sparsity, Diversity Feasibility Optimization | Causal?
. c Yes [120]
WatcherCF [55] [rab / 1g] [Alo: C¥] ttnrorm] x vw x x Gradient Descent x
Prototype c Yes [120] Ww Ww ~
Counterfactuals (rat (Tab /Img] | [Algor CFProte] | [1.,/Ly-norm] v [ka-trees] ~ x FISTA x
FACE [56] (rep * img) Yes [122] x ww x w ww ¢ -graphs x
Weighted Counterfactual [101] [rab] No tu ~ : x vw x x Gradient Descent x
4-norm
Instance-Centric | ce [102, 103, 104] [rab / Tae / Tg] Yes [123] a ~ : x vw x x Growing Spheres x
o-norm
DICE [50] c Yes [124] wv x w wv vw Gradient Descent x
[Tab] [L-norm] [hinge loss]
CRUDS [116] c No ~ ww x ~ vw x
[Tab] [Lp-norm] [Variation Autoencoders]
ARES C/R Ww x Maximum a Posterior
Global [125] Tab/ Txt No x Probabilistic x x Estimate x
U
: CIR
PRINCE [i] prab/txt] Yes [126] wv x [ Randam Wark [ x x PageRank x
Probabilistic-Centrie | C-CHVAE (112, 113] c Yes [127] wv x w x x Integer Programming | )&
[Tab] [Variation Autoencoders] Optimization
ABELE [15] c Yes [128] vw x w~ x x x
[Img] [Variation Autoencoders]
RECOURSE [17] c Yes [129] wv wv w ww ww Gradient Descent wv
[Tab] [Variation Autoencoders]
MC-BRP [ia] rr] Yes [130] 7 x Y ww x Monte Carlo x
: c Hill Climbing 7
gic {195] [Tab] No v x vw x x Genetic Algorithms x
MACE [93] c Yes [131] ~ v w~ v wv SMT x
[Tab] [Lo/Ly/Linf-norm] [constraint satisfaction]
Coherent Counterfactuals [98] CIR Yes [132] ~ wv w~ ww vv Gurobi x
ab / Txt Ly-norm mixed polytopes! timization
[Tab / Txt] {Ly 1 b polytopes] Op’
MOCE [92] c Yes [133] ~ x x x NSCAI x
[Tab] [Ly-norm] [min feature changes]
CERTIFAL [107] c Yes [134] wv x x ~ x Fitness. x
. [Tab / Img J [L1-norm / SSIM] [mutations]
Genetic Centric LORE [106] c Yes [135] x x w x DecisionTree x
[Tab] [Ly-norm / Match] [mutations] Model
: CTR ‘Additive Feature
LIME-C 1136, 199] | crab tat y Img} | Yee [1371 x x x x x Mtuibution x
SED-C [95] * Yes [138] x x x x x x
R P [Txt] [cosine similarity]
legression-Centric S wy w
CLEAR 108, ‘Yes[139] x x x Ri x
{re8] [Tab] [139] [L-norm] [min feature changes] Seression
SHAP-C 136,109] | coat me img] |__Yl401 x x x x x Shapley Values x
Game Theory :
Centric SHAP-CC. [19] fr ol No x x x x x Shapley Values x
CBR for Good c Ww a Nearest Unlikely
Case Based Reasoning | Counterfactuals [a] [Tab / Txt] Ne [Li-norm| ~ [counterfactual potential] “ ~ Neighbour x

 

 

 

 

 

 

 

 

 

 

 

 

 

Table 2: Classification of collected model-agnostic counterfactual algorithms for XAI based on different properties, theoretical
backgrounds and applications.

7. Counterfactual Approaches to Explainable AI: Applications

This work is motivated by Holzinger et al. [22] hypothesis, which states that for a system to provide
understandable human explanations, the user needs to achieve a specified level of causal understanding
with effectiveness, efficiency, and satisfaction in a specified context of use [48, 47, 141, 142, 83, 143]. One
way to achieve this causal understanding is through counterfactuals.

One of the main areas that showed a need for counterfactual explanations is in medical decision support

systems. As pointed by Holzinger et al. [144], medical decision-making faces several challenges ranging

39
from small labelled datasets to the integration, fusion, and mapping of heterogeneous data to appropriate
visualisations [145]. Structured causal models that provide explanatory factors of the data could be used to
support medical experts. However, learning causal relationships from observational data is a very difficult
problem [146, 147].

XAI is very relevant for industries [148]. Another area of application of counterfactuals in XAI that is
highly mentioned in the literature is loan credit evaluation. Grath et al. [101] developed a model-agnostic
counterfactual explainer with an interface that uses weights generated from feature importance to generate
more compact and intelligible counterfactual explanations for end users. Lucic et al. [114] also developed a
model-agnostic counterfactual explanation in the context of a challenge from the finance industry’s interest
in exploring algorithmic explanations [149].

Recently, interfaces that generate counterfactuals as explanations have been proposed in the literature.
The ”What-IF tool” [150] is an open-source application that allows practitioners to probe, visualize, and
analyze machine learning systems with minimal coding. It also enables the user to investigate decision
boundaries and explore how general changes to data points affect the prediction.

ViCE (Visual Counterfactual Explanations for Machine Learning Models) [151] is an interactive visual
analytics tool that generates instance-centric counterfactual explanations to contextualize and evaluate
model decisions in a home equity line of credit scenario. ViCE highlights counterfactual explanations and
equips users with interactive methods to explore both the data and the model.

DECE (Decision Explorer with Counterfactual Explanations for Machine Learning Models) [152] is
another example of an interactive visual analytics tool that generates counterfactuals at an instance and
subgroup levels. The main difference to ViCE is that DECE allows users to interact with the counterfactuals
in order to find more actionable counterfactuals that suit their needs. DECE showed effectiveness in

supporting decision exploration tasks and instance explanations.

8. Towards Causability: Opportunities for Research

Although the demand for providing XAI systems that promote the causability, the literature is very
scarce in this aspect. We only found one recent article that proposed an explanation framework (FATE)
based on causability [153]. This framework focuses on human interaction, and the authors used the system
causability scale proposed by [22] to validate the effectiveness of their system’s explanations.

Shin [153] highlight that causability represents the quality of explanations and emphasize that it is an
antecedent role to explainability. Furthermore, they found that properties such as transparency, fairness,
and accuracy, play a critical role in improving user trust in the explanations. In general, this framework
is a guideline for developing user-centred interface design from the perspective of user interaction and

examines the effects of explainability in terms of user trust. This framework does not refer to XAI algorithms

40
underpinned by a theory of causality, neither on how to achieve causability from such mathematical
constructs. In the next section, we provide a set of conditions that we find are crucial elements for an XAI
system to promote causability. However, FATE causability system is not underpinned by any formal theory
of causality, and the causability metrics applied in this work focused on the interaction of the human with
the system.

Holzinger et al. [22] proposed a theoretical framework with a set of guidelines to promote causability
in XAI systems in the medical domain. One of the policies put forward is in creating new visualization
techniques that can be trainable by medical experts, as the specialists can survey the underlying explanatory
factors of the data. Another point is to formalize a structural causal model of human decision-making
and delineating features in the model. Holzinger [47] argue that a human-Al interface with counterfactual
explanations will help achieve causability. An open research opportunity is to extend human-Al explainable
interfaces with causability by allowing a domain expert to interact and ask “what-if” questions (counterfac-
tuals). This will enable the user to gain insights into the underlying explanatory factors of the predictions.
Holzinger et al. [83] propose a system causability scale framework as an evaluation tool for causability in
XAI systems.

We conclude our systematic literature review by highlighting what properties should an XAI system have
to promote causability. We find that the process of generating explanations that are human-understandable
needs to go beyond the minimisation of some loss function as proposed by the majority of the algorithms
in the literature. Explainability is a property that implies the generation of human mental representations
that can provide some degree of human-understandability of the system and, consequently, allow users to
trust it. As Guidotti et al. [106] stated, explainability is the ability to present interpretations in a meaningful
and effective way to a human user. We argue that for a system do be both explainable and promote
causability, then it cannot be resumed to a minimisation optimisation problem. Doing so would imply a
simplistic and objective explanation process that needs to be necessarily human-centric to achieve human
understandability [154]. We argue that, for a system to promote causability, the following properties should

be satisfied:

8.1. The Main Characteristics of a Causability System

* Causality. The analysis we conducted revealed that current model-agnostic explainable AI algorithms
lack a foundation on a formal theory of causality. Causal explanations are a crucial missing ingredient
for opening the black box to render it understandable to human decision-makers since knowing about
the cause/effect relationships of variables can promote human understandability. We argue that causal
approaches should be emphasised in XAI to promote a higher degree of interpretability to its users
and causability, although some authors challenge this notion. Zheng et al. [119] found that providing

causal information to human users in some tasks, resulted in poor decision-making. Zheng et al. [119]

41
conducted studies to investigate whether presenting causal explanations to a user would lead to better
decision-making. Their results were mixed. They found that if the user has prior and existing domain
knowledge, then presenting causal information did not improve the decision-making quality of the
user. On the other hand, if the user did not have any prior knowledge or beliefs about a specific task,
then causal information enabled the user to make better decisions. More work is needed on whether
causal information leads to better decisions. Many unstudied factors may contribute to this diverse
literature on the topic ranging from human cognitive bias to the way the explanations are presented in

the interface (supporting interactivity or not).

Counterfactual. Explanations generated by a causability system needs to be counterfactual. Cognitive
scientists agree that counterfactual reasoning is a crucial ingredient in learning and a key for explaining
adaptive behaviour in a changing environment [155]. Counterfactual reasoning induces mental
representations of an event that happened and representations of some other event alternative to
it [58]. It has been recognised in the literature that counterfactuals tend to help humans make
causal judgments [156]. Additionally, humans tend to think in a cause/effect way, but not in a strict
probabilistic sense [157]. It follows that for a machine to achieve a certain degree of human intelligence,
explainability systems need to provide counterfactual explanations. Additionally, for a system to
achieve causability, the counterfactual explanations need to be underpinned by a formal theory of
causality [22, 83]. Properties to generate good counterfactuals, such as diversity, feasibility, and

plausibility, should also be considered to increase the level of human understanding.

Human-Centric. Explanations need to be adapted to the information needs of different users. For
instance, in medical decision-making, a doctor is interested in certain aspects of an explanation, while
a general user is interested in other types of information. Adapting the information for the type
of user is a crucial and challenging point currently missing in XAI literature. There is the need to
bring the human user back to the optimisation process with human-in-the-loop strategies [158, 144]
containing contextual knowledge and domain-specific information. This interactive process can
promote causability since it will allow the user to create mental representations of the counterfactual

explanations in a symbiotic process between the human and the counterfactual generation process.

Inference. To promote the system’s user understandability, we argue that a causability framework
should be equipped with causal inference mechanisms to interact with the system and ask queries to the
generated explanations. Queries such as "given that I know my patient has a fever, what changes this
information induces in the explanation?”. This type of interaction can be highly engaging for the user
and promote more transparency in the system. It can enable more human-centric understandability of

the system since the user asks questions (performs inference) over variables of interest.

42
* Semantic annotations. One of the major challenges in XAI and a current open research problem is
to convert the sub-symbolic information extracted from the black-box into human-understandable
explanations. Incorporating semantic contextual knowledge and domain-specific information are
crucial ingredients that are currently missing in XAI. We argue that story models and narratives
are two important properties that need to be considered to generate human-understandable and
human-centric explanations. Story models and narratives can promote higher degrees of believability

in the system [159] and consequently achieve causability.

9. Answers to Research Questions

This section summarises the key points presented throughout this work by providing answers to the

research questions that guided our research.

9.1. RQI & RQ2: What are the main theoretical approaches and algorithms for counterfactuals in XAI?

Our systematic literature review revealed many different counterfactual algorithms proposed in the
literature. We were able to identify key elements shared by these algorithms based on how the optimisation
problem was framed and by considering the counterfactual generation process. We classified the existing

model-agnostic-XAI by their ’master theoretical algorithm” from which each algorithm derived:

* Instance-Centric. These approaches are based on random feature permutations and on finding
counterfactuals closed to the original instance by some distance function. These approaches are
relatively straightforward to implement. However, the generated counterfactuals are susceptible to
fail the plausibility and the diversity properties, although some algorithms incorporate mechanisms to
overcome this issue. Examples of algorithms that fall in this category are WatcherCF [55], prototype
counterfactuals [100], weighted counterfactuals [101],FACE [56], DiCE [50], and TRUCE [103].

* Constraint-Centric. These approaches are modelled as a constraint satisfaction problem. Coun-
terfactuals based on these approaches can easily satisfy different properties such as diversity and
plausibility. Examples of algorithms that fall in this category are MACE [93], GIC [105], and Coherent

Counterfactuals [98].

* Genetic-Centric. These approaches generate counterfactuals using the principles of genetic algorithms.
Due to genetic principles such as mutation or crossover, these approaches can satisfy counterfactual
properties such as diversity and plausibility. Examples of algorithms that fall in this category are

CERTIEFAI [107], MOCE [92], and LORE [106].

* Regression-Centric. These approaches have LIME as their underlying framework, and they use

linear regression to fit a set of permutated features. Counterfactuals based on these approaches have

43
difficulties satisfying several properties such as plausibility and diversity. Examples of algorithms that

fall in this category are LIME-C [109], SED-C [95], and CLEAR [108].

Game Theory Centric. These approaches have SHAP as their underlying framework, and they use
Shapley values to determine the local feature relevance. Counterfactuals based on these approaches
also have difficulties satisfying several properties such as plausibility and diversity. Examples of

algorithms that fall in this category are SHAP-C [109], and SHAP-CC [110].

Case-Based Reasoning. These approaches are inspired in the case-based reasoning paradigm of artifi-
cial intelligence and cognitive science. Since they store in-memory examples of good counterfactuals,
these approaches tend to satisfy different counterfactual properties, such as plausibility and diversity.

An examples of an algorithms that fall in this category is CBR Explanations by Keane and Smyth [96].

Probabilistic-Centric. These approaches mainly use probabilistic models to find the nearest coun-
terfactuals. Approaches such as recourse [117] have the potential to generate causal counterfactuals
based on the causality framework proposed by Pearl [44]. However, as the authors acknowledge, it is
challenging to learn causal relationships from observational data without introducing assumptions in

the causal model.

This research suggests that current model-agnostic counterfactual algorithms for explainable AI are
not grounded on a causal theoretical formalism and, consequently, might not promote causability to a
human decision-maker. Our findings show that the explanations derived from most of the model-agnostic
algorithms in the literature provide spurious correlations rather than cause/effects relationships, leading
to sub-optimal, erroneous, or even biased explanations. This opens the door to new research directions on
incorporating formal causal theories of causation in XAI. The closest work that we found that meets this goal
is the Recouse algorithm [117], however, research is still needed to investigate the extraction of structured
causal models from observational data.

There are also novel model-agnostic approaches proposed in the literature of XAI based on probabilistic
graphical models. For instance, Moreira et al. [160] proposed to learn a local Bayesian network that enables
the user to see which features are correlated (or conditional independent from) the class variable. They
found four different rules that can measure the degree of confidence of the interpretable model over the
explanations and provide specific recommendations for the user. However, this model is not causal, and

further research is needed to understand if such structures can be mapped into structured causal models.

44
9.2

for

9.3.

. RQ3: What are the sufficient and necessary conditions for a system to promote causability (Applications)?

The main purpose of this research work is to highlight some properties that find relevant and necessary

causability systems. We proposed the following properties.

Explanations need to be grounded on a structured and formal theory of Causality. This will enable the
usage of a full framework of algorithms of causal discovery that have been proposed throughout the

years [46].

Explanation algorithms need to be computed in the form of Counterfactuals. Due to the evidence
from cognitive science and social sciences, counterfactuals are among the best approaches to promote

human understandability and interpretability [24], although some authors challenge this [119].

Explanations need to be Human-Centric. Explanations need to be specific to the user’s needs: a medical

doctor will be interested in different explanations from a standard user.

The user should be able to interact with the generated explanations. The interaction with explanations
can help the user increase the levels of understandability and interpretability of the internal workings
of the XAI algorithm. Probabilistic inference is a promising tool to provide answers to users’ questions

regarding explanations.

Explainable AI systems need to be complemented with semantic annotations of features and domain
knowledge. To achieve explainability, contextual knowledge and domain-specific information need to

be included in the system.

RQ4: What are the pressing challenges and research opportunities in XAI systems that promote Causability?

This literature review enabled us to understand the current pressing challenges and opportunities

involved in creating XAI models that promote causability. We identified the following research opportunities

that can be used for future research in the area.

* Causal Theories for XAI. Pearl [88] argues that causal reasoning is indispensable for machine learning
to reach the human-level artificial intelligence since it is the primary mechanism of humans to be aware
of the world. As a result, the causal methodology gradually becomes a vitally important component in
explainable and interpretable machine learning. However, most current interpretability techniques
pay attention to solving the correlation statistic rather than the causation. Therefore, causal approaches
should be emphasized to achieve a higher degree of interpretability. The reason why causal approaches
for XAIJ are scarce is that finding causal relationships from observational data is very hard and still an

open research question [146].

45
* Standardized Evaluation Metrics for XAI. The field of metrics for XAI is also a topic that needs
development. Measures such as stability or fidelity [161] are not very clear for counterfactuals [31].
Ultimately, XAI metrics should be able to answer the following question: how does one know whether the
explanation works and the user has achieved a pragmatic understanding of the AI? [32] We highlight that
one research concern in XAI should be to develop generalised and standardised evaluation protocols
for XAI in different levels: Objective Level (user-free), Functional Level (functionality-oriented), and
User Level (human-centric). The main challenges consist in deriving standardised protocols that could
fit so many algorithms underpinned by different master theoretical approaches and at so many different

levels, although some interesting works have already been proposed in terms of causability [47, 83, 48].

Intelligent Interfaces for Causability in XAI. XAI’s basilar applications lie at the core of Intelligent
User Interfaces (IUIs). Rather than generating explanations as linear symbolic sequences, graphical
interfaces enable people to visually explore ML systems to understand how they perform over different
stimuli. The What-If tool [150] provides an excellent example, enabling people to visualize model
behavior across multiple models and subsets of input data and for different ML fairness metrics.
Such visual techniques leverage the human visual channel’s high bandwidth to explore probabilistic
inference, allowing humans to interact with explanations while recommending different descriptions.
Taking advantage of the innate human ability to spot patterns, these methods can provide better

quality answers than purely automatic approaches.

In a related direction van der Waa et al. [162] propose a framework that considers users’ experience
and reactions to explanations and evaluates these effects in terms of understanding, persuasive power,
and task performance. This user-centric approach is crucial to assess assumptions and intuitions to

yield more effective explanations effectively.

Recent Intelligent Exploration Interfaces focus on making explanations accessible to non-expert
users to interpret the underlying models better. Hoque and Mueller [163] argue that predictive and
interactive models based on causality are inherently interpretable and self-contained. They developed
Outcome Explorer, a causality-guided interactive interface that allows experts and non-experts alike

to acquire a comprehensive understanding of the models.

10. Conclusion

We conducted a systematic literature review to determine the modern theories underpinning model-

agnostic counterfactual algorithms for XAI and analyse if any existing algorithms can promote causability.

We extended the current literature by proposing a new taxonomy for model-agnostic counterfactuals based

on six approaches: instance-centric, constraint-centric, genetic- centric, regression-centric, game theory

46
centric, case-based reasoning centric, and probabilistic-centric. Our research also showed that model-
agnostic counterfactuals are not based on a formal and structured theory of causality as proposed by [44].
For that reason, we argue that these systems cannot promote a causal understanding to the user without
the risk of the explanations being biased, sub-optimal, or even erroneous. Current systems determine
relationships between features through correlation rather than causation.

We conclude this survey by highlighting new key points to promote causability in XAI systems, which
derive from formal theories of causality such as inference, counterfactuals, and probabilistic graphical
models. Causal models are a new research area, bursting with exciting new research challenges and
opportunities for XAI approaches grounded on probabilistic theories of causality and graphical models.
Indeed this field is highly relevant to Intelligent User Interfaces (IUIs) [164] by its very nature, both in
terms of content generation engines and user interface architecture. Therefore, more than a contraption
powered by robust and effective causal models, XAI can be seen as a cornerstone for next-generation IUIs.
This can only be achieved by marrying sound explanations delivered by fluid storytelling to persuasive
and articulate argumentation and a harmonious combination of different interaction modalities. These will

usher in powerful engines of persuasion, ultimately leading to the rhetoric of causability.

11. Acknowledgement

This work was partially supported by Portuguese government national funds through FCT, Fundacgdo
para a Ciéncia e a Tecnologia, under project UIDB/50021/2020.
This work was also partially supported by Queensland University of Technology (QUT) Centre for Data

Science First Byte Funding Program and by QUT’s Women in Research Grant Scheme.

References

[1] W. Tan, P. Tiwari, H. M. Pandey, C. Moreira, A. K. Jaiswal, Multi-modal medical image fusion algorithm
in the era of big data, Neural Computing and Applications (2020).

[2] Z. C. Lipton, The mythos of model interpretability, Communications ACM 61 (2018) 36-43.

[3] D. Doran, S. Schulz, T. R. Besold, What does explainable ai really mean? a new conceptualization of
perspectives, in: Proceedings of the First International Workshop on Comprehensibility and Explana-
tion in AI and ML 2017 co-located with 16th International Conference of the Italian Association for

Artificial Intelligence, 2017. arXiv: 1710.00794.

[4] C. T. Ramaravind K. Mothilal, Amit Sharma, Examples are not enough, learn to criticize! criti-
cism for interpretability, in: Proceedings of the 2020 Conference on Fairness, Accountability, and

TransparencyJanuary, 2020.

47
[5] B. Goodman, S. Flaxman, European union regulations on algorithmic decision-making and a “right to

explanation”, AI Magazine 38 (2017) 50-57.

[6] C. O’Neil, Weapons of math destruction: How big data increases inequality and threatens democracy,

Broadway Books, 2017.

[7] Z. Obermeyer, B. Powers, C. Vogeli, S. Mullainathan, Dissecting racial bias in an algorithm used to

manage the health of populations, Science 366 (2019) 447-453.

[8] A. Lau, E. Coiera, Do people experience cognitive biases while searching for information?, Journal of

the American Medical Informatics Association 14 (2007) 599 - 608.

[9] G. Saposnik, D. Redelmeier, C. C. Ruff, P. N. Tobler, Cognitive biases associated with medical decisions:
a systematic review, BMC Medical Informatics and Decision Making 16 (2016) 138.

[10] J. R. Zech, M. A. Badgeley, M. Liu, A. B. Costa, J. J. Titano, E. K. Oermann, Variable generalization
performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional

study, PLOS Medicine 15 (2018) 1-17.

[11] J. Buolamwini, T. Gebru, Gender shades: Intersectional accuracy disparities in commercial gender
classification, in: Proceedings of the 1st Conference on Fairness, Accountability and Transparency,

2018, pp. 77-91.

[12] C. Moreira, B. Martins, P. Calado Using rank aggregation for expert search in academic digital libraries,

in: Simpdsio de Informatica, INForum, 2011, pp. 1-10.
[13] C. Moreira, Learning to rank academic experts, Master Thesis, Technical University of Lisbon, 2011.

[14] T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, A. Kalai, Man is to computer programmer as woman
is to homemaker? debiasing word embeddings, in: Proceedings of the 30th Conference on Neural

Information Processing Systems, 2016.

[15] N. Garg, L. Schiebinger, D. Jurafsky, J. Zou, Word embeddings quantify 100 years of gender and
ethnic stereotypes, Proceedings of the National Academies of Science of the United States of America

115 (2018) 3635-3644.

[16] A. Caliskan, J. J. Bryson, A. Narayanan, Semantics derived automatically from language corpora

contain human-like biases, Science 356 (2017) 183-186.

[17] M. Kosinski, Y. Wang, Deep neural networks are more accurate than humans at detecting sexual

orientation from facial images, Journal of Personality and Social Psychology 114 (2018) 246-257.

48
[18] H. Lakkaraju, E. Kamar, R. Caruana, J. Leskovec, Faithful and customizable explanations of black box
models, in: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, ATES, 2019,
pp. 131-138.

[19] F. Doshi-Velez, B. Kim, Towards a rigorous science of interpretable machine learning, arxiv:

1702.08608 (2017).

[20] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, L. Kagal, Explaining explanations: An overview
of interpretability of machine learning, CoRR abs/1806.00069 (2018).

[21] J. W. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, B. Yu, Definitions, methods, and applications in
interpretable machine learning, Proceedings of the National Academy of Sciences 116 (2019) 22071 -
22080.

[22] A. Holzinger, G. Langs, H. Denk, K. Zatloukal, H. Miller, Causability and explainability of artificial
intelligence in medicine, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 9

(2019) e1312.

[23] A. Pdez, The pragmatic turn in explainable artificial intelligence (xai), Minds and machines (Dor-

drecht) 29 (2019) 441-459.

[24] T. Miller, Explanation in artificial intelligence: Insights from the social sciences, Artificial Intelligence

267 (2019) 1-38.

[25] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods for
explaining black box models, ACM Computing Surveys 51 (2018) 93:1-93:42.

[26] A. Das, P. Rad, Opportunities and challenges in explainable artificial intelligence (xai): A survey, 2020.
arXiv:2006. 11371.

[27] A. Adadi, M. Berrada, Peeking inside the black-box: A survey on explainable artificial intelligence
(XAI), IEEE Access 6 (2018) 52138-52160.

[28] A. Barredo Arrieta, N. Diaz-Rodriguez, J. Del Ser, A. Bennetot, S$. Tabik, A. Barbado, S. Garcia,
S. Gil-Lopez, D. Molina, R. Benjamins, R. Chatila, KF Herrera, Explainable artificial intelligence (xai):
Concepts, taxonomies, opportunities and challenges toward responsible ai, Information Fusion 58

(2020) 82-115.

[29] S. Mohseni, N. Zarei, A multidisciplinary survey and framework for design and evaluation of

explainable ai systems, CoRR cs. HC/1811.11839 (2020) 1-45.

49
[30] J. Zhou, A. H. Gandomi, F. Chen, A. Holzinger, Evaluating the quality of machine learning explana-

tions: A survey on methods and metrics, Electronics 10 (2021) 593.

[31] D. V. Carvalho, E. M. Pereira, J. S. Cardoso, Machine learning interpretability: A survey on methods

and metrics, Electronics 8 (2019) 832.

[32] R. R. Hoffman, S. T. Mueller, G. Klein, J. Litman, Metrics for explainable ai: Challenges and prospects,
2019. arXiv: 1812.04608.

[33] D. Alvarez-Melis, T. S. Jaakkola, On the robustness of interpretability methods, 2018.
arXiv: 1806 .08049.

[34] J. Chen, H. Dong, X. Wang, F. Feng, M. Wang, X. He, Bias and debias in recommender system: A
survey and future directions, 2020. arXiv: 2010 .03240.

[35] S. Serrano, N. A. Smith, Is attention interpretable?, in: Proc. of the 57th Conference of the Association

for Computational Linguistics, ACL, Association for Computational Linguistics, 2019, pp. 2931-2951.

[36] M. T. Ribeiro, S. Singh, C. Guestrin, ”"Why Should I Trust You?”: Explaining the predictions of
any classifier, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, 2016, pp. 1135-1144.

[37] S. Lundberg, S.-I. Lee, A unified approach to interpreting model predictions, in: Proceedings of the
31st Annual Conference on Neural Information Processing Systems (NIPS), 2017, pp. 4765-4774.

[38] C. Rudin, Stop explaining black box machine learning models for high stakes decisions and use

interpretable models instead, Nature Machine Intelligence 1 (2019) 206-215.

[39] J. G. Richens, C. M. Lee, S. Johri, Improving the accuracy of medical diagnosis with causal machine

learning, Nature Communications 11 (2020) 3923-3932.

[40] N. Kilbertus, M. Rojas-Carulla, G. Parascandolo, M. Hardt, D. Janzing, B. Schélkopf, Avoiding dis-
crimination through causal reasoning, in: Proceedings of the 31st Conference on Neural Information

Processing Systems, 2017.

[41] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan

Kaufmann Publishers, 1988.

[42] R. M. J. Byrne, Counterfactuals in explainable artificial intelligence (xai): Evidence from human
reasoning, in: Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intel-
ligence, IJCAI-19, International Joint Conferences on Artificial Intelligence Organization, 2019, pp.

6276-6282.

50
[43] B. Lake, T. Ullman, J. Tanenbaum, S. Gershman, Building machines that learn and think like humans,
Brain and Behavioural Sciences 40 (2017) e253.

[44] J. Pearl, Causality: Models, Reasoning and Inference, Cambridge University Press, 2009.

[45] S. Gershman, E. Horvitz, J. Tenenbaum, Computational rationality: A converging paradigm for

intelligence in brains, minds, and machines, Science 349 (2015) 273-278.

[46] J. Peters, D. Janzing, B. Schélkopf, Elements of Causal Inference Foundations and Learning Algorithms,

MIT Press, 2017.
[47] A. Holzinger, Explainable ai and multi-modal causability in medicine, i-com 19 (2020) 171 - 179.

[48] A. Holzinger, B. Malle, A. Saranti, B. Pfeifer, Towards multi-modal causability with graph neural

networks enabling information fusion for explainable ai, Information Fusion 71 (2021) 28-37.

[49] M. N. Hoque, K. Mueller, Outcome-explorer: A causality guided interactive visual interface for

interpretable algorithmic decision making, arxiv: 2101.00633 (2021).

[50] R. Mothilal, A. Sharma, C. Tan, Explaining machine learning classifiers through diverse counterfactual
explanations, in: Proceedings of the 2020 Conference on fairness, accountability, and transparency,

2020, pp. 607-617.

[51] J. Halpern, J. Pearl, Causes and explanations: A structural-model approach. part i: Causes, The British

Journal for the Philosophy of Science 56 (2005) 889-911.
[52] S. Psillos, Causation and Explanation, MPG Books Group, 2002.
[53] D. Hume, A Treatise of Human Nature, London: John Noon, 1739.
[54] D. Lewis, Causation, Journal of Philosophy 70 (1973) 113-126.

[55] W. Sandra, M. Brent, R. Chris, Counterfactual explanations without opening the black-box: Automated

decisions and the gdpr, Harvard journal of law & technology 31 (2018).

[56] R. Poyiadzi, K. Sokol, R. Santos-Rodriguez, T. De Bie, P. Flach, Face: Feasible and actionable counter-
factual explanations, in: Proceedings of the AAAI/ACM Conference on ai, ethics, and society, 2020,
pp. 344-350.

[57] S. Verma, J. Dickerson, K. Hines, Counterfactual explanations for machine learning: A review, arxiv:

2010.10596 (2020).

51
[58] I. Stepin, J. M. Alonso, A. Catala, M. Pereira-Farifia, A survey of contrastive and counterfactual
explanation generation methods for explainable artificial intelligence, IEEE Access 9 (2021) 11974—
12001.

[59] A. Karimi, G. Barthe, B. Schélkopf, I. Valera, A survey of algorithmic recourse: definitions, formula-

tions, solutions, and prospects, arXiv: 2010.04050 (2021).

60] V. Belle, I. Papantonis, Principles and practice of explainable machine learning, arXiv:2009.11698
Pp Pp Pp p g
(2020).

[61] C. Molnar, Interpretable Machine Learning: A Guide for Making Black Box Models Explainable,
Leanpub, 2018.

62] M. Siering, A. V. Deokar, C. Janze, Disentangling consumer recommendations: Explaining and
g ging Pp g
predicting airline recommendations based on online reviews, Decision Support Systems 107 (2018)

52 - 63.

[63] B. Kim, J. Park, J. Suh, Transparency and accountability in ai decision support: Explaining and
visualizing convolutional neural networks for text information, Decision Support Systems 134 (2020)

113302.

[64] M. A.-M. Radwa Elshawi, Youssef Sherif, S$. Sakr, Interpretability in healthcare a comparative study of
local machine learning interpretability techniques, in: Proceedings of IEEE Symposium on Computer-

Based Medical Systems (CBMS), 2019.

[65] M. Stiffler, A. Hudler, E. Lee, D. Braines, D. Mott, D. Harborne, An analysis of the reliability of lime
with deep learning models, in: Proceedings of the Dstributed Analytics and Information Science

International Technology Alliance, 2018.

[66] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: Computer Vision

~ECCV 2014, 2014, pp. 818-833.

[67] S. Lapuschkin, $. Waldchen, A. Binder, G. Montavon, W. Samek, K.-R. Miiller, The pragmatic turn in

explainable artificial intelligence (xai), Nature communications 10 (2019) 1096-1096.

[68] H. F. Tan, K. Song, M. Udell, Y. Sun, Y. Zhang, Why should you trust my interpretation? understanding

uncertainty in lime predictions, 2019.

[69] M. Badhrinarayan, P. Ankit, K. Faruk, Explainable deep-fake detection using visual interpretability
methods, in: 2020 3rd International Conference on Information and Computer Technologies (ICICT),

2020, pp. 289-293.

52
[70] A. Preece, Asking ‘why’ in ai: Explainability of intelligent systems - perspectives and challenges,

International journal of intelligent systems in accounting, finance & management 25 (2018) 63-72.

[71] R. Turner, A model explanation system, in: IEEE 26th International Workshop on Machine Learning

for Signal Processing, 2016.
[72] B. Osbert, K. Carolyn, B. Hamsa, Interpretability via model extraction, arxiv: 1705.08504 (2017).

[73] T. J. J, K. Bhavya, S. Prasanna, R. K. Natesan, Treeview: Peeking into deep neural networks via

feature-space partitioning, Nature communications (2019).

[74] R. Sindhgatta, C. Moreira, C. Ouyang, A. Barros, Interpretable predictive models for business
processes, in: Proceedings of the 18th Internation Conference on Business Process Management

(BPM), 2020.

[75] R. Sindhgatta, C. Ouyang, C. Moreira, Exploring interpretability for predictive process analytics, in:
Proceedings of the 18th International Conference on Service Oriented Computing (ICSOC), 2020.

[76] M. T. Ribeiro, S. Singh, C. Guestrin, Anchors: High-precision model-agnostic explanations, in:

Proceedings of the 32nd AAAI International Conference on Artificial Intelligence, 2018.
[77] L.S. Shapley, A value for n-person games, Rand coporation (1952) 15.

[78] E. Strumbelj, I. Kononenko, Explaining prediction models and individual predictions with feature

contributions, Knowledge and Information Systems 41 (2013) 647-665.

[79] A. C. Miller Janny Ariza-Garzon, Javier Arroyo, M.-J. Segovia-Vargas, Explainability of a machine

learning granting scoring model in peer-to-peer lending, in: Proceedings of IEEE Access, 2020.

[80] A. B. Parsa, A. Movahedi, H. Taghipour, S. Derrible, A. (Kouros)Mohammadian, Toward safer
highways, application of xgboost and shap for real-time accident detection and feature analysis,

Accident Analysis & Prevention 136 (2020) 105405.

[81] J. Wang, J. Wiens, S. Lundberg, Shapley flow: A graph-based approach to interpreting model
predictions, in: Proceedings of the 24th International Conference on Artificial Intelligence and

Statistics, 2021. arXiv: 2010. 14592.

[82] H. Y. Teh, A. W. Kempa-Liehr, K. L-K. Wang, Sensor data quality: a systematic review, Journal of Big
Data 7 (2020) 11.

[83] A. Holzinger, A. Carrington, H. Miiller, Measuring the quality of explanations: The system causability
scale (scs), KI - Ktinstliche Intelligenz 34 (2020) 193-198.

53
[84] R. Byrne, Cognitive processes in counterfactual thinking about what might have been, The psychology

of learning and motivation: Advances in research and theory 37 (1997) 105-154.

[85] D. Wesberg, A. Gopnik, Pretense, counterfactuals, and bayesian causal models: Why what is not real

really matters, Cognitive Science 37 (2013) 1368-1381.

[86] L. M. Pereira, A. B. Lopes, Cognitive prerequisites: The special case of counterfactual reasoning,

Machine Ethics. Studies in Applied Philosophy, Epistemology and Rational Ethics 53 (2020).

[87] M. Prosperi, Y. Guo, M. Sperrin, J. S. Koopman, J. S. Min, X. He, S. Rich, M. Wang, I. E. Buchan, J. Bian,
Causal inference and counterfactual prediction in machine learning for actionable healthcare, Nature

Machine Intelligence 2 (2020) 369-375.

[88] J. Pearl, The seven tools of causal inference, with reflections on machine learning, Communications of

ACM 62 (2019) 7.

[89] K. Sokol, P. Flach, Explainability fact sheets: a framework for systematic assessment of explainable
approaches, in: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,

2020.

[90] A. Fernandez, F. Herrera, O. Cordon, M. Jose del Jesus, F. Marcelloni, Evolutionary fuzzy systems
for explainable artificial intelligence: Why, when, what for, and where to?, IEEE computational

intelligence magazine 14(1) (2019).
[91] D. Lewis, Counterfactuals, Oxford: Blackwell, 1973.

[92] S. Dandl, C. Molnar, M. Binder, B. Bischl, Multi-objective counterfactual explanations, Lecture Notes
in Computer Science (2020) 448-469.

[93] A.-H. Karimi, G. Barthe, B. Balle, I. Valera, Model-agnostic counterfactual explanations for conse-
quential decisions, in: Proceedings of the 23rd International Conference on Artificial Intelligence and

Statistics (AISTATS), 2020, pp. 895-905.

[94] M. T. Keane, B. Smyth, Good counterfactuals and where to find them: A case-based technique for

generating counterfactuals for explainable ai (xai), arxiv: 2005.13997 (2020).
[95] D. Martens, F. Provost, Explaining data-driven document classifications, MIS quarterly 38(1) (2014).

[96] M. T. Keane, B. Smyth, Good counterfactuals and where to find them: A case-based technique
for generating counterfactuals for explainable ai (xai), in: Case-Based Reasoning Research and

Development, Springer International Publishing, 2020.

54
[97]

[98]

[99]

[100]

[101]

[102]

[103

[104]

[105]

[106]

[107]

M. Pawelczyk, K. Broelemann, G. Kasneci, On counterfactual explanations under predictive mul-
tiplicity, in: Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence,

2020.

C. Russell, Efficient search for diverse coherent explanations, in: Proceedings of the Conference on

Fairness, Accountability, and Transparency, 2019, pp. 20-28.

P. Domingos, The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake
Our World, Penguin, 2017.

A. V. Looveren, J. Klaise, Interpretable counterfactual explanations guided by prototypes, 2019.
arXiv: 1907 .02584.

R. M. Grath, L. Costabello, C. L. Van, P. Sweeney, F. Kamiab, Z. Shen, F. Lecue, Interpretable credit
application predictions with counterfactual explanations, in: Proceedings of the 32nd Annual

Conference on Neural Information Processing Systems (NIPS), 2018.

T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, M. Detyniecki, Comparison-based inverse classifica-
tion for interpretability in machine learning, in: Proceedings of the International Conference on
Information Processing and Management of Uncertainty in Knowledge-Based Systems. Theory and

Foundations, 2018, pp. 100-111.

T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, M. Detyniecki, The dangers of post-hoc interpretability:
Unjustified counterfactual explanations, in: Proceedings of the Twenty-Eighth International Joint

Conference on Artificial Intelligence, 2019.

T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, M. Detyniecki, Unjustified classification regions and
counterfactual explanations in machine learning, in: Proceedings of the European Conference on

Machine Learning and Knowledge Discovery in Databases, 2019.

M. T. Lash, Q. Lin, N. Street, J. G. Robinson, J. Ohlmann, Generalized inverse classification, Proceed-
ings of the 2017 Society for Industrial and Applied Mathematics International Conference on Data
Mining (2017) 162-170.

R. Guidotti, A. Monreale, 5. Ruggieri, D. Pedreschi, F. Turini, E Giannotti, Local rule-based explana-
tions of black box decision systems, arxiv: 1805.10820 (2018).

S. Sharma, J. Henderson, J. Ghosh, Certifai: Counterfactual explanations for robustness, transparency,

interpretability, and fairness of artificial intelligence models, arxiv: 1905.07857 (2019).

55
[108] A. White, A. d’Avila Garcez, Measurable counterfactual local explanations for any classifier, arxiv:

1908.03020 (2019).

[109] Y. Ramon, D. Martens, F. Provost, T. Evgeniou, A comparison of instance-level counterfactual
explanation algorithms for behavioral and textual data: Sedc, lime-c and shap-c, Advances in Data

Analysis and Classification 1(1) (2020).

[110] S. Rathi, Generating counterfactual and contrastive explanations using shap, 2019.

arXiv: 1906 .09293.

[111] A. Ghazimatin, O. Balalau, R. Saha Roy, G. Weikum, Prince: provider-side interpretability with
counterfactual explanations in recommender systems, in: Proceedings of the 13th International

Conference on Web Search and Data Mining, 2020, pp. 196-204.

[112

M. Pawelczyk, K. Broelemann, G. Kasneci, Learning model-agnostic counterfactual explanations for

tabular data, in: Proceedings of the World Wide Web Conference 2020, 2020.

[113

M. Pawelczyk, J. Haug, K. Broelemann, G. Kasneci, Towards user empowerment, in: Proceedings
of the Thirty-third Annual Conference on Neural Information Processing Systems, Workshop on

Human-Centric Machine Learning, 2019.

[114

A. Lucic, H. Haned, M. de Rijke, Why does my model fail?: contrastive local explanations for retail

forecasting, in: FAT* ’20: Conference on Fairness, Accountability, and Transparency, 2020.

[115] R. Guidotti, A. Monreale, S. Matwin, D. Pedreschi, Black box explanation by learning image exemplars
in the latent feature space, in: Proccedings of the Joint European Conference on machine Learning

and Knowledge Discovery in Databases, 2020.

[116] M. Downs, J. L. Chu, Y. Yacoby, F. Doshi-Velez, W. Pan, Cruds: Counterfactual recourse using
disentangled subspaces, ICML WHI 2020 (2020) 1-23.

[117

A. Karimi, B. J. von Kiigelgen, B. Schélkopf, I. Valera, Algorithmic recourse under imperfect causal
knowledge: a probabilistic approach, in: Advances in Neural Information Processing Systems 33:

Annual Conference on Neural Information Processing Systems, 2020.

[118] S. Barocas, A. D. Selbst, M. Raghavan, The hidden assumptions behind counterfactual explanations
and principal reasons, in: Proceedings of the 2020 Conference on Fairness, Accountability, and

Transparency, 2020.

[119

M. Zheng, J. K. Marsh, J. V. Nickerson, S. Kleinberg, How causal information affects decisions,

Cognitive Research: Principles and Implications 5 (2020).

56
[120] Alibi, 2019. URL: https: //github.com/SeldonI0/alibi.

[121] A. V. Looveren, J. Klaise, Interpretable counterfactual explanations guided by prototypes, arxiv:

1907.02584 (2019).
[122] Face, 2020. URL: https: //github.com/sharmapulkit.
[123] Truce, 2019. URL: https: //github.com/thibaultlaugel/ truce.
[124] Dice, 2020. URL: https: //github.com/interpretml /DiCE.

[125] K. Rawal, Himabindu, Beyond individualized recourse: Interpretable and interactive summaries of
actionable recourse, in: Proceedings of the 34th International Conference on Neural Information

Processing Systems, 2020.
[126] Prince, 2019. URL: https: //github.com/azinmatin/prince/.
[127] C-chvae, 2020. URL: https: //github.com/MartinPawel /c-chvae.
[128] Abele, 2020. URL: https: //github.com/riccotti/ABELE.
[129] Recourse, 2020. URL: https: //github.com/amirhk/ recourse.
[130] Mc-brp, 2019. URL: https: //github.com/a-lucic/mc-brp.
[131] Mace, 2019. URL: https: //github.com/amirhk /mace.

[132] Coherent counterfactuals, 2019. URL: https: //bitbucket.org/ChrisRussell1/

diverse-coherent-explanations/src/master/.
[133] Moce, 2020. URL: https: //github.com/susanne-207/moc.
[134] Certifai, 2020. URL: https: //github.com/Ighina/CERTIFAI.
[135] Lore, 2018. URL: https://github.com/riccotti/LORE.

[136] Y. Ramon, D. Martens, F. Provost, T. Evgeniou, Counterfactual explanation algorithms for behavioral

and textual data, 2019. arXiv: 1912.01819.
[137] Lime counterfactual, 2020. URL: https: //github.com/yramon/LimeCounterfactual.
[138] Sed counterfactual, 2020. URL: https: //github.com/yramon/edc.
[139] Clear, 2020. URL: https: //github.com/ClearExplanationsAI /CLEAR.

[140] Shap counterfactual, 2020. URL: https: //github.com/yramon/ShapCounterfac tual.

57
[141] A. Holzinger, C. Biemann, C. Pattichis, D. Kell, What do we need to build explainable ai systems for
the medical domain?, 2017. arXiv: 1712.09923.

[142] A. Holzinger, From machine learning to explainable ai, in: Proceedings of the 2018 World Symposium

on Digital Intelligence for Systems and Machines, 2018.

[143] G. Xu, T. D. Duong, Q. Li, S. Liu, X. Wang, Causality learning: A new perspective for interpretable
machine learning, 2020. arXiv: 2006. 16789.

[144] A. Holzinger, M. Plass, M. Kickmeier-Rust, K. Holzinger, G. C. Crisan, C.-M. Pintea, V. Palade,
Interactive machine learning: experimental evidence for the human in the algorithmic loop, Applied

Intelligence 49 (2019) 2401-2414.

[145] A. Holzinger, Trends in interactive knowledge discovery for personalized medicine: Cognitive science

meets machine learning, IEEE Intelligent Informatics Bulletin 15 (2014) 6-14.

[146] Q. Zhao, T. Hastie, Causal interpretations of black-box models, Journal of Business & Economic

Statistics (2019) 1-10.
[147] O. Peters, The ergodicity problem in economics, Nature Physics 15 (2019) 1216-1221.

[148] J. Rehse, N. Mehdiyev, P. Fettke, Towards explainable process predictions for industry 4.0 in the
dfki-smart-lego-factory, Kiinstliche Intelligenz 33 (2019) 181-187.

[149] Fico, 2017. URL: https: //community.fico.com/s/explainable-machine-learning-challenge.

[150] J. Wexler, M. Pushkarna, T. Bolukbasi, M. Wattenberg, F. Viegas, J. Wilson, The what-if tool: Interactive
probing of machine learning models, IEEE Transactions on Visualization and Computer Graphics

(2019) 1-1.

[151] O. Gomez, S. Holter, J. Yuan, E. Bertini, Vice: Visual counterfactual explanations for machine learning
models, in: Proceedings of the 25th International Conference on Intelligent User Interfaces, 2020, p.

531-535.

[152] F. Cheng, Y. Ming, H. Qu, Dece: Decision explorer with counterfactual explanations for machine

learning models, in: Proceedings of the IEEE VIS 2020, 2020.

[153] D. Shin, The effects of explainability and causability on perception, trust, and acceptance: Implications

for explainable ai, International Journal of Human-Computer Studies 146 (2021) 102551.

[154] R. Confalonieri, L. Coba, B. Wagner, T. R. Besold, A historical perspective of explainable artificial
intelligence, WIREs Data Mining and Knowledge Discovery 11 (2021) e1391.

58
[155] J. Paik, Y. Zhang, P. Pirolli, Counterfactual reasoning as a key for explaining adaptive behavior in a

changing environment, Biologically Inspired Cognitive Architectures 10 (2014) 24-29.

[156] T. Gerstenberg, M. F. Peterson, N. D. Goodman, D. A. Lagnado, J. B. Tenenbaum, Eye-tracking
causality, Psychological science 28 (2017) 1731-1744.

[157] E. Goldvarg, P. Johnson-Laird, Naive causality: a mental model theory of causal meaning and

reasoning, Cognitive Science 25 (2001) 565-610.

[158] A. Holzinger, Interactive machine learning for health informatics: When do we need the human-in-

the-loop?, Brain Informatics 3 (2016) 119-131.

[159] R.N. Yale, Measuring narrative believability: Development and validation of the narrative believability

scale (nbs-12), Journal of Communication 63 (2013) 578-599.

[160] C. Moreira, Y.-L. Chou, M. Velmurugan, C. Ouyang, R. Sindhgatta, P. Bruza, Linda-bn: An inter-
pretable probabilistic approach for demystifying black-box predictive models, Decision Support
Systems (in press) (2021) 113561.

[161] M. Velmurugan, C. Ouyang, C. Moreira, R. Sindhgatta, Evaluating explainable methods for predictive
process analytics: A functionally-grounded approach, in: Proceedings of the 33rd International

Conference on Advanced Information Systems Engineering Forum, 2020.

[162] J. van der Waa, E. Nieuwburg, A. Cremers, M. Neerincx, Evaluating xai: A comparison of rule-based

and example-based explanations, Artificial Intelligence 291 (2021) 103404.

[163] M. N. Hoque, K. Mueller, Outcome-explorer: A causality guided interactive visual interface for

interpretable algorithmic decision making, 2021. arXiv:2101.00633.

[164] S. T. Vélkel, C. Schneegass, M. Eiband, D. Buschek, What is “intelligent” in intelligent user interfaces?
a meta-analysis of 25 years of iui, in: Proceedings of the 25th International Conference on Intelligent

User Interfaces, 2020, p. 477-487.

59
15 Feb 2021

What Do We Want From Explainable Artificial
Intelligence (XAI)?

A eholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary

XAI Research

Markus Langer, Daniel Oster, Timo Speith

2.07817v1 gcs.AT]

Helger Hermanns, Lena Kastner, Eva Schmidt, Andreas Sesing, & Kevin Baum

2

Vv

“4
Thisitfa preprint version of the paper “What do we want from Explainabile Artificial Intelligence (XAD?-A
stakelfoider perspective on XAI and a conceptual model guiding interdisciplinary XAI research.” accepted at
Artificial Intelligence. The published version might differ from this version. Please cite this as: Langer M.,
Oster, D., Speith, T., Hermanns, H., Kastner, L., Schmidt, E., Sesing, A., & Baum, K. (2021). What do we
want from Explainabile Artificial Intelligence (XAI)? — A stakeholder perspective on XAI and a conceptual

model guiding interdisciplinary XAI research. Artificial Intelligence. doi: 10.1016/j.artint.202 1.103473

 

© 2021. This manuscript version is made available under the Creative Commons

Attribution-NonCommercial-NoDerivatives 4.0 International (CC-B Y-NC-ND 4.0) License.

C\OSO

 

 
What Do We Want From Explainable Artificial Intelligence (XAI)?
A Stakeholder Perspective on XAI and a Conceptual Model Guiding
Interdisciplinary XAI Research

Markus Langer!
Department of Psychology, Saarland University, Saarbriicken, Germany
Daniel Oster!, Lena Kastner
Institute of Philosophy, Saarland University, Saarbriicken, Germany

Timo Speith', Kevin Baum

Institute of Philosophy, Saarland University, Saarbriicken, Germany

Department of Computer Science, Saarland University, Saarbriicken, Germany

Holger Hermanns

Department of Computer Science, Saarland University, Saarbriicken, Germany

Institute of Intelligent Software, Guangzhou, China
Eva Schmidt
Institute of Philosophy and Political Sciences, Technical University Dortmund, Germany

Andreas Sesing

Institute of Legal Informatics, Saarland University, Saarbriicken, Germany

 

Abstract

Previous research in Explainable Artificial Intelligence (XAD) suggests that a main aim of explainability ap-
proaches is to satisfy specific interests, goals, expectations, needs, and demands regarding artificial systems
(we call these ‘stakeholders’ desiderata’) in a variety of contexts. However, the literature on XAI is vast,
spreads out across multiple largely disconnected disciplines, and it often remains unclear how explainability
approaches are supposed to achieve the goal of satisfying stakeholders’ desiderata. This paper discusses

the main classes of stakeholders calling for explainability of artificial systems and reviews their desiderata.

 

* Work on this paper was funded by the Volkswagen Foundation grants AZ 95143, 98509, 98510, 98511, 98512, 98513, and 98514
“Explainable Intelligent Systems” (EIS), by the DFG grant 389792660 as part of TRR 248, and by the ERC Advanced Grant 695614
(POWVER). The authors thank the anonymous reviewers for feedback on initial drafts of this paper.

'Markus Langer, Daniel Oster, and Timo Speith have contributed equally to this article and share the first authorship.

Preprint submitted to Artificial Intelligence February 17, 2021
We provide a model that explicitly spells out the main concepts and relations necessary to consider and in-
vestigate when evaluating, adjusting, choosing, and developing explainability approaches that aim to satisfy
stakeholders’ desiderata. This model can serve researchers from the variety of different disciplines involved
in XAI as acommon ground. It emphasizes where there is interdisciplinary potential in the evaluation and
the development of explainability approaches.

Keywords: Explainable Artificial Intelligence, Explainability, Interpretability, Explanations,

Understanding, Interdisciplinary Research, Human-Computer Interaction

 

1. Introduction

1.1. Background, Motivation, and Related Work

Explainable Artificial Intelligence (XAD) is — once again [1-4] — a burgeoning multidisciplinary area
of research. In general, XAI can be perceived as the topic or research field concerned with developing
approaches to explain and make artificial systems understandable to human stakeholders [5-7].

This puts several central aspects into the focus of XAI research. First, artificial systems are the primary
objects of investigation. Such systems can range from systems following a predefined set of rules, to expert
and knowledge-based systems, to systems relying on machine learning. Insights from XAI research become
important when these systems are too complex to allow for human oversight or are inherently opaque, which
precludes human insight [8]. Second, this view on XAI emphasizes the importance of approaches that en-
able or provide insights into artificial systems, their functioning, and their outputs. These approaches (we
call them ‘explainability approaches’) encompass methods, procedures, and strategies to provide explana-
tory information helping us to better understand artificial systems. Third, there is a decisive need for XAI
because there are human stakeholders (e.g., users, developers, regulators)” whose interests, goals, expecta-
tions, needs, and demands regarding artificial systems (e.g., to have fair or trustworthy systems [9, 10]) call
for greater understandability of artificial systems. We call such conglomerations of stakeholders’ interests,
goals, expectations, needs, and demands regarding artificial systems ‘stakeholders’ desiderata’.

A large part of previous XAI research was mainly concerned with developing new explainability ap-

proaches without evaluating whether these methods are useful to satisfy stakeholders’ desiderata (except

 

?Generally, we speak of single stakeholders here. Since we cannot consider each stakeholder individually, we treat them as repre-

sentative members of specified stakeholder classes.
maybe the desiderata of developers) [9, 11-13]. In fact, only a minority of papers concerned with ex-
plainability approaches also evaluated the proposed methods [12, 13]. In contrast, nowadays an increasing
number of researchers strongly suggest putting human stakeholders in the center of attention when evaluat-
ing and developing explainability approaches. For instance, researchers have proposed to comprehensively
examine the perspectives of all stakeholders involved in the discussions around XAI (e.g., [14—16]) or they
have introduced evaluation methods and metrics to systematically and empirically investigate the effects of
explainability approaches on human stakeholders and their desiderata (e.g., [9, 17]).

This paper reinforces and extends the focus on human stakeholders as well as on the development and
evaluation of explainability approaches, and provides three main contributions. First, we propose that when
evaluating, adjusting, choosing, and developing explainability approaches, research needs to pay more atten-
tion to stakeholders’ specific desiderata in given contexts. This is crucial as the success of explainability ap-
proaches depends on how well they satisfy these desiderata. Current measures and metrics focus on how well
explainability approaches calibrate trust or how much they increase human-machine performance (see, e.g.,
[17]). However, these are just two of many desiderata driving XAI research and although there is research
that investigates which classes of stakeholders hold essential desiderata for XAI (e.g., [14, 15, 18, 19]), there
is a lack of research identifying, defining, and empirically investigating these desiderata, let alone research
that links them to explainability approaches suitable for their satisfaction. Our paper identifies desiderata of
different classes of stakeholders and calls for systematic empirical research investigating how explainability
approaches, through the facilitation of understanding, lead to the satisfaction of these desiderata.

Second, we emphasize the central role of understanding as a path through which explainability ap-
proaches satisfy stakeholders’ desiderata. Although research has highlighted the importance of human un-
derstanding to XAI (e.g., [17, 20]), understanding sometimes seems to be considered as just one of many
important outcomes of explainability approaches [21]. We claim that increasing human understanding is not
just one of many important effects of explainability approaches, but crucial for the satisfaction of desiderata
in general. For this reason, we introduce a model that emphasizes the critical importance of human under-
standing as a mediator between explainability approaches and the satisfaction of desiderata (for a related
model focused on user performance, see Hoffman et al. [17]).

Third, we propose that our model can be used to guide evaluating, adjusting, choosing, and develop-
ing explainability approaches. In particular, our model highlights the main concepts and their relations of
how explainability approaches are supposed to lead to the satisfaction of stakeholders’ desiderata. Clearly

defining, analyzing, and capturing these concepts, as well as clarifying their relations are central for the
systematic evaluation of the success of explainability approaches, as this helps to identify potential reasons
for why an explainability approach did not satisfy given desiderata. Similarly, considering these concepts
and their relations is crucial for the choice between different explainability approaches or for the successful
development of such approaches, because this supports the derivation of requirements for an explainability
approach that has the potential to satisfy stakeholders’ desiderata. Furthermore, our model is useful to de-
tect where input from disciplines outside of computer science (e.g., psychology, philosophy, law, sociology;
[22]) is crucial when evaluating or developing explainability approaches. Thus, our model serves to identify
interdisciplinary potential and is aimed to establish a common ground for different disciplines involved in

XAL Overall, the current paper is intended for an interdisciplinary readership interested in XAI.

1.2. A Conceptual Model of the Relation Between Explainability Approaches and Stakeholders’ Desiderata

For the purposes of this paper, we introduce a conceptual model (see Figure 1) that organizes and makes
explicit the central concepts of how explainability approaches relate to the satisfaction of stakeholders’
desiderata, as well as the relations between these concepts. The main concepts in this model are: ‘explain-
ability approach’, ‘explanatory information’, ‘stakeholders’ understanding’, ‘desiderata satisfaction’, and

‘(given) context’.

motivate and guide

Explanation Process Desigerata

feeds back

moderates
Desiderata Satisfaction
Explanatory
Information

Explainability Stakeholders’
A :
pproach Understanding

Figure 1: Our proposed model of how explainability approaches relate to the satisfaction of stakeholders’ desiderata.

 

 

   
       
   

Status of Desiderata

Satisfaction

 

 
   

  
 
 

   
     

  
  
 

   

provides facilitates affects

The overall idea of our model is that the success of an explainability approach depends on the satisfaction
of stakeholders’ desiderata (consisting of the substantial and the epistemic facet of desiderata satisfaction,
see Section 3.1). Desiderata satisfaction, thus, motivates an explanation process including explainability ap-
proaches, explanatory information, and stakeholders’ understanding. Specifically, in the explanation process
we assume that explainability approaches provide explanatory information to human stakeholders. Human
stakeholders engage with the information to facilitate their understanding of an artificial system, its func-

tioning, and outputs. As a consequence, the adjusted understanding of the stakeholders affects the extent to
which their desiderata are satisfied. The context in which the human stakeholder and the artificial system
operate and interact affects the relations between the other concepts (i.e., influences the relation between
explanatory information and stakeholder understanding as well as the relation between understanding and
desiderata satisfaction). Identifying, defining, as well as capturing and empirically examining the concepts
and their relations should guide evaluating, adjusting, choosing, and developing of explainability approaches
that aim to satisfy stakeholders’ desiderata.

With a focus on stakeholders’ desiderata, the following sections will elaborate on the model’s concepts
and their relations in more detail, as well as explicate shortcomings of the current view on these concepts
and their relations. This paper is structured as follows. We will start on the right side of Figure 1 and
will continue to work backwards from stakeholders’ desiderata. In Section 2, we will describe different
classes of stakeholders and provide examples of their pertinent desiderata. We will elaborate on the central
role of understanding for desiderata satisfaction in Section 3. Evoking understanding, in turn, requires
explanatory information, as we will illuminate in Section 4. Section 5 will shed light on the connection
between explainability approaches and explanatory information. Throughout these sections, we will point
towards interdisciplinary potential that becomes apparent with the transition from one concept to the next in
our model. In Section 6, we will, then, exemplify how our model can be used to evaluate, adjust, choose,

and develop explainability approaches.

2. Stakeholders’ Desiderata

Stakeholders’ desiderata are one, if not the, reason for the rising popularity of XAI (see also [14, 19]).
Since stakeholders in combination with their concrete desiderata motivate, guide, and affect the explanation
process depicted in Figure 1, we propose that identifying and clarifying desiderata of the various classes
of stakeholders related to artificial systems is a crucial first step when evaluating, adjusting, choosing, and

developing explainability approaches for an artificial system in a given context.

2.1. Stakeholder Classes

The need for explainability starts with the increasing societal impact of artificial systems and the fact
that many such systems still have to be operated by humans or affect human lives. This indicates that there
are various groups of people with different interests in the explainability of artificial systems: people operate

systems, try to improve them, are affected by decisions based on their output, deploy systems for everyday
tasks, and set the regulatory frame for their use. These people are commonly called stakeholders.?

Previous research has discussed varying classes of stakeholders in the context of XAI. For instance,
Preece et al. [18] distinguish between four main classes of stakeholders: developers, theorists, ethicists,
and users. Arrieta et al. [14] categorize the main classes of stakeholders into domain experts/users, data
scientists/developers/product owners, users affected by model decisions, managers/executive board mem-
bers, and regulatory entities (see also [15, 19, 24]). We follow Arrieta et al. and distinguish five classes of

stakeholders: users, (system) developers, affected parties, deployers, and regulators (see Figure 2).

Regulator

   

Deployer

Developer Affected Parties

Figure 2: The different classes of stakeholders associated with artificial systems and their relations.

Clearly, one person can be a member of several stakeholder classes. A user, for instance, can be affected
by the outputs of the system she operates. Additionally, these are just prototypical classes of stakeholders and
more fine-grained distinctions into sub-classes of stakeholders are possible [15]. For example, there is not
one prototypical developer, but developers differ in their expertise and in other factors (e.g., personality). A
novice developer may have different desiderata than an expert. In a similar way, a lay user’s desiderata might
differ from those of an expert user (more on this in Section 4.2). Moreover, we want to emphasize that this
list of stakeholders is not necessarily exhaustive because our distinction is based on previous research that

mainly comes from a computer scientific background. Thus, it might neglect other classes of stakeholders.

2.2. Exemplary Stakeholders’ Desiderata

The desiderata arising from the five classes of stakeholders are diverse. Based on a term search, we
identified more than 100 peer-reviewed journal and conference publications that postulate XAI as being

indispensable when it comes to satisfying the different desiderata (see Table 1).

 

3 According to the Merriam-Webstar dictionary, a stakeholder is, among others, someone “who is involved in or affected by a course

of action” [23]. For this reason, we use this as a general term, but refer to specific stakeholder classes where appropriate.
“QueOyTUsIs
JOU seM UOTORIsNes S,LUMeJapIsap eB Uo YoRosdde Arypiqeurerdxa ue Jo iayja ay) “S-a) aouapraa yeordiua aarsnpouoour Juasaid iy) asou) pure ‘(s]oayJa OU aIe AIA] SIIYIO JO] svarayM ‘UONIvysTIeS
S ,lUNJeIapIsap UAAIs B UO s]dajJa aansod aye aray) sayovordde Ayyiqeurejdxa awios oj 3a) aouapraa [Boda paxiw apraoid yey] aso) “(uOTIIeJsNes s,LuNIeJaprsap B paloayye yovoidde Ayrypiqeureydxa
ue “3'3) SOUSPIAS [BOLD MOYs Tey} BSOU] ‘SLUTETD ISU) JO UOTSNSIAUT [BOLIIGUS OU SpLAold JY] SSO OIUT SdINOS BY) AJISSE[D BAA “IBIIPISSp Isay} JO UOTORJsHES BY) 0} 9INGLNUOS ued sjndjno pue

sSurpuy sit pur (sayovoidde Aqypiqeureydxa uo “3'a) yoreasal parejai-TVX Jey) Moys Jo ‘asodoud ‘wey Jey] saomos pure ‘eIVJapIsap asay) Surproy siappoysyeis ‘eIeJapisap Jo ist] ATe;dwoxa uy 2] aqeL

 

[rll

‘T6 ‘ZB ‘OS “TE IZ]

[te]

[er]

[zg]

[ag ‘99 ‘79 ‘9S ‘SS ‘vr ‘er ‘EE ‘TE OT ‘OL ‘HI ‘LI Jedojaaag op 01 pesoddns st 11 yey seop ureisfs oy] Joey elenpRae 0] e[qe og UOTIROYLISA,

[sv ‘1¢] [iz ‘€9 ‘ep ‘9¢ ‘ve ‘e1] Jes) suiaisAs [nyash sAREy ssauynjasy

LOLI ‘801 “POL ‘8z ‘£9 ‘Z9 ‘LS ‘Eh ‘VE “EE ‘BZ ‘TZ ‘EL] Jes] suraysXs aqusn oaup Aes)

[Sol ‘POl ‘6 ‘16 ‘Zz “pe ‘69 ‘09 ‘Ir ‘BE ‘Ee ‘ZI ‘Z) JoR[Nsey, SSOUTYLOMISILY] S,We]SAS OY] @SBAIOUI PUR ssessy SSAUTYIO MISH,
[e11-901
“COL ‘OOL ‘86 “O6-€6 ‘16 ‘88-98 “E8 ‘18 “6L-LL ‘SL-69 “29 “€9 ‘79 ‘6S

[gr ‘ov ge] [6¢ ‘og] | —Ss ‘ES ‘1S ‘Gb ‘Le—-pr ‘Zh ‘LE-ZE ‘6T-ST ‘7 “OZ “OT ‘ST “EL ‘ZI ‘L-S) Jakoideq ‘Test wesXs ay) ut ist) oyeudosdde ayeiqueg qsnyL,
[801 ‘LOI “P6 ‘26 ‘O8 ‘BL

Los ‘ev ‘le ‘8Z] | ‘sé pL ‘29 ‘€9 ‘79 ‘LS “SS “Gr ‘Ob ‘Pr ‘oE ‘LE ‘VE ‘CE 67 OZ ‘Sz “OI ‘EI] Josey stualshs juaredsuey oavy Agueredsuesy,

[ps ‘ze ‘v1] sedojeseq S]X@]UO JAY]O O] e[quIaJsuBN] [epOUL peta] s,talsXs B exRIAL Aiqureysuesy,

[ool ‘sz ‘Lg ‘ss] uv Ajundes s,walsXs B esveloUl puy ssessy Ayunseg

[901 ‘GOI ‘26 ‘28 ‘OL ‘29 “99 ‘£9 ‘09 ‘Lr ‘vr ‘LE ‘SE “PL ‘ZI ‘L‘S] Jes] walscs oY] WoL SYSISUT SY NUATOS UTED eoUaIog

[ZOl ‘v6 ‘62 ‘82 ‘Sr ‘Er ‘OE “OE “EE “BZ “PZ ‘ST “EL ‘S] Jes] sutaysAs SULAJstes CAREY uonoyysneg

[sol ‘gz ‘vz ‘Oz ‘69 ‘8S ‘rr] Jes ‘exojdeq Ajayes S,We]SAS 8 eSBAIOUI pue ssessy Ayeyes

(uon
[sol ‘ss ‘vi ‘s] Jedojesoq | -e[ndiueur [euesieapy jsurese “32) ssoujsnqol s,welsks ¥ aseeJOUI pue ssessy ssaujsnqoy

Ayyiqisuodsas peateo

[go] [pol ‘09 ‘zg ‘9 ‘er ‘Oz ‘EI ‘9] Joy[nsey | -sed esvesour 0} Jo efqisuodses ureules suvuINy Je] 0] suveut oyeLidoiddy eptaoig Ayyiqisuodsay
[86 ‘82 ‘91 ‘rt] Jest) seonovid Aovaud s,wesXs B esvelOUl puy ssessy AoBAUg
[eat ‘r¢] [ZO1 ‘O01 “96 ‘16 “re ‘Zz ‘99 ‘1S ‘OS ‘Bh ‘Er ‘OP ‘BE ‘9E ‘ee ‘ZE ‘OZ ‘ST] dadopanagq wajsXs ¥ Jo courunoyied ay) esveioul pue ssessy aouvuLlopieg
[gz ‘69 ‘09 ‘RS ‘ZS ‘SS ‘ES ‘LP ‘PP ‘LE “Pe ‘67 ‘OI ‘VI ‘ZI ‘9) JOWP[NSey ‘perayy SPIepur]s [BoIYJe PUB [RIOUL YIM eouvI[dur0s s,wialsXs B esveIOUI puB ssessy SOTYTAY AI [RIOW]
[ZO1-O0I ‘86 ‘16 ‘28 “18 ‘9L-VL ‘OL-89 ‘99 ‘€9 ‘79
“gS “OS “SS ‘Lb ‘Hh ‘SE “PE ‘OE “6% “LZ “97 “PZ ‘TZ ‘IZ “OI ‘SI ‘ZI ‘LE ‘S) rekoydeq wesfs ¥ Jo souvl[duios [ese] ey] esveJoUI puw ssessy | souvtduiog [eseT
[es ‘9g ‘s1] | soiensey ‘paraysy SUOISIZED S,tlasAs B SUILIEDUOD ]UASUOD PAULIOJUT Jey] ears O] suBUINY e[quugq | JUesuOD peuLIOJUT

[001-96 ‘28 ‘98 ‘18 ‘BL ‘OL 69

[39 ‘ev ‘6z] ‘LO “V9 *€9 ‘09 ‘RE ‘OS ‘SS ‘ES “IS ‘Hh “OP ‘RE ‘ZE “PT ‘ZZ “OL “VI-TI ‘8] Jolmpnsey ‘peasy SSaLLIT¥y ([BM9¥) s,WalsXs ¥ esveloUl Puy ssassy SSOUITey
[te] [s6-€6 ‘SZ ‘PL ‘Ss “6 ‘8z ‘E1] Jes ‘edojesoq, wesXs 8 YIM Apuatoyse YOM fAUaTOYJe S,Ue]SAS B OSBAIOUI PUL Ssessy AQuatoOysa

[or ‘Le ‘gz] [S6-€6 ‘SZ ‘LL ‘PL ‘LO ‘S9 ‘Gr ‘SP ‘Tr ‘6E “OE “PE ‘97 ‘EI] Jasc ‘tedojaaoq weisks B YIM AT@ATDaJe YIOM ‘ssousatseyje S,WalsXs B OSveIOUI PUB ssessy SSQUaATIOAISA

[Z6 ‘16 ‘88 ‘O8 ‘PL ‘6r ‘6E “OE ‘EE ‘LE ‘PZ ‘EI ‘91 Jes) seyuernoed s,wesXs pue ulelsXs 8 asn 0} Moy Wee] uoneonpy

[o6 “68 ‘Z8] | [88-8 *pZ ‘29 P9-19 ‘8S ‘9S ‘1S ‘BP ‘LP ‘py ‘ee ‘O ‘Sz ‘OI ‘EL ‘ZI ‘2 ‘S) tedojeseq, sang pur sole xy pue Ajnuepl Aqedngeq

[ev] [6¢] [pz ‘09 ‘96 ‘6b ‘rr ZZ ‘EL ‘ZI 9) Jes) walsks ¥ SUILI9UOD [ON UOS UBUENY (ale[dwo9) UeIEY ATGeTONUOD
[I8-Lz ‘pL ‘€L ‘69 ‘8S ‘LS ‘Gr ‘Br ‘6E ‘BE ‘OEE ‘TE ‘BZ ‘9I-VI ‘S] yes) welsXs ¥ ZUISN Lay JUepyuod suBUINY syZPA] aouepyuod

[9i-rz ‘09 ‘iS ‘9S ‘ES ‘pr SST] 43s walsks B YIM Sunoee]UT ueyM AWLOUOINE Jey] UTeIer 0] suBUINY e[quUg Auiouoiny

[2-69 ‘Ss ‘IS ‘Or ‘Be ‘EE ‘Sz ‘s] Jedopeseq Agvinsoe eatjoipald s,waysXs B esvelOUl puy ssessy Ageinaay

[g9 ‘er] [L9-I16 ‘6% ‘ZZ ‘OZ ‘91 “ST ‘2) JoR[Nsey, Q[quIUNODIV SI OYM aUTULIE]ap 0] suveut eeudoidde apracig Ayyiqeiunosoy

[os] [6r] [sp-sz ‘Sl ‘EL ‘s] | solensey ‘sehoydeq suashs Jo eounjdasov asorduy eourldesoy

 

eats ouoouy ‘dur

 

pexy] durg | eoueprag ‘dur uonesnseauy (durg) jourdurg nou, Jepjousyzig uondusseq eaneiuay, unjesapiseq

 

 

 

 

 

 
Table 1 presents exemplary desiderata that we have extracted from this literature review. Each row con-
tains a desideratum, partnered with sources that claim, propose, or show that XAI-related research (e.g., on
explainability approaches) and its findings and outputs can contribute to the satisfaction of this desideratum.
Furthermore, this table presents stakeholder classes that may be most prone to have one or more of these
desiderata. Whenever we could not extract stakeholder classes from the respective papers, we did our own
(mostly common-sense) mappings. Notably, most of the sources we present in this table only claim that
XAlI-related research can contribute to satisfy the respective desiderata with only a subset of these papers
(for instance, [28, 31, 45]) providing empirical evidence for their claims (e.g., regarding the mapping of
desiderata to stakeholder classes or regarding the relation of explanatory information and desiderata). In
what follows, we present two important exemplary desiderata for each class of stakeholders.

Users. Most papers concerning stakeholders in XAI have this class of stakeholders in common (see, e.g.,
[14, 18, 19]). Among others, users take into account recommendations of artificial systems to make decisions
[24]. Some prototypical members of this stakeholder class are medical doctors, loan officers, judges, or
hiring managers. Usually, users are no experts regarding the technical details and the functioning of the
systems they use. However, they can work more effectively if they form adequate expectations concerning
the systems’ functioning. In case they cannot do so, and in cases where their expectations are violated, they
need information that goes beyond the knowledge of purely operating the system. This motivates at least the
following two central desiderata of users: usability [22, 36] and trust (29, 96, 115].

In many cases, a system is more usable if it offers meaningful information alongside its outputs. This
information can help users to adequately link their knowledge and assessment of a given situation to the
information used by a system, can help them to make decisions more quickly, or to increase decision quality
[116]. All of this can contribute to the usefulness (another important desideratum of users) of a system and
is important in high-stakes scenarios where a user decides on the basis of a system’s recommendations.

This is closely linked to the desideratum of adequately calibrating trust in systems. Both undertrust
and overtrust can negatively affect the appropriate use of systems [117]. In the case of undertrust, users
may constantly try to supervise a system’s behavior or even attempt to intervene in a system’s processes,
thereby undermining the effectiveness of the human-system interaction [118]. In the case of overtrust, people
may use a system without questioning its behavior [118-120]. This can again decrease the effectiveness
of the human-system interaction, as humans rely on the system’s outputs even in situations where they
should challenge them [117, 121]. Explainability approaches have the potential to provide means to let users

adequately calibrate their trust in artificial systems [17].
Developers. Individuals who design, program, and build artificial systems are the developers. Naturally,
they count as a class of stakeholders, as without them the systems would not exist in the first place. Generally,
developers have a high expertise concerning the systems and an interest in improving them.

An especially important desideratum of developers is verification, that is, to check whether a system
works as intended [7, 122-124]. There are many ML-based classifiers that consider, for instance, irrelevant
inputs as relevant (see, e.g., [125, 126]). Increasing insights into the system’s decision-making processes by
using certain explainability approaches can help developers to recognize and correct such mistakes. Accord-
ingly, there are cases where XAI contributes to determine whether a system works as intended and, thus,
explainability approaches can support verification of the system.

Another important desideratum for developers is performance. There are many ways in which a system
can achieve a better performance. For example, the predictive accuracy of an ML algorithm can be seen
as a performance measure. Although there are some claims that explainability and accuracy are difficult
to combine [40, 44], there is also the opposite view, which sees XAI as a way to actually make systems
more accurate and, in particular, to help developers estimate system accuracy [70, 73]. By means of getting
information of what led to a system’s outcomes, developers can detect underrepresented or erroneous training
data and, thus, fine-tune the learning process to achieve higher accuracy. Another way in which performance
can be understood is user-system interaction. The better users can interact with a system, the better they,
the system, and the combination of user and system perform. To this end, insights about a system, its
functioning, and its outputs are a fruitful way to improve user-system performance [14, 17, 48].

Affected Parties. The influence of artificial systems is constantly growing and decisions about people are
increasingly automated — often without their knowing. Affected parties are such (groups of) people in the
scope of a system’s impact. They are stakeholders, as for them much hinges on the decision of an artificial
system. Patients, job or loan applicants, or defendants at court are typical examples of this class.

Crucial desiderata of affected parties are fairness [7, 9, 11, 122] and morality/ethics [6, 11, 127]. These
desiderata are closely related. If a system is fair, for instance, the influence of protected attributes (e.g.,
gender or ethnicity) is adequately limited or controlled in the systems’ decision-making processes. In the
case of ethical systems, their decision-making processes rely on morally permissible considerations (e.g.,
according to certain moral theories, an autonomous car in a dilemma situation should never let affected
parties’ age contribute to its decision-making process, see [128]).

Considerations of fairness and ethics have evolved because there is an increasing number of affected

parties. This can lead to discrimination of individuals (e.g., concerning the distribution of jobs, loans, or
healthcare), not on the basis of their own actions or characteristics but on the basis of actions or character-
istics of social groups to which they belong (e.g., women, ethnic minorities, older people) [54]. One hope
of establishing automated decision-processes was to make decisions less prone to human bias [129]. How-
ever, it is commonly acknowledged that artificial systems can reproduce and, in this process, even intensify
human biases (see, e.g., [54] and [130]). To counteract biases, it is, therefore, crucial to enable their detec-
tion. Explainability approaches may aid in this regard by providing means to track down factors that may
have contributed to unfair and unethical decision-making processes and either to eliminate such factors, to
mitigate them, or at least to be aware of them.

Deployers. People who decide where to employ certain systems (e.g, a hospital manger decides to
implement a diagnosis system in her hospital) are deployers. We count them as another class of stakeholders
because their decisions influence many other classes of stakeholders. For example, users have to work with
the deployed systems and, consequently, new people fall inside of the range of affected parties.

Deployers want the systems they bring into use to be accepted [5, 11, 96, 131]. In the eyes of deployers,
the worst case in terms of acceptance is that users reject appropriately working systems so that the systems
will end up never being used [132]. Therefore, low acceptance undermines what deployers intend to achieve
when providing systems to users. Previous research claims that explainability approaches can aid in this case
by providing people with more insights into systems, which can improve their acceptance [11, 115, 131].

Another desideratum of deployers is the system’s legal compliance. As deployers bear a certain degree of
responsibility for systems they bring into use, they have to ensure that these systems comply with legislation.
Non-discrimination and safety of a system are two important factors for its legal compliance. Explainability
approaches promise to enable deployers and other stakeholders to check whether the system is indeed safe
and non-discriminatory. Moreover, the European General Data Protection Regulation (GDPR) and the often
discussed Right to Explanation [101] (arguably) explicitly require explanations.

Regulators. Finally, there are regulators stipulating legal and ethical norms for the general use, deploy-
ment, and development of systems. This class of stakeholders occupies a somewhat extraordinary role, since
they have a ‘watchdog’ function not only with regard to systems, but to the whole interaction process of
systems and the other stakeholder classes. This class consists of ethicists, lawyers, and politicians, who
must have the know-how to assess, control, and regulate the whole process of using artificial systems.

Regulators call, for instance, for trustworthy systems [5, 6, 10, 11, 29, 96, 115, 131]. However, the
concept of trustworthiness is still only vaguely defined [133]. For example, the High Level Expert group

on Artificial Intelligence (HLEGAI) initiated by the European Commission does not provide a common

10
definition for trustworthiness, but it only proposes that trustworthy systems have three properties: they are
lawful, ethical, and robust [10]. Without examining trustworthiness more closely, the HLEGAI emphasizes
the significance of trustworthy artificial systems by stating that the trustworthiness of systems is imperative
for the realization of potentially vast social and economic benefits. Regulators such as the EU, as well as
previous research on artificial intelligence that calls for trustworthy systems (e.g., as described in [53]), agree
that explainability approaches are one central way to facilitate the trustworthiness of systems [10, 53].
Accountability is another important desideratum of regulators [20, 56]. Accountability is about being
able to identify who is blamable or culpable for a mistake. With increasing use of artificial systems, ac-
countability gaps might emerge [134, 135]. For instance, when the use of an artificial system harms a
person, it may not be clear who is accountable, as there are many parties that may have contributed to the
harm. Opaque artificial systems only amplify this issue. For example, a person acting on the outputs of a
system may not (be able to) know that this output was erroneous, so blaming her for ensuing problems might
inadequately ignore the contribution of the artificial systems. Overall, regulators want to avoid situations in
which existing legislation is hard to apply or where no one is (or feels) accountable for a mistake. In such
cases, explainability approaches may restore accountability by making errors and causes for unfavorable

outcomes detectable and attributable to the involved parties.

2.3. Interdisciplinary Potential

Artificial systems will continue to influence humans in every part of their lives, thus it is likely that
new desiderata will emerge. Further desiderata might evolve from societal, legal, political, philosophical,
or psychological needs regarding artificial systems (e.g., for competence, relatedness, or autonomy; [136]).
For example, with artificial systems in healthcare [137] there is a pressing need for formulating relevant
ethical and legal desiderata. In addition, it is also possible that explanatory information provided by artificial
systems does not only aim to improve task achievement but also to entertain users [138].

We conducted a literature review to derive an overview of stakeholders’ desiderata, but it will clearly
be possible to extend our list in Table 1. In fact, further developing and refining this list of desiderata is
an important point that reveals interdisciplinary potential. First, most of the sources referred to in Table 1
only claim that these desiderata are relevant for stakeholders. There needs to be a more thorough empirical
investigation, probably done by interdisciplinary teams of psychologists, philosophers, and scholars from
law to show the actual importance of these desiderata for certain stakeholder classes.

Furthermore, in our overview, the desiderata’s denotations stem (in most cases) directly from the source

11
papers. However, some of these desiderata are closely related and, especially given the interdisciplinary
research contributing to XAI, it is plausible that different authors actually mean to refer to the same desider-
atum but give it a different term or use the same term to refer to different desiderata. Consistent terminology
and conceptual clarity for the desiderata are pivotal and there is a need to explicate the various desiderata
more precisely. Different disciplines like law, philosophy, and psychology need to come together to discuss
their conceptions of various desiderata to agree on common definitions of these desiderata. Without this,
insights from different disciplines regarding the respective desiderata (e.g., what kind of explanatory infor-
mation is required to satisfy a given desideratum) might not be adequately integrated into a common stream
of research.

Additionally, we need research that more explicitly analyzes society’s stakeholder classes affected by
artificial systems. For instance, collaborating with sociologists could offer a broader or more nuanced picture
of the classes of stakeholders that have to be considered within the scope of XAI. In any case, in order to
comprehensively address the stakeholders’ desiderata, we need a more detailed understanding of stakeholder
classes and sub-classes. For this, it is promising to consult disciplines outside of computer science focusing
on society (i.e., sociology) as well as individual differences within groups of society (i.e., psychology).

Furthermore, researchers from different disciplines may be able to take the perspective of certain stake-
holder classes. By doing so, they can help to refine the list of desiderata. For instance, computer scientists
can take the perspective of developers. Psychologists can take the perspective of users and affected people.
Management scholars could take the perspective of deployers. Philosophers, political scientists, as well as
researchers from law can take the perspective of regulators.

Working together in interdisciplinary teams can, thus, contribute to a comprehensive consideration of
important desiderata in a given context. However, comprehensiveness is just one side of the coin, the jus-
tification of desiderata is another. Concerning this justification, there are two main perspectives: one from
ethics and one from jurisprudence. From an ethical perspective, we can judge whether a desideratum is
compatible with some, many, or even all established moral theories. Similarly, legislation can be consulted
to assess whether there are laws demanding (or prohibiting) to meet certain desiderata. When engaging in
thorough moral and legal justification, we might conclude that there will be desiderata that are not justifi-
able. In high-stakes decisions, for instance, each individual user might want systems to do what is best for
her. In the case of autonomous cars, drivers will probably want a car to decide in a way that makes it more
likely that they will survive if the car faces an imminent accident [139]. There might be cases where such

a decision is neither morally nor legally justifiable. In a less drastic example, users may ask for an expla-

12
nation of why they received a low score on a personnel selection test. However, providing this explanation
might render the given test obsolete because the explanation possibly enables participants to game the test
[8]. We suggest that the given context, as well as moral and legal considerations are decisive factors when

determining whether certain stakeholders’ desiderata can be justified.

3. Desiderata Satisfaction requires Understanding

In the previous section, we have introduced our claim that the need for XAI arises from stakeholders’
desiderata. More precisely, the need arises in cases where certain stakeholders’ desiderata are not (suffi-
ciently) satisfied [7, 14, 15, 18, 53]. For this reason, we have to take a look at what it means for a desideratum

to be satisfied.

3.1. Facets of Desiderata Satisfaction

We propose that the satisfaction of each desideratum can take two facets. We call these facets epistemic
and substantial desiderata satisfaction, respectively. On the one hand, stakeholders want systems to have
certain properties that make them actually fair, transparent, or usable. In line with this, a desideratum
(e.g., fairness) is substantially satisfied if a system sufficiently possesses the corresponding properties. On
the other hand, stakeholders want to know or be able to assess whether a system (substantially) satisfies
a certain desideratum (i.e., whether the system has the required properties). So, the epistemic facet of the
fairness desideratum is satisfied for a stakeholder, if she is in a position to assess or know whether and to what
extent the system is fair. Naturally, for XAI the epistemic facet is the most important one, since explanatory
information can contribute to the satisfaction of the epistemic facet of every desideratum, whereas this is not
the case for the substantial facet.

As an example, take the desideratum of having usable systems. A successful explanation process as
depicted in our model may enable users to recognize whether a system is usable, and, optimally, also increase
the system’s usability to a certain degree. In this case, the epistemic satisfaction consists in the stakeholders
being able to check whether a system or its outputs are usable for the task at hand. To a lesser extent,
however, an explanation process can also contribute to the substantial satisfaction of the desideratum, since
it provides additional knowledge about the system that makes it more usable for the stakeholder. For larger
deficits in usability to be addressed, however, explanatory information might not directly help; for this, the

entire artificial system may need to be redesigned.

13
Depending on the desideratum, the two facets are correlated to a certain degree (possibly even com-
pletely, when satisfying the epistemic facet also completely satisfies the substantial facet). To illustrate,
consider the desideratum of retaining user autonomy in human-in-the-loop scenarios. Let us assume that an
explanation process has helped to satisfy the epistemic facet of this desideratum to a certain degree, as it has
enabled the user to assess the extent to which she can retain her autonomy in making decisions based on the
recommendations of the system. Additionally, the more understanding a user has about a system’s output,
the more autonomous she can decide based on it. Thus, the explanation process has helped to satisfy the
epistemic and the substantial facet of this desideratum. Hence, in this case, the substantial and the epistemic
facet of desiderata satisfaction are highly correlated.

Now, consider the desideratum that systems adhere to certain ethical principles. When having sufficient
information about a system, regulators can evaluate whether this system complies with ethical standards.
Again, the explanation process serves to satisfy the epistemic facet of this desideratum. However, this
does not directly make the system’s processes and outputs more likely to comply with ethical standards.
Consequently, explanation processes can at most indirectly satisfy the substantial facet of this desideratum:
based on the understanding obtained by the explanation process, faults can be identified and steps to improve
systems regarding their ethical properties can be initiated. In this case, the epistemic and the substantial facet
of desiderata satisfaction are only loosely correlated.

On the one hand, the distinction of these two facets shows that explanation processes can contribute to the
satisfaction of all epistemic facets of desiderata concerning artificial systems. On the other hand, it shows
that an explanation process alone does sometimes not suffice to satisfy the substantial facet of desiderata
concerning artificial systems. In many cases, however, the epistemic satisfaction enables the substantial one.
This means that even if a better understanding of the systems triggered by explanatory information does
not always directly lead to the substantial satisfaction of the desiderata, it can form the necessary basis for
achieving it. As the epistemic satisfaction of a desideratum is closely linked to a better understanding of a

system, understanding is the pivotal point for all endeavors of satisfying desiderata.

3.2. Understanding

Throughout the history of XAI research, authors have highlighted the central importance of understand-
ing in XAI (e.g., [2, 20, 115, 140-142]). The overall goal of XAT is to advance human understanding of
artificial systems in order to satisfy a given desideratum. There is an ongoing debate in the philosophical

literature about what constitutes understanding [143-145], and a comprehensive review of this concept is

14
beyond the scope of the current paper (see, for instance, [144] for a review on understanding, [146, 147]
for papers on the concept of understanding, [138, 148, 149] for the relation between explanations and un-
derstanding, or [150] for the related topic of cognitive processes in knowledge acquisition; furthermore, see
[151] for a broad overview on the theoretical basics of understanding relevant for XAI research). Some
aspects of understanding, however, are typically agreed upon: There are different depths and breadths of
understanding (in the following, we will use the term degree of understanding to address depth and breadth
of understanding, similar to [144, 152]), and there are different kinds of understanding [20, 146].

For the evaluation of explainability approaches it will, thus, be crucial to determine stakeholders’ under-
standing of artificial systems. Examining understanding of software has a long history in human-computer
interaction and education [108, 153]. Typically, when referring to the understanding of an artificial system,
these disciplines also refer to users’ mental models of systems. A mental model of a system can be un-
derstood as a mental representation of this system and its functioning. According to Rouse et al., mental
models allow humans “to generate descriptions of system purpose and form, explanations of system func-
tioning and observed system states, and predictions of future states” [154]. In other words, mental models
allow humans to mentally simulate aspects of a system, for instance, in order to understand the causes of its
decision-making [17].

Translated to the case of artificial systems, this means that we could examine understanding by investi-
gating stakeholders’ mental models of a system [17, 108]: how well does a person’s mental model mirror the
actual system? Are there gaps in the current understanding of a system’s functioning? Are people overcon-
fident that they understand how a system works (when they actually only have an illusion of understanding)
[155]? Are there learned misconceptions about systems and their outputs that need to be revised [156]? For
example, it is possible to investigate a stakeholder’s mental model through think-aloud techniques where
stakeholders are tasked to describe systems and their inner workings, or by letting stakeholders draw their
mental model of a given system (for an overview of methods to elicit mental models, see [17]). Similarly, it
is possible to measure understanding by capturing what humans’ mental models enable them to do in relation
to a system and its outputs. For example, Kulesza et al. [157] used an explanation task to assess whether
participants understood what kinds of information are used to predict outcomes. Other studies used predic-
tion tasks to assess, for instance, whether participants can anticipate which predictive model would produce
better outcomes [115], whether participants can predict what outcome a predictive model would produce for
a person with a given profile [21], or whether participants can foresee the influence of a given feature on

an outcome [21]. Another possibility would be to use manipulation tasks in order to assess whether people

15
understood what kind of information might add to the predictive accuracy of a model (e.g., like in [115]).
Further possible tasks might be perception tasks (e.g., naming of recognized characteristics of a model) or
imagination tasks (e.g., estimating what a model would predict for a given input), all of which would reflect
different degrees of understanding. Furthermore, all of these tasks can reveal misconceptions of a system’s
functioning or knowledge gaps that need to be adjusted or filled with additional or alternative explanatory
information. Additionally, what all of these ways to capture stakeholder understanding have in common is
that they might help us to examine whether a given desideratum has the potential to be satisfied. For exam-
ple, if a developer has understood a system in a way that she can imagine situations under which a system
might fail, her ability to make the system more robust most likely increases.

Furthermore, there is an initial degree to which stakeholder understand artificial systems. Specifically,
stakeholders without any prior experience with a given system will likely start with a degree of understanding
that corresponds to their (background) knowledge of artificial systems or arose from initial instructions they
have received regarding the system [17, 158]. Thus, they will have an incomplete or even faulty mental model
of the given system [20]. For instance, a stakeholder might know (or might be informed) that machine-
learning based systems are usually trained on historical data in order to predict new data. This degree
of understanding can, then, be augmented (e.g., with explanatory information generated from explainability
approaches). With a higher degree of understanding (and, consequently, a more detailed and accurate mental
model of a system), a stakeholder might understand what kind of training data underlie a given system, what
kind of algorithm is used for a given system, or what kind of output data a system produces [17, 63, 159].
Thus, with increasing degrees of understanding, stakeholders will be able to assess whether a given system
has desired characteristics and adequate processes, or produces expected outcomes. In other words, an
increasing degree of understanding will satisfy the epistemic facet of more and more desiderata. For the
satisfaction of the desideratum’s substantial facet, however, the opposite might sometimes be the case, as we

will discuss in the next section.

3.3. The Relation Between Understanding and Desiderata Satisfaction

In certain cases, a stakeholder’s degree of understanding and the extent of desiderata satisfaction are
positively correlated. For instance, if the desideratum is to retain autonomy in interaction with a system,
usually a higher degree of understanding satisfies the epistemic and substantial facets of this desideratum to a
greater extent. However, there are also more complex cases. Assume that the desideratum is to trust a certain

system. Acquiring a higher degree of understanding will increase a stakeholder’s epistemic satisfaction of

16
this desideratum (i.e. she can better assess whether and to what extent to trust the system), but the substantial
facet (i.e., her actual trust) can be influenced in a negative way. When a stakeholder still possesses a low
degree of understanding, she is likely to be unaware of problematic features a system has in certain contexts
(e.g., in complex environments) or with certain kinds of input data (e.g., noisy inputs). So, with a low degree
of understanding, a stakeholder is likely to trust a system (although inadequately) [29, 38]. In contrast, with
a higher degree of understanding, the stakeholder is able to recognize or even explain the conditions under
which a system will tend to fail. Therefore, she is more aware of the system’s problematic features, and this
may, consequently, decrease her trust in it [21, 38].

Additionally, it can happen that understanding contributes to the satisfaction of a single desideratum of a
stakeholder to a greater extent, while the satisfaction of other desiderata for the same stakeholder suffers [63].
To illustrate, take the trade-off between transparency and non-gameability of systems. Deployers of systems
want to comply with legislation and, consequently, want their systems to be transparent. Understanding is
a necessary condition for perceived transparency. However, making systems more transparent can diminish
another desideratum of deployers, the systems’ non-gameability [8]. In other words, it should not be possible
for particular users to manipulate a system in such a way that they can systematically evoke beneficial outputs
for them. However, more transparency caused by a higher degree of understanding may enable some people
to exploit the system [63]. In a personnel selection test, for example, a better understanding of the selection
system may enable participants to game the test, preventing its proper use (i.e., selecting suitable applicants).

This points to another potential trade-off to be considered regarding the relation between an advanced
understanding and desiderata satisfaction. Felzmann and colleagues [15] argue that different stakeholders
might hold different expectations regarding the extent to which a single desideratum has to be satisfied. It
is furthermore possible that, while desiderata of one stakeholder are influenced positively by an increase
in understanding, desiderata of other stakeholders suffer. An example of such a case was described by
Langer et al. [160]. They provided additional information accompanying an automated personnel selection
system for their participants, which resulted in more perceived transparency, but at the same time reduced
acceptance of the system (for a similar finding see [161]). In such cases, it can happen that the two desiderata
of transparency and acceptance arise from different perspectives and characteristics of stakeholders. For
instance, legislation (i.e., a regulator) might call for transparency of systems, whereas a company using a

system (i.e., a deployer) desires the system to be accepted. Explaining the system will, then, lead to the

17
satisfaction of the legal desideratum, but at the price of impairing the company’s desideratum.*

These examples indicate that the degree and kind of understanding of artificial systems which explain-
ability approaches should evoke may depend on trade-offs between a variety of desiderata from a variety of
stakeholders. Consequently, the development, implementation, and use of explainability approaches should
go hand in hand with a case-by-case evaluation of the relevant stakeholders’ desiderata. While estimating
the effects of explainability approaches, it is central to investigate the perspective of not only one, but all

stakeholders who potentially have a variety of (conflicting) desiderata with regard to a given system.

3.4. Factors Influencing the Relation Between Understanding and Desiderata Satisfaction

Characteristics of the context in which artificial systems operate moderate the relation between under-
standing and desiderata satisfaction and may affect the degree and kind of understanding necessary to satisfy
a given desideratum. There is no agreed-upon definition of the term “context” [162] and a deeper dive into
the discussion about the term goes beyond what we can achieve in this paper (for discussions on this topic,
see [162, 163]). Following Dourish [163], we hold that the context is set by a given situation, in the inter-
action between a stakeholder, an artificial system, a given activity or task, and an environment. This makes
it impossible to anticipate all contextual influences that will affect the process of how explainability ap-
proaches aim to satisfy stakeholders’ desiderata without knowing the concrete situation. Nevertheless, it is
crucial to consider and anticipate these contextual influences when evaluating and developing explainability
approaches.

For instance, what is at stake in a given situation can affect the relation between understanding and
stakeholders’ desiderata. That is, whether a context is a high or low stakes scenario may determine the degree
of understanding necessary to satisfy a given desideratum. Research indicates that certain situations tend to
require a greater understanding of an event than other situations. Specifically, situations where instrumental,
relational, moral, or legal values are at stake might be more likely to require extensive understanding [164—
166]. Instrumental values are at stake when there are (personal, economical etc.) benefits or losses to expect
in a specific situation (e.g., when an artificial system handles financial transactions). Relational values are at
stake when important interpersonal relationships might be affected by an event (e.g., when artificial systems
are used for employee layoff). Moral values are at stake when moral rights might be violated (e.g., when

using artificial systems for sentencing in court). Finally, legal values are at stake when legal rights might be

 

4Note that this example also describes a situation where two desiderata of the company are in conflict: user acceptance and adhering

to legislation.

18
violated (e.g., when an artificial systems outputs conflict with the right of non-discrimination). Depending
on the concrete situation (e.g., being in an autonomous car), instrumental, relational, moral, and legal values
determine the stakes of a situation. Additionally, identifying which of these values stakeholders regard as
relevant or which values are indeed relevant in which situations may allow for drawing inferences about
whether a situation is (considered) high or low stakes. Consequently, these values serve as an orientation for
when stakeholders are more likely to demand higher degrees of understanding. Specifically, this means that
for the satisfaction of a given desideratum in a low stakes scenario, a lower degree of understanding might
be sufficient compared to high stakes scenarios.

Furthermore, contexts involving artificial systems may differ in their ‘outcome favorability’ [167, 168].
In the case of an unfavorable outcome, people are more likely to call for additional information in order
to understand the reasons for the outcome, and so to be able to better assess and control further similar
outcomes [36, 167]. On the one hand, it may be that increasing the extent of understanding in the case of
favorable outcomes has little potential to positively affect a desideratum’s satisfaction. On the other hand,
a better understanding can be central to positively affecting a desideratum’s satisfaction in the case of a
unfavorable outcome. For instance, perceived fairness is a central desideratum for many decision situations
[169]. While understanding may have negligible effects on perceived fairness under favorable outcomes,
it can improve perceived fairness under unfavorable outcomes [170]. Supporting this claim, Kizilcec [38]
found that available explanatory information only affected perceived fairness when people’s expectations
concerning an outcome were violated. There may, however, be certain conditions under which a better un-
derstanding (negatively) affects perceived fairness even when people experience favorable outcomes [166].

What is at stakes in a situation and outcome favorability are just two of many possible contextual influ-
ences on the relation between understanding and desiderata satisfaction. Further candidates are the applica-
tion context (e.g., at home, at work), time constraints [171], and social circumstances (e.g., whether there

are other people present in a given situation; [163]).

3.5. Interdisciplinary Potential

The interdisciplinary potential we see in the relation between understanding and desiderata satisfaction is
described in research questions such as: (a) How does stakeholders’ initial degree of understanding or prior
knowledge of artificial systems relate to their desiderata satisfaction? (b) What are the trade-offs between
desiderata of a single stakeholder and/or desiderata of multiple stakeholders and what are the implications

of understanding regarding these trade-offs? (c) How does the degree and kind of understanding relate to

19
the satisfaction of desiderata? (d) How do task or context affect the relation between understanding and
desiderata satisfaction in a given situation?

Scholars from educational sciences could collaborate with computer scientists in order to investigate how
to design adequate instructions to achieve a proper basic understanding or background knowledge of artificial
systems that can serve as a general basis to partially satisfy a large variety of desiderata. Furthermore, it is
necessary to involve psychologists in order to experimentally examine the relation between understanding
and desiderata satisfaction as well as influences affecting this relation. In this regard, it will be central to
determine what it means for a certain desideratum to be satisfied. This requires finding an adequate way
of measuring the satisfaction of this desideratum (e.g., using self-report measures, expert interviews, legal
analyses), as well as understanding the requirements for it to be satisfied (e.g., defining minimum legal
standards, enabling stakeholders to perform a specific task successfully). In practice, this involves having
an elaborated research design, clear conceptual definitions, appropriate operationalization and measurement
methods, and fitting research disciplines for iterative (empirical) research.

Finally, scholars from requirement engineering may help to understand relationships between several
desiderata. Based on their results, scholars from law and from philosophy can help to determine which
trade-offs are morally and/or legally justified. In general, an interdisciplinary collaboration can contribute to

being aware of potential relationships and trade-offs between certain desiderata.

4. Understanding Requires Explanatory Information

Providing explanatory information of a given phenomenon is the default procedure to facilitate its un-
derstanding [147, 172, 173]. Explanatory information helps humans to navigate complex environments by
facilitating better understanding, predictions, and control of situations [138, 174]. Such information narrows
down possible reasons for events, decreases uncertainty, corrects misconceptions, facilitates generalization
and reasoning, and enables a person to draw causal conclusions [138, 155, 174, 175].

In the context of XAI, explanatory information puts stakeholders in a position to grasp generalizable pat-
terns underlying the production of a system’s outcomes (e.g., LIME [115]). These patterns allow for drawing
inferences about (potentially causal) connections between the system’s inputs and outputs, or for narrowing
down the possible ways in which the system might have failed. Additionally, these patterns may help stake-
holders to tell apart correct but unexpected behavior from malfunctions in order to to debug the system. In
general, all of this can reduce people’s uncertainty concerning a system (e.g., uncertainty concerning how to

behave towards it, how to react to it, what to think of it, whether to recommend it, or whether to disseminate

20
it; [63]). Overall, explanatory information should lead to a better understanding, which should, in turn, posi-
tively affect the satisfaction of stakeholders’ desiderata. Depending on the explanatory information, different
degrees and kinds of understanding can be acquired [20, 152]. For this reason, we have to shed more light

on what characteristics of explanatory information are.

4.1. Characteristics of Explanatory Information

Important characteristics of explanatory information concern its kind and its presentation format. There
are various kinds of explanatory information: teleological (i.e., information that appeals to the function of the
explanandum; [176]), nomological (i.e., information that refers to laws of nature; [177]), statistical relevance
(i.e., information that is statistically relevant to the explanandum; [178, 179]), contrastive (i.e., information
that highlights why event P happened and not event Q; [6]), counterfactual (i.e., information that appeals to
hypothetical cases in which things went differently; [180]), mechanistic (i.e., information that appeals to the
mechanisms underlying a certain process; [181]), causal (i.e., information that appeals to the causes of an
event; [182, 183]), network (i.e., information that appeals to the topological properties of a network model
describing a system; [184]), and many more.

Concerning the presentation format, there are also various possibilities. Roughly, we can distinguish
between text-based and multimedia presentation [36]. A text-based presentation can be a natural language
text, a rule extracted from a rule-based system, an execution trace, or simply the program’s source code. A
multimedia presentation can include graphics, visualizations, images, and animations. For instance, heat-
maps of neural activity are a popular presentation format for explanatory information of neural networks
[14].

Aside from the very general characteristics of kinds and presentation format, there are further character-
istics of explanatory information that influence how and whether it evokes understanding: soundness (i.e.,
how accurate the information is), completeness, novelty (i.e., whether the information is new for the re-
cipient), and complexity (e.g., depending on the number of features and on the interrelation between the
features the information contains; [171]) are just some of the various examples of further characteristics of
explanatory information (for more, see [63]).

The importance of acknowledging the characteristics of explanatory information is highlighted by re-
search in cognitive and educational psychology that shows that effects of explanatory information can vary
depending on their characteristics [138, 150, 166]. Take complexity as an example: studies show that peo-

ple prefer simpler information (e.g., information mentioning fewer causes; [185]). Another example is the

21
finding that explanatory information that aligns with a stakeholder’s goals in a certain situation is preferred
[186]. Vasilyeva et al. [186] showed that people evaluate teleological explanatory information compared
to mechanistic explanatory information as more useful when asked to name the function of a phenomenon,
and, conversely, perceive mechanistic explanatory information as more useful when asked to name the cause

of an event.

4.2. Factors Influencing the Relation Between Explanatory Information and Understanding

The relation between explanatory information and understanding is influenced by a variety of charac-
teristics of the stakeholders, the context, and interactions between these characteristics [164, 166, 187].

Considering these influences is central to evaluate and develop explainability approaches.

Characteristics of Stakeholders. Since every stakeholder possesses an individual degree of understanding
of a system, an individual ability to understand, and an individual set of desiderata that are or need to be sat-
isfied to a certain extent, the characteristics of stakeholders who receive explanatory information influence
the relation between explanatory information and understanding [18]. Some of the characteristics that most
obviously influence this relation are the stakeholders’ background knowledge, beliefs, learning capacities,
and desiderata they have concerning respective systems [63, 188-190]. For instance, explanatory informa-
tion including technical details might increase an expert developer’s degree of understanding while technical
details can hamper understanding for novice developers or other (non-expert) stakeholders [175].
Furthermore, desiderata that are salient for a respective stakeholder can influence the relation between
explanatory information and understanding. Specifically, as stakeholders engage with information in order
to advance their understanding of artificial systems, their motivation and prior beliefs may affect how they
interpret a given set of information [67]. For instance, if a stakeholder’s primary desideratum is to ensure
that a system provides fair outputs, they will scrutinize explanatory information for signs of bias that might
lead to unfair outcomes. In contrast, if a stakeholder’s primary desideratum is to improve a system’s predic-
tive accuracy, they might pay special attention to information providing insights on how to improve system
performance. This indicates that, depending on the given desideratum, the same amount and kind of infor-
mation can lead to different degrees and kinds of understanding. Consequently, it is important to provide the
appropriate information for the given purpose. These assumptions are supported by research proposing that
human reasoning processes can be strongly influenced by explanatory information. Such information has
the potential to improve human decision-making but may also hamper it (e.g., when explanatory information

attenuates human biases or when humans need to invest too much effort to use the information [36, 67]).

22
The list of individual differences influencing the relation between explanatory information and under-
standing goes beyond the scope of this article and covers personality traits such as conscientiousness [191],
need for cognition’ [192, 193], and need for closure [194, 195], as well as people’s preferences for detailed

versus coarse explanations [196], or age differences between stakeholders [197].

Characteristics of the Context. Time pressure can be a relevant contextual influence [63, 171]. For a stake-
holder under high time pressure, the same explanatory information may lead to less understanding as com-
pared to situations where she is under low time pressure [198]. Workload is another contextual influence
[199]. In situations of high perceived workload, the same explanatory information can lead to a different
degree of understanding compared to low workload conditions. Similar things are true for situations where
it is more likely that stakeholders will experience higher levels of stress [200, 201] (e.g., high stake and high
risk situations, multitasking environments; [171, 202]).

In general, depending on the situation and task at hand, the effects of explanatory information on un-
derstanding may differ. Therefore, it is important to investigate the contextual conditions of a stakeholder’s
interaction with an artificial system in detail when theorizing about how explanatory information can best
improve understanding. Given the impact of the context and given the fact that it might not be possible
to anticipate all relevant contextual influences [163], it is especially important to assess the effects of ex-
planatory information in laboratory and in field settings [9, 63]. Although it has advantages to investigate
the effects of, for instance, stress on the relation between explanatory information and understanding in con-
trolled laboratory settings, such results may not translate to field settings. This finding is in line with calls for
experiments involving human participants in proxy tasks in order to show that given explanatory information
not only elicits understanding when simulating a context, but also under real world conditions (that is, to
investigate whether given explanatory information is equally valuable in the wild) [9, 63]. Although it will
be impossible to fully anticipate how the context will alter the relation between explanatory information and
understanding (e.g., because the interpretation of contextual influences depends on the relevant stakeholders
[162]), at least it is crucial to be aware that contextual influences may be central for the success or failure of

explanatory information.

 

5Need for cognition is a personality trait that distinguishes people who like to put effort into cognitive activities from those who

prefer less cognitively demanding activities.

23
Interactions Between Characteristics of Explanatory Information, Stakeholders, and the Context. Finally,
characteristics of explanatory information, stakeholders, and the context can interact in ways that affect how
stakeholders engage with explanatory information to advance their understanding of artificial systems. For
instance, the level of detail of explanatory information can interact with the prior knowledge of a stakeholder.
Whereas expert developers’ understanding may benefit to a higher degree from explanatory information with
much detail, this level of detail can have the contrary effect for novice developers [36, 166, 203, 204]. At
the same time, when novice users want to learn how to use a system, they might want detailed explanatory
information whereas when expert users want to use the system for task fulfillment, every unnecessary piece
of information could lead to the rejection of the system [36]. Further, if the context changes, the relations be-
tween explanatory information and understanding are prone to change as well. For instance, as soon as there
is time pressure in the aforementioned situations, it is plausible that neither expert nor novice developers or

users pragmatically benefit from too detailed explanatory information.

4.3. Interdisciplinary Potential

The following research questions reinforce calls for extensive validation and experimental studies in-
vestigating the effects of explainability approaches in different contexts and in relation to different stake-
holders [9, 63] (we refer readers to Sokol et al. [63] for a description of further important characteristics
of explanations, stakeholders, and contexts that affect the relations between explanations and stakeholders’
understanding): (a) How should we classify explanatory information? (b) How does different explanatory
information lead to understanding? (c) How do different stakeholders engage with explanatory information?
(d) How can we optimally evoke understanding through explanatory information? (e) How do stakeholder
differences (e.g., background knowledge, personality characteristics) affect understanding? (f) How do con-
textual influences (e.g., different levels of risk, multi-tasking environments, time pressure) affect the relation
between explanatory information and understanding? (g) How should explanatory information be designed?
What should it include? What kind of presentation format (e.g., textual, graphical) is appropriate? How can
stakeholders interactively engage with this information?

On the one hand, these research questions call for a unified classification of explanatory information.
Often, computer scientist classify explanatory information based on its presentation format or based on
the explainability approach it originated from (e.g., [205]). Philosophers and psychologists, however, usu-
ally classify explanatory information in terms of the kinds mentioned above (e.g., causal or nomological;

[206]). In order to prevent the debate from drifting apart, philosophers, psychologists, and computer scien-

24
tists should collaborate to find standardized ways to classify explanatory information. On the other hand,
these research questions call for empirical evaluation of the value of different kinds of explanatory informa-
tion for different stakeholders, under a variety of contexts and under the consideration of different desiderata.
This may primarily call for empirically working psychologists, as they have the tools and expertise to design
experimental studies, for philosophers to determine the qualities of good explanatory information, and for
computer scientists to adjust the presentation of explanatory information as well as possible. Furthermore,
cognitive scientists might need to examine what exactly understanding of artificial systems actually means
and how to measure it (see [17] for ideas of how to capture human understanding of artificial systems).
However, deriving explanatory information of artificial systems is a task that in itself requires a lot of
interdisciplinary research. The following section completes the specification of the concepts and relations in
our model by describing that, in order to get the required explanatory information from the systems, there is

the need for fitting explainability approaches.

5. Explanatory Information Requires Explainability Approaches

In order to provide explanatory information that facilitates understanding and, thus, affects the satisfac-
tion of the desiderata of the different classes of stakeholders, XAI research has developed a wide variety of
explainability approaches. These approaches encompass methods, procedures, and strategies that provide
explanatory information to help stakeholders better understand artificial systems, their inner workings, or
their outputs. A specific explainability approach is characterized by all steps and efforts that are undertaken
to extract explanatory information from a system and to adequately provide it to the stakeholders in a given
context. Explainability approaches can take many guises and the literature commonly distinguishes two

families of approaches (e.g., [12, 14, 63, 131, 205, 207]): ante-hoc and post-hoc approaches.

5.1. Families of Explainability Approaches

Ante-hoc approaches aim at designing systems that are inherently transparent and explainable. They
rely on systems being constructed on the basis of models that do not require additional procedures to extract
meaningful information about their inner workings or their outputs. For example, decision-trees, rule-based
models, and linear approximations are commonly seen as inherently explainable (given they have a limited
size) [7, 131]. A human can, in principle, directly extract information from these models in order to enhance
her understanding of how the system works or of how the system arrived at a particular output. Unfortunately,

this way of deriving explanatory information from transparent models might only be useful for stakeholders

25
with a certain expertise [16, 208]. For this reason, ante-hoc approaches make systems directly explainable
only for developers and members of other stakeholder classes that possess enough expertise about artificial
systems. Furthermore, ante-hoc explainability can also lead to a loss of predictive power [14, 15] and not all
systems can be designed inherently explainable.

Post-hoc approaches try to circumvent the aforementioned shortcomings. Such approaches are, in princi-
ple, applicable to all kinds of models. The difference to ante-hoc approaches is that post-hoc approaches do
not aim at the design-process of a particular system, but at procedures and methods that allow for extracting
explanatory information from a system’s underlying model, which is usually not inherently transparent or
explainable in the first place [11, 63, 131, 207]. Post-hoc approaches are, for example, based on input-output
analyses or the approximation of opaque models by models that are inherently explainable.

In many cases, however, post-hoc approaches are restricted with respect to how they present explanatory
information. That is, given a specific model or one of its outputs, the information an approach will provide on
repeated usage (and the format in which the approach provides the information) will be similar. Hence, for
post-hoc approaches the same holds as for ante-hoc approaches: it is not guaranteed that all stakeholders are
able to understand the provided information in the given format [16, 208]. So, the explanatory information
accessible from both, post-hoc and ante-hoc approaches is often only interpretable for developers or other
expert stakeholders. This means that this information does not directly facilitate understanding for non-
experts [16].

One solution to this is to combine several explainability approaches in order to cover a broad range of
different information and presentation modes. Another solution that has received increasing attention is to
have recourse on interactive explainability approaches [12, 67]. Interactive approaches are based on the idea
that the user or some other stakeholder is provided with more in-depth information concerning a system if
the information she initially received does not suffice. Based on her needs, the person interacting with the
system can call for information about specific aspects of a decision or request a presentation in a different
format. To date, however, approaches that are fully interactive remain rare [12, 205].

A third solution is to have a human facilitator (e.g., an expert stakeholder) explain a system to other
stakeholders. For instance, when regulators want to satisfy their desiderata concerning artificial systems,
there will be cases where they do not directly interact with an artificial system. Instead, a human facilitator
(e.g., an expert user or a developer) will do so and derive suitable explanatory information for the regulator.
In a sense, this process introduces a desiderata hierarchy based on the stakeholders into Figure 2. In other

words, one desideratum might have to be satisfied for one stakeholder class before some other desideratum

26
(for another stakeholder class) can be satisfied. For example, the facilitator’s desideratum (in this case: to be
able to explain a system to a regulator), must be satisfied first as a precondition for the satisfaction of a regu-
lator’s desideratum (e.g., to be able to assess the fairness of a system). Thus, we have the complete processes
in Figure 1 nested within the explainability approach. Having a human facilitator (e.g., a developer) deriving
explanatory information (assisted by an explainability approach) for another stakeholder (e.g., a regulator)

can be considered a hybrid human-system approach to explainability.

5.2. Factors Influencing the Relation between Explanatory Information and Explainability Approaches

There are further characteristics of explainability approaches that are worth mentioning, since they are
likely to influence the provided kind of explanatory information or its presentation format. First, it is im-
portant to distinguish post-hoc approaches that work regardless of the underlying model type (so-called
model-agnostic approaches) from ones that only work for specific (types of) models (so-called model-specific
approaches). Model-agnostic approaches aim to deliver explanatory information about a system solely by
observing input/output pairs [63, 115, 131, 207]. Model-specific approaches do so while also factoring
in specific features of the model at hand (e.g., by creating prototype vectors in a support vector machine)
[63, 131, 207]. Model-agnostic approaches have the advantage of working for all types of models, but have
the drawback that they tend to be less efficient, less accurate as well as less explanatory powerful (i.e., the
explanatory information’s level of detail is lower with regard to individual phenomena) than the former.

Second, previous research distinguished the scope of an explainability approach. Some approaches
provide information about only single predictions of the model. The scope of these approaches is local
[63, 115, 131, 207]. Often, they offer visualized prototype outcome examples (e.g., [209, 210]). The more
general type of approaches has a global scope [63, 131, 207]. These approaches are designed to uncover the
overall decision processes in the model. Here, the usual way to provide this information is by approximating
complex models with simpler ones that are inherently explainable.

Depending on the explainability approach, the explanatory information provided will differ. Global
explainability approaches, for instance, are likely to produce more complex information that requires more
background knowledge by stakeholders to be understood. Local explanability approaches, on the other hand,
only show a limited picture of a system’s inner working and may not be representative of its overall decision-
making processes. Based on these differences we can conclude that certain explainabilty approaches are
more suitable for the satisfaction of given desiderata of specific stakeholder classes than others.

To elaborate, take users who want to calibrate their trust in a system. They will need a different kind

27
of information compared to users who want to have usable systems. In the first case, the explainability
approach will likely need to extract information about the robustness of a system, about conditions under
which outputs of the system are trustworthy and situations where users have to be aware that outputs might be
misleading. In the case of usable systems, users might want information that directly matches their specific
goals in a given task. Another example: if a desideratum of users is learning how to use a system, they may
need a different kind of explanatory information compared to when they simply want to fulfill tasks with
the help of an artificial system [211]. This is because users who want to learn how to use a system need
more details whereas users who want to fulfill tasks need directly useful information in order not to reduce
their productivity through overly detailed information. Differences in the information needed can also be
due to differences in the perspective of stakeholders. Take the desideratum of fairness. Affected parties
will more likely focus on aspects of individual fairness which may call for explainability approaches that
facilitate local explainability. Regulators, however, may focus on more general notions of fairness calling

for explainability approaches facilitating global explainability.

5.3. Interdisciplinary Potential

The design of explainability approaches and the goal of providing adequate explanatory information
with the potential to affect stakeholders desiderata, again, hold untapped interdisciplinary potential, with re-
search questions such as: (a) How can we pinpoint what explanatory information an explainability approach
should provide in which case? (b) How can we design interactive explainability approaches? (c) How can
explainability approaches involving human facilitators be optimally designed? (d) How can we guarantee
that an explainability approach provides the required explanatory information? (e) How should stakeholders’
desiderata and their degrees of understanding be taken into account when generating explanatory information
or when developing new explainability approaches?

For all of these research questions, we see a potential for collaboration between computer scientists,
philosophers, psychologists, and cognitive scientists. For instance, philosophers and psychologist have to
determine which information is needed to assess whether a system is fair, whereas computer scientists de-
velop explainability approaches that provide this information, are aware of trade-offs between different ap-
proaches as well as of technical constraints. Furthermore, investigating how the examination of stakeholders
and their desiderata narrows down the options of possibly successful explainability approaches might be a
fruitful area for future research.

A more thorough discussion about which desiderata of what stakeholder class call for what kind of ex-

28
plainability approach goes far beyond what a single paper can achieve. However, in Section 6, we will show
that our model could inspire work that is necessary to evaluate the usefulness of an explainability approach
in relation to a given desideratum. Furthermore, we will show that the model supports the development
of new explainability approaches to satisfy a given set of desiderata. Thus, the next section aims to show
how our model can lead to actionable insights for XAI research by analyzing the concepts and relations that
we propose in Figure 1. By means of hypothetical application scenarios, we want to stimulate ideas about

specific applications of our model.

6. Bringing It All Together: Hypothetical Application Scenarios

Following, we will present how the previous sections come together within hypothetical application
scenarios. These scenarios highlight the importance of different stakeholder classes and their desiderata.
Furthermore, these scenarios emphasize that understanding affects the satisfaction of these desiderata, that
explanatory information provided by explainability approaches facilitate understanding, and that analyzing
and investigating these concepts and their relations is central for the aims of XAI as well as for the develop-
ment of explainability approaches that can successfully satisfy stakeholder desiderata.

The main application of our model is derived from the idea that if an explanation process does not
change a certain desideratum’s extent of satisfaction, the corresponding explainability approach might not
be a suitable means for satisfying the desideratum in the given context. Such a discovery (e.g., resulting from
stakeholder feedback or from empirical investigation) can provide feedback regarding which explainability
approaches and what kinds of explanatory information work for which desiderata in which contexts. Thus,
this feedback can serve as an input for the improvement of explanation processes and, consequently, helps
to evaluate, adjust, choose, and develop explainability approaches for a given purpose and context.

We propose that each step in our model (Figure 1: Explainability Approaches — Explanatory Infor-
mation — Understanding — Desiderata Satisfaction) allows for drawing inferences about the explanation
process involving explainability approach(es), kind(s) of explanatory information, and stakeholder under-

standing. The following questions, which arise at different points in our model, are of particular interest:

¢ Who are the relevant stakeholders and what are their specific characteristics? Which are the relevant

desiderata in a specific context and are they satisfied?

¢ Have the stakeholders acquired a sufficient degree and the right kind of understanding that allows for

assessing whether given desiderata are satisfied and to facilitate their satisfaction?

29
¢ Does the provided explanatory information and its format of presentation facilitate stakeholders’ un-
derstanding in a given context and in consideration of the stakeholder characteristics?

¢ Is the explainability approach able to provide the right kind and amount of explanatory information in
the right presentation format?

¢ Are there contextual influences hindering or promoting the satisfaction of desiderata through the ex-

planation process?

Investigating these questions requires empirical research, hypothesis testing and interdisciplinary coop-
eration, but should, eventually, aid in evaluating, adjusting, choosing, and developing explainability ap-
proach(es) and finding explanatory information in order to adequately satisfy stakeholders’ desiderata.
With this in mind, we believe that our model is useful for several important application scenarios. First,
our model is useful for choosing adequate explainability approaches for novel application contexts of arti-
ficial systems and for guiding the development of new explainability approaches. Specifically, our model
can be used to inform projects on how to develop explainability approaches to satisfy certain desiderata for
a given class of stakeholders. Second, our model is useful for evaluating why and at which stage an ex-
plainability process failed to contribute to satisfying the relevant desiderata. Let us assume that the use of
an explainability approach does not lead to the satisfaction of a certain desideratum. Why is this the case?
Is the explainability approach not suitable for the satisfaction of the desideratum (e.g., the explainability
approach provides the false kind of explanatory information) and should be replaced by another approach?
Or does the error lie somewhere else in the explanation process? In some cases, we may be able to adjust

the explainability approach appropriately to achieve its intended purpose.

6.1. General Application Scenarios

For structured attempts to evaluate, adjust, choose, or develop explainability approaches for a given
context we roughly distinguish two scenarios, which we call the evaluation and the discovery scenario. In
the evaluation scenario, we want to investigate whether the use of a specific explainability approach was
adequate, and if not, what is needed to fix its shortcomings. In the discovery scenario, we want to find
an adequate explainability approach to satisfy stakeholders’ desiderata. This can take one of two forms:
choosing among existing approaches or, if no adequate approach is available, developing a new one.

Stakeholder and Desiderata. Both evaluation and discovery scenarios start with examining the stake-
holders and clarifying their desiderata in the given context. In the discovery scenario, we have to examine

what the relevant classes of stakeholders are and which desiderata they have concerning the application of a

30
system under consideration. In the evaluation scenario, we have to check — even in scenarios where already
identified desiderata are satisfied — whether the explanation process fits all relevant classes of stakeholders
and all of their desiderata (and ensure that we did not overlook important stakeholders or desiderata).

For this, we need input from a wide variety of disciplines including but not limited to scholars from law,
sociology, psychology, philosophy, and computer science. Such a combination of expertise and perspectives
helps to identify relevant stakeholder classes and list their desiderata pertaining to a given context. Defining
these desiderata falls within the expertise of philosophers. The elaboration of the desiderata’s relevant moral
and legal aspects is a task for ethicists and scholars from law. Furthermore, assessing contextual peculiari-
ties and stakeholder characteristics relevant in a given context will require psychologists (on the individual
level), sociologist (on the group level), as well as domain experts such as judges or personnel managers (for
particular application scenarios).

Desiderata Satisfaction. For the discovery scenario, the next step is to determine what it means that
stakeholders’ desiderata are satisfied in a specific context: to make estimates, to provide guidelines, and
to create measures for when the identified desiderata are satisfied. For the evaluation scenario, we have to
check whether the relevant desiderata are satisfied or not. If they are, the explanation process was successful
and the chosen explainability approach appropriate. If they are not substantially satisfied, there are two
possible cases. First, the necessary understanding was not acquired and, thus, the desideratum is also not
epistemically satisfied. Then, an improvement of the stakeholders’ understanding is required. Second, an
adequate degree and kind of understanding is reached and, thus, the epistemic facet of the desideratum is
satisfied. In this case, we may conclude that the regarded desideratum is not directly substantially satisfiable
by means of explainability approaches. Regardless of the case, at this point we have to move on to investigate
stakeholders’ understanding.

Determining conditions for desiderata satisfaction will be an interdisciplinary task for psychologists,
philosophers, and scholars from law. Furthermore, computer scientists and domain experts can give practical
input on satisfaction conditions for the desiderata in their specific domain. Making the satisfaction of these
desiderata measurable and examining their extent of satisfaction will be a job for psychologists who develop
measures or tasks that help to assess the extent of desiderata satisfaction. Additionally, it can be a task for
scholars from law or philosophers to provide clear guidelines for when a desideratum is satisfied.

Stakeholders’ Understanding. The discovery scenario continues by investigating and defining require-
ments for the stakeholders’ understanding needed to satisfy the desiderata under consideration. Specifically,

we have to determine the appropriate degree and kind of understanding concerning the system and its out-

31
put that promise to enable the epistemic facet of desiderata satisfaction. At the same time, this means that
we need to assess (e.g., in studies using tasks to measure stakeholders’ mental models of a system, or in
studies using tasks to reveal whether stakeholders were able to, for instance, explain a systems’ functioning
or predict a systems’ behavior and outcomes) stakeholders’ actual degree and kind of understanding. This
is especially important in the evaluation scenario when deficits in the substantial desiderata satisfaction be-
come apparent. This results from the circumstance that assessing whether the required degree and kind of
understanding that has been achieved allows for drawing inferences as to whether there is a fundamental
gap between the explanatory process and the substantial desiderata satisfaction, or whether the provided
information is not appropriate to evoke understanding. This is due to the fact that the explanation process
can only serve to maximize understanding. If a desideratum still remains (substantially) unsatisfied, its full
satisfaction goes beyond the scope of any explainability approach.

Philosophers can help to investigate and explicate what it means to have a certain degree or kind of
understanding. Building on this, psychologists can design ways to empirically assess and measure such
understanding (e.g., is it necessary for a given desideratum that stakeholders are able to predict a system’s
outputs? Is it necessary that stakeholders are able to anticipate situations when systems will likely fail?).
Furthermore, scholars from law can contribute conditions for traceability and auditability.

Explanatory Information. The next step in the discovery scenario is to pin down what explanatory
information has the potential to facilitate the right kind and degree of understanding in a predetermined
context. This implies an evaluation of different dimensions of explanatory information with respect to the
expected effects within an explanation process. To do so, we have to pay attention to, for instance, the
kind of information, its presentation format [36], its quality [17], its amount [171], its completeness, its
complexity, or its adequacy for the given context [171]. In the evaluation scenario, we have to check whether
an explainability approach provides explanatory information that sufficiently meets the previously identified
requirements. Sometimes there will also be a need to re-evaluate whether the requirements concerning the
information are indeed adequate.

In this respect, philosophers and other explanation scientists can help to distinguish between different
kinds and features of explanatory information [6, 7]. Furthermore, scholars from law can examine current
legislation to find out whether it prescribes certain kinds of explanatory information. In the case of the
GDPR, for instance, they have to specify what it means to “provide [...] meaningful information about
the logic involved” (GDPR Art. 13 (2)(f); [101]). Finally (and based on this differentiation), educational

or cognitive psychologist have the task to characterize the explanatory information that is best suited to

32
facilitate the required kind and degree of understanding for a certain context.

Explainability Approach. Insights from the assessment of the concepts and relations in our model can
guide and inform the requirements for explainability approaches that aim to satisfy given desiderata. This is
of particular importance for the discovery scenario, where the primary objective is to identify which approach
is expected to be most appropriate for providing specific information. Assume that, in the evaluation scenario
we are at a point where the desiderata are not satisfied, the adequate degree and kind of understanding is not
evoked, and the required explanatory information is not delivered. In this case, it is necessary to investigate
whether the explainability approach is even capable of producing explanatory information with the right
features at all. All the insights that are available at this point can indicate whether an existing explainability
approach provides explanatory information that is sufficient to satisfy stakeholders’ desiderata.

Additionally, we can learn whether a given explainability approach has the potential to derive explanatory
information that can satisfy stakeholders’ desiderata to a certain degree, whether it is necessary to adjust the
explainability approach, whether it is sufficient to choose another one, or whether we need to develop an
entirely new one. At this stage, computer scientists who can improve, adjust, and design explainability
approaches are the main contributors integrating the aforementioned insights. First, they have the abilities to
assess what is technically feasible and possible. Second, they can actually implement the demands regarding

the explainability approach resulting from the previous steps.

6.2. Specific Application Scenario

We will conclude our thoughts about the application of our model by means of a specific example.
Consider a situation where users want a system that produces fair outputs (substantial facet). For this,
we first need an explanation process that enables them to assess whether the system produces fair outputs
(epistemic facet) and, consequently, we need to find an adequate explainability approach. We first clarify
the relevant user sub-classes and their prototypical characteristics. Will it be, for instance, novice or expert
users? Then, we determine whether other stakeholder classes also have to be considered. For example, do
we need to consider the perspectives of regulators regarding fairness? Furthermore, we have to anticipate the
context. Are we talking about a personnel selection tasks or court cases with completely different contextual
peculiarities?

Subsequently, we determine what users mean when they desire fair outputs (i.e., we clarify the satisfac-
tion conditions of the desideratum’s substantial facet). What kind of algorithmic fairness do they expect?

At the same time, it is important to be aware of other relevant desiderata as the satisfaction of other user

33
desiderata could be affected when trying to satisfy the fairness desideratum. Similarly, there may be unan-
ticipated effects on the stakeholders’ desiderata. For instance, will using a specific explainability approach
that proves suitable to assess system fairness also affect predictive accuracy of the artificial system?

Next, we consider the desideratum’s epistemic facet. Under which circumstances would users be enabled
to assess whether the outputs of a system are fair? What do they need to understand with regard to a
system’s functioning or outputs? To answer these questions, we must determine what degree and what kind
of understanding is appropriate for the epistemic satisfaction of the fairness desideratum and we need a
detailed investigation of how contextual influences moderate this relation. Furthermore, we must be aware
of given stakeholder characteristics (e.g., users’ background knowledge).

When we have estimated what degree and kind of understanding is required to enable users to assess
whether a system’s outputs are fair, we can determine what kind of explanatory information facilitates this
understanding. For example, we might need explanatory information about the influence of features based
on protected attributes (e.g., race) on the system’s outputs. Alternatively, we might need information that
contrasts the treatment of different groups of people (e.g., minorities and majorities). With this knowledge,
we can determine what explainability approach provides this kind of explanatory information. Most likely,
it would be post-hoc, local approaches that provide explanatory information that either highlights feature
relevance or that allow users to compare subsets of instances regarding their predicted outcomes. As a result,
we have specific demands regarding the explainability approach for satisfying the fairness desideratum in
the case under consideration.

With this knowledge we can either choose an adequate explainability approach from existing ones or
design a new one. Afterwards, we can investigate whether the explanatory information resulting from the
respective explainability approach leads to a better understanding of the system and its outputs. This means
that we are now in a position to empirically evaluate the success of the selected approach and the corre-
sponding explanation process.

For this, we may conduct a stakeholder study and find that, given our definitions of desiderata satisfac-
tion, the epistemic facet may be satisfied (e.g., users can actually and justifiedly assess whether the system
produces fair outputs; users can explain whether and why the respective system’s outputs are fair; users can
predict what kind of inputs will lead to fair or unfair outcomes) but not the substantial one (i.e., the system’s
outputs are actually not fair). Then, we may have to conclude that the substantial facet of the desidera-
tum is not satisfiable by altering the explainability approach. Nevertheless, satisfying the epistemic facet

of the desideratum can help us to figure out how to satisfy the substantial facet of the desideratum beyond

34
XAlI-related strategies. For instance, we may obtain information that a developer can use to improve system
fairness. In this case, the artificial system has to be adjusted or changed to additionally satisfy the substantial
facet of desiderata satisfaction.

On the other hand, if the stakeholder study reveals that the epistemic facet of the desideratum is not
satisfied (e.g., users fail in explaining whether and why the respective system’s outputs are fair), we can
conclude that the users’ degree or kind of understanding of the system and its outputs does not suffice. In
many situations this may be the case because the explanatory information was not suitable to evoke the
necessary understanding (e.g., the explainability approach did not provide explanatory information suitable
to allow assessing whether the system produces fair outcomes). Depending on the context, the task, and
stakeholders’ individual characteristics, it may be necessary to iteratively adjust the characteristics of the ex-
planatory information so that stakeholders engaging with this information achieve the right kind and degree
of understanding. Alternatively, it may be necessary to adjust the explainability approach if it is not suitable

to provide explanatory information useful for facilitating stakeholder understanding.

7. Conclusion

With increasing numbers of people affected by artificial systems, the number of stakeholders’ desiderata
will continue to grow. Although the focus of XAI research has shifted towards human stakeholders and
the evaluation of explainability approaches, this shift still needs to incorporate a comprehensive view of all
stakeholders and their desiderata when artificial systems are used in socially relevant contexts as well as
empirical investigation of explainability approaches with respect to desiderata satisfaction. This engenders
an even more pressing need for interdisciplinary collaboration in order to consider all stakeholders’ per-
spectives and to empirically evaluate and optimally design explainability approaches to satisfy stakeholders’
desiderata. The current paper has introduced a model highlighting the central concepts and their relations
along which explainability approaches aim to satisfy stakeholders’ desiderata. We hope that this model
inspires and guides future interdisciplinary evaluation and development of explainability approaches and,

thereby, further advances XAI research concerning the satisfaction of stakeholders’ desiderata.

References

[1] D. C. Brock, Learning from artificial intelligence’s previous awakenings: The history of expert sys-

tems, AI Magazine 39 (3) (2018) 3-15. doi:10.1609/aimag.v3913.2809.

35
[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

W. J. Clancey, The epistemology of a rule-based expert system — A framework for explanation, Arti-

ficial Intelligence 20 (3) (1983) 215-251. doi:10.1016/0004-—3702 (83) 90008-5.

W. R. Swartout, Xplain: A system for creating and explaining expert consulting programs, Artificial

Intelligence 21 (3) (1983) 285-325. doi:10.1016/S0004—3702 (83) 80014-9.

H. Johnson, P. Johnson, Explanation facilities and interactive systems, in: Proceedings of the Ist
International Conference on Intelligent User Interfaces (IUI), Association for Computing Machinery,

New York, NY, USA, 1993, pp. 159-166. doi:10.1145/169891.169951.

O. Biran, C. Cotton, Explanation and justification in machine learning: A survey, in: Proceedings of

the ICAI 2017 Workshop on Explainable Artificial Intelligence (XAI), 2017, pp. 8-13.

T. Miller, Explanation in artificial intelligence: Insights from the social sciences, Artificial Intelli-

gence 267 (2019) 1-38. doi:10.1016/j.artint.2018.07.007.

B. D. Mittelstadt, C. Russell, S. Wachter, Explaining explanations in AI, in: Proceedings of the 2019
Conference on Fairness, Accountability, and Transparency, Association for Computing Machinery,

New York, NY, USA, 2019, pp. 279-288. doi:10.1145/3287560.3287574.

J. Burrell, How the machine ‘thinks’: Understanding opacity in machine learning algorithms, Big

Data & Society 3 (1) (2016) 1-12. doi:10.1177/2053951715622512.

F. Doshi-Velez, B. Kim, Towards a rigorous science of interpretable machine learning, CoRR

abs/1702.08608. arXiv:1702.08608.

EU High-Level Expert Group on Artificial Intelligence, Ethics guidelines for trustworthy AI (2019).
URL https: //ec.europa.eu/digital-—single-market/en/news/

ethics-guidelines-trustworthy-ai

Z. C. Lipton, The mythos of model interpretability, Commun. ACM 61 (10) (2018) 36-43. doi:
10.1145/3233231.

A. Adadi, M. Berrada, Peeking inside the black-box: A survey on explainable artificial intelligence

(XAT), IEEE Access 6 (2018) 52138-52160. doi:10.1109/ACCESS.2018.2870052.

36
[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

I. Nunes, D. Jannach, A systematic review and taxonomy of explanations in decision support and
recommender systems, User Modeling and User-Adapted Interaction 27 (3-5) (2017) 393-444. doi:
10.1007/s11257-017-9195-0.

A. B. Arrieta, N. Diaz-Rodriguez, J. D. Ser, A. Bennetot, S. Tabik, A. Barbado, S. Garcia, S. Gil-
Lopez, D. Molina, R. Benjamins, R. Chatila, F. Herrera, Explainable artificial intelligence (XAI):
Concepts, taxonomies, opportunities and challenges toward responsible AI, Information Fusion 58

(2020) 82-115. doi:10.1016/}j.inffus.2019.12.012.

H. Felzmann, E. F. Villaronga, C. Lutz, A. Tamo-Larrieux, Transparency you can trust: Transparency
requirements for artificial intelligence between legal norms and contextual concerns, Big Data &

Society 6 (1) (2019) 1-14. doi: 10.1177/2053951719860542.

L. H. Gilpin, C. Testart, N. Fruchter, J. Adebayo, Explaining explanations to society, in: NIPS
Workshop on Ethical, Social and Governance Issues in AI, Vol. abs/1901.06560, 2018, pp. 1-6.
arXiv:1901.06560.

R. R. Hoffman, S. T. Mueller, G. Klein, J. Litman, Metrics for explainable AI: Challenges and
prospects, CoRR abs/1812.04608. arXiv:1812.04608.

A. D. Preece, D. Harborne, D. Braines, R. Tomsett, S. Chakraborty, Stakeholders in explainable AI,
CoRR abs/1810.00184. arXiv:1810.00184.

A. Weller, Transparency: Motivations and challenges, in: W. Samek, G. Montavon, A. Vedaldi, L. K.
Hansen, K.-R. Mtiller (Eds.), Explainable AI: Interpreting, Explaining and Visualizing Deep Learn-
ing, Springer, 2019, pp. 23-40. doi:10.1007/978-3-030-28954-6_2.

A. Paez, The pragmatic turn in explainable artificial intelligence (XAD, Minds & Machines 29 (2019)
441-459. doi:10.1007/s11023-019-09502-w.

H.-F. Cheng, R. Wang, Z. Zhang, F. O’Connell, T. Gray, F M. Harper, H. Zhu, Explaining decision-
making algorithms through ui: Strategies to help non-expert stakeholders, in: Proceedings of the 2019
chi conference on human factors in computing systems, Association for Computing Machinery, New

York, NY, USA, 2019, pp. 1-12. doi:10.1145/3290605.3300789.

37
[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

A. Abdul, J. Vermeulen, D. Wang, B. Y. Lim, M. Kankanhalli, Trends and trajectories for explainable,
accountable and intelligible systems: An HCI research agenda, in: Proceedings of the 2018 Confer-
ence on Human Factors in Computing Systems (CHD, Association for Computing Machinery, New

York, NY, USA, 2018, pp. 1-18. doi:10.1145/3173574.3174156.

M.-W. Dictionary, Stakeholder, accessed: 30 July 2020 (2020).

URL https://www.merriam—webster.com/dictionary/stakeholder

M. Hind, D. Wei, M. Campbell, N. C. F Codella, A. Dhurandhar, A. Mojsilovié, K. N. Ramamurthy,
K. R. Varshney, Ted: Teaching ai to explain its decisions, in: Proceedings of the 2019 AAAI/ACM
Conference on AI, Ethics, and Society, Association for Computing Machinery, New York, NY, USA,
2019, pp. 123-129. doi:10.1145/3306618. 3314273.

S. Anjomshoae, K. Fraimling, A. Najjar, Explanations of black-box model predictions by contextual
importance and utility, in: Explainable, Transparent Autonomous Agents and Multi-Agent Systems,

Springer, 2019, pp. 95-109. doi:10.1007/978-3-030-30391-4_6.

M. Atzmueller, Towards socio-technical design of explicative systems: Transparent, interpretable and
explainable analytics and its perspectives in social interaction contexts information, in: Proceedings
of the 2019 Workshop on Affective Computing and Context Awareness in Ambient Intelligence (Af-
CAD, 2019, pp. 1-8.

I. Baaj, J.-P. Poli, W. Querdane, Some insights towards a unified semantic representation of explana-
tion for explainable artificial intelligence, in: Proceedings of the 2019 Workshop on Interactive Natu-
ral Language Technology for Explainable Artificial Intelligence (NL4XAD), Association for Compu-
tational Linguistics, 2019, pp. 14-19. doi:10.18653/v1/W19-8404.

K. Balog, F. Radlinski, $. Arakelyan, Transparent, scrutable and explainable user models for per-
sonalized recommendation, in: Proceedings of the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval, Association for Computing Machinery, New

York, NY, USA, 2019, pp. 265-274. doi:10.1145/3331184.3331211.

R. Binns, M. Van Kleek, M. Veale, U. Lyngs, J. Zhao, N. Shadbolt, It’s reducing a human being to a
percentage’: Perceptions of justice in algorithmic decisions, in: Proceedings of the 2018 Conference
on Human Factors in Computing Systems (CHD, Association for Computing Machinery, New York,

NY, USA, 2018, pp. 1-14. doi:10.1145/3173574.3173951.

38
[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

T. Chakraborti, S. Sreedharan, S. Grover, S. Kambhampati, Plan explanations as model reconciliation,
in: 14th ACM/IEEE International Conference on Human-Robot Interaction (HRD, IEEE, 2019, pp.
258-266. doi:10.1109/HRI.2019.8673193.

L. Chen, D. Yan, F. Wang, User evaluations on sentiment-based recommendation explanations, ACM

Transactions on Interactive Intelligent Systems (TiiS) 9 (4) (2019) 1-38. doi:10.1145/3282878.

K. Cotter, J. Cho, E. Rader, Explaining the news feed algorithm: An analysis of the” news feed
fyi” blog, in: Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in
Computing Systems (CHI EA), Association for Computing Machinery, New York, NY, USA, 2017,
pp. 1553-1560. doi:10.1145/3027063.3053114.

K. Darlington, Aspects of intelligent systems explanation, Universal Journal of Control and Automa-

tion 1 (2) (2013) 40-51. doi:10.13189/ujca.2013.010204.

K. Ehrlich, S. E. Kirk, J. Patterson, J. C. Rasmussen, S. I. Ross, D. M. Gruen, Taking advice from in-
telligent systems: The double-edged sword of explanations, in: Proceedings of the 16th International
Conference on Intelligent User Interfaces (IUD, Association for Computing Machinery, New York,

NY, USA, 2011, pp. 125-134. doi: 10.1145/1943403.1943424.

A. A. Freitas, Comprehensible classification models: A position paper, SIGKDD Explorations
Newsletter 15 (1) (2014) 1-10. doi:10.1145/2594473.2594475.

S. Gregor, I. Benbasat, Explanations from intelligent systems: Theoretical foundations and implica-

tions for practice, MIS Quarterly 23 (4) (1999) 497-530. doi:10.2307/249487.

J. Hois, D. Theofanou-Fuelbier, A. J. Junk, How to achieve explainability and transparency in human
ai interaction, in: International Conference on Human-Computer Interaction (HCI), Springer, 2019,

pp. 177-183. doi:10.1007/978-3-030-23528-4 25.

R. F. Kizilcec, How much information? effects of transparency on trust in an algorithmic interface, in:
Proceedings of the 2016 Conference on Human Factors in Computing Systems (CHD, Association for
Computing Machinery, New York, NY, USA, 2016, pp. 2390-2395. doi:10.1145/2858036.
2858402.

39
[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

S. Nagulendra, J. Vassileva, Providing awareness, explanation and control of personalized filtering in
a social networking site, Information Systems Frontiers 18 (1) (2016) 145-158. doi:10.1007/
s10796-015-9577-y.

A. Papenmeier, G. Englebienne, C. Seifert, How model accuracy and explanation fidelity influence
user trust in ai, in: Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence
(XAD), 2019, pp. 94-100.

R. Pierrard, J.-P. Poli, C. Hudelot, A new approach for explainable multiple organ annotation with
few data, in: Proceedings of the ICAI 2019 Workshop on Explainable Artificial Intelligence (XAI),
2019, pp. 101-107.

V. Putnam, L. Riegel, C. Conati, Towards personalized xai: A case study in intelligent tutoring sys-
tems, in: Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAI),
2019, pp. 108-114.

E. Rader, K. Cotter, J. Cho, Explanations as mechanisms for supporting algorithmic transparency, in:
Proceedings of the 2018 Conference on Human Factors in Computing Systems (CHI), Association
for Computing Machinery, New York, NY, USA, 2018, pp. 1-13. doi:10.1145/3173574.
3173677.

A. Rosenfeld, A. Richardson, Explainability in human—agent systems, Autonomous Agents and

Multi-Agent Systems 33 (6) (2019) 673-705. doi:10.1007/s10458-019-09408-y.

M. Sato, K. Nagatani, T. Sonoda, Q. Zhang, T. Ohkuma, Context style explanation for recommender
systems, Journal of Information Processing 27 (2019) 720-729. doi:10.2197/ipsjjip.27.
720.

J. Vig, S. Sen, J. Riedl, Tagsplanations: Explaining recommendations using tags, in: Proceedings
of the 14th International Conference on Intelligent User Interfaces (IUI, Association for Computing

Machinery, New York, NY, USA, 2009, pp. 47-56. doi:10.1145/1502650.1502661.

X. Watts, F. Lécué, Local score dependent model explanation for time dependent covariates, in: Pro-
ceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAD, 2019, pp. 129-
135.

40
[48]

[49]

[50]

[51]

[52]

[53]

[54]

[55]

J. Zhou, H. Hu, Z. Li, K. Yu, F. Chen, Physiological indicators for user trust in machine
learning with influence enhanced fact-checking, in: International Cross-Domain Conference for
Machine Learning and Knowledge Extraction, Springer, 2019, pp. 94-113. doi:10.1007/
978—-3-030-29726-8_7.

J. L. Herlocker, J. A. Konstan, J. Riedl, Explaining collaborative filtering recommendations, in: Pro-
ceedings of the 2000 ACM Conference on Computer Supported Cooperative Work (CSCW), As-
sociation for Computing Machinery, New York, NY, USA, 2000, pp. 241-250. doi:10.1145/
358916.358995.

H. Cramer, V. Evers, S. Ramlal, M. Van Someren, L. Rutledge, N. Stash, L. Aroyo, B. Wielinga, The
effects of transparency on trust in and acceptance of a content-based art recommender, User Modeling

and User-adapted interaction 18 (5) (2008) 455. doi:10.1007/s811257-008-9051-3.

R. M. Byrne, Counterfactuals in explainable artificial intelligence (xai): Evidence from human rea-
soning, in: Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence

(IJCAI-19), 2019, pp. 6276-6282. doi:10.24963/ijcai.2019/876.

P. B. De Laat, Algorithmic decision-making based on machine learning from big data: Can trans-
parency restore accountability?, Philosophy & Technology 31 (4) (2018) 525-541. doi:10.1007/
$13347-017-0293-z.

L. Floridi, J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, R. Madelin,
U. Pagallo, F. Rossi, B. Schafer, P. Valcke, E. Vayena, Al4people—An ethical framework for a good
Al society: Opportunities, risks, principles, and recommendations, Minds and Machines 28 (4) (2018)
689-707. doi:10.1007/811023-018-9482-5.

B. Lepri, N. Oliver, E. Letouzé, A. Pentland, P. Vinck, Fair, transparent, and accountable algorithmic
decision-making processes, Philosophy & Technology 31 (4) (2018) 611-627. doi:10.1007/
$13347-017-0279-x.

S. M. Mathews, Explainable artificial intelligence applications in nlp, biomedical, and malware clas-
sification: A literature review, in: Intelligent Computing — Proceedings of the Computing Conference,

Springer, 2019, pp. 1269-1292. doi:10.1007/978-3-030-22868-2_90.

41
[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]

B. D. Mittelstadt, P. Allo, M. Taddeo, S. Wachter, L. Floridi, The ethics of algorithms: Mapping the
debate, Big Data & Society 3 (2) (2016) 1-21. doi:10.1177/2053951716679679.

W. Pieters, Explanation and trust: what to tell the user in security and ai?, Ethics and Information

Technology 13 (1) (2011) 53-64. doi:10.1007/810676-010- 9253-3.

G. Ras, M. van Gerven, P. Haselager, Explanation methods in deep learning: Users, values, concerns
and challenges, in: Explainable and Interpretable Models in Computer Vision and Machine Learning,

Springer, 2018, pp. 19-36. doi:10.1007/978-3-319-98131-4_2.

M. O. Riedl, Human-centered artificial intelligence and machine learning, Human Behavior and

Emerging Technologies 1 (1) (2019) 33-36. doi:10.1002/hbe2.117.

S. Robbins, A misdirected principle with a catch: Explicability for ai, Minds and Machines 29 (4)
(2019) 495-514. doi:10.1007/s11023-019-09509-3.

R. Sheh, Different xai for different hri, in: AAAI Fall Symposium, AAAI Press, 2017, pp. 114-117.

R. Sheh, I. Monteath, Defining explainable ai for requirements analysis, KI-Kiinstliche Intelligenz

32 (4) (2018) 261-266. doi:10.1007/s13218-018-0559-3.

K. Sokol, P. A. Flach, Explainability fact sheets: A framework for systematic assessment of ex-
plainable approaches, in: Proceedings of the 2020 Conference on Fairness, Accountability, and
Transparency, Association for Computing Machinery, New York, NY, USA, 2020, pp. 56-67.
doi:10.1145/3351095.3372870.

K. Sokol, P. Flach, One explanation does not fit all, KI-Kiinstliche Intelligenz 34 (2) (2020) 235-250.
doi:10.1007/s13218-020-00637-y.

M. Sridharan, B. Meadows, Towards a theory of explanations for human-robot collaboration, KI-

Kiinstliche Intelligenz 33 (4) (2019) 331-342. doi:10.1007/s13218-019-00616-y.

A. Vellido, The importance of interpretability and visualization in machine learning for ap-
plications in medicine and health care, Neural Computing and Applicationsdoi:10.1007/

s00521-019-04051-w.

D. Wang, Q. Yang, A. Abdul, B. Y. Lim, Designing theory-driven user-centric explainable ai, in:

Proceedings of the 2019 Conference on Human Factors in Computing Systems (CHI), Association

42
[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

[76]

for Computing Machinery, New York, NY, USA, 2019, pp. 1-15. doi:10.1145/3290605.
3300831.

M. K. Lee, A. Jain, H. J. Cha, S. Ojha, D. Kusbit, Procedural justice in algorithmic fairness, Proceed-
ings of the 2019 ACM on Human-Computer Interaction 3 (2019) 1-26. doi:10.1145/3359284.

D. Doran, S. Schulz, T. R. Besold, What does explainable ai really mean? a new conceptualization of
perspectives, in: CEUR Workshop Proceedings, Vol. 2071, CEUR, 2018, pp. 1-8. arXiv:1710.
00794.

M. Krishnan, Against interpretability: A critical examination of the interpretability problem in ma-

chine learning, Philosophy & Technology (2019) 1-l6doi:10.1007/s13347-019-00372-9.

S. K. Peddoju, M. Saravanan, S. Suresh, Explainable classification using clustering in deep learning
models, in: Proceedings of the ICAI 2019 Workshop on Explainable Artificial Intelligence (XAI),
2019, pp. 115-121.

N. F. Rajani, R. J. Mooney, Using explanations to improve ensembling of visual question answering
systems, in: Proceedings of the (CAI 2017 Workshop on Explainable Artificial Intelligence (XAD,
2017, pp. 43-47.

J. Zhou, F. Chen, Towards trustworthy human-ai teaming under uncertainty, in: Proceedings of the

IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAD, 2019, pp. 143-147.

S. Anjomshoae, A. Najjar, D. Calvaresi, K. Framling, Explainable agents and robots: Results
from a systematic literature review, in: Proceedings of the 18th International Conference on Au-
tonomous Agents and MultiAgent Systems (AAMAS), International Foundation for Autonomous
Agents and Multiagent Systems, Richland, SC, USA, 2019, p. 1078-1088. doi:10.5555/
3306127.3331806.

M. Fox, D. Long, D. Magazzeni, Explainable planning, in: Proceedings of the ICAI 2017 Workshop
on Explainable Artificial Intelligence (XAD, 2017, pp. 24-30.

S. Jasanoff, Virtual, visible, and actionable: Data assemblages and the sightlines of justice, Big Data

& Society 4 (2) (2017) 1-15. doi: 10.1177/2053951717724477.

43
[77]

[78]

[79]

[80]

[81]

[82]

[83]

[84]

[85]

[86]

G. Friedrich, M. Zanker, A taxonomy for generating explanations in recommender systems, AI Mag-

azine 32 (3) (2011) 90-98. doi:10.1609/aimag.v32i3.2365.

A. Holzinger, G. Langs, H. Denk, K. Zatloukal, H. Miiller, Causability and explainability of artificial
intelligence in medicine, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery

9 (4) (2019) 1-13. doi:10.1002/widm.1312.

R. Sevastjanova, F. Beck, B. Ell, C. Turkay, R. Henkin, M. Butt, D. A. Keim, M. El-Assady, Going
beyond visualization: Verbalization as complementary medium to explain machine learning models,

in: VIS Workshop on Visualization for AI Explainability (VISxAI), 2018, pp. 1-6.

F. Sermo, J. Cassens, A. Aamodt, Explanation in case-based reasoning—perspectives and goals, Arti-

ficial Intelligence Review 24 (2) (2005) 109-143. doi:10.1007/s10462-005-4607-7.

J. Zerilli, A. Knott, J. Maclaurin, C. Gavaghan, Transparency in algorithmic and human decision-
making: is there a double standard?, Philosophy & Technology 32 (4) (2019) 661-683. doi:10.
1007/813347-018-0330-6.

A. Lucic, H. Haned, M. de Rijke, Contrastive explanations for large errors in retail forecasting predic-
tions through monte carlo simulations, in: Proceedings of the ICAI 2019 Workshop on Explainable

Artificial Intelligence (XAI), 2019, pp. 66-72.

H. K. Dam, T. Tran, A. Ghose, Explainable software analytics, in: Proceedings of the 40th In-
ternational Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),
Association for Computing Machinery, New York, NY, USA, 2018, pp. 53-56. doi:10.1145/
3183399.3183424.

J. De Winter, Explanations in software engineering: The pragmatic point of view, Minds and Ma-

chines 20 (2) (2010) 277-289. doi:10.1007/s11023-010-9190-2.

Z. Juozapaitis, A. Koul, A. Fern, M. Erwig, F Doshi-Velez, Explainable reinforcement learning via
reward decomposition, in: Proceedings of the ICAI 2019 Workshop on Explainable Artificial Intel-
ligence (XAI), 2019, pp. 47-53.

L. Michael, Machine coaching, in: Proceedings of the ICAI 2019 Workshop on Explainable Artificial
Intelligence (XAI), 2019, pp. 80-86.

44
[87]

[88]

[89]

[90]

[91]

[92]

[93]

[94]

[95]

K. Sokol, P. A. Flach, Conversational explanations of machine learning predictions through class-
contrastive counterfactual statements, in: Proceedings of the Twenty-Seventh International Joint Con-
ference on Artificial Intelligence (IJCAI-18), 2018, pp. 5785-5786. doi:10.24963/ijcal.
2018/836.

H. Wicaksono, C. Sammut, R. Sheh, Towards explainable tool creation by a robot, in: Proceedings of

the ICAI 2017 Workshop on Explainable Artificial Intelligence (XAD, 2017, pp. 63-67.

T. Eiter, Z. G. Saribatur, P. Schiiller, Abstraction for zooming-in to unsolvability reasons of grid-cell
problems, in: Proceedings of the I}CAI 2019 Workshop on Explainable Artificial Intelligence (XAT),
2019, pp. 7-13.

T. Kulesza, S. Stumpf, W.-K. Wong, M. M. Burnett, S. Perona, A. Ko, I. Oberst, Why-oriented
end-user debugging of naive bayes text classification, ACM Transactions on Interactive Intelligent

Systems (TiiS) 1 (1) (2011) 1-31. doi:10.1145/2030365.2030367.

R. R. Hoffman, G. Klein, S. T. Mueller, Explaining explanation for “explainable ai”, Proceedings
of the Human Factors and Ergonomics Society Annual Meeting 62 (1) (2018) 197-201. doi:10.
1177/1541931218621047.

F Nothdurft, T. Heinroth, W. Minker, The impact of explanation dialogues on human-computer trust,
in: International Conference on Human-Computer Interaction (HCD, Springer, 2013, pp. 59-67.

doi:10.1007/978-3-642-39265-8_7.

C. Brinton, A framework for explanation of machine learning decisions, in: Proceedings of the IJCAT

2017 Workshop on Explainable Artificial Intelligence (XAD, 2017, pp. 14-18.

N. Tintarev, Explanations of recommendations, in: Proceedings of the 2007 ACM Conference on
Recommender Systems, Association for Computing Machinery, New York, NY, USA, 2007, pp. 203-
206. doi:10.1145/1297231.1297275.

R. O. Weber, H. Hong, P. Goel, Explaining citation recommendations: Abstracts or full texts?, in:
Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAD, 2019, pp.
136-142.

45
[96]

[97]

[98]

[99]

[100]

[101]

[102]

[103]

[104]

[105]

L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, L. Kagal, Explaining explanations: An
overview of interpretability of machine learning, in: IEEE 5th International Conference on Data

Science and Advanced Analytics DSAA, 2018, pp. 80-89. doi:10.1109/DSAA.2018.00018.

J. C.-T. Ho, How biased is the sample? reverse engineering the ranking algorithm of facebook’s
graph application programming interface, Big Data & Society 7 (1) (2020) 1-15. doi:10.1177/
2053951720905874.

F. Hohman, A. Head, R. Caruana, R. DeLine, S. M. Drucker, Gamut: A design probe to understand
how data scientists understand machine learning models, in: Proceedings of the 2019 Conference on
Human Factors in Computing Systems (CHI), Association for Computing Machinery, New York, NY,
USA, 2019, pp. 1-13. doi:10.1145/3290605.3300809.

M. Veale, R. Binns, Fairer machine learning in the real world: Mitigating discrimination
without collecting sensitive data, Big Data & Society 4 (2) (2017) 1-17. doi:10.1177/
2053951717743530.

C. Zednik, Solving the black box problem: A normative framework for explainable artificial intelli-

gence, Philosophy & Technology (2019) 1-24doi:10.1007/s13347-019-00382-7.

B. Goodman, S. Flaxman, European union regulations on algorithmic decision-making and a right

to explanation”, AI magazine 38 (3) (2017) 50-57. doi:10.1609/aimag.v3813.2741.

E. I. Sklar, M. Q. Azhar, Explanation through argumentation, in: Proceedings of the 6th International
Conference on Human-Agent Interaction (HAD, Association for Computing Machinery, New York,

NY, USA, 2018, pp. 277-285. doi: 10.1145/3284432.3284470.

I. Lage, D. Lifschitz, F Doshi-Velez, O. Amir, Exploring computational user models for agent policy
summarization, in: Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence
(XAD), 2019, pp. 59-65.

E. S. Dahl, Appraising black-boxed technology: the positive prospects, Philosophy & Technology
31 (4) (2018) 571-591. doi:10.1007/8s13347-017-0275-1.

B. Ghosh, D. Malioutov, K. S. Meel, Interpretable classification rules in relaxed logical form, in:
Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAD, 2019, pp.
14-20.

46
[106]

[107]

[108]

[109]

[110]

[111]

[112]

[113]

[114]

M. T. Stuart, N. J. Nersessian, Peeking inside the black box: a new kind of scientific visualization,

Minds and Machines 29 (1) (2019) 87-107. doi:10.1007/s11023-018-9484-3.

J. Clos, N. Wiratunga, S. Massie, Towards explainable text classification by jointly learning lexicon
and modifier terms, in: Proceedings of the ICAI 2017 Workshop on Explainable Artificial Intelli-
gence (XAD, 2017, pp. 19-23.

J. Zhu, A. Liapis, S. Risi, R. Bidarra, G. M. Youngblood, Explainable ai for designers: A human-
centered perspective on mixed-initiative co-creation, in: IEEE Conference on Computational Intelli-

gence and Games (CIG), IEEE, 2018, pp. 1-8. doi:10.1109/CIG.2018.8490433.

M.-A. Clinciu, H. Hastie, A survey of explainable ai terminology, in: Proceedings of the 1st Workshop
on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019),
Association for Computational Linguistics, 2019, pp. 8-13. doi:10.18653/v1/W19-8403.

C. Henin, D. Le Métayer, Towards a generic framework for black-box explanation methods, in: Pro-

ceedings of the ICAI 2019 Workshop on Explainable Artificial Intelligence (XAD, 2019, pp. 28-34.

P. Madumal, T. Miller, L. Sonenberg, F. Vetere, A grounded interaction protocol for explainable arti-
ficial intelligence, in: Proceedings of the 18th International Conference on Autonomous Agents and
MultiAgent Systems (AAMAS), International Foundation for Autonomous Agents and Multiagent

Systems, Richland, SC, USA, 2019, pp. 1033-1041. doi:10.5555/3306127.3331801.

M. L. Olson, L. Neal, F. Li, W.-K. Wong, Counterfactual states for atari agents via generative deep
learning, in: Proceedings of the I}CAI 2019 Workshop on Explainable Artificial Intelligence (XAI),
2019, pp. 87-93.

Z. Zeng, C. Miao, C. Leung, J. J. Chin, Building more explainable artificial intelligence with argumen-
tation, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18),
AAAI Press, 2018, pp. 8044-8046.

URL https://www.aaai.org/ocs/index.php/AAAT/AAAT18/paper/view/16762

P. Madumal, T. Miller, L. Sonenberg, F. Vetere, Explainable reinforcement learning through a causal
lens, in: Proceedings of the ICAI 2019 Workshop on Explainable Artificial Intelligence (XAD, 2019,
pp. 73-79.

47
[115]

[116]

[117]

[118]

[119]

[120]

[121]

[122]

[123]

[124]

[125]

M. T. Ribeiro, S. Singh, C. Guestrin, ’Why should I trust you?”: Explaining the predictions of any
classifier, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, Association for Computing Machinery, New York, NY, USA, 2016, pp.
1135-1144. doi:10.1145/2939672.2939778.

M. R. Endsley, From here to autonomy, Human Factors: The Journal of the Human Factors and

Ergonomics Society 59 (1) (2017) 5-27. doi:10.1177/0018720816681350.

J. D. Lee, K. A. See, Trust in automation: Designing for appropriate reliance, Human Factors 46 (1)

(2004) 50-80. doi:10.1518/hfes.46.1.50_30392.

R. Parasuraman, V. Riley, Humans and automation: Use, misuse, disuse, abuse, Human Factors: The
Journal of the Human Factors and Ergonomics Society 39 (2) (1997) 230-253. doi:10.1518/
001872097778543886.

K. A. Hoff, M. Bashir, Trust in automation, Human Factors 57 (3) (2014) 407-434. doi:10.1177/
0018720814547570.

R. Parasuraman, D. H. Manzey, Complacency and bias in human use of automation: An attentional

integration, Human Factors 52 (3) (2010) 381-410. doi:10.1177/0018720810376055.

A. Kunze, S. J. Summerskill, R. Marshall, A. J. Filtness, Automation transparency: Implications
of uncertainty communication for human-automation interaction and interfaces, Ergonomics 62 (3)

(2019) 345-360. doi:10.1080/00140139.2018.1547842.

W. Samek, T. Wiegand, K.-R. Miiller, Explainable artificial intelligence: Understanding, visualizing

and interpreting deep learning models, CoRR abs/1708.08296. arXiv:1708.08296.

G. Montavon, W. Samek, K.-R. Miiller, Methods for interpreting and understanding deep neural net-

works, Digital Signal Processing 73 (2018) 1-15. doi:10.1016/ 4.dsp.2017.10.011.

S. Becker, M. Ackermann, S. Lapuschkin, K. Miiller, W. Samek, Interpreting and explaining deep
neural networks for classification of audio signals, CoRR abs/1807.03418. arXiv:1807.03418.

S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller, W. Samek, Analyzing classifiers: Fisher vec-
tors and deep neural networks, in: Proceedings of the IEEE Conference on Computer Vision and

Pattern Recognition, 2016, pp. 2912-2920. doi:10.1109/CVPR.2016.318.

48
[126]

[127]

[128]

[129]

[130]

[131]

[132]

[133]

[134]

R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, N. Elhadad, Intelligible models for healthcare: Pre-
dicting pneumonia risk and hospital 30-day readmission, in: Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, Association for Computing Ma-

chinery, New York, NY, USA, 2015, pp. 1721-1730. doi:10.1145/2783258.2788613.

K. Baum, H. Hermanns, T. Speith, From machine ethics to machine explainability and back, in:
International Symposium on Artificial Intelligence and Mathematics (ISAIM), 2018, pp. 1-8.
URL http://isaim2018.cs.virginia.edu/papers/ISAIM2018 Ethics_Baum_

etal.pdf

C. Luetge, The german ethics code for automated and connected driving, Philosophy & Technology

30 (2017) 547-558. doi:10.1007/s13347-017-0284-0.

S. L. S. Purkiss, P. L. Perrewé, T. L. Gillespie, B. T. Mayes, G. R. Ferris, Implicit sources of bias
in employment interview judgments and decisions, Organizational Behavior and Human Decision

Processes 101 (2) (2006) 152-167. doi:10.1016/4.0bhdp.2006.06.005.

A. Caliskan, J. J. Bryson, A. Narayanan, Semantics derived automatically from language corpora

contain human-like biases, Science 356 (2017) 183-186. doi:10.1126/science.aal4230.

R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods for
explaining black box models, ACM Comput. Surv. 51 (5) (2019) 1-42. doi:10.1145/3236009.

V. Venkatesh, M. G. Morris, G. B. Davis, F. D. Davis, User acceptance of information technology:
Toward a unified view, Management Information Systems Quarterly 27 (3) (2003) 425-478. doi:
10.2307/30036540.

C. McLeod, Trust, in: E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy, fall 2015 Edition,
Metaphysics Research Lab, Stanford University, 2015, pp. 143.

I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud, D. Theron,
P. Barnes, Closing the ai accountability gap: Defining an end-to-end framework for internal algo-
rithmic auditing, in: Proceedings of the 2020 Conference on Fairness, Accountability, and Trans-
parency (FAT*), Association for Computing Machinery, New York, NY, USA, 2020, pp. 33-44.
doi:10.1145/3351095.3372873.

49
[135]

[136]

[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

A. Matthias, The responsibility gap: Ascribing responsibility for the actions of learn-
ing automata, Ethics and information technology 6 (3) (2004) 175-183.  doi:10.1007/
s10676-004—-3422-1.

E. L. Deci, A. H. Olafsen, R. M. Ryan, Self-determination theory in work organizations: The state
of a science, Annual Review of Organizational Psychology and Organizational Behavior 4 (1) (2017)

19-43. doi:10.1146/annurev—orgpsych-032516-113108.

C. Longoni, A. Bonezzi, C. K. Morewedge, Resistance to medical artificial intelligence, Journal of

Consumer Research 46 (4) (2019) 629-650. doi:10.1093/jcr/ucz013.

F.C. Keil, Explanation and understanding, Annual Review of Psychology 57 (1) (2006) 227-254.

doi:10.1146/annurev.psych.57.102904.190100.

J.-F Bonnefon, A. Shariff, I. Rahwan, The social dilemma of autonomous vehicles, Science 352

(2016) 1573-1576. doi:10.1126/science.aaf2654.

B. G. Buchanan, E. H. Shortliffe, Rule-based expert systems: The MYCIN experiments of the Stan-
ford Heuristic Programming Project, Addison-Wesley, 1984.

J. S. Dhaliwal, I. Benbasat, The use and effects of knowledge-based system explanations: Theoretical
foundations and a framework for empirical evaluation, Information Systems Research 7 (3) (1996)

342-362. doi:10.1287/isre.7.3.342.

M. A. Kohl, K. Baum, M. Langer, D. Oster, T. Speith, D. Bohlender, Explainability as a non-
functional requirement, in: TEEE 27th International Requirements Engineering Conference (RE),

2019, pp. 363-368. doi:10.1109/RE.2019.00046.

H. W. De Regt, Understanding scientific understanding, Oxford University Press, 2017. doi:10.
1093/0s0/9780190652913.001.0001.

C. Baumberger, C. Beisbart, G. Brun, What is understanding? An overview of recent debates in
epistemology and philosophy of science, in: S. G. C. Baumberger, S. Ammon (Eds.), Explaining
Understanding: New Perspectives from Epistemolgy and Philosophy of Science, Routledge, 2017,
pp. 1-34.

50
[145]

[146]

[147]

[148]

[149]

[150]

[151]

[152]

[153]

[154]

[155]

F I. Malfatti, On understanding and testimony, Erkenntnis (2019) 1-21doi:10.1007/
s10670-019-00157-8.

C. Baumberger, Types of understanding: Their nature and their relation to knowledge, Conceptus

40 (98) (2014) 67-88. doi:10.1515/cpt—2014-0002.

K. Lambert, On whether an answer to a why-question is an explanation if and only if it yields scientific
understanding, in: G. G. Brittan (Ed.), Causality, method, and modality, Vol. 48, Springer, Dordrecht,
Netherlands, 1991, pp. 125-142. doi:10.1007/978-94-011-3348-7_8.

T. Lombrozo, S. Carey, Functional explanation and the function of explanation, Cognition 99 (2)

(2006) 167-204. doi:10.1016/}.cognition.2004.12.009.

M. T. Chi, N. De Leeuw, M.-H. Chiu, C. Lavancher, Eliciting self-explanations improves understand-
ing, Cognitive Science 18 (3) (1994) 439-477. doi:10.1207/s815516709cog1803\_3.

R. E. Mayer, Cognition and instruction: Their historic meeting within educational psychology, Journal

of educational Psychology 84 (4) (1992) 405-412. doi:10.1037/0022-0663.84.4.405.

S. T. Mueller, R. R. Hoffman, W. Clancey, A. Emrey, G. Klein, Explanation in human-ai systems: A
literature meta-review, synopsis of key ideas and publications, and bibliography for explainable ai,

CoRR abs/1902.01876. arXiv:1902.01876.

C. Kelp, Understanding phenomena, Synthese 192 (12) (2015) 3799-3816. doi:10.1007/
$11229-014-0616-x.

P. J. Feltovich, R. L. Coulson, R. J. Spiro, Learners’ (mis)understanding of important and difficult
concepts: A challenge to smart machines in education, in: Smart Machines in Education: The Coming

Revolution in Educational Technology, MIT Press, Cambridge, MA, USA, 2001, pp. 349-375.

W. B. Rouse, N. M. Morris, On looking into the black box: Prospects and limits in the search for
mental models, Psychological bulletin 100 (3) (1986) 349-363. doi:10.1037/0033-2909.
100.3.349.

L. Rozenblit, F. Keil, The misunderstood limits of folk science: An illusion of explanatory depth,

Cognitive Science 26 (5) (2002) 521-562. doi:10.1207/815516709c0g2605_1.

31
[156]

[157]

[158]

[159]

[160]

[161]

[162]

[163]

[164]

[165]

D. Kuhn, How do people know?, Psychological science 12 (1) (2001) 1-8. doi:10.1111/
1467-9280.00302.

T. Kulesza, S. Stumpf, M. Burnett, S. Yang, I. Kwan, W.-K. Wong, Too much, too little, or just right?
Ways explanations impact end users’ mental models, in: IEEE Symposium on Visual Languages and

Human Centric Computing, IEEE, 2013, pp. 3-10. doi:10.1109/VLHCC.2013.6645235.

J. Tullio, A. K. Dey, J. Chalecki, J. Fogarty, How it works: A field study of non-technical users
interacting with an intelligent system, in: Proceedings of the 2007 Conference on Human Factors in
Computing Systems (CHD, Association for Computing Machinery, New York, NY, USA, 2007, pp.
31-40. doi:10.1145/1240624.1240630.

M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji,
T. Gebru, Model cards for model reporting, in: Proceedings of the 2019 Conference on Fairness,
Accountability, and Transparency, Association for Computing Machinery, New York, NY, USA, 2019,
p. 220-229. doi:10.1145/3287560.3287596.

M. Langer, C. J. Konig, A. Fitili, Information as a double-edged sword: The role of computer ex-
perience and information on applicant reactions towards novel technologies for personnel selection,

Computers in Human Behavior 81 (2018) 19-30. doi:10.1016/4.chb.2017.11.036.

D. T. Newman, N. J. Fast, D. J. Harmon, When eliminating bias isn’t fair: Algorithmic reductionism
and procedural justice in human resource decisions, Organizational Behavior and Human Decision

Processes 160 (2020) 149-167. doi:10.1016/4.obhdp.2020.03.008.

M. Bazire, P. Brézillon, Understanding context before using it, in: A. Dey, B. Kokinov, D. Leake,
R. Turner (Eds.), Modeling and Using Context, Springer, 2005, pp. 29-40. doi:10.1007/
11508373_3.

P. Dourish, What we talk about when we talk about context, Personal and Ubiquitous Computing 8 (1)

(2004) 19-30. doi:10.1007/s00779-003-0253-8.

D.R. Bobocel, A. Zdaniuk, How can explanations be used to foster organizational justice, Handbook

of organizational justice (2005) 469-498.

R. Folger, R. Cropanzano, Fairness theory: Justice as accountability, Advances in organizational

justice 1 (2001) 1-55.

52
[166]

[167]

[168]

[169]

[170]

[171]

[172]

[173]

[174]

[175]

J.C. Shaw, E. Wild, J. A. Colquitt, To justify or excuse?: A meta-analytic review of the effects of ex-
planations, Journal of Applied Psychology 88 (3) (2003) 444-458. doi:10.1037/0021-9010.
88.3.444.

J. Brockner, B. M. Wiesenfeld, An integrative framework for explaining reactions to decisions:
Interactive effects of outcomes and procedures, Psychological Bulletin 120 (2) (1996) 189-208.
doi:10.1037/0033-2909.120.2.189.

R. Wang, EF M. Harper, H. Zhu, Factors influencing perceived fairness in algorithmic decision-
making: Algorithm outcomes, development procedures, and individual differences, in: Proceedings
of the 2020 Conference on Human Factors in Computing Systems (CHI), Association for Computing

Machinery, New York, NY, USA, 2020, pp. 1-14. doi:10.1145/3313831.3376813.

E. A. Lind, K. van den Bos, When fairness works: Toward a general theory of uncer-
tainty management, Research in Organizational Behavior 24 (2002) 181-223. doi:10.1016/
s0191-3085 (02) 24006-x.

J. A. Colquitt, J. M. Chertkoff, Explaining injustice: The interactive effect of explanation and outcome
on fairness perceptions and task motivation, Journal of Management 28 (5) (2002) 591-610. doi:

10.1177/014920630202800502.

P. Liu, Z. Li, Task complexity: A review and conceptualization framework, International Journal of

Industrial Ergonomics 42 (6) (2012) 553-568. doi:10.1016/4.ergon.2012.09.001.

D. A. Wilkenfeld, Functional explaining: A new approach to the philosophy of explanation, Synthese
191 (2014) 3367-3391. doi:10.1007/s11229-014-0452-z.

D. A. Wilkenfeld, D. Plunkett, T. Lombrozo, Depth and deference: When and why we attribute under-
standing, Philosophical Studies 173 (2016) 373-393. doi:10.1007/s11098-015-0497-y.

T. Lombrozo, The instrumental value of explanations, Philosophy Compass 6 (8) (2011) 539-551.
doi:10.1111/4.1747-9991.2011.00413.x.

J. J. Williams, T. Lombrozo, Explanation and prior knowledge interact to guide learning, Cognitive

Psychology 66 (1) (2013) 55-84. doi:10.1016/4.cogpsych.2012.09.002.

53
[176]

[177]

[178]

[179]

[180]

[181]

[182]

[183]

[184]

[185]

[186]

[187]

T. Lombrozo, B. Rehder, Functions in biological kind classification, Cognitive Psychology 65 (4)
(2012) 457-485. doi:10.1016/4.cogpsych.2012.06.002.

C. G. Hempel, Deductive-nomological explanation, in: Aspects of Scientific Explanation, Free Press,

1965, pp. 335-376.

W. C. Salmon, Scientific explanation and the causal structure of the world, Princeton University Press,

1984.
P. Gardenfors, Knowledge in flux: Modeling the dynamics of epistemic states, MIT Press, 1988.

S. Wachter, B. Mittelstadt, C. Russell, Counterfactual explanations without opening the black box:

Automated decisions and the GDPR, Harv. JL & Tech. 31 (2). doi:10.2139/ssrn.3063289.

C. F. Craver, Explaining the Brain, Oxford University Press, Oxford, 2007. doi:10.1093/
acprof:080/9780199299317.001.0001.

J. Pearl, Causality: Models, reasoning, and inference, Cambridge University Press, 2009, 2nd edition.

doi:10.1017/CB09780511803161.

P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction and Search, MIT Press, 2001, 2nd edition.

doi:10.7551/mitpress/1754.001.0001.

D. Borsboom, A. Cramer, A. Kalis, Brain disorders? Not really... Why network structures block
reductionism in psychopathology research, Behavioral and Brain Sciences 42 (2018) 1-54. doi:

10.1017/S0140525xX17002266.

T. Lombrozo, Simplicity and probability in causal explanation, Cognitive Psychology 55 (3) (2007)
232-257. doi:10.1016/4.cogpsych.2006.09.006.

N. Vasilyeva, D. Wilkenfeld, T. Lombrozo, Contextual utility affects the perceived quality
of explanations, Psychonomic Bulletin & Review 24 (2017) 1436-1450. doi:10.3758/
s13423-017-1275-y.

V. Bellotti, K. Edwards, Intelligibility and accountability: Human considerations in context-
aware systems, Human-Computer Interaction 16 (2-4) (2001) 193-212. doi:10.1207/
$15327051hci16234_05.

34
[188]

[189]

[190]

[191]

[192]

[193]

[194]

[195]

[196]

[197]

K. Hartley, L. D. Bendixen, Educational research in the internet age: Examining the role
of individual characteristics, Educational researcher 30 (9) (2001) 22-26. doi:10.3102/
0013189xX030009022.

H. Kauffman, A review of predictive factors of student success in and satisfaction with online learning,

Research in Learning Technology 23. doi:10.3402/rl1t.v23.26507.

D. S. McNamara, E. Kintsch, N. B. Songer, W. Kintsch, Are good texts always better? interactions of
text coherence, background knowledge, and levels of understanding in learning from text, Cognition

and Instruction 14 (1) (1996) 1-43. doi:10.1207/81532690xcil401\_1.

L. Goldberg, Language and individual differences: The search for universals in personality lexicons,
in: W. L. (Ed.), Review of Personality and Social Psychology, vol. 2 Edition, SAGE Publications,
1981, pp. 141-166.

J. T. Cacioppo, R. E. Petty, The need for cognition, Journal of Personality and Social Psychology
42 (1) (1982) 116-131. doi:10.1037/0022-3514.42.1.116.

C. P. Haugtvedt, R. E. Petty, Personality and persuasion: Need for cognition moderates the persistence
and resistance of attitude changes, Journal of Personality and Social Psychology 63 (2) (1992) 308-
319. doi:10.1037/0022-—3514.63.2.308.

T. K. DeBacker, H. M. Crowson, The influence of need for closure on learning and teaching, Educa-

tional Psychology Review 21 (2009) 303-323. doi:10.1007/s10648-009-9111-1.

D. M. Webster, A. W. Kruglanski, Individual differences in need for cognitive closure, Journal of
Personality and Social Psychology 67 (6) (1994) 1049-1062. doi:10.1037/0022-3514.67.
6.1049.

P.M. Fernbach, S. A. Sloman, R. S. Louis, J. N. Shube, Explanation fiends and foes: How mechanistic
detail determines understanding and preference, Journal of Consumer Research 39 (5) (2012) 1115-

1131. doi:10.1086/667782.

L. Hasher, R. T. Zacks, Working memory, comprehension, and aging: A review and a new view,
in: Psychology of Learning and Motivation, Elsevier, 1988, pp. 193-225. doi:10.1016/
s0079-7421 (08) 60041-9.

55
[198]

[199]

[200]

[201]

[202]

[203]

[204]

[205]

[206]

R. Ackerman, T. Lauterman, Taking reading comprehension exams on screen or on paper? A
metacognitive analysis of learning texts under time pressure, Computers in Human Behavior 28 (5)

(2012) 1816-1828. doi:10.1016/4.chb.2012.04.023.

M. S. Prewett, R. C. Johnson, K. N. Saboe, L. R. Elliott, M. D. Coovert, Managing workload in
human-robot interaction: A review of empirical studies, Computers in Human Behavior 26 (5) (2010)

840-856. doi:10.1016/ 4.chb.2010.03.010.

K. Starcke, O. T. Wolf, H. J. Markowitsch, M. Brand, Anticipatory stress influences decision making
under explicit risk conditions, Behavioral Neuroscience 122 (6) (2008) 1352-1360. doi:10.1037/
a0013281.

S. Lupien, F Maheu, M. Tu, A. Fiocco, T. Schramek, The effects of stress and stress hormones on
human cognition: Implications for the field of brain and cognition, Brain and Cognition 65 (3) (2007)

209-237. doi:10.1016/4.bandc.2007.02.007.

P. A. Hancock, On the process of automation transition in multitask human-machine systems, [EEE
Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 37 (4) (2007) 586-
598. doi:10.1109/tsmca.2007.897610.

L. Chazette, O. Karras, K. Schneider, Do end-users want explanations? Analyzing the role of explain-
ability as an emerging aspect of non-functional requirements, in: JEEE 27th International Require-

ments Engineering Conference (RE), 2019, pp. 223-233. doi:10.1109/RE.2019.00032.

L. Chazette, K. Schneider, Explainability as a non-functional requirement: challenges and
recommendations, Requirements Engineering 25 (4) (2020) 493-514. doi:10.1007/
s00766-020—-00333-1.

V. Arya, R. K. E. Bellamy, P.-Y. Chen, A. Dhurandhar, M. Hind, S. C. Hoffman, S. Houde, Q. V.
Liao, R. Luss, A. Mojsilovi¢é, S. Mourad, P. Pedemonte, R. Raghavendra, J. T. Richards, P. Sat-
tiger, K. Shanmugam, M. Singh, K. R. Varshney, D. Wei, Y. Zhang, One explanation does not
fit all: A toolkit and taxonomy of AI explainability techniques, CoRR abs/1909.03012. arXiv:
1909.03012.

J. Woodward, Scientific explanation, in: E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy,
winter 2019 Edition, Metaphysics Research Lab, Stanford University, 2019, pp. 1-101.

56
[207] M. Hall, D. Harborne, R. Tomsett, V. Galetic, S. Quintana-Amate, A. Nottle, A. Preece, A systematic
method to understand requirements for explainable AI (XAI) systems, in: Proceedings of the IJCAI

2019 Workshop on Explainable Artificial Intelligence (XAD, 2019, pp. 21-27.

[208] T. Miller, P. Howe, L. Sonenberg, Explainable AI: Beware of inmates running the asylum. or: How
I learnt to stop worrying and love the social and behavioural sciences, in: Proceedings of the IJCAI

2017 Workshop on Explainable Artificial Intelligence (XAD, 2017, pp. 36-42.

[209] B. Kim, C. Rudin, J. A. Shah, The bayesian case model: A generative approach for case-based rea-
soning and prototype classification, in: Advances in Neural Information Processing Systems, 2014,

pp. 1952-1960.

[210] B. Kim, R. Khanna, S. Koyejo, Examples are not enough, learn to criticize! Criticism for inter-

pretability, in: Advances in Neural Information Processing Systems, 2016, pp. 2280-2288.

[211] J. M. Carroll, M. B. Rosson, Paradox of the active user, in: Interfacing Thought: Cognitive Aspects
of Human-Computer Interaction, MIT Press, Cambridge, MA, USA, 1987, pp. 80-111.

57
The Knowledge Engineering Review, Vol. 36, e5, 1 of 35. © The Author(s), 2021. Published by
Cambridge University Press. This is an Open Access article, distributed under the terms of the Creative
Commons Attribution licence (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted
re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
doi:10.1017/S026988892 1000011

Argumentation and explainable artificial intelligence:
a survey

ALEXANDROS VASSILIADES ©, NICK BASSILIADES' ©, and THEODORE PATKOS’

"School of Informatics, Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece; e-mail: valexande@csd.auth.gr,
nbassili@csd.auth.gr

2
Institute of Computer Science, Foundation for Research and Technology - Hellas, 70013, Heraklion, Greece; e-mail:
patkos @ics.forth.gr

Abstract

Argumentation and eXplainable Artificial Intelligence (XAI) are closely related, as in the recent years,
Argumentation has been used for providing Explainability to AI. Argumentation can show step by step
how an AI System reaches a decision; it can provide reasoning over uncertainty and can find solutions
when conflicting information is faced. In this survey, we elaborate over the topics of Argumentation
and XAI combined, by reviewing all the important methods and studies, as well as implementations
that use Argumentation to provide Explainability in AI. More specifically, we show how Argumentation
can enable Explainability for solving various types of problems in decision-making, justification of an
opinion, and dialogues. Subsequently, we elaborate on how Argumentation can help in constructing
explainable systems in various applications domains, such as in Medical Informatics, Law, the Semantic
Web, Security, Robotics, and some general purpose systems. Finally, we present approaches that combine
Machine Learning and Argumentation Theory, toward more interpretable predictive models.

1 Introduction

Explainability of an Artificial Intelligence (AI) system (i.e., tracking the steps that lead to the decision)
was an easy task in the early stages of AI, as the majority of the systems were logic-based. For this
reason, it was easy to provide transparency to their decisions by providing explanations and therefore to
gain the trust of their users. This changed in the last 20 yr, when data-driven methods started to evolve and
became part of most AI systems, giving them computational capabilities and learning skills that cannot
easily be reached by means of logic languages alone. Eventually, the steadily increasing complexity of
computational evolution of AI methods resulted in more obscure systems.

Therefore, a new research field appeared in order to make AI systems more explainable, called
eXplainable Artificial Intelligence (XAI). The graph is presented in Figure 1, showing the Google
searches that contain the keyword XAI is very interesting, as it shows that people’s searches are steadily
increasing since the mid of 2016, indicating the interest in explaining decisions in AI (the picture was
part of the study Adadi & Berrada 2018). Capturing an accurate definition of what can be considered
an explanation is quite challenging, as can be seen in Miller (2019). Among many definitions, some
commonly accepted ones are:

e An explanation is an assignment of causal responsibility (Josephson & Josephson 1996).

e An explanation is both a process and a product that is it is the process and the result of a Why?
question, Lombrozo (2006).

e An explanation is a process to find meaning or create shared meaning, Malle (2006).

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press © CrossMark
2 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

A A

Figure 1. Google searches with XAI (Adadi & Berrada 2018)

Providing explanations to an AI system has two directions: the first one is to gain trust in a system
or convince a user, and the other is for the scientists to understand how a data-driven model reaches
a decision. The first case has many real-word implementations which explain the decision of a system
to the user. A significant amount of work can be found in the fields of Medical Informatics (Holzinger
et al. 2017; Tjoa & Guan 2019; Lamy er al. 2019), Legal Informatics (Waltl & Vogl 2018; Deeks 2019),
Military Defense Systems (Core et al. 2005; Core er al. 2006; Madhikermi et al. 2019; Keneni er al.
2019), and Robotic Platforms (Sheh 2017; Anjomshoae et al. 2019). In the second case, the studies try to
enhance transparency in the data-driven model (Bonacina 2017; Yang & Shafto 2017; Gunning & Aha
2019; Samek & Miiller 2019; Fernandez er al. 2019); in some cases, visualization are also used (Choo &
Liu 2018).

In the last decade, Argumentation has achieved significant impact to XAI. Argumentation has strong
Explainability capabilities, as it can translate the decision of an AI system in an argumentation proce-
dure, which shows step by step how it concludes to a result. Every Argumentation Framework (AF) is
based upon well-defined mathematical models, from which the basic definitions are close to Set Theory,
extended with some extra properties between the elements. The advantages of Argumentation, which
give aid to XAI, are that given a set of possible decisions, the decisions can be mapped to a graphical
representation, with predefined attack properties that subsequently will lead to the winning decision and
will show the steps that were followed in order to reach it.

This study provides an overview over the topics of Argumentation and XAI combined. We present a
survey that describes how Argumentation enables XAI in AI systems. Argumentation combined with XAI
is a Wide research field, but our intention is to include the most representative relevant studies. We classify
studies based on the type of problem they solve such as Decision-Making, Justification of an opinion, and
Dialogues between Human—System and System—System scenarios and show how Argumentation enables
XAI when solving these problems. Then, we delineate on how Argumentation has helped in providing
explainable systems, in the application domains of Medical Informatics, Law Informatics, Robotics, the
Semantic Web (SW), Security, and some other general purpose systems. Moreover, we get into the field
of Machine Learning (ML) and address how transparency can be achieved with the use of an AF. The
contributions of our study are the following:

1. We present an extensive literature review of how Argumentation enables XAI.

2. We show how Argumentation enables XAI, for solving problems in Decision-Making, Justification,
and Dialogues.

3. We present how Argumentation has helped build explainable systems in the application domains of
Medical Informatics, Law, the SW, Security, and Robotics.

4. We show how Argumentation can become the missing link between ML and XAI.

The remainder of this study is organized as follows. Section 2 presents the motivation and the
contributions of this survey. Section 3 discusses related works and describes other surveys related to
Argumentation or XAI. Section 4 contains the background needed for the terms in the subsequent
sections. In Section 5, we present how Argumentation enables Explainability in Decision-Making,
Justification, and Dialogues. Moreover, we present how agents can use Argumentation to enhance
their Explainability skills and what principles they must follow, in order not to be considered biased.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 3

Subsequently, Section 6 shows how Argumentation helped build explainable systems in various applica-
tion fields. Section 7 elaborates on studies that combine Argumentation and ML, in order to show how
Argumentation can become the missing link between ML and XAI. Finally, we discuss our findings and
conclude in Section 8.

2 Motivation

Argumentation Theory is developing into one of the main reasoning methods to capture Explainability
in AI. The quantity of theoretical studies that use Argumentation to enable Explainability, as well as the
plurality of explainable systems that use Argumentation to provide explanations in so many application
fields that are presented in this survey, is proofs for the importance of Argumentation Theory in XAI.
Nevertheless, previous surveys over Argumentation either do not point out its Explainability capabilities
or present the Explainability capabilities of Argumentation only for a specific domain (see Section 3).
Therefore, we believe that there is a need in the literature for a survey over the topic of Argumentation
and XAI combined, for various problem types and applications domains.

First, we want to present an extensive literature overview of how Argumentation enables XAI. For
this reason, we classify studies based on the most important practical problem types that Argumentation
solves, such as decision-making, justification of an opinion, and explanation through dialogues. Our goal
is to show how Argumentation enables XAI in order to solve such problems. We believe that such a
classification is more interesting for the reader who tries to locate which research studies are related to
the solution of specific problem types.

Second, we want to point out the capabilities of Argumentation in building explainable systems.
We can see that any AI system that chooses Argumentation as its reasoning mechanism for explain-
ing its decision can gain great Explainability capabilities and provide explanations which are closer to
the human way of thinking. Henceforth, using Argumentation for providing explanations makes an AI
system friendlier and more trustworthy to the user. Our goal is to show that Argumentation for build-
ing explainable systems is not committed to one application domain. Therefore, we present an overview
of studies in many domains such as Medical Informatics, Law, the SW, Security, Robotics, and some
general purpose systems.

Finally, our intention is to connect ML, the field that brought to the surface XAI, with Argumentation.
The literature review over studies that combine Argumentation with ML to explain the decision of data-
driven models revealed how closely related those two fields are. For this reason, we wanted to show that
Argumentation can act as a link between ML and XAI.

3 Related work

In this section, we present surveys that are related to Argumentation or XAI. Our intention is to help the
reader to obtain an extensive look in the field of Argumentation or XAI and become aware of its various
forms, capabilities, and implementations. One could read the surveys of Modgil et al. (2013) in order to
understand the uses of Argumentation, Baroni et al. (2011) to understand how Abstract AFs are defined
and their semantics, Amgoud er al. (2008) to understand how Bipolar AFs are defined, Doutre and Mailly
(2018) to understand the dynamic enforcement that Argumentation Theory has over a set of constrains,
and Bonzon et al. (2016) to see how we can compare set of arguments. Moreover, most studies in this
section indicate what is missing in Argumentation Theory or XAI in general and how the gaps should be
filled.

Argumentation is becoming one of the main mechanisms when it comes to explaining the decision
of an AI system. Therefore, understanding how an argument is defined as acceptable within an AF is
crucial. An interesting study to understand such notions is presented in Cohen et al. (2014), where the
authors present a survey which analyzes the topic of support relations between arguments. The authors
talk about the advantages and disadvantages of the deductive support, necessity support, evidential sup-
port, and backing. Deductive support captures the intuition: if an argument A supports argument B, then

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
4 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

the acceptance of A implies the acceptance of B and, as a consequence, the non-acceptance of B implies
the non-acceptance of A. Necessity support triggers the following constraint: if argument A supports
argument B, it means that A is necessary for B. Hence, the acceptance of B implies the acceptance of
A (conversely the non-acceptance of A implies the non-acceptance of B). Evidential support states that
arguments are accepted if they have a support that will make them acceptable by the other participants
in a conversation. Backing provides support to the claim of the argument. The authors showed that each
support establishes different constraints to the acceptability of arguments.

An important survey for solving reasoning problems in an AF is introduced in Charwat er al. (2015),
where the authors show the different techniques of solving implementation issues for an AF. The authors
group the techniques into two different classes. First, the reduction-based techniques where the argu-
mentation implementation problem is transformed into another problem, a satisfiability problem in
propositional logic (Biere et al. 2009), or a constraint-satisfaction problem (Dechter & Cohen 2003),
or an Answer Set Programming (ASP) problem (Fitting 1992; Lifschitz 2019). Reduction-based tech-
niques have the following advantages: (i) they are directly adapted with newer versions of solvers and
(ii) they can be easily adapted to specific needs which an AF may need to obey. While, the other category
called direct approaches refers to systems and methods implementing AF from scratch, thus allowing
algorithms to realize argumentation-specific optimizations.

Argumentation and ML are fused in Longo (2016), Cocarascu and Toni (2016). Longo (2016) in
his study considers that any AF should be divided into sub-components, to make the training of ML
classifiers easier when they are asked to build an AF from a set of arguments and relations between them.
He considers that there should be five different classifiers, one for understanding the internal structure of
arguments, one for the definition of conflicts between arguments, another for the evaluation of conflicts
and definition of valid attacks, one for determining the dialectical status of arguments, and a last one to
accrue acceptable arguments. Thus, he provides in his survey any ML classifier that has been built for
each component, studies that have defined a formalization for the elements of any component, and studies
that provide data for training. Nevertheless, he mentions that there is a lack of data to train a classifier
for each sub-component. On the other hand, Cocarascu and Toni (2016) analyze the implementation of
Argumentation in ML, categorizing them by the data-driven model they augment, the arguments, the AF,
and semantics they deploy. Therefore, they show real-life systems of ML and Reinforcement Learning
models that are aided by Argumentation in the scope of Explainability. Kakas and Michael (2020), in their
survey, elaborate over the topic of Abduction and Argumentation, which are presented as two different
forms of reasoning that can play a fundamental role in ML. More specifically, the authors elaborate
on how reasoning with Abduction and Argumentation can provide a natural-fitting mechanism for the
development of explainable ML models.

Two similar surveys are Moens (2018), Lippi and Torroni (2016), where the authors elaborate over the
topic of Argumentation Mining (AM), from free text, and dialogues through speech. AM is an advanced
form of Natural Language Processing (NLP). The classifiers in order to understand an argument inside
a piece of text or speech must first understand the whole content of the conversation, the topic of the
conversation, as well as the specific key phrases that may indicate whether an argument exists. The
aforementioned actions facilitate the identification of the argument in a sentence or dialog. Further anal-
ysis is necessary to clarify what kind of argument has been identified (i.e. opposing, defending, etc.).
There are two key problems identified for AM systems in both surveys: (i) the fact that they cannot
support a multi-iteration argumentation procedure, since it is hard for them to extract argument from a
long argumentation dialogue, (ii) the lack of training data to train argument annotators, apart from some
great efforts such as: The Debater’ > Debatepedia , Idebate’ , VBATES’, and ProCon’. Moreover, Moens
(2018) talks about studies where facial expressions are also inferred through a vision module to better
understand the form of the argument. Another survey on AM is Lawrence and Reed (2020).

https://www.research.ibm.com/artificial-intelligence/project-debater.

1
2 http:/Avww.debatepedia.org.
3 https://idebate.org.

4 http://vbate.idebate.org.

5

https://www.procon.org.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 5

Finally, the explanation of Case-Based Reasoning (CBR) systems is explored in the survey of Sérmo
et al. (2005). Even though the authors do not include Argumentation in their survey, CBR works similarly
to Case-Based Argumentation. Therefore, one could find interesting information about Explainability
with CBR. Moreover, the authors extend the study of Wick and Thompson (1992), in which reasoning
methods that take into consideration the desires of the user and the system are presented, in order to
follow explanation pipelines. The pipelines capture the different methods that can help a system reach a
decision:

. Transparency: Explain How the System Reached the Answer.

. Justification: Explain Why the Answer is a Good Answer.

. Relevance: Explain Why a Question Asked is Relevant.

. Conceptualization: Clarify the Meaning of Concepts.

. Learning: Teach the user about the domain to state the question better.

mB WN

On the other hand, the literature of surveys for XAI is also rich. A smooth introduction to XAT is the
paper of Miller (2019), where the author reviews relevant papers from philosophy, cognitive psychology,
and social psychology and he argues that XAI can benefit from existing models of how people define,
generate, select, present, and evaluate explanations. The paper drives the following conclusions: (1) Why?
questions are contrastive; (2) Explanations are selected in a biased manner; (3) Explanations are social;
and (4) Probabilities are not as important as causal links. As an extension of these ideas, we can see the
extensive survey of Atkinson er al. (2020) on the topic of Explainability in AI and Law.

Fundamentally, XAI is a field that came to the surface when AI systems moved from logic-based to
data-driven models with learning capabilities. We can see this in the survey of Adadi and Berrada (2018),
where the authors show how XAI methods have developed during the last 20 yr. As it was natural, data-
driven models increased the complexity of tracking the steps to reach a decision. Thus, the Explainability
of a decision was considered as a ‘black box’. For this reason, a lot of studies have tried to provide
even more Explainability to data-driven models especially in the last decade. We can see many similar
surveys that describe the Explainability methods which are considered state of the art, for the decision
of various data-driven models in MoZina et al. (2007), DoSilovié et al. (2018), Schoenborn and Althoff
(2019), Guidotti et al. (2018), Collenette er al. (2020). Nevertheless, there are many AI systems that still
have not reached the desired transparency for the way they reach their decisions. A survey with open
challenges in XAI can be found in the study of Das and Rad (2020).

Deep learning is the area of ML that is the most obscure to explain its decision. Even though many
methods have been developed to achieve the desired level of transparency, there are still a lot of open
challenges in this area. A survey that gathers methods to explain the decision of a deep learning model, as
well as the open challenges, can be found in Carvalho er al. (2019). In this scope, we can find other more
practical surveys that talk about Explainability of decision-making for data-driven models in Medical
Informatics (Tjoa & Guan 2019; Pocevititté er al. 2020) or in Cognitive Robotic Systems (Anjomshoae
et al. 2019). The last three studies present how data-driven models give explanations for their decision in
the field of Medical Informatics in order to recommend a treatment, to make a diagnosis (with the help of
an expert making the final call), and image analysis to classify an illness, for example through magnetic
resonance images to classify if a person has some type of cancer.

A theoretical scope on why an explanation is desired for the decision of an AI system can be found
in the study of Koshiyama er al. (2019), where the authors argue that a user has the right to know every
decision that may change her life. Hence, they gather AI systems that offer some method of providing
explanations for their decisions and interact with human users. This study was supported 2 yr ago by the
European Union which has defined new regulations about this specific topic (Regulation 2016). Another,
human-centric XAI survey is that of Paez (2019), where the author shows the different types of acceptable
explanations of a decision based on cognitive psychology and groups the AI systems according to the
type of explanation they provide. Moreover, the author reconsiders the first grouping based on the form
of understanding (i.e., direct, indirect, etc) the AI systems offer to a human.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
6 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

4 Background

The theoretical models of Argumentation obtained a more practical substance in 1958 by Toulmin
through his book The Uses of Argumentation (Toulmin 1958), where he presented how we can
use Argumentation to solve every day problems. Yet, the demanding computational complexity lim-
ited the applicability of AFs for addressing real-world problems. Fortunately, during the last 20 yr,
Argumentation Theory was brought back to the surface, new books were introduced (Walton 2005;
Besnard & Hunter 2008), and mathematical formalizations were defined. The Deductive Argumentation
Framework (DAF) (Besnard & Hunter 2001) is the first mathematical formalization describing an AF
and its non-monotonic reasoning capabilities.

In this section, we will give the basic definitions of an Abstract Argumentation Framework (AAF)
(Dung 1995; Dung & Son 1995) and we will introduce several extensions of the AAF. More specifically,
we are going to talk about the Structured Argumentation Framework (SAF) (Dung 2016), the Label-
Based Argumentation Framework (LBAF) (Caminada 2008), the Bipolar Argumentation Framework
(BAF) (Cayrol & Lagasquie-Schiex 2005), the Quantitative Bipolar Argumentation Framework (QBAF)
(Baroni et al. 2018), and the Probabilistic Bipolar Argumentation Framework (PBAF) (Fazzinga et al.
2018). All these frameworks are used in studies mentioned in the next sections, so to avoid over analysis,
we will give the definitions of the AAF and we will mention the dimension being extended from its
variations.

Deductive Argumentation Framework: DAF is defined only on first-order logic rules and terms, and
all the aforementioned frameworks are built upon it. DAF considers an argument as a set of formulae
that support a claim, using elements from a propositional (or other type) language A. Thus, arguments
in DAF are represented as (T', 6), where T C A denotes the set of formulae and is called the support of
the argument, which help establish the claim ¢. The following properties hold: (1) T c A, Gi) TF ¢, (iii)
I KL, and (iv) I is minimal with respect to set inclusion for which (i), (ii), and (iii) hold. Furthermore,
there exist two types of attack, undercut and rebut.

1. For any propositions ¢ and y, ¢ attacks yw when ¢ = —7% (the symbol — denotes the strong negation).
2. Rebut: (IT), #1) rebuts (Tz, $2), if @; attacks ¢.

3. Undercut: (T), @;) undercuts (2, $2), if attacks some w ET.

4. (Ty, 1) attacks (T'2, $2), if it rebuts or undercuts it.

Abstract Argumentation Framework: An AAF is a pair (A, R), where A is a set of arguments
and RC Ax A a set of attacks, such that Va,be.A the relation (a, b)¢R means a attacks b
(equivalently (b, a) € R means b attacks a). Let, SC A we call:

1. S conflict free, if Va, b € S holds (a, b) € R (or (b, a) ER).
2. S defends an argument a € A if Vb € A such that (b, a) ER, Je € S and (c, NER.
3. Sis admissible if is conflict free, and Va € S, Vb € A, such that (b, a) ER, Jc €S holds (c, b) Ee R.

Next, the semantics of AAF are specific subsets of arguments, which are defined from the aforemen-
tioned properties. But first, we need to introduce the function 7, where F : 2A -s 2A, such that for § CA,
F (S) = {a|a is defended by S}. The fixpoint of a function F given a set S is a point where the input of
the function is identical to the output, F (S) = S.

1. Stable: Let S C A, S is a stable extension of (A, R), iff S is conflict free and Va € A \ S, 4b € S such
that (b, a) ER.

2. Preferred: Let S C A, Sis a preferred extension of (A, R), iff S is maximal for the set inclusion among
the admissible sets of A.

3. Complete: Let S C A, S is a complete extension of (A, R), iff S is conflict-free fixpoint of F.

4. Grounded: Let S C A, S is a grounded extension of (A, R), iff S is the minimal fixpoint of F.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 7

Structured Argumentation Framework: SAF represents the arguments in the form of logical rules (Rule
(1)), and it introduces the constraints of preference between arguments. First, we need to define some
new concepts. A theory is a pair (7, P) whose sentences are formulae in the background monotonic
logic (L, ) of the form L<,...,L, where L,L,,..., L, are ground literals. Henceforth, 7 is a
set of ground literals, and P is a set of rules which follow the general form label : claim < premise
(Rule (1)).

ria<b,..., by (1)

forné€N, anda, b),...,b,€T.

Rule (1) is understood as if the facts b;,..., 5, are true, then its claim a is true, otherwise if any
of the facts is false, the claim is false. Additionally, if we have two rules r,r’ similar to Rule (1), we
define the preference of r over r’ by prefer(r,r’). The arguments in SAF have the same format similar to
Rule (1). Therefore, when we say we prefer an argument a over b, we mean the relation prefer(a,b). If the
preference rules are removed from the framework, then it is also called Assumption-Based Argumentation
Framework (ABA) (Dung et al. 2009), where a set of assumptions (i.e. body of rule) support a claim (i.e.
head of rule). Additionally, ABA can tackle incomplete information because if we do not have all the
literals from the body of a rule, we can make some assumptions, to fill the missing information.

Label-Based Argumentation Framework: LBAF is a framework where the arguments are character-
ized by a label, which defines the acceptability of an argument. Briefly:

1. an argument is labeled in if all of its attackers are labeled out and is called acceptable.
2. an argument is labeled out if at least one of its attackers is labeled in.
3. an argument is labeled undec, when we cannot label it neither in nor out.

Keeping this in mind, let (A, R) be an LBAF, then the framework can be represented through a function
£:A-— {in, out, undec}. The function £ must be an injection (i.e. Va € A, IL(@) € {in, out, undec}).

Bipolar Argumentation Framework: A BAF is a triplet (A, R*, R-), where as before A is a set of
arguments, R* C A x Aisa binary relation called support relation, and R7~ C A x Aisa binary relation
called attack relation. Therefore, Va, b € A if (a, b) © R™, we say the argument a supports argument b,
equivalently Va, b € Aif (a,b) ER”, we say the argument a attacks argument b.

Quantitative Bipolar Argumentation Framework: QBAF is an extension of BAF and is a 5-tuple
(A, R*,R-,T, o), where A, R*, R™ are the same as in BAF, and t: A> K isa base score function.
The function t gives initial values to the arguments from a preorder set of numerical values K, mean-
ing that K is equipped with a function <, such that Va, b EK if a <b, then b <a. Another important
component of QBAF is the strength of an argument, which is defined by a total function o : A> K.

Probabilistic Bipolar Argumentation Framework: PBAF is the last AF we will mention and is also
an extension of BAF. PBAF is quadruple (A, R*, R~, P) where (A, Rt, R-) isa BAF and P isa
probability distribution function over the set PD = {(A’, R'T, R'") | A! CAAR™ C(A'x ANN ROA
Rt C(A’ x AOR}. The elements in PD ((A, RR, P)) called possible BAFs are the possible
scenario that may occur and are represented through a BAF which was extended with probabilities.

5 Argumentation and Explainability

Explainability serves a much bigger goal than just the desire of computer scientists to make their system
more transparent and understandable. Apart from the fact that Explainability is an aspect that justifies the
decision of an AI system, it is also a mandatory mechanism of any AI system that can take decisions which
affect the life of a person (Core et al. 2006). For example, by making an automated charge to our credit
card for a TV show that we are subscribers of, or booking an appointment to a doctor that we asked our
personal AI helper to make last month, among others. The European Union has defined regulations that
obligate a system with this kind of characteristics to provide explanations over their decisions (Regulation
2016). Therefore, in this section, we describe how Argumentation enables Explainability. We can see

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
8 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

that explaining a decision with argumentation is not something that emerged in recent years but existed
in Argumentation Theory from its beginning (Pavese 2019).

We are going to divide this chapter based on the most important baselines that can provide explanation
through Argumentation; Decision-Making, Justification, and Dialogues. Moreover, we will present some
basic studies that establish the field of XAI through Argumentation. But first, we offer a literature review
on how agents can use Argumentation, and the Ethics that Argumentation should follow. We consider
these two subsections important, in order to show how XAI can be implemented in an agent through
Argumentation, and what principles the agent must follow in order to be considered unbiased.

5.1 Agents and argumentation

Single agent systems (SAS) and multi-agent systems (MAS) can be built upon various forms of logic,
such as first-order logic (Smullyan 1995), description logic (Nute 2001), propositional logic (Buvac &
Mason 1993), and ASP (Fitting 1992; Lifschitz 2019). One could read the book of Wooldridge (2009) to
see the connections of SAS and MAS with first-order logic and other forms of logic, as well as to take a
glimpse to Agents Theory in general.

At this point, we are going to describe the most important studies that share Argumentation Theory
and Agents Theory either in SAS or MAS. One of the first studies that addressed this issue was that
of Kakas and Moraitis (2003, 2006), in which the authors presented an AF to support the decisions
of an agent. They consider a dynamic framework where the strength of arguments is defined by the
context and the desires of the agent. Also, the concept of abduction is used by the agents in this
framework. When they are faced with incomplete information, the agents can make hypotheses based
on assumptions. Another important aspect is that of the personality of an agent. Based on definitions
from cognitive psychology, the authors give to each agent its own beliefs and desires translated into the
AF as preferences rules. For instance, let the two arguments a = ‘I will go for football after work’, and
b = ‘Bad news, we need to stay over hours today, to finish the project’. Obviously, these two arguments
are in conflict; thus, we need a meta-argument preference hierarchy. If, additionally, we knew that argu-
ment b is stated by the employer, and argument a by the employee, then it would hold prefer(b,a). We
can see implementations of such theoretical frameworks in Panisson et al. (2014), Panisson and Bordini
(2016), where the authors use the AgentSpeak programming language to create agents that argue over a
set of specific beliefs and desires using description logic, as well as in Spanoudakis et al. (2016b) where
agents argue in a power saving optimization problem between different stakeholders.

Next, Amgoud provides two studies (Amgoud & Serrurier 2007, 2008) where agents are able to argue
and explain classification. They consider a set of examples 2’, a set of classes C, and a set of hypothesis
H. which are governed by a pre-ordered relation < that defines which hypothesis is stronger. Then, an
example x € 4’ is classified in a class c €C by the hypothesis # € H,, and an argument for this statement is
formalized as a = (h, x, c). It is easily understood that other hypotheses could classify the same example
to other classes; thus, when all the arguments are created, an AAF is generated.

An important need for agents which use AFs is the capability of understanding natural language and
performing conversations with humans (Kemke 2006; Liu et al. 2019). Understanding natural language
and having a predefined protocol for conversation ease the exchange of arguments (Willmott et al. 2006;
Panisson et al. 2015), allow the agents to perform negotiations (Pilotti et al. 2015), be more persuasive
(Black & Atkinson 2011; Rosenfeld & Kraus 2016b), and to explain in more detail how they reached a
decision (Laird & Nielsen 1994).

The recent study of Ciatto et al. (2015) proposes an AAF for an MAS focusing on the notions of
Explainability and Interpretation. The authors define the notion of interpreting an object O (i.e., inter-
acting with an object by performing an action), as a function / that gives a score in [0,1] with respect to
how interpretable the object O is to the agent, when the agent wants to perform an action. The authors
consider an explanation as the procedure to find a more interpretable object x’ from a less interpretable x.
Thus, when a model M : 4’ — Y maps an input set of objects 1’ to an output set of actions .V, the model
is trying to construct a more interpretable model M’. An AAF can then use this procedure to explain why
a set of objects is considered more interpretable.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 9

5.2 Argumentation and ethics

An agent in order to be trustworthy when it argues about a topic, it needs to be unbiased. For an agent to
be considered unbiased, it must not support only its personal interest through the argumentation dialogue
with other agent(s). Personal interests are usually supported through fallacies, such as exaggerations of
the truth, or with unethical/fake facts. For this reason, the notions of ethics and argumentation are closely
related to the problem of tackling biased agents (Correia 2012). Moreover, knowing the ethics that an
agent follows when it argues is crucial as it enhances the Explainability, by allowing the opposing party
to see how the agent supports its personal interests, and to accurately create counter arguments.

The combination of ethics and argumentation to construct unbiased agents who achieve argumenta-
tional integrity (Schreier et al. 1995) is mostly used for persuading the opposing agent(s) (Ripley 2005).
Ethics in argumentation is also implemented in legal cases to conduct fair trials (Czubaroff 2007), and in
medical cases for patients privacy (Labrie & Schulz 2014).

The existence of ethics in argumentation is very important in decision-making problems that have
conflicting interests between the participants. Especially in scenarios where the proper relations are
mandatory, Ethics in Argumentation becomes a necessity. The authors in Mosca er al. (2020) propose a
model where an agent works as a supervisor over decision-making problems where conflicting interests
exist, for sharing content online. The agent takes into consideration the personal utility and the moral
values of each participant and justifies a decision. A similar study is Langley (2019) for more generic
scenarios.

E-Democracy is an evolving area of interest for governments wishing to engage with citizens through
the use of new technologies. E-Democracy goal is to motivate young persons to become active members
of the community by voting over decisions for their community through web applications and argue if
they disagree with a decision that is at stake. It is easily understood that ethics is an important aspect in
the argumentation dialogues of an e-Democracy application. Citizens and the government should not be
biased only in favor of their own personal interest but for the interest of the community. More specifically,
citizens should think if a personal request affects negatively the other members of the community, while
the government should consider if the decision that it is proposing has indeed positive results to the
community or is only good for the popularity of the members of the government. In Cartwright and
Atkinson (2009), Wyner er al. (2012b), we can see many web applications for e-Democracy, while in
Atkinson et al. (2005a), e-Democracy is used to justify the proposal of an action.

The idea of e-Democracy goes one step further with Wyner et al. (2012a), where the authors propose
a model to critique citizens proposals. The authors use Action-based Alternative Translating scheme
(Wooldridge & Van Der Hoek 2005) to accept or reject the justification of a proposal and automatically
provide a critique on the proposal, using practical reasoning. The critique is in the form of critical ques-
tions which are provided by the Action-based Alternative Translating scheme. Similarly, Atkinson et ai.
(2011) use AF with values to critique citizens proposals, and Wardeh et al. (2013) provide web-based
tools for this task.

5.3 XAI through Argumentation

In this section, we will present some important studies that lead to the conclusion that Argumentation
Theory is one of the most suitable models to provide explanations to AI systems.

The studies of Fan and Toni are very important in this field, as they provide a methodology for com-
puting explanation in AAF (Fan & Toni 2014) and ABA (Fan & Toni 2015a). In the former, the authors
define the notion of explanation as: given an argument A defended by a set of arguments S, A is called
topic of S and S is the explanation of A. Then, they call compact explanation of A the smallest S with
respect to subset relation, equivalently verbose explanation of A the largest S with respect to subset rela-
tion. Moreover, based on the notion of Dispute Trees, they provide another method of explanation, with
respect to the acceptability semantics of Dispute Trees. A Dispute Tree is defined as follows: (i) the
root element is the topic of discussion, (ii) each node at odd depth is an opponent, (iii) each node at even
depth is a proponent, and (iv) there does not exist node which is opponent and proponent at the same time.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
10 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

Then, Admissible Disputed Trees are those that each path from the root to the leaf has even length, and
the root argument is called acceptable. Furthermore, Dispute Forest is the set of all Admissible Disputed
Trees. It is easily seen that the set of Dispute Forest contains all the explanations for why an argument A
is acceptable. While in the next study (Fan & Toni 2015a), the authors extend these definitions to ABA.

Next, two studies present a formalization on how to model Explainability into an AF, in the context
of solving scientific debates. In SeSelja and StraSer (2013), the authors consider the formalization of an
explanation as an extra relation and a set in the AF. More specifically, given an AF (A, ) an explainable
AF is a 5-tuple (A X,RR', R) such that 4’ is the set of topics that receive arguments from A as
explanations, the relation R’ defines that an argument a € A is part of the explanation of an element
from x € X, and finally states that some arguments may not exist simultaneously in an explanation
for a topic because their co-occurrence brings inconsistencies. Therefore, an explanation is a set € that
contains all the arguments from A that are connected through ’ with an element from x € V and do
not co-exist in R. On the same, principles are the formalization of Sakama (2018). These formalizations
were used for Abduction in Argumentation Theory, to model criticism inherent to scientific debates in
terms of counter-arguments, to model alternative explanations, and to evaluate or compare explanatory
features of scientific debates.

Case Base Reasoning (CBR) is one of the most commonly used methods in providing explanations
about decisions of an AI system. Many studies use CBR to provide explanations in combination with
Argumentation. In Cyras et al. (2016a); Cyras et al. (2016b), the authors use CBR to classify arguments
to a set of possible options, and when new information is inserted to the Knowledge Base (KB), the class
of the argument may change.

Imagine we have the options O, = book this hotel, O27 = do not book this hotel, and our criteria are
that the hotel should be close to the city center and cheap.

We find the hotel H to be close to the city center. Then, we have an argument for booking hotel H, but

when we look at the price, we see that it is too expensive for us. Then, we have a new argument not to
book the hotel H.
We can understand that this method is close to explanation through dialogues, where each step adds new
information to the KB. Therefore, the authors also provide an illustration of their framework with Dispute
Trees. Another study that uses CBR and Argumentation is Cyras er al. (2019), where the authors give a
framework that explains why certain legislation passes and others not, based on a set of features. They
use the features of: (i) The Type, if the legislation is proposed by the Government, Private Sector, etc,
(ii) The Starting House Parliament (it is a UK study; thus, the authors consider the House of Lords and
the House of Commons), (iii) The number of legislations that are proposed, (iv) Ballot Number, and (v)
Type of Committee. Another CBR model that classifies arguments based on precedents and features, for
legal cases, is presented in Bex et al. (2011). The authors use a framework that takes in consideration
information from the KB in order to classify the argument. More specifically, given a verdict that a person
stole some money, under specific circumstances, the system must classify the argument if the defendant is
guilty or not. The system will search for similar cases in its KB to make an inference based on important
features such as type of crime, the details of the legal case, the age of the defendant, and if the defendant
was the moral instigator.

The aspect of explanation of query failure using arguments is studied in Arioua et al. (2014). More
specifically, the authors elaborate on query failures based on Boolean values in the presence of knowledge
inconsistencies, such as missing and conflicting information within the ontology KB. The framework
supports a dialectical interaction between the user and the reasoner. The ontology can also construct
arguments from the information in the ontology on the question. The user can request for explanations
on why a question Q holds or not, and it can follow up with questions to the explanation provided by the
framework.

5.4. Decision-making with argumentation

Argumentation is highly related to Decision-Making. In fact, it has been stated that Argumentation
was proposed in order to facilitate Decision-Making (Mercier & Sperber 2011). The contributions of

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 11

Argumentation in Decision-Making are plenty, with the most important being support or opposition of a
decision, reasoning for a decision, tackling KBs with uncertainty, and recommendations.

The problem of selecting the best decision from a variety of choices is maybe the most popular among
studies that combine Argumentation and Decision-Making. In Amgoud and Prade (2009), the authors
present the first AAF for Decision-Making used by MAS. They propose a two-step method of mapping
the decision problem in the context of AAF. First, the authors consider beliefs and opinions as arguments.
Second, pairs of options are compared using decision principles. The decision principles are: (i) Unipolar
refers only to the arguments attacks or defenses; (ii) Bipolar takes into consideration both; and (iii) Non-
Polar is those that given a specific choice (the opinion), an aggregation occurs, such that arguments
pros and cons disappear in a meta-argument reconsideration of the AAF. Moreover, the authors test
their framework under optimistic and pessimistic decision criteria (i.e., a decision may be more desirable
or less than other), and decision-making under uncertainty. Decision-Making under uncertainty is also
presented in Amgoud and Prade (2006), where the authors try to tackle uncertainty over some decisions
by comparing alternative solutions. Pessimistic and optimistic criteria are also part of the study.

In Zhong er al. (2014), the authors define an AF that takes into consideration information from the
KB to make a decision using similar decisions. The framework first parses the text of the argument
and extracts the most important features (nouns, verbs). Then, it compares with the decisions in the KB
and returns the most similar decision, with respect to the quantity of common features. The framework
can back up its decision with arguments on how similar the two cases are and uses arguments which
were stated for the similar case in the KB. On the other hand, in Zeng er al. (2018), the authors use a
Decision Graph with Context (DGC), to understand the context, in order to support a decision. A DGC is
a graph, where the decisions are represented as nodes, and the interactions between them (attacking and
supporting) as edges. The authors map the DGC in an ABA by considering decisions as arguments and
the interactions between them as attack and support relations. Then, if a decision is accepted in the ABA,
it is considered a good decision.

Decision-Making and Argumentation are also used to support and explain the result of a recommenda-
tion system (Friedrich & Zanker 2011; Rago et al. 2018). Recommendation systems with Argumentation
resemble feature selection combined with user evaluation on features. A recommendation system is

6-tuple (M, AT, T, £,U, R) such that:

1. M isa finite set of items.

2. AT isa finite, non-empty set of artributes for the items.
3. 7 aset of types for the attributes.

4. the sets M and AT are pairwise disjoint.

5. Y=MUAT.

6. £C(M x AT) is a symmetrical binary relation.

7. U isa finite, non-empty set of users.

8. R:U x X > [-1, 1] a partial function of ratings.

Mapping the recommendation system to an AF is done after the ratings have been given by a variety
of users. Arguments are the different items from M, and positive and negative ratings to the attributes
related to an item from M as supports or attacks.

Decision-Making for MAS in an ABA is presented in Fan et al. (2014). The authors consider that
agents can have different goals and decisions hold attributes that are related to the goal of each agent. In
their case, the best decision is considered as an acceptable argument in the joint decision framework of
two different agents. Moreover, the authors define trust between agents, meaning that the arguments of
an agent are stronger than the arguments of others in the scope of some scenario.

5.5 Justification through argumentation

Justification is a form of explaining an argument, in order to make it more convincing and persuade the
opposing participant(s). Justification uses means of supporting an argument with background knowledge,

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
12 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

defensive arguments from the AF, and external knowledge. One important study in this field is Cyras
et al. (2017), where with the help of ABA and Dispute Trees, the authors show how easy it is to justify
if an argument is acceptable or not, just by reasoning over the Dispute Tree. Similarly, Schulz and Toni
(2016) provide two methods of Justification for a claim that is part of an ASP, both using correspondence
between ASP and stable extensions of an ABA. The first method relies on Attack Trees. The authors
consider an Attack Tree as: given an argument A, the root of the tree, and the children being the attackers
of A, iteratively each node in the tree has as children only its attackers. An Attack Tree is constructed for
the stable extension of an ABA and is using admissible fragments of the ASP. If the literals that form the
argument are part of the fragment, then the argument is justified. The second justification method relies
on the more typical method of checking if there exists an Admissible Dispute Tree for the argument.

Preference rules are usually used to justify the acceptability of an argument. We can see such studies
in Melo et al. (2016), Cerutti et al. (2019). Acceptability of an argument is easily explained through
preference rules, due to the fact that preference rules are a sequence of preferences between logic rules.
Melo er al. (2016) present preferences over arguments formed from information of different external
sources by computing the degree of trust each agent has for a source. The authors define the trust of
a source ¢ as a function tr(¢) € [0, 1]. Given an argument A with supporting set S={¢1,..., @,} from
different external sources, the trust of an argument is given by Equation (2), where @ is a generic operator
(ie., it could be any operator according to the characteristics of the problem we try to solve).

1r(A) =tr(b1) @... @tr(Gn) (2)

Moreover, the authors consider two different types of agent’s behaviors: (i) Credulous agents trust only
the most trustworthy source (the one with the biggest score from tr), and (ii) Skeptical agents consider
all the sources from which they received information. Their study was based on Tang et al. (2012),
where the authors also define trust of arguments in MAS. Cerutti er al. (2019) designed the ArgSemSAT
system that can return the complete labelings of an AAF and is commonly used for the justification of
the acceptability of an argument. ArgSemSAT is based on satisfiability solvers (SAT), and its biggest
innovations are: (i) it can find a global labeling encoding which describes better the acceptability of an
argument, (ii) it provides a method where if we compute first the stable extensions we can optimize the
procedure of computing the preferred extensions, and (iii) it can optimize the labeling procedure and
computation of extensions of an AAF, with the help of SAT solvers and domain-specific knowledge.

Justification for Argument-Based Classification has been the topic of the study in Thimm and Kersting
(2017). The authors propose a method of justifying the classification of a specific argument, based on the
features that it possesses. For instance, X should be classified as a penguin because it has the features
black, bird, not(fly), eatsfish. An advantage of using classification based on features is that it makes
explanation an easy task.

One common method to justify an argument is by adding values to the AF. There are cases where
we cannot be conclusive that either party is wrong or right, in a situation of practical reasoning. The
role of Argumentation in a case like this is to persuade the opposing party rather than to lead to mutually
satisfactory compromises. Therefore, the values that are added to an AF are social values, and whether an
attack of one argument on another succeeds depends on the comparative strength of the values assigned
to the arguments. For example, consider the argument A/ from BBC, and the argument A2 from Fox
News.

Al = The weather tomorrow will be shinny
A2 = The weather tomorrow will be rainy

Obviously, those two arguments are in conflict; yet, we wish to reach to some conclusion about the
weather, even an uncertain one, in order to plan a road trip. In this case, adding social values to the AF
will solve the problem. For instance, a naive way is to define a partial order by relying on an assignment
of trustworthiness: if we trust information arriving from BBC more than from Fox News, we can use
this order to reach to the conclusion. Another way is to take a third opinion and consider valid the
argument that is supported by two sources. These ideas were implemented in an AF by Bench-Capon in

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 13

Bench-Capon (2003a); Bench-Capon (2002), where the author extends an AAF by adding values (AFV).
Subsequently, AFVs were used to solve legal conflicts (Bench-Capon 2003b; Bench-Capon et al. 2005),
to infer inconsistency between preferences of arguments (Amgoud & Cayrol 2002a), and to produce
acceptable arguments (Amgoud & Cayrol 2002b).

The notions of AFVs were extended by Modgil in Hierarchical Argumentation Frameworks (Modgil

2006a, b, 2009). Intuitively, given a set of values {a,, ..., a,}, an extended AAF (i.e., attacks and defence
relations exist) is created for each value ((A;, R1), ---, (Ay, Ry)). A; contains arguments whose value
is a; and R, attacks which are related to the arguments of A;, for ie {1,..., a}. This mapping helped

to accommodate arguments that define preferences between other arguments, thus incorporating meta
level argumentation-based reasoning about preferences at the object level. Additionally, the studies of
Coste-Marquis et al. (2012a, b) and Bistarelli et ad. (2009) depict more accurately how the social values
are translated into numerical values in a single AAF, extending the studies of Dunne et al. (2009), Dunne
et al. (2011).

AFVs had a significant impact contributed significantly to practical reasoning, that is, reasoning about
what action is better for an agent to perform in a particular scenario. The authors in Atkinson and Bench-
Capon (2007b) justify the choice of an action through an argumentation scheme, which is subjected to
a set of critical questions. In order for the argument scheme and critical question to be given correct
interpretations, the authors use the Action-Based Alternating Transition System as the basis of their def-
initions. The contributions of AFVs are for the justification of an action, to show how preferences based
upon specific values emerge through practical reasoning. The authors use values in the argumentation
scheme to denote some descriptive social attitude or interest, which an agent (or a group of agents) wish
to hold. Moreover, the values provide an explanation for moving from one state to another, after an action
is performed. Therefore, values in this argumentation scheme obtain a qualitative, rather than a quanti-
tative meaning. Two extensions of this study are Atkinson and Bench-Capon (2007a) and (2018). In the
former, the agent must take into consideration the actions of another agent when it wants to perform an
action. While in the latter, the agent must take in consideration the actions of all the agents that exist in a
framework. An implementation of this argumentation scheme for formalizing the audit dialogue in which
companies justify their compliance decisions to regulators can be found in Burgemeestre et al. (2011).

5.6 Dialogues and argumentation for XAI

Explaining an opinion by developing an argumentation dialogue has its roots in Argumentation Dialogue
Games (ADG) (Levin & Moore 1977), which existed long before Dung presented the AAF (Dung 1995).
These dialogues occur between two parties which argue about the tenability of one or more claims or
arguments, each trying to persuade the other participant to adopt their point of view. Hence, such dia-
logues are also called persuasion dialogues. Dung’s AAF (Dung 1995) enhanced the area of ADG and
helped many scientists to implement the notions of ADG in an AF (Hage et al. 1993; Bench-Capon 1998;
Bench-Capon et al. 2000).

Nevertheless, the AAF of Dung helped only to some extent because it could not capture all the aspects
of an ADG. For instance, Dung’s AAF could not capture the notion of a clear reply structure, where each
party waits for its turn in order to place a new argument. Henry Prakken identified this disadvantages and
introduced a new AF which could capture all the aspects of an ADG (Prakken & Sartor 1998; Prakken
2005b). More specifically, the author constructed an AF with a clear reply structure, where each dia-
logue moves either attacks or surrenders, following a preceding move of the other participant, and allows
for varying degrees of coherence and flexibility when it comes to maintaining the focus of a dialogue.
Moreover, the framework can be implemented in various logics.

Subsequently, Verheij (2003) implemented the ideas of ADG and constructed two argument assistance
tools, to guide the user in the computation of arguments. The author considers a context of argumentation
in law. Moreover, the author uses defeasible reasoning, meaning that each argument no matter how
commonly accepted it is, it can be questioned. Similar studies where defeasible argumentation is used are
Gordon (1993) and Loui and Norman (1995).

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
14 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

McBurney and Parson in McBurney and Parsons (2002) study offered a review on the protocols that
ADG have in MAS, classifying them based on the task they intent to solve, which are: (i) Information
Seeking Dialogues, where one participant seeks the answer to some question(s) from another participant,
who is believed by the former to know the answer(s), (ii) Inquiry Dialogues, where the participants
collaborate to answer some question(s) whose answers are not known to any party, (iii) Persuasion
Dialogues, which involve one participant seeking to persuade another to accept a proposition she does
not currently endorse, (iv) Negotiation Dialogues, where the participants bargain over the division of
some resource, (v) Deliberation Dialogues, where agents collaborate to decide what action or course of
actions should be adopted in some situation, and (vi) Eristic Dialogues, where participants quarrel ver-
bally as a substitute for physical fighting, aiming to vent perceived grievances. An extension of this study
is McBurney and Parsons (2009), where the syntax and semantics in these protocols are analyzed, to help
software engineering specification, design an implementation in MAS. In the latter study, McBurney
comes to two important conclusions. First, people or agents in a dialogue have an ostensible purpose, but
their own goals or the goals of the other participants may not be consistent with this purpose. Second,
both humans and agents involve mixtures of the dialogue protocols when they are in a dialogue. Analysis
of protocols to purchase negotiations using ADG is also the topic of Mcburney et al. (2003). Close to the
aforementioned studies is Atkinson et al. (2005b), where a protocol for ADG is presented, which enables
participants to rationally propose, attack, and defend, an action or course of actions.

Using Argumentation-Based Dialogues (ABD) to explain an opinion is maybe the most natural method
of providing an explanation (Kraus et al. 1998; Girle et al. 2003; Garcia et al. 2007; Luis-Argentina 2008;
Lucero et al. 2009). Dialogues are the most common way of displaying arguments, but it is not an easy
task to define semantics that need to be followed by agents, in order to find the winning participant.
Nevertheless, many attempts have been proposed, in order to make SAS and MAS more explainable,
using ABD. Usually, ABD are performed on a strict set of rules, or otherwise known as protocols, which
supervise the procedure of conversation by defining: whose turn is to speak; what knowledge can be used;
when a conversation ends or begins; who the winner is; and what type of arguments must the agents use.
But even these are not flawless, when faced with domain-specific information. Studies, such as Cogan
et al. (2005), indicate that some reconsideration should be applied on the protocols when the agent(s) are
faced with domain-specific knowledge.

One study that addresses such technicalities is Panisson (2019), in which the author presents MAS
as an organization-oriented paradigm, where social relationships are considered. The author considers
various organization models that share the characteristics, such as: (i) agents use a common language,
(ii) agents are characterized by roles, (iii) explicit representation between roles exists, and (iv) activities
can be either decomposed and asserted to individuals or can be solved as a whole. Furthermore, due to
the nature of MAS, the author defines preferences between the opinions of agents in a dialogue, by the
level of authority. Social relations and Argumentation are also the topic of Liao et al. (2018), for action
selection of a robotic platform.

The Hilton (1990) conversational model, which was extended by Antaki and Leudar (1992) from
dialogues to arguments, shows that many statements made in explanations are actually argumentative
claim supporters, that is, justifying that a particular cause holds (or does not hold) when a statement is
made. Therefore, explanations are used to support claims, which are arguments. The authors extend the
conversational model to a wider class of contrast cases, as well as explaining causes. Thus, explanations
extend not just to the state of affairs external to the dialogue but also to the internal attributes of the
dialogue itself. These notions were supported by Slugoski et al. (1993).

In Bex et al. (2012), Bex and Walton (2016), the authors consider a different approach for
Argumentation and Explainability. They state that Argumentation and Explainability should consider
two different aspects in an ABD. More specifically, Argumentation should only play the role of opposing
the opinion of another participant, while Explainability should provide evidence to support an argument.
The authors consider that this distinction can help an agent restrict the range of possible answers in an
argumentation dialogue because they will have to choose between a set of explanations or arguments
in responses. Moreover, the authors demonstrate such concepts using the Argumentation Interchange

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 15

Format (AIF) (Chesnevar er al. 2006). A similar approach is found in Letia and Groza (2012), where the
authors propose an AF for MAS, in which the notion of arguments is separated from the explanation.
The authors provide a formalization for the agents to understand evidence that is needed to support an
argument, and formalization to understand and explain why an event has occurred. The authors use their
method in an ABD, where the aforementioned components are extracted from free text.

As mentioned, formalizing AFs to support dialectical explanation is not an easy task and this was
understood from the beginning in the research of Explanation through Dialogues. Therefore, many studies
propose different semantics on how to constrain Argumentation Theory into a dialogue. In the study
(Garcia et al. 2013), in the context of description logics, the authors propose a method using Dispute
Trees, where the topic of conversation is the root argument, and each level of the tree represents a step in
the conversation. The authors also provide a method that justifies an argument A through its supporting
arguments, the arguments that support not(A), and the arguments that attack A.

Finally, a study that provides an explanation of non-acceptable arguments with Dispute Trees (Fan &
Toni 2015b). The core idea of the study is that given an AF = (A, 72), a non-acceptable argument a € A
can become acceptable if a specific set of arguments A C A and a set of attacks R C R are removed from
the framework. The authors call the set A argument explanation of a, and the set R attack explanation of
a. The contribution of this paper is very important, due to the minimality of the definitions, meaning that
only a specific set of arguments and attacks if removed will make an argument acceptable. Moreover, an
agent which can use these semantics can understand which arguments must be attacked in order to make
an argument acceptable, in a dialogue.

6 Argumentation and explainable systems

Argumentation from its beginning helped in the development of explainable systems, and today is becom-
ing one of the most important reasoning methods to capture Explainability. We can see this in this section,
if we consider how many AI systems choose Argumentation as their reasoning mechanism to enhance
Explainability. We capture the topics of Argumentation in Medical Informatics, Law, the SW, Security,
and some General Purpose AI systems (i.e., systems that can be implemented in various fields). Moreover,
we include a subsection about Robotics, in which not all of the studies follow the strict rules of an AF,
but they use arguments to gain the trust of the user, or to perform an argumentation dialogue, or even to
help at cooperative decision-making between humans and robots.

6.1 General purpose argumentation systems

As General Purpose Argumentation Systems should be considered, all the systems use Argumentation to
explain a result. These systems are not restricted to a specific scientific field. Systems like these could be
implemented in various scientific fields only by changing the KB. One of the most well-known frame-
works based on SAF is ASPIC (Modgil & Prakken, 2014; Dauphin & Cramer, 2017) which can provide
explanations and arguments based on mathematical deduction.

Gorgias is an argumentation framework (Kakas & Moraitis, 2003; Noél & Kakas, 2009; Spanoudakis
et al. 2016b), where the arguments are represented in the form of rules; also, it supports the notion of
preference between logic rules and assumptions, when we have the case of missing information. Gorgias
is implemented in Prolog, and it can explain if an argument is acceptable using the Dispute Tree. Later
versions of Gorgias, called Gorgias-B, offer a friendlier user environment with a GUI in Java, where
the user can give a set of arguments and facts in the form of logic rules and make questions for the
acceptability of any argument. Gorgias can support decision-making in more advanced scenarios, such as
choosing the best policy when we have an iterative set of restrictions. In detail, a primary decision based
on some restrictions will lead to a specific set of new decisions that are governed again by restrictions,
and iteratively, the new choice will lead to a new set with restrictions, until we reach a final decision
(Kakas et al. 2019). Gorgias was also used in SAS (Spanoudakis & Moriaitis, 2009) for choosing the
best policy on pricing products based on a set of restrictions, and affecting relations between products

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
16 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

from a KB. MASs (Bassiliades et al. 2018) have also used Gorgias for choosing the best policy in energy
saving among conflict policies of agents and are translated to argument preferences.

Two tools that use visualization to help empower the explanation capabilities of an AF can be found
in Betz et al. (2019), Green et al. (2019). The first one is OpMap, a tool for visualizing large-scale
opinions spaces into a geographical map. The goal of OpMap is to build a tool that can handle multi-
opinion against or in favor of a topic, based on a SAF. OpMap first clusters the opinions using the GMap
algorithm, which extends force directed algorithms and constructs visualizations resembling a geographic
map. After the clusters are created, OpMap maps the opinions-arguments and the clusters into a 2D map.
The clusters are considered as columns and each argument as a row. Then, the arguments are given
the label of True, False, or Judgement Suspension (i.e., Undefined). The second one is called AVIZE
and is a tool for constructing arguments in the domain of international politics. AVIZE tries to create a
triplet (topic, claim, premise) for each argument, and it represents this as a table with a column for each
component of the triplet. Obviously, AVIZE is a tool that helps to understand the structure of an argument
and make it easier for the user to find ways that can attack or support it. Moreover, AVIZE is a supervised
tool where the user must evaluate the quality of the triplet, and it can be used in other domains.

An AF that evaluates alternatives and explains them with a scoring function, coupled also with a visual
representation, is presented in Baroni et al. (2015). The framework is based on a QBAF and can support
argument dialogues. The authors consider a scoring function SF, which takes into consideration the base
score of an argument a, and the sum scores of its attackers and supporters. This will help the debater
to use specific arguments based on the final score. For example, if we know that S7(a) > SF (b), it is
wiser to use argument a instead of b. The framework is also compared with an AAF in terms of finding
the appropriate argument to win a conversation and is projected to a visual representation using Visual
Understanding Environment .

Another important aspect when we provide an explanation is the level of trust we have for an argu-
ment. ArgTrust (Sklar et al. 2016) is maybe the most well-known system for quantifying the trust of an
argument, to facilitate decision-making. The system given an extended AF, and a set of weights for the
relations between arguments, computes the trust level of the arguments in the framework by taking into
consideration the negative and positive impact the other arguments have. Argument trust is also addressed
in Tang et al. (2012). Given a MAS, the authors construct an AF that can return the trust level of any
argument. The framework is based on LBAF, and the agent has to elaborate why to support an opinion,
by displaying the premises it relies on. The framework offers an implementation in Java that infers the
acceptability or non-acceptability of an argument, which is derived based on the beliefs of the agents and
relations between the arguments.

6.2 Law

Argumentation and Law are strongly connected even outside the scope of Computer Science because
Law could be considered as a discipline in which a lawyer tries to obtain knowledge that will help him
oppose each argument against his client. Henry Prakken was the first computer scientist that managed
to define formalizations for AF used in Law. Among his many studies, the most relevant to this survey
are Prakken (2005a); Prakken (2017), Prakken et al. (2015). In the first two studies, the author provides
a formalization of a legal case into Argumentation Theory. The legal case is first given to the ASPIC
framework that tries to produce defeasible rules, which are considered as arguments. The procedure of
translating logical rules into arguments is identical to Caminada er al. (2015). After this, we can easily
compile an AF. In the last study, the author proposes to represent legal cases as Dispute Trees, over
description logics. Of similar nature is the study of Bench-Capon (2020), where the impact of AAF in AI
and Law is discussed.

We can easily envision the dialogue that takes place during a trial, as an argumentation dia-
logue, where a lawyer provides arguments in favor of his client and the opposing lawyer against him,
and this process goes on until one lawyer cannot issue any more arguments. This was the idea in

6  https://vue.tufts.edu.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 17

Al-Abdulkarim et al. (2016a, b, 2019), where the argumentation dialectical procedure is projected into
a Dispute Tree. On the other hand, Sklar and Azhar (2015) consider legal reasoning as an interchange
between two or more agents based on LBAF. The most important aspect of this study is the meta level
argumentation semantics. The authors provide a set of logic rules with preferences that can be imple-
mented in a LBAF and change the procedure of how an admissible argument is derived. The preferences
are called Social Biases and capture the notion of: (1) Argument from Authority, meaning that the deci-
sion of an agent might be stronger than others, (ii) Epistemic Entrenchment, when an agent is totally sure
about its opinion and nothing will change it, (iii) Stereotyping, when an agent makes assumptions about
the beliefs of another agent, and (iv) Defiance, when an agent constructs arguments from propositions
which are against its beliefs.

Interesting is the study of Zhong er al. (2019), in which the authors present a framework that is meant
to help judges to define a sentence. Their framework compares current cases with past performed cases
and returns the verdicts that were taken for similar cases. Also, the framework provides arguments and
explanations to the judge, by showcasing the Redundant Attributes. Redundant Attributes are the parts in
the description of a case that may contribute to the verdict, such as: the type of crime, amount of stolen
items, condition of the accusant, crime evidence, number of abettors, and if the accusant is also the moral
instigator.

The implementation of Argumentation in the field of AI and Law has resulted into many soft-
ware systems with important Explainability capabilities. For instance, TAXMAN (McCarty 1976),
which can develop argumentation dialogues in favor or against a side in a specific legal case (Eisner
v Macomber Clark 1919), HYPO (Rissland & Ashley, 1987), CATO (Aleven 1997), and ANGELIC
(Al-Abdulkarim et al. 2016c), which are used for legal CBR, and Gordon and Walton (2009), which uses
the Argumentation schemes of Walton (2005), in order to construct and search for legal arguments.

6.3 Medical informatics

Argumentation has emerged recently in the field of Medical Informatics, mostly to support the decision
of AI systems. For instance, in Longo and Hederman (2013), Defeasible Reasoning and Argumentation
Theory are used for Decision-Making in a health care scenario, using data about Breast Cancer. The study
shows how to translate clinical evidence in the form of arguments, add support and defeat relations, and
apply Defeasible Reasoning. The authors represent arguments as rules and take the clinical evidence as
support. Using this representation, defeat can be derived through undercut and rebut attacks. Another
similar approach is Spanoudakis er al. (2017), where the same notions are used to determine the level of
access to a patient’ s medical record. Moreover, the clinical evidence and medical record of an admissible
argument are given as an explanation. Looking at MoZina et al. (2007), Chapman et al. (2019), Kékciyan
et al. (2019), Kokctyan et al. (2018), Sassoon et al. (2019), we can find a decision-making AF for patients
that suffer from chronic diseases to help them decide how they can prevent worsening their health. More
specifically, the framework involves sensors that record the health state. Then, if an anomaly in the
blood pressure occurs, the framework can recommend a treatment, by computing an argument based on
embedded data which was bounded with the input given by the anomaly in the health recorder. In Cyras
et al. (2018), Cyras and Oliveira (2019), Oliveira er al. (2018), the authors translate clinical evidence into
arguments for an ABA and track patients health state to suggest a treatment in an emergency scenario.
Due to the probability distribution which is part of the framework, an ABA which can handle uncertainty
can tackle the demands of stochastic framework.

Argumentation and decision-making are also presented in Qurat-ul-ain Shaheen and Bowles (2020). In
this study, decision-making through argumentation dialogues is used in order to recommend a treatment
to patients with multi-morbidity (i.c., multiple chronic health conditions). The complexity for making
a decision in this case is high, as a treatment for a chronic disease may affect another chronic dis-
ease. Nevertheless, the authors propose a novel approach to justify a decision with Satisfiability Modulo
Theories solvers in an interactive way through argumentation-based dialogues. Moreover, the authors
provide two different ways of explanation: the first one is called passive, where the patient accepts all the

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
18 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

arguments which point to a specific treatment, while the other one is called active, where the patient can
ask why a treatment is recommended.

Argumentation in Medical Informatics is also used to justify a decision. Donadello er al. (2019) use
an OWL ontology with information about the dietary habits that a patient with chronic diseases should
have. The information is recorded into the KB of the framework by a domain expert. The framework
keeps track of the daily meals of the patient, and if it finds inconsistencies, it notifies the patient in the
form of arguments. More specifically, predefined SPARQL templates exist that get for example what the
user ate and are addressed to the ontology. For instance, if a patient is diagnosed with diabetes, eating
food with high sugar consistency will trigger an argument that explains why the user should not eat so
much sugar. Very similar is the study of Grando et al. (2013); the authors also provide a medical purpose
mechanism that receives data about the health anomaly that occurred. Then, it addresses a SPARQL query
to an underlying OWL ontology and returns an explanation in the form of arguments and the supporting
facts. The ontology is constructed by domain experts, and the framework offers a GUI to receive data.

Two similar studies in argument classification for medical purposes are Mayer et al. (2018), Prentzas
et al. (2019). In the former, the authors try to classify arguments, by finding evidence and claims from
free text, which can be useful information to an expert, in order to lead him to a potential treatment.
Moreover, the authors created their own annotated corpus with arguments called Random Clinical Trials.
For the evaluation, they use three methods for classifying the arguments in the texts: (i) SubSet Tree
Kernel, (ii) SVM with Bag of Words features weighted by TF-IDF, and (iii) a Kernel mechanism that
combines (i) and (ii). While in the latter, they propose a methodology for applying Argumentation on top
of ML to build an XAI system for stroke prediction. The authors trained a Random Forest ML model
on the Asymptomatic Carotid Stenosis and Risk of Stroke Study data set (Nicolaides et ai. 2010). Then,
the produced rules are extracted in the form of IF-THEN statements and are given to Gorgias. Gorgias,
provided with a set of facts and arguments represented as logical rules, generates an explanation through
the Dispute Tree, if an argument is admissible.

Zeng et al. (2018) propose an explainable model based on Argumentation for detection of dementia.
The framework uses a Convolutional Neural Network to extract features from images, such as the size,
the region, and the drawing test performance. The framework can then to explain its decision through
arguments using these features and the medical history. The explainable model is a combination of a
graphical representation for modeling decision problems in various contexts, and a reasoning mechanism
for context-based decision computed with an ABA.

6.4 Robotics

The recent evolution in the field of Robotics, which makes it easier for people to deploy a robotic assistant
in a household environment, increased the urgency of making the robotic platforms decisions totally
explainable. Argumentation has been used in several cases in the field of robotics, to explain decisions.
The tasks of trust gaining, persuasion, and combined decision-making between a human and a robot are
the main reason for which a robotic platform will use an AF to explain its decisions.

Argumentation for shared decision-making between a robotic platform and a human is the topic in
Sklar and Azhar (2015, 2018), Azhar and Sklar (2016, 2017). To the best of our knowledge, these were
the first studies on argumentation-based dialogue games between a human and a robotic platform. The
authors developed an LBAF along with a GUI that helps humans in cooperation with a robotic platform
to reach a shared decision at each step of an activity, more specifically in a Treasure Hunt Game (Sklar
& Azhar, 2015). Next, in Azhar and Sklar (2016, 2017), Sklar and Azhar (2018), the authors extent the
Treasure Hunt Game with different methodologies for implementing multiple types of argumentation-
based dialogues. The framework can explain which dialogues are appropriate given the beliefs of the
participants and how multiple dialogues can occur simultaneously while containing consistency in the
general dialogue. Interesting is the fact that they manage to define conditions for when the two partic-
ipants are in a state of agreement, disagreement, one side lacks knowledge, and both lack knowledge.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 19

Additionally, the dialogue protocols are separated into three classes: (i) Persuasion, where one of the
two participants tries to convince the other one about its beliefs, (ii) Information Seeking, when one
participant tries to extract information from the other one, and (iii) Inquiry, when one participant asks
information that the other participant does not have. The last dialogue protocol might seem odd, but the
idea behind it is that even though one participant might not know the correct answer, he might give some
secondary information which might be useful.

Next, we elaborate on two studies that can be considered as argumentation dialogue frameworks. In
Modsgil et al. (2018), a chatbot is used to collect arguments, counterarguments, and supporting arguments
from users over a large variety of topics. The robotic platform can then recall past received arguments
to explain to the user its opinion on a topic. The biggest benefits of this system are: (i) The ability of
capturing the probability of different people to provide same or similar arguments on a topic, (ii) the
ability of initiating argumentation with zero knowledge about the topic, and (iii) the ability to identify
and use a counter-argument for similar arguments. Similar arguments are considered those that use the
same evidence to support or attack a claim.

On the other hand, the chatbot in Wanner et al. (2017) uses an OWL ontology to perform advanced
dialogue planning, much like an AF which can help choose specific responses to elaborate and possibly
justify a topic. More importantly, the study addresses the problem of recommendation with arguments.
The idea behind it is the following: from the moment that we reached a point where the explanations
exist for each system’s decision, we can trust it to give us recommendations. The robotic system receives
verbal as well as visual signals, such as gestures. After textual parsing and visual inferencing occurs,
the robotic system tries to understand: (i) The form of conversation it is having (i.e., if the user wants
explanation, recommendation, or information about something), or (ii) if it needs to return information
about the question using the Statistical Speech Model Vocapia’ from its internal KB. Very similar is the
recent study of Meditskos et al. (2019), with one significant extension; the authors use web repositories
such as DBpedia’ and WordNet’ to enrich the KB of the system.

Close to the two aforementioned studies are Torres et al. (2019), Fischer et al. (2018), in which the
authors use argumentation in the form of recommendation. These studies are fully developed robotic
platforms that can also perform other tasks, but it is interesting to see how the robotic platform uses
external knowledge, information from its sensors, and information in its internal KB, to compute an
argument. In Torres et al. (2019), the authors use preference in a non-monotonic KB with a closed word
assumption. Also, the conflicting information which comes from non-monotonic logic is tackled with the
principle of specificity according to which between two conflict propositions we shall always choose the
more specific one (i.e., the one with more information). One example should make things clear, on how
the recommendation in the form of arguments is constructed:

When the user asks for a Coca-Cola, the system starts looking in the environment for objects that can
be classified as a can or bottle of Coca-Cola, but it already knows that the user has diabetes and today
he already had a lot of sugar, so it recommends bringing him tea instead.

In Fischer et al. (2018), the authors use a static ontology and external knowledge from WordNet
and Wikidata’’, to recommend tools to perform an action. The framework first tries to understand what
activity the user wants to perform, for example ‘J want to cut this wooden block’, and then it annotates the
action cut and the object on which he wants to perform the action on, in this case wooden block. Then,
the framework is asked to give an argument recommendation on which tool is the most appropriate for
the action from a collection of objects. The framework uses the internal knowledge from its ontology for
actions and objects, Wikidata for hierarchical relations and WordNet for synonyms, to infer the desired
tool. It can interact with the user via speech and even provide an explanation containing a log file with
the internal steps it performed to reach this recommendation.

7
8
9

http://www.vocapia.com/.
https://wordnet.princeton.edu.
https://wordnet.princeton.edu.

10 https:/Awww.wikidata.org/wiki/Wikidata:Main_Page.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
20 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

6.5 Semantic web

In this subsection, we will describe some argumentation systems that use SW technologies, or try to solve
problems that are related to the Web through Argumentation.

One of the first studies in this domain concerns an OWL ontology, which is used to annotate arguments
in natural language and to provide explanation on how the arguments were annotated (Rahwan et al.
2011). The authors use the AIF to understand the structure of arguments in text. Other important aspects
are that the authors manage to capture the support relations between arguments and that the ontology can
automatically create new classes which take as instances arguments with specific structures. AIF is also
used in the study (Indrie & Groza, 2010) to model interaction between arguments. Semantic MediaWiki’
(SMW) is used to translate arguments in specific templates for coherency in their structure. Next, in order
to exploit the ontology of AIF in the SMW, the authors map the concepts and roles from the ontology to
the internal structuring mechanisms available in SMW. This study offers a method of explaining how an
argument was classified based on: (i) the structure of the argument, (ii) patterns (for instance the opinion
of experts), (iti) relation of specific Wikipedia terms, (iv) domain arguments that support a specific topic
in the ontology KB, (v) support level, and (vi) context.

An approach to empower commonsense reasoning and make it more explainable with Argumentation
is given in Botschen et al. (2018). The authors investigate whether external knowledge of event-based
frames and fact-based entities can contribute to decompose an argument as stated in the Abstraction and
Reasoning Corpus (ARC) task’. In the ARC task, the system must find the correct cause that derives
a claim given some data. It is similar to finding the warrant of an argument in Hunter’s argumentation
model (Besnard & Hunter 2001; Besnard & Hunter 2009). The outline of the study is that the authors use
a Bi-LSTM trained on ARC KB to annotate the frames and entities. FrameNet’ is used to make semantic
connections between frames and Wikidata to offer more information for the entities. The difference
between frames and entities is that entities in a sentence could only have the role of subject and object,
while frames can be any part of speech.

Online dialogues can be a rich source of argumentation dialogues, and researchers are interested in
using such repositories, to give explanations on how people consider an acceptable argument (Snaith
et al. 2010; Reed et al. 2010). These types of repositories are the most precious in understanding how
Argumentation Theory works in real life because they give data that are in the purest form. Using the
Web as a source of arguments was originally introduced in Bex et al. (2013), where Bex envisioned
the Argument of Web, a Web platform combining linked argument data with software tools that perform
online debates. Inspired by the aforementioned study, a mechanism which attempts to find arguments
that are against or in favor of a topic in a conversation was built (BoltuzZié & Snajder. 2014b, 2015). The
mechanism relies on an SVM trained on the custom made ComArg corpus (Boltuzié & Snajder 2014a),
a data set of arguments supported by comments, using the data of ProCon’™ and Idebate’. Then, the
system is evaluated on data from ProCon and Idebate. Additionally, the authors annotate the similarity
of arguments using Bag of Words, Skip-Gram, and Semantic Textual Similarity methods. Identical is the
study (Swanson et al. 2015), where the authors train three different regressors: (i) Linear Least Squared
Error, (ii) Ordinary Kriging, and (iii) an SVM on a data set created from CreateDebate'®, to identify
arguments in online dialogues on the topic of gay marriage, gun control, and death penalty. A powerful
tool for visualizing a BAF can be found in the web application of Baroni et al. (2018), where the user can
build an argumentation graph and see the justification of an argument.

https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki.
https://competitions.codalab.org/competitions/17327.

‘3 http://framenet.icsi.berkeley.edu/fndrupal.
https://www.procon.org.

'5  https://idebate.org.

https://www.createdebate.com.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 21

6.6 Security

Preserving the user’s privacy is maybe the most difficult task that computer science has to solve, as
having users that do not trust the systems will make them reluctant to use digital products or services.
Especially with the rise of the Internet of Things, more and more devices are connected to each other
and need to communicate and collaborate. Such a setting makes the devices even more vulnerable to an
attack. Argumentation has been used in recent years to provide security in such cases, mostly because
Argumentation can use persuasion in order for an agent to understand if it receives an attack (Rowe et al.
2012; Murukannaiah et al. 2015; Santini & Yautsiukhin, 2015; Panisson et al. 2018).

In Kékciyan er al. (2017), the authors use an ABA, for agents to decide whether they should share the
content of a file. The authors consider that when the content of file is shared by a user, other users who
might get affected must agree in order for the file to be uploaded, otherwise privacy constraints might
be violated. In order for this to be achieved, the authors model the users as agents in a social network
and represent their users privacy constraints as semantic rules. The agents can argue between them on
propositions that enable their privacy rules using assumptions from their personal ontology. Also, agents
can ask for help from other agents, in order to enrich their ontology. This study offers a MAS with
personal ontologies for each agent that contain domain knowledge, and semantic rules which describe
privacy constraints such that the knowledge can be used to perform argumentation. Moreover, it offers
an algorithm that allows agents to carry a dialogue such that each agent can attack the assumptions of
others. K6kciyan and Yolum (2017) study is an extension in the context of the Internet of Things. The
study (Bassiliades et al. 2018), based on the studies of Spanoudakis er al. (2016a, 2007), focuses also on
the same principles for accessing patient’s data.

Similar to the previous studies are Fogues et al. (2017a, b), Shakarian et al. (2015), where the users
with the help of Argumentation find a sharing policy, when conflicting interests between many users
exist. The authors in Fogues er al. (2017a, b) develop a computational model that understands how peo-
ple decide the appropriate sharing policy in multi-user scenarios where arguments exist and predicts an
appropriate sharing policy for a given scenario. In Shakarian et al. (2015), the authors offer a framework
for recommending sharing policies.

In Karafili et al. (2017, 2018a, b, 2020), the authors propose a novel argumentation-based reasoner
for analyzing cyber-attacks, to help the cyber security analysts understand from where a cyber attack
came. The framework gives possible culprits of the attack and hints about missing evidence. The AF
that is used is a SAF, and the Argumentation-based reasoner, which is part of their framework, is taking
into consideration the logic rules with their preferences, as well as the social biases (or background
knowledge) that may occur with the cyber attack. The running examples in the studies give a good
understanding of the social biases: two countries that have some conflicting financial interests are more
likely to perform a cyber attack against each other, therefore indicating a potential source of the cyber
attack. The same principles were used in Karafili et al. (n.d.) for decision-making over actions in drones.

The studies of Nunes et al. (2016b, c) are very interesting as they propose an argumentation model
based on defeasible logic programming, designed to help the analyst find the source of a cyber-attack.
These studies are based on Nunes et al. (2016a), where the authors construct a data set with real-life
cyber-attack scenarios from hackers, collected from the DEFCON" competition. The studies of Nunes
et al. are some of the few which are tested in real-life cyber-attack cases. The experiments show that using
argumentation can significantly reduce the number of potential culprits that the analyst must consider and
that the reduced set of culprits, used in conjunction with classification, leads to improved cyber-attribution
decisions. In the same field are the studies of Genitsaridi et al. (2013), Bikakis and Antoniou (2010),
which are based on the principles of defeasible logic programming and offer a high level authorization
language for access control policies.

Finally, Bandara et al. (2006, 2009) use Argumentation and preference reasoning for firewall policy
analysis and automatically generate firewall policies from higher-level requirements. These studies man-
aged to show that the non-monotonic reasoning with conflict rules that Argumentation offers permits

7 https:/Awww.defcon.org/htmV/links/dc-ctf html.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
22 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

the analysis and generation of anomaly free firewall configuration rules. The modularity of this policy
generator framework is that it allows for customization according to characteristics of a network, and the
usage of deductive and abductive reasoning offers explanatory power which can trace the source of an
attack. Additionally, the authors state that their framework can be used to: (i) review a firewall configu-
ration by querying the formal model for reachable nodes, (ii) analyse a firewall configuration in order to
detect anomalies, and (iii) generate a firewall configuration according to the characteristics of a specific
network.

Table 1 displays all the aforementioned systems. We classified the systems based on which task they
tackle: Decision-Making, Justification, Explanation though Dialogue, or Argument Classification.

7 Argumentation and machine learning for explainability

ML could be considered as the field that brought XAI to the surface, due to the fact that the results
returned by data-driven models were considered as ‘black boxes’ whose rationale is incomprehensible to
most human users. The new regulation established by the EU that any system which can take a decision
that can affect our life must have the capability to explain its decision (Regulation 2016), led to the
need of new methods that can provide Explainability even in this field. As we can see in this section,
Argumentation can become the link between ML and XAI.

Cocarascu and Toni (2018) introduce a deep learning method for argument mining to extract attack
and support relations, in order to explain how news headlines support tweets and whether reviews are
deceptive by analyzing the influence these texts have on people. Thus, the authors elaborate on the level
of trustworthiness, persuasion, and explainability of arguments that exist in these texts. Exploiting the
knowledge relations that hold between arguments units carries great potential of explaining why an argu-
ment holds (or does not hold) when presenting with supporting or attacking evidence. The method could
be considered a pipeline classification problem with an LSTM trained over the argument corpus AIFdb ;
to capture whether two different texts support, attack, or are neutral among each other. Furthermore, the
framework obtains state-of-the-art scores over small data sets such as the Hotel Dataset (Ott er al. 2013).
Due to the fact that the method is based on mining a BAF, the authors can automatically map the attack
and support relations on a BAF.

CBR or Instance-based Learning is one of the most commonly used methods of ML, which was used
from the the early days of the research in ML. Therefore, there are many studies which use CBR and
Argumentation to achieve Explainability. In Cyras et al. (2016a); Cyras et al. (2016b), the authors use
CBR to classify arguments to set possible options, and when new information is inserted to the KB,
the class of the argument may change. Another study that uses CBR and Argumentation is Cyras er al.
(2019), where the authors construct a framework that explains why certain legislation passes and other
not, based on a set of features (see Section 5). A CBR model that classifies arguments based on precedents
and features, for legal cases, is also presented in Bex et al. (2011). The authors use a framework that takes
in consideration information from the KB in order to classify the argument. More specifically, given a
verdict, the system must classify the argument if the defendant is guilty or not. Hence, it searches for
similar cases in its KB to make an inference.

One of the first studies that use Argumentation and ML together with external knowledge from domain
experts is MoZina et al. (2007). In this study, the authors try to tackle the fact that domain expert opinion
may not be global for the domain as there might exist exceptions under specific circumstances. Therefore,
the experts explain with examples why a specific argument is acceptable or not under specific circum-
stances. The examples work as templates for the characteristics of the domain. More specifically, if
specific properties hold in the environment, then the reasoning for the acceptability of an argument will
be different if other properties would exist. A data-driven model tries to learn this relation between
properties and different type of reasoning, in order to help an AF decide over the acceptability of an
argument.

‘8 http://corpora.aifdb.org/.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
ssaid Aysieain aBpyiquies Aq auljuo paysiqnd | |000017688869Z0S/Z 10101 /S10°1lopy/sdiy

Table 1.

Overview of argumentation systems for XAI

 

 

Domain Decision making Justification Explanation through Argument classification
dialogue
Law Bench-Capon (2020) Prakken (2017) Sklar et al. (2013), Prakken et al. (2015,
Al-Abdulkarim et al. 2005a), Rissland and
(2016a); Al-Abdulkarim Ashley (1987), Aleven
et al. (2016b); (1997), Al-Abdulkarim
Al-Abdulkarim et al. et al. (2016c), Gordon
(2019), McCarty (1976) and Walton (2009)
Medical Longo and Hederman (2013), Grando et al. (2013), Donadello Qurat-ul-ain Shaheen and Prentzas ef al. (2019),
informatics Spanoudakis ef al. (2017), Chapman et al. (2019), Zeng et al. Bowles (2020) Mayer et al. (2018)
et al. (2019) (2018)
Cyras and Oliveira (2019), Oliveira et al.
(2018)
Robotics Azhar and Sklar (2016); Azhar and Sklar Torres et al. (2019), Fischer Modgil ef al. (2018),
(2017), Sklar and Azhar (2015) et al. (2018) Wanner et al. (2017),
Meditskos et al. (2019)
SW Botschen ef al. (2018) Baroni et al. (2018) Boltuzié and Snajder Swanson et al. (2015),
(2014b), Bex et al. Indrie and Groza (2010),
(2013), Boltuzié and Rahwan et al. (2011)
Snajder (2015)
Security Koékciyan et al. (2017), Kékciyan and Karafili et al. (2017, 201 8b, Rowe et al. (2012), Panisson et al. (2018)
Yolum (2017), Fogues et al. (2017a); 2020, 2018a, n.d.), Nunes Murukannaiah et al.
Fogues et al. (2017b), Shakarian ef al. et al. (2016c); Nunes ef al. (2015)

General purpose

(2015)

Dauphin and Cramer (2017), Modgil and
Prakken (2014), Green et al. (2019),
Kakas et al. (2019), Bassiliades et al.
(2018), Noél and Kakas (2009),
Spanoudakis et al. (2016b)

(2016b); Nunes et al. (2016a),

Genitsaridi ef al. (2013),

Bikakis and Antoniou (2010),

Bandara et al. (2006);

Bandara et al. (2009), Santini

and Yautsiukhin (2015)

Tang et al. (2012), Baroni et al.

(2015), Spanoudakis and
Moriaitis (2009)

Chesnevar et al. (2006),
Betz et al. (2019)

Bex et al. (2010)

 

a2uasIaqUl [DIIYNAD ajqouIDjdxa Puv UOLIDIUaUNSAY

€o
24 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

Three studies that use ML and Argumentation to explain a claim while relying on an external KB can
be found in Samadi et al. (2016), Potash et al. (2017), Habernal & Gurevych (2016). ClaimEval is pre-
sented in Samadi er al. (2016), as a mechanism that given a specific topic extracts a set of supporting and
attacking arguments from various websites with the help of Bing’. ClaimEval relies on a Probabilistic
Logic that allows it to state and incorporate different forms of already existing knowledge. The authors
take into consideration the credibility of the source by mapping each source into a graph. Then, the
authors propagate the credibility of the graphs of different sources based on some prior knowledge, which
is defined as a set of rules to reach joint source credibility. Also, the authors use an SVM to evaluate if
evidence is supporting or attacking for a specific topic and achieve state-of-the-art results at this classi-
fication task. On the other hand, in Potash ez ai. (2017), the authors present various data-driven models
which are trained to find the most convincing argument for a topic. These models are very useful because
they can explain how the form of an argument should be, in order to be considered convincing for a topic.
The models are evaluated on the argument convincingness data set UKPConvArg (Habernal & Gurevych
2016). Furthermore, the authors give four supervised models that achieve state-of-the-art results on the
same data set: (i) An Bi-LSTM that receives as input the vector of the concatenation argument-topic pairs,
using Glove embeddings’ , (ii) A method with Bag of Words given the term-frequency representation of
each pair, (iii) A method using Bag of Words and term-frequency of the triplet (argument, topic, most
related wiki article with respect to wiki metric), and (iv) A probability distribution between arguments
and Wikipedia articles. Also, the authors provide the largest data set of annotated arguments in Wikipedia
atticles’ .

Another study that uses external knowledge from ConceptNet” and DBpedia, NLP methods, and KB
features to predict the type of relations between arguments is Kobbe et al. (2019), similar to Cocarascu
and Toni (2018). The authors classify the relation between two arguments A and B using a pretrained
Bi-LSTM. The Neural Network receives the vector representations of the words that each argument
is composed of and returns a vector representation for A (denoted by emb(A)) and B (denoted by
emb(B)). The relation between the vectors is r(A, B) = emb(A) — emb(B), where the operation is per-
formed element-wise. External knowledge can be used to enrich the obtained representation r(A,B) with
relevant information for knowledge relation about concepts and entities mentioned in the two argumen-
tative units. If vg (A, B) is the new vector with external features, then the authors add element-wise to get
a new vector for the relation r’(A, B) = r(A, B) © vx(A, B).

Argumentative discussion where agents recommend arguments to people to justify their opinion is
addressed in Rosenfeld and Kraus (2016a). The authors train three different ML models, an SVM, a
Decision Tree, and a Multi-Layered Neural Network, in three different scenarios to predict the most
appropriate candidate argument that may justify the opinion of a person. The first scenario is a predefined
conversation on topics, such as ‘Why should I buy the car x?’. Each classifier is trained with data collected
from Amazon Mechanical Turk. The second scenario uses data from Penn Treebank Corpus’, which
contains real argument conversations, not annotated by workers as in the first case. The third scenario
uses data from medical corpus, and the classifiers are executed to give the pros and cons of each topic.

An interesting implementation of AFVs to enhance the Explainability on the decisions of data-driven
models is presented in Garcez et al. (2005). The authors establish a relationship between neural networks
and argumentation networks, combining reasoning and learning in the same argumentation framework.
The authors present a neural argumentation algorithm for translating argumentation networks into stan-
dard neural networks. The algorithm can translate acyclic and circular AFs into neural networks, and it
enables the learning of arguments, as well as the parallel computation of arguments.

Cocarascu et al. (2018) present an architecture that combines artificial neural networks for feature
selection and AAF, for effective predictions, explainable both logically and dialectically. More specifi-
cally, the authors train an auto-encoder to rank features from examples. The auto-encoder is trained on

19
20

www.bing.com.
https://nlp.stanford.edu/projects/glove/.

21 https://github.com/UKPLab/.

22 https://conceptnet.io.

23 https://catalog.ldc.upenn.edu/LDC99T42.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 25

the Mushroom Data set (Dheeru & Taniskidou 2017), which contains types of mushrooms paired with
a group of features for each one of them. Using the most important features, an AAF is produced to
classify and explain if a mushroom is poisonous or not. The arguments in the AAF are composed of the
set of important features and the class for the mushroom. The method produces a set of logical rules to
explain the classification. The method outperforms a Decision Tree model and the combination of an
auto-encoder with an artificial neural network, when trained on the same data set. An interesting imple-
mentation of Hunter’s argumentation model (Besnard & Hunter, 2001, 2009) can be found in Mollas
et al. (2020). The authors use a feature importance technique, in order to extract untruthful parts in the
explanation of a data-driven model. Moreover, the authors use this methodology to find the less untruthful
explanation among many explanations of various data-driven models.

8 Discussion and conclusion

In this survey, we elaborated over the topic of Argumentation combined with XAI. Our goal was to report
the most important methods that appeared in the literature to achieve Explainability in AI Systems, as well
as their implementations. For this reason, we presented how Argumentation enables Explainability when
tackling problems, such as decision-making, justification of an opinion, and argumentation through dia-
logues. Moreover, we give an extensive literature overview on how Argumentation can be implemented
into an agent, in order to solve the aforementioned problems, and what principles they must follow, in
order not to be considered biased. Also, we showed how Argumentation can be used to construct explain-
able systems in the application domains of Medical Informatics, Law, the SW, Security, Robotics, and
some general purpose systems. In our last chapter, we showed ML models that use Argumentation Theory
to unlock the black box of Explainability in ML. The main contribution of this survey is the extensive
literature overview of theoretical studies that use Argumentation to enhance Explainability, the literature
overview of Argumentation implementations to build explainable systems. In Section 7, it becomes clear
that Argumentation can work as a link between ML and XAI.

This survey revealed that not many studies exist which address the topic of commonsense knowl-
edge fused into Argumentation Theory. Fusing commonsense knowledge into Argumentation with strict
definitions will empower the Explainability capability of an AF because it will allow it to reason with
methods closer to human thinking and therefore being more persuasive in a dialogue. The studies that
attempt to fuse commonsense knowledge into Argumentation Theory consider commonsense knowledge
as: commonly accepted knowledge on which they use preferences (Cyras 2016), text analytics (Moens
2016; Zhang et al. 2017), Event Calculus (Almpani & Stefaneas 2017), and even knowledge from exter-
nal Web Knowledge Graphs (Kobbe et al. 2019). One study (Vassiliades et al. 2020) gives a notion of
how an argument with commonsense knowledge could be defined but it is at a preliminary level.

Furthermore, using Argumentation Theory to explain why an event started, or what led to a decision,
is a reasoning capability that an AF can offer and it can enhance its Explainability power. Causality
could be achieved by reasoning over each step that led to a decision and explain why alternatives were
left out. Nevertheless, we see that not many works exist that combine Argumentation and causality for
this purpose, apart from Collins et al. (2019) where the authors use argumentation to explain planning.

For future work, we plan to focus on arguments with commonsense knowledge, an interesting area
that has not yet received much attention. More specifically, we will extent our survey over the topic of
multi-argumentation frameworks that include arguments with commonsense knowledge and the various
types of attack relations between them, which can be used to model, among other things, exceptions to
commonsense knowledge (Vassiliades et al. 2020). Arguments that can use commonsense knowledge can
enhance the Explainability capabilities of Argumentation. Moreover, a literature overview of arguments
with commonsense knowledge may provide models to represent commonsense knowledge that can be
used in other research areas, such as agent theory, robotics, and even data-driven models in NLP to help
mine arguments from human text dialogues, which could subsequently be used in a human—machine
dialogue.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
26 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

Acknowledgment

This project has received funding from the Hellenic Foundation for Research and Innovation (HFRI) and
the General Secretariat for Research and Technology (GSRT), under grant agreement No 188.

References

Adadi, A. & Berrada, M. 2018. Peeking inside the black-box: a survey on explainable artificial intelligence (xai).
IEEE Access 6, 52138-52160.

Al-Abdulkarim, L., Atkinson, K. & Bench-Capon, T. 2016a. A methodology for designing systems to reason with
legal cases using abstract dialectical frameworks. Artificial Intelligence and Law 24(1), 1-49.

Al-Abdulkarim, L., Atkinson, K. & Bench-Capon, T. 2016b. Statement Types in Legal Argument. IOS Press.

Al-Abdulkarim, L., Atkinson, K. & Bench-Capon, T. J. 2016c. Angelic secrets: bridging from factors to facts in us
trade secrets. In JURIX, 113-118.

Al-Abdulkarim, L., Atkinson, K., Bench-Capon, T., Whittle, S., Williams, R. & Wolfenden, C. 2019. Noise induced
hearing loss: building an application using the angelic methodology. Argument & Computation 10(1), 5-22.

Aleven, V. A. 1997. Teaching Case-Based Argumentation Through a Model and Examples. Citeseer.

Almpani, S. & Stefaneas, P. S. 2017. On proving and argumentation. In AJC, 72-84.

Amgoud, L. & Cayrol, C. 2002a. Inferring from inconsistency in preference-based argumentation frameworks.
Journal of Automated Reasoning 29(2), 125-169.

Amgoud, L. & Cayrol, C. 2002b. A reasoning model based on the production of acceptable arguments. Annals of
Mathematics and Artificial Intelligence 34-3), 197-215.

Amgoud, L., Cayrol, C., Lagasquie-Schiex, M.-C. & Livet, P. 2008. On bipolarity in argumentation frameworks.
International Journal of Intelligent Systems 23(10), 1062-1093.

Amgoud, L. & Prade, H. 2006. Explaining qualitative decision under uncertainty by argumentation. In Proceedings
of the National Conference on Artificial Intelligence, 21, 219. AAAI Press, MIT Press, 1999.

Amgoud, L. & Prade, H. 2009. Using arguments for making and explaining decisions. Artificial Intelligence
1733-4), 413-436.

Amgoud, L. & Serrurier, M. 2007. Arguing and explaining classifications. In International Workshop on
Argumentation in Multi-Agent Systems, 164-177, Springer.

Amgoud, L. & Serrurier, M. 2008. Agents that argue and explain classifications. Autonomous Agents and Multi-Agent
Systems 16(2), 187-209.

Anjomshoae, S., Najjar, A., Calvaresi, D. & Framling, K. 2019. Explainable agents and robots: results from a system-
atic literature review. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent
Systems, 1078-1088. International Foundation for Autonomous Agents and Multiagent Systems.

Antaki, C. & Leudar, I. 1992. Explaining in conversation: towards an argument model. European Journal of Social
Psychology 22(2), 181-194.

Arioua, A., Tamani, N., Croitoru, M. & Buche, P. 2014. Query failure explanation in inconsistent knowledge
bases: a dialogical approach. In International Conference on Innovative Techniques and Applications of Artificial
Intelligence, 119-133. Springer.

Atkinson, K. & Bench-Capon, T. 2007a. Action-based alternating transition systems for arguments about action. In
AAAI, 7, 24-29.

Atkinson, K. & Bench-Capon, T. 2007b. Practical reasoning as presumptive argumentation using action based
alternating transition systems. Artificial Intelligence 171(10-15), 855-874.

Atkinson, K. & Bench-Capon, T. 2018. Taking account of the actions of others in value-based reasoning. Artificial
Intelligence 254, 1-20.

Atkinson, K., Bench-Capon, T. & Bollegala, D. 2020. Explanation in ai and law: past, present and future. Artificial
Intelligence, 103387.

Atkinson, K., Bench-Capon, T. J. & McBumey, P. 2005a. Multi-agent argumentation for edemocracy. In EUMAS,
35-46.

Atkinson, K., Bench-Capon, T. & Mcburney, P. 2005b. A dialogue game protocol for multi-agent argument over
proposals for action. Autonomous Agents and Multi-Agent Systems 11(2), 153-171.

Atkinson, K. M., Bench-Capon, T. J., Cartwright, D. & Wyner, A. Z. 2011. Semantic models for policy deliberation.
In Proceedings of the 13th International Conference on Artificial Intelligence and Law, 81-90.

Azhar, M. Q. & Sklar, E. I. 2016. Analysis of empirical results on argumentation-based dialogue to support shared
decision making in a human-robot team. In 2016 25th IEEE International Symposium on Robot and Human
Interactive Communication (RO-MAN), 861-866. IEEE.

Azhar, M. Q. & Sklar, E. I. 2017. A study measuring the impact of shared decision making in a human-robot team.
The International Journal of Robotics Research 36(5—-7), 461-482.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 27

Bandara, A. K., Kakas, A. C., Lupu, E. C. & Russo, A. 2009. Using argumentation logic for firewall configuration
management. In 2009 IFIP/IEEE International Symposium on Integrated Network Management, 180-187. IEEE.

Bandara, A. K., Kakas, A., Lupu, E. C. & Russo, A. 2006. Using argumentation logic for firewall policy specifi-
cation and analysis. In International Workshop on Distributed Systems: Operations and Management, 185-196.
Springer.

Baroni, P., Borsato, S., Rago, A. & Toni, F. 2018. The “games of argumentation” web platform. In COMMA,
447-448.

Baroni, P., Caminada, M. & Giacomin, M. 2011. An introduction to argumentation semantics. Knowledge
Engineering Review 26(4), 365.

Baroni, P., Rago, A. & Toni, F. 2018. How many properties do we need for gradual argumentation? In Thirty-Second
AAAI Conference on Artificial Intelligence.

Baroni, P., Romano, M., Toni, F., Aurisicchio, M. & Bertanza, G. 2015. Automatic evaluation of design alternatives
with quantitative argumentation. Argument & Computation 6(1), 24-49.

Bassiliades, N., Spanoudakis, N. I. & Kakas, A. C. 2018. Towards multipolicy argumentation. In Proceedings of the
10th Hellenic Conference on Artificial Intelligence, 1-10.

Bench-Capon, T. 2002. Value based argumentation frameworks. arXiv preprint cs/0207059.

Bench-Capon, T., Atkinson, K. & Chorley, A. 2005. Persuasion and value in legal argument. Journal of Logic and
Computation 15(6), 1075-1097.

Bench-Capon, T. J. 1998. Specification and implementation of toulmin dialogue game. In Proceedings of JURIX,
98, 5-20.

Bench-Capon, T. J. 2003a. Persuasion in practical argument using value-based argumentation frameworks. Journal
of Logic and Computation 13(3), 429-448.

Bench-Capon, T. J. 2003b. Try to see it my way: modelling persuasion in legal discourse. Artificial Intelligence and
Law 11(4), 271-287.

Bench-Capon, T. J. 2020. Before and after dung: argumentation in ai and law. Argument & Computation (Preprint),
1-18.

Bench-Capon, T. J. M., Geldard, T. & Leng, P. H. 2000. A method for the computational modelling of dialectical
argument with dialogue games. Artificial Intelligence and Law 8(2-3), 233-254.

Besnard, P. & Hunter, A. 2001. A logic-based theory of deductive arguments. Artificial Intelligence 128(1-2),
203-235.

Besnard, P. & Hunter, A. 2008. Elements of Argumentation, 47. MIT Press.

Besnard, P. & Hunter, A. 2009. Argumentation based on classical logic. In Argumentation in Artificial Intelligence,
133-152. Springer.

Betz, G., Hamann, M., Mchedlidze, T. & von Schmettow, S. 2019. Applying argumentation to structure and visualize
multi-dimensional opinion spaces. Argument & Computation 10(1), 23-40.

Bex, F., Bench-Capon, T. J. & Verheij, B. 2011. What makes a story plausible? the need for precedents. In JURIX,
23-32.

Bex, F., Budzynska, K. & Walton, D. 2012. Argumentation and explanation in the context of dialogue. Explanation-
aware Computing ExaCt 2012 9, 6.

Bex, F., Lawrence, J., Snaith, M. & Reed, C. 2013. Implementing the argument web. Communications of the ACM
56(10), 66-73.

Bex, F., Prakken, H. & Reed, C. 2010. A formal analysis of the AIF in terms of the ASPIC framework. In COMMA,
99-110.

Bex, F. & Walton, D. 2016. Combining explanation and argumentation in dialogue. Argument & Computation 7(1),
55-68.

Biere, A., Heule, M. & van Maaren, H. 2009. Handbook of Satisfiability, 185. JOS Press.

Bikakis, A. & Antoniou, G. 2010. Defeasible contextual reasoning with arguments in ambient intelligence. JEEE
Transactions on Knowledge and Data Engineering 22(11), 1492-1506.

Bistarelli, $., Pirolandi, D. & Santini, F. 2009. Solving weighted argumentation frameworks with soft constraints. In
International Workshop on Constraint Solving and Constraint Logic Programming, 1-18. Springer.

Black, E. & Atkinson, K. 2011. Choosing persuasive arguments for action. In The 10th International Conference
on Autonomous Agents and Multiagent Systems-Volume 3, 905-912. International Foundation for Autonomous
Agents and Multiagent Systems.

Boltuzié, F. & Snajder, J. 2014a. Back up your stance: recognizing arguments in online discussions. In
Proceedings of the First Workshop on Argumentation Mining, 49-58. Association for Computational Linguistics.
http://www.aclweb.org/anthology/W 14-2107.

Boltuzié, F. & Snajder, J. 2014b. Back up your stance: recognizing arguments in online discussions. In Proceedings
of the First Workshop on Argumentation Mining, 49-58.

Boltuzié, F. & Snajder, J. 2015. Identifying prominent arguments in online debates using semantic textual similarity.
In Proceedings of the 2nd Workshop on Argumentation Mining, 110-115.

Bonacina, M. P. 2017. Automated reasoning for explainable artificial intelligence. In ARCADE@ CADE, 24-28.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
28 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

Bonzon, E., Delobelle, J., Konieczny, $. & Maudet, N. 2016. A comparative study of ranking-based semantics for
abstract argumentation. In Proceedings of the AAAI Conference on Artificial Intelligence, 30.

Botschen, T., Sorokin, D. & Gurevych, I. 2018. Frame-and entity-based knowledge for common-sense argumentative
reasoning. In Proceedings of the 5th Workshop on Argument Mining, 90-96.

Burgemeestre, B., Hulstijn, J. & Tan, Y.-H. 2011. Value-based argumentation for justifying compliance. Artificial
Intelligence and Law 19(2-3), 149.

Buvac, S. & Mason, I. A. 1993. Propositional logic of context. In AAAI, 412-419.

Caminada, M. 2008. A gentle introduction to argumentation semantics. Lecture Material, Summer.

Caminada, M., Sa, S., Alcantara, J. & Dvofdk, W. 2015. On the equivalence between logic programming semantics
and argumentation semantics. International Journal of Approximate Reasoning 58, 87-111.

Cartwright, D. & Atkinson, K. 2009. Using computational argumentation to support e-participation. [EEE Intelligent
Systems 24(5), 42-52.

Carvalho, D. V., Pereira, E. M. & Cardoso, J. 8. 2019. Machine learning interpretability: a survey on methods and
metrics. Electronics 8(8), 832.

Cayrol, C. & Lagasquie-Schiex, M.-C. 2005. On the acceptability of arguments in bipolar argumentation frameworks.
In European Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty, 378-389.
Springer.

Cerutti, F., Giacomin, M. & Vallati, M. 2019. How we designed winning algorithms for abstract argumentation and
which insight we attained. Artificial Intelligence 276, 1-40.

Chapman, M., Balatsoukas, P., Ashworth, M., Curcin, V., Kékciyan, N., Essers, K., Sassoon, I., Modgil, S., Parsons,
8. & Sklar, E. I. 2019. Computational argumentation-based clinical decision support. In Proceedings of the 18th
International Conference on Autonomous Agents and MultiAgent Systems, 2345-2347. International Foundation
for Autonomous Agents and Multiagent Systems.

Charwat, G., Dvorak, W., Gaggl, S. A., Wallner, J. P. & Woltran, S. 2015. Methods for solving reasoning problems
in abstract argumentation—a survey. Artificial Intelligence 220, 28-63.

Chesnevar, C., McGinnis, J., Modgil, S., Rahwan, I, Reed, C., Simari, G., South, M., Vreeswijk, G. & Willmott, 8.
2006. Towards an argument interchange format. The Knowledge Engineering Review 21(4), 293-316.

Choo, J. & Liu, S. 2018. Visual analytics for explainable deep learning. IEEE Computer Graphics and Applications
38(4), 84-92.

Ciatto, G., Calvaresi, D., Schumacher, M. I. & Omicini, A. 2015. An Abstract Framework for Agent-Based
Explanations in AI. Springer.

Clark, C. E. 1919. Eisner v Macomber and some income tax problems. Yale LJ 29, 735.

Cocarascu, O., Cyras, K. & Toni, F. 2018. Explanatory predictions with artificial neural networks and argumentation.
In Proceedings of the 2nd Workshop on Explainable Artificial Intelligence (XAI 2018).

Cocarascu, O. & Toni, F. 2016. Argumentation for machine learning: a survey. In COMMA, 219-230.

Cocarascu, O. & Toni, F. 2018. Combining deep learning and argumentative reasoning for the analysis of social
media textual content using small data sets. Computational Linguistics 44(4), 833-858.

Cogan, E., Parsons, S. & McBumey, P. 2005. What kind of argument are we going to have today?. In Proceedings
of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, 544-551.

Cohen, A., Gottifredi, S., Garcia, A. J. & Simari, G. R. 2014. A survey of different approaches to support in
argumentation systems. The Knowledge Engineering Review 29(5), 513-550.

Collenette, J., Atkinson, K. & Bench-Capon, T. 2020. An explainable approach to deducing outcomes in european
court of human rights cases using ADFs. Frontiers in Artificial Intelligence and Applications 326, 21-32.

Collins, A., Magazzeni, D. & Parsons, S. 2019. Towards an argumentation-based approach to explainable planning.
In ICAPS 2019 Workshop XAIP Program Chairs.

Core, M. G., Lane, H. C., Van Lent, M., Gomboc, D., Solomon, S. & Rosenberg, M. 2006. Building explainable
artificial intelligence systems. In AAAI, 1766-1773.

Core, M. G., Lane, H. C., Van Lent, M., Solomon, S., Gomboc, D. & Carpenter, P. 2005. Toward question answer-
ing for simulations. In Proceedings of the IJCAI 2005 Workshop on Knowledge and Reasoning for Answering
Questions (KRAQOS5). Citeseer.

Correia, V. 2012. The ethics of argumentation. Informal Logic 32(2), 222-241.

Coste-Marquis, S., Konieczny, S., Marquis, P. & Quali, M. A. 2012a. Selecting extensions in weighted argumentation
frameworks. In COMMA, 12, 342-349.

Coste-Marquis, S., Konieczny, S., Marquis, P. & Ouali, M. A. 2012b. Weighted attacks in argumentation frame-
works. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning.
Cyras, K. 2016. Argumentation-based reasoning with preferences. In International Conference on Practical

Applications of Agents and Multi-Agent Systems, 199-210. Springer.

Cyras, K., Birch, D., Guo, Y., Toni, F., Dulay, R., Turvey, S., Greenberg, D. & Hapuarachchi, T. 2019. Explanations

by arbitrated argumentative dispute. Expert Systems with Applications 127, 141-156.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 29

Cyras, K., Delaney, B., Prociuk, D., Toni, F., Chapman, M., Dominguez, J. & Curcin, V. 2018. Argumentation
for explainable reasoning with conflicting medical recommendations. In Proceedings of the Joint Proceedings of
Reasoning with Ambiguous and Conflicting Evidence and Recommendations in Medicine (MedRACER 2018).

Cyras, K., Fan, X., Schulz, C. & Toni, F. 2017. Assumption-based argumentation: disputes, explanations,
preferences. Journal of Applied Logics-ifcolog Journal of Logics and their Applications 4(8), 2407-2456.

Cyras, K. & Oliveira, T. 2019. Resolving conflicts in clinical guidelines using argumentation. In Proceedings of
the 18th International Conference on Autonomous Agents and MultiAgent Systems, 1731-1739. International
Foundation for Autonomous Agents and Multiagent Systems.

Cyras, K., Satoh, K. & Toni, F. 2016a. Abstract argumentation for case-based reasoning. In Fifteenth International
Conference on the Principles of Knowledge Representation and Reasoning.

Cyras, K., Satoh, K. & Toni, F. 2016b. Explanation for case-based reasoning via abstract argumentation. In
International Conference on the Principles of Argumentation.

Czubaroff, J. 2007. Justice and argument: toward development of a dialogical argumentation theory. Argumentation
and Advocacy 44(1), 18-35.

Das, A. & Rad, P. 2020. Opportunities and challenges in explainable artificial intelligence (xai): a survey. arXiv
preprint arXiv:2006.11371.

Dauphin, J. & Cramer, M. 2017. Aspic-end: structured argumentation with explanations and natural deduction. In
International Workshop on Theorie and Applications of Formal Argumentation, 51-66. Springer.

Dechter, R. & Cohen, D. 2003. Constraint Processing. Morgan Kaufmann.

Deeks, A. 2019. The judicial demand for explainable artificial intelligence. Columbia Law Review 119(7),
1829-1850.

Dheeru, D. & Taniskidou, E. K. 2017. UCI machine learning repository: mushroom data set.

Donadello, I, Dragoni, M. & Eccher, C. 2019. Persuasive explanation of reasoning inferences on dietary data. In
Contributo in Atti di Convegno (Proceeding).

Dosilovié, F. K., Bréié, M. & Hlupié, N. 2018. Explainable artificial intelligence: a survey. In 2018 41st International
Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO),
0210-0215. IEEE.

Doutre, S. & Mailly, J.-G. 2018. Constraints and changes: a survey of abstract argumentation dynamics. Argument
& Computation 9(3), 223-248.

Dung, P. M. 1995. An argumentation-theoretic foundation for logic programming. The Journal of Logic
Programming 22(2), 151-177.

Dung, P. M. 2016. An axiomatic analysis of structured argumentation with priorities. Artificial Intelligence 231,
107-150.

Dung, P. M., Kowalski, R. A. & Toni, F. 2009. Assumption-based argumentation. In Argumentation in Artificial
Intelligence, 199-218. Springer.

Dung, P. M. & Son, T. C. 1995. Nonmonotonic inheritance, argumentation and logic programming. In International
Conference on Logic Programming and Nonmonotonic Reasoning, 316-329. Springer.

Dunne, P. E., Hunter, A.. McBumey, P., Parsons, S. & Wooldridge, M. 2011. Weighted argument systems: basic
definitions, algorithms, and complexity results. Artificial Intelligence 175(2), 457-486.

Dunne, P. E., Hunter, A.. McBurney, P., Parsons, S. & Wooldridge, M. J. 2009. Inconsistency tolerance in weighted
argument systems. In AAMAS (2), 851-858.

Fan, X. & Toni, F. 2014. On computing explanations in abstract argumentation. In ECA/, 1005-1006.

Fan, X. & Toni, F. 2015a. On computing explanations in argumentation. In Twenty-Ninth AAAI Conference on
Artificial Intelligence.

Fan, X. & Toni, F. 2015b. On explanations for non-acceptable arguments. In International Workshop on Theory and
Applications of Formal Argumentation, 112-127. Springer.

Fan, X., Toni, F., Mocanu, A. & Williams, M. 2014. Dialogical two-agent decision making with assumption-based
argumentation. In Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent
Systems, 533-540.

Fazzinga, B., Flesca, 8. & Furfaro, F. 2018. Probabilistic bipolar abstract argumentation frameworks: complexity
results. In LICAT, 1803-1809.

Fermandez, A., Herrera, F., Cordon, O., del Jesus, M. J. & Marcelloni, F. 2019. Evolutionary fuzzy systems
for explainable artificial intelligence: why, when, what for, and where to?. JEEE Computational Intelligence
Magazine 14(1), 69-81.

Fischer, L., Hasler, 8., Deigmdller, J., Schniirer, T., Redert, M., Pluntke, U., Nagel, K., Senzel, C., Ploennigs, J.,
Richter, A. & Eggert, J. 2018. Which tool to use? grounded reasoning in everyday environments with assistant
robots. In CogRob@ KR, 3-10.

Fitting, M. 1992. The stable model semantics for logic programming.

Fogues, R. L., Murukannaiah, P. K., Such, J. M. & Singh, M. P. 2017a. Sharing policies in multiuser pri-
vacy scenarios: incorporating context, preferences, and arguments in decision making. ACM Transactions on
Computer-Human Interaction (TOCHI) 24(1), 1-29.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
30 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

Fogues, R. L., Murukannaiah, P. K., Such, J. M. & Singh, M. P. 2017b. Sosharp: recommending sharing policies in
multiuser privacy scenarios. IEEE Internet Computing 21(6), 28-36.

Friedrich, G. & Zanker, M. 2011. A taxonomy for generating explanations in recommender systems. Al Magazine
32(3), 90-98.

Garcez, A. S., Gabbay, D. M. & Lamb, L. C. 2005. Value-based argumentation frameworks as neural-symbolic
learning systems. Journal of Logic and Computation 15(6), 1041-1058.

Garcia, A., Chesfievar, C., Rotstein, N. & Simari, G. 2007. An abstract presentation of dialectical explanations in
defeasible argumentation. In ArgNMRO7, 17-32.

Garcia, A. J., Chesfievar, C. I, Rotstein, N. D. & Simari, G. R. 2013. Formalizing dialectical explanation support for
argument-based reasoning in knowledge-based systems. Expert Systems with Applications 40(8), 3233-3247.
Genitsaridi, I., Bikakis, A. & Antoniou, G. 2013. Deal: a distributed authorization language for ambient intelli-
gence. In Pervasive and Ubiquitous Technology Innovations for Ambient Intelligence Environments, 188-204.

IGI Global.

Girle, R., Hitchcock, D., McBurney, P. & Verheij, B. 2003. Decision support for practical reasoning. In
Argumentation Machines, 55-83. Springer.

Gordon, T. F. 1993. The pleadings game. Artificial Intelligence and Law 2(4), 239-292.

Gordon, T. F. & Walton, D. 2009. Legal reasoning with argumentation schemes. In Proceedings of the 12th
International Conference on Artificial Intelligence and Law, 137-146.

Grando, M. A., Moss, L., Sleeman, D. & Kinsella, J. 2013. Argumentation-logic for creating and explaining medical
hypotheses. Artificial Intelligence in Medicine 58(1), 1-13.

Green, N. L., Branon, M. & Roosje, L. 2019. Argument schemes and visualization software for critical thinking
about international politics. Argument & Computation 10(1), 41-53.

Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F. & Pedreschi, D. 2018. A survey of methods for
explaining black box models. ACM Computing Surveys (CSUR) 51(5), 1-42.

Gunning, D. & Aha, D. W. 2019. Darpa’s explainable artificial intelligence program. AJ Magazine 40(2), 44-58.

Habernal, I. & Gurevych, I. 2016. What makes a convincing argument? empirical analysis and detecting attributes of
convincingness in web argumentation. In Proceedings of the 2016 Conference on Empirical Methods in Natural
Language Processing, 1214-1223.

Hage, J. C., Leenes, R. & Lodder, A. R. 1993. Hard cases: a procedural approach. Artificial Intelligence and Law
2(2), 113-167.

Hilton, D. J. 1990. Conversational processes and causal explanation. Psychological Bulletin 107(1), 65.

Holzinger, A., Malle, B., Kieseberg, P., Roth, P. M., Miiller, H., Reihs, R. & Zatloukal, K. 2017. Towards the
augmented pathologist: challenges of explainable-ai in digital pathology. arXiv preprint arXiv:1712.06657.

Indrie, S. M. & Groza, A. 2010. Enacting argumentative web in semantic wikipedia. In 9th RoEduNet IEEE
International Conference, 163-168. IEEE.

Josephson, J. R. & Josephson, S. G. 1996. Abductive Inference: Computation, Philosophy, Technology. Cambridge
University Press.

Kakas, A. C., Moraitis, P. & Spanoudakis, N. I. 2019. Gorgias: applying argumentation. Argument & Computation
10(1), 55-81.

Kakas, A. & Michael, L. 2020. Abduction and argumentation for explainable machine learning: a position survey.
arXiv preprint arXiv:2010.12896.

Kakas, A. & Moraitis, P. 2003. Argumentation based decision making for autonomous agents. In Proceedings of the
Second International Joint Conference on Autonomous Agents and Multiagent Systems, 883-890.

Kakas, A. & Moraitis, P. 2006. Adaptive agent negotiation via argumentation. In Proceedings of the Fifth
International Joint Conference on Autonomous Agents and Multiagent Systems, 384-391.

Karafili, E., Kakas, A. C., Spanoudakis, N. I. & Lupu, E. C. 2017. Argumentation-based security for social good. In
2017 AAAI Fall Symposium Series.

Karafili, E., Lupu, E. C., Arunkumar, 8. & Bertino, E. n.d.. Policy analysis for drone systems: an argumentation-based
approach.

Karafili, E., Spanaki, K. & Lupu, E. C. 2018a. An argumentation reasoning approach for data processing. Computers
in Industry 94, 52-61.

Karafili, E., Wang, L., Kakas, A. C. & Lupu, E. 2018b. Helping forensic analysts to attribute cyber-attacks: an
argumentation-based reasoner. In International Conference on Principles and Practice of Multi-Agent Systems,
510-518. Springer.

Karafili, E., Wang, L. & Lupu, E. C. 2020. An argumentation-based reasoner to assist digital investigation and
attribution of cyber-attacks. Forensic Science International: Digital Investigation 32, 300925.

Kemke, C. 2006. An architectural framework for natural language interfaces to agent systems. In Computational
Intelligence, 371-376.

Keneni, B. M., Kaur, D., Al Bataineh, A., Devabhaktuni, V. K., Javaid, A. Y., Zaientz, J. D. & Marinier, R. P. 2019.
Evolving rule-based explainable artificial intelligence for unmanned aerial vehicles. IEEE Access 7, 17001-17016.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 31

Kobbe, J., Opitz, J., Becker, M., Hulpus, I, Stuckenschmidt, H. & Frank, A. 2019. Exploiting background knowledge
for argumentative relation classification. In 2nd Conference on Language, Data and Knowledge (LDK 2019).
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.

Kékciyan, N., Chapman, M., Balatsoukas, P., Sassoon, I., Essers, K., Ashworth, M., Curcin, V., Modgil, S., Parsons,
S. & Sklar, E. I. 2019. A collaborative decision support tool for managing chronic conditions. In Medinfo,
644-648.

Kokciyan, N., Sassoon, L, Young, A. P., Chapman, M., Porat, T., Ashworth, C., Modgil, S., Parsons, S. & Sklar,
E. 2018. Towards an argumentation system for supporting patients in self-managing their chronic conditions. In
AAAT.

Koékciyan, N., Yaglikci, N. & Yolum, P. 2017. An argumentation approach for resolving privacy disputes in online
social networks. ACM Transactions on Internet Technology (TOIT) 17(3), 1-22.

Kokciyan, N. & Yolum, P. 2017. Context-based reasoning on privacy in internet of things. In [JCAI, 4738-4744.

Koshiyama, A., Kazim, E. & Engin, Z. 2019. Xai: digital ethics. In HeXAI Workshop.

Kraus, S., Sycara, K. & Evenchik, A. 1998. Reaching agreements through argumentation: a logical model and
implementation. Artificial Intelligence 104(1—2), 1-69.

Labrie, N. & Schulz, P. J. 2014. Does argumentation matter? a systematic literature review on the role of
argumentation in doctor-patient communication. Health Communication 29(10), 996-1008.

Laird, J. E. & Nielsen, E. 1994. Coordinated behavior of computer generated forces in TacAir-Soar. AD-A280 063
1001, 57.

Lamy, J.-B., Sekar, B., Guezennec, G., Bouaud, J. & Séroussi, B. 2019. Explainable artificial intelligence for breast
cancer: a visual case-based reasoning approach. Artificial Intelligence in Medicine 94, 42-53.

Langley, P. 2019. Explainable, normative, and justified agency. In Proceedings of the AAAI Conference on Artificial
Intelligence, 33, 9775-9779.

Lawrence, J. & Reed, C. 2020. Argument mining: a survey. Computational Linguistics 45(4), 765-818.

Letia, I. A. & Groza, A. 2012. Interleaved argumentation and explanation in dialog. In The 12th workshop on
Computational Models of Natural Argument, 44.

Levin, J. A. & Moore, J. A. 1977. Dialogue-games: metacommunication structures for natural language interaction.
Cognitive Science 1(4), 395-420.

Liao, B., Anderson, M. & Anderson, S. L. 2018. Representation, justification and explanation in a value driven agent:
an argumentation-based approach. arXiv preprint arXiv:1812.05362.

Lifschitz, V. 2019. Answer Set Programming. Springer International Publishing.

Lippi, M. & Torroni, P. 2016. Argumentation mining: state of the art and emerging trends. ACM Transactions on
Internet Technology (TOIT) 16(2), 1-25.

Liu, X., Eshghi, A., Swietojanski, P. & Rieser, V. 2019. Benchmarking natural language understanding services for
building conversational agents. arXiv preprint arXiv: 1903.05566.

Lombrozo, T. 2006. The structure and function of explanations. Trends in Cognitive Sciences 10(10), 464-470.

Longo, L. 2016. Argumentation for knowledge representation, conflict resolution, defeasible inference and its
integration with machine learning. In Machine Learning for Health Informatics, 183-208. Springer.

Longo, L. & Hederman, L. 2013. Argumentation theory for decision support in health-care: a comparison with
machine learning. In International Conference on Brain and Health Informatics, 168-180, Springer.

Loui, R. P. & Norman, J. 1995. Rationales and argument moves. Artificial Intelligence and Law 3(3), 159-189.

Lucero, M. J. G., Chesnevar, C. I. & Simari, G. R. 2009. On the accrual of arguments in defeasible logic
programming. In Twenty-First International Joint Conference on Artificial Intelligence.

Luis-Argentina, §. 2008. Decision rules and arguments in defeasible decision making. In Computational Models of
Argument: Proceedings of COMMA 2008, 172, 171.

Madhikermi, M., Malhi, A. K. & Framling, K. 2019. Explainable artificial intelligence based heat recycler fault
detection in air handling unit. In International Workshop on Explainable, Transparent Autonomous Agents and
Multi-Agent Systems, 110-125. Springer.

Malle, B. F. 2006. How the Mind Explains Behavior: Folk Explanations, Meaning, and Social Interaction. MIT
Press.

Mayer, T., Cabrio, E., Lippi, M., Torroni, P. & Villata, S. 2018. Argument mining on clinical trials. In COMMA,
137-148.

McBurney, P. & Parsons, 8. 2002. Dialogue games in multi-agent systems. Informal Logic 22(3).

McBumey, P. & Parsons, S$. 2009. Dialogue games for agent argumentation. In Argumentation in Artificial
Intelligence, 261-280, Springer.

Mcburney, P., Van Eijk, R. M., Parsons, S. & Amgoud, L. 2003. A dialogue game protocol for agent purchase
negotiations. Autonomous Agents and Multi-Agent Systems 7(3), 235-273.

McCarty, L. T. 1976. Reflections on taxman: an experiment in artificial intelligence and legal reasoning. Harvard
Law Review 90, 837.

Meditskos, G., Kontopoulos, E., Vrochidis, 8. & Kompatsiaris, I. 2019. Converness: ontology-driven conversational
awareness and context understanding in multimodal dialogue systems. Expert Systems, ¢12378.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
32 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

Melo, V. S., Panisson, A. R. & Bordini, R. H. 2016. Argumentation-based reasoning using preferences over sources
of information. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent
Systems, 1337-1338.

Mercier, H. & Sperber, D. 2011. Why do Humans Reason? Arguments for an Argumentative Theory. Cambridge
University Press.

Miller, T. 2019. Explanation in artificial intelligence: insights from the social sciences. Artificial Intelligence 267,
1-38.

Modgil, 8S. 2006a. Hierarchical argumentation. In European Workshop on Logics in Artificial Intelligence, 319-332.
Springer.

Modgil, S. 2006b. Value based argumentation in hierarchical argumentation frameworks. COMMA 144, 297-308.

Modgil, S. 2009. Reasoning about preferences in argumentation frameworks. Artificial Intelligence 173(9-10),
901-934.

Modgil, S., Budzynska, K. & Lawrence, J. 2018. Argument harvesting using chatbots. In Computational Models of
Argument: Proceedings of COMMA 2018, 305, 149.

Modgil, S. & Prakken, H. 2014. The aspic+ framework for structured argumentation: a tutorial. Argument &
Computation 5(1), 31-62.

Modgil, S., Toni, F., Bex, F., Bratko, ., Chesfievar, C. I., Dvorak, W., Falappa, M. A., Fan, X., Gaggl, S. A., Garcia,
A. J., Gonzalez, M. P., Gordon, T. F., Leite, J., Mozina, M., Reed, C., Simari, G. R., Szeider, S., Torroni, P. &
Woltran, S. 2013. The added value of argumentation. In Agreement Technologies, 357-403. Springer.

Moens, M.-F. 2016. Argumentation mining: how can a machine acquire world and common sense knowledge?. In
COMMA, 4.

Moens, M.-F. 2018. Argumentation mining: how can a machine acquire common sense and world knowledge?.
Argument & Computation 9(1), 1-14.

Mollas, L, Bassiliades, N. & Tsoumakas, G. 2020. Altruist: argumentative explanations through local interpretations
of predictive models. arXiv preprint arXiv:2010.07650.

Mosca, F., Sarkadi, S., Such, J. M. & McBurney, P. 2020. Agent expri: licence to explain. In International Workshop
on Explainable, Transparent Autonomous Agents and Multi-Agent Systems, 21-38. Springer.

MozZina, M., Zabkar, J. & Bratko, I. 2007. Argument based machine learning. Artificial Intelligence 171(10-15),
922-937.

Murukannaiah, P. K., Kalia, A. K., Telangy, P. R. & Singh, M. P. 2015. Resolving goal conflicts via argumentation-
based analysis of competing hypotheses. In 20/5 IEEE 23rd International Requirements Engineering Conference
(RE), 156-165. IEEE.

Nicolaides, A. N., Kakkos, S. K., Kyriacou, E., Griffin, M., Sabetai, M., Thomas, D. J., Tegos, T., Geroulakos, G.,
Labropoulos, N., Doré, C. J., Morris, T. P., Naylor, R. & Abbott, A. L. 2010. Asymptomatic internal carotid artery
stenosis and cerebrovascular risk stratification. Journal of Vascular Surgery 52(6), 1486-1496.

Noél, V. & Kakas, A. 2009. Gorgias-c: extending argumentation with constraint solving. In International Conference
on Logic Programming and Nonmonotonic Reasoning, 535-541. Springer.

Nunes, E., Kulkarni, N., Shakarian, P., Ruef, A. & Little, J. 2016a. Cyber-deception and attribution in capture-the-flag
exercises. In Cyber Deception, 149-165. Springer.

Nunes, E., Shakarian, P. & Simari, G. I. 2016b. Toward argumentation-based cyber attribution. In 30th AAAI
Conference on Artificial Intelligence, AAAI 2016, 177-184. AI Access Foundation.

Nunes, E., Shakarian, P., Simari, G. L & Ruef, A. 2016c. Argumentation models for cyber attribution. In 2016
IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 837-844.
IEEE.

Nute, D. 2001. Defeasible logic. In International Conference on Applications of Prolog, 151-169. Springer.

Oliveira, T., Dauphin, J., Satoh, K., Tsumoto, 8. & Novais, P. 2018. Argumentation with goals for clinical deci-
sion support in multimorbidity. In Proceedings of the 17th International Conference on Autonomous Agents and
MultiAgent Systems.

Ott, M., Cardie, C. & Hancock, J. T. 2013. Negative deceptive opinion spam. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
497-501.

Paez, A. 2019. The pragmatic turn in explainable artificial intelligence (xai). Minds and Machines 29(3), 441-459.

Panisson, A. R. 2019. Towards an organisation-centred semantics for argumentation-based dialogues. In 2019 Sth
Brazilian Conference on Intelligent Systems (BRACIS), 491496. IEEE.

Panisson, A. R., Ali, A., McBurney, P. & Bordini, R. H. 2018. Argumentation schemes for data access control. In
COMMA, 361-368.

Panisson, A. R. & Bordini, R. H. 2016. Knowledge representation for argumentation in agent-oriented programming
languages. In 2016 5th Brazilian Conference on Intelligent Systems (BRACIS), 13-18. IEEE.

Panisson, A. R., Meneguzzi, F., Vieira, R. & Bordini, R. H. 2014. An approach for argumentation-based reasoning
using defeasible logic in multi-agent programming languages. In //th International Workshop on Argumentation
in Multiagent Systems, 1-15.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 33

Panisson, A. R., Meneguzzi, F., Vieira, R. & Bordini, R. H. 2015. Towards practical argumentation-based dialogues
in multi-agent systems. In 20/5 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent
Agent Technology (WI-IAT), 2, 151-158. IEEE.

Pavese, C. 2019. The semantics and pragmatics of argumentation. Academia.

Pilotti, P., Casali, A. & Chesfievar, C. 2015. A belief revision approach for argumentation-based negotiation agents.
International Journal of Applied Mathematics and Computer Science 25(3), 455-470.

Poceviciuté, M., Eilertsen, G. & Lundstrém, C. 2020. Survey of XAI in digital pathology. In Artificial Intelligence
and Machine Learning for Digital Pathology, 56-88, Springer.

Potash, P., Bhattacharya, R. & Rumshisky, A. 2017. Length, interchangeability, and external knowledge: observa-
tions from predicting argument convincingness. In Proceedings of the Eighth International Joint Conference on
Natural Language Processing (Volume 1: Long Papers), 342-351.

Prakken, H. 2005a. Ai & law, logic and argument schemes. Argumentation 19(3), 303-320.

Prakken, H. 2005b. Coherence and flexibility in dialogue games for argumentation. Journal of Logic and
Computation 15(6), 1009-1040.

Prakken, H. 2017. Logics of Argumentation and the Law. Cambridge University Press.

Prakken, H. & Sartor, G. 1998. Modelling reasoning with precedents in a formal dialogue game. In Judicial
Applications of Artificial Intelligence, 127-183. Springer.

Prakken, H., Wyner, A., Bench-Capon, T. & Atkinson, K. 2015. A formalization of argumentation schemes for legal
case-based reasoning in aspic+. Journal of Logic and Computation 25(5), 1141-1166.

Prentzas, N., Nicolaides, A., Kyriacou, E., Kakas, A. & Pattichis, C. 2019. Integrating machine learning with
symbolic reasoning to build an explainable AI model for stroke prediction. In 2019 IEEE 19th International
Conference on Bioinformatics and Bioengineering (BIBE), 817-821. IEEE.

Qurat-ul-ain Shaheen, A. T. & Bowles, J. K. 2020. Dialogue games for explaining medication choices. In Rules
and Reasoning: 4th International Joint Conference, RuleML+ RR 2020, Oslo, Norway, June 29-July 1, 2020,
Proceedings, 97. Springer Nature.

Rago, A., Cocarascu, O. & Toni, F. 2018. Argumentation-based recommendations: fantastic explanations and how
to find them. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence.

Rahwan, I, Banihashemi, B., Reed, C., Walton, D. & Abdallah, S. 2011. Representing and classifying arguments on
the semantic web. The Knowledge Engineering Review 26(4), 487-511.

Reed, C., Wells, S., Budzynska, K. & Devereux, J. 2010. Building arguments with argumentation: the role of
illocutionary force in computational models of argument. Jn COMMA, 415-426.

Regulation, P. 2016. Regulation (eu) 2016/679 of the European Parliament and of the Council. REGULATION (EUV),
679.

Ripley, M. L. 2005. Arguing for the ethics of an ad: an application of multi-modal argumentation theory.

Rissland, E. L. & Ashley, K. D. 1987. A case-based system for trade secrets law. In Proceedings of the Ist
International Conference on Artificial Intelligence and Law, 60-66.

Rosenfeld, A. & Kraus, S. 2016a. Providing arguments in discussions on the basis of the prediction of human
argumentative behavior. ACM Transactions on Interactive Intelligent Systems (TiiS), 6(4), 1-33.

Rosenfeld, A. & Kraus, S. 2016b. Strategical argumentative agent for human persuasion. In Proceedings of the
Twenty-second European Conference on Artificial Intelligence, 320-328. IOS Press.

Rowe, J., Levitt, K., Parsons, 8., Sklar, E., Applebaum, A. & Jalal, S$. 2012. Argumentation logic to assist in security
administration. In Proceedings of the 2012 New Security Paradigms Workshop, 43-52.

Sakama, C. 2018. Abduction in argumentation frameworks. Journal of Applied Non-Classical Logics 28(2-3),
218-239.

Samadi, M., Talukdar, P., Veloso, M. & Blum, M. 2016. Claimeval: integrated and flexible framework for claim
evaluation using credibility of sources. In Thirtieth AAAI Conference on Artificial Intelligence.

Samek, W. & Miiller, K.-R. 2019. Towards explainable artificial intelligence. In Explainable AI: Interpreting,
Explaining and Visualizing Deep Learning, 5-22. Springer.

Santini, F. & Yautsiukhin, A. 2015. Quantitative analysis of network security with abstract argumentation. In Data
Privacy Management, and Security Assurance, 30-46. Springer.

Sassoon, I, Kékciyan, N., Sklar, E. & Parsons, S. 2019. Explainable argumentation for wellness consultation. In
International Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Systems, 186-202.
Springer.

Schoenborn, J. M. & Althoff, K.-D. 2019. Recent trends in XAI: a broad overview on current approaches,
methodologies and interactions. In JCCBR Workshops, 51-60.

Schreier, M., Groeben, N. & Christmann, U. 1995. ‘that’s not fair! argumentational integrity as an ethics of
argumentative communication. Argumentation 9(2), 267-289.

Schulz, C. & Toni, F. 2016. Justifying answer sets using argumentation. Theory and Practice of Logic Programming
16(1), 59-110.

SeSelja, D. & StraBer, C. 2013. Abstract argumentation and explanation applied to scientific debates. Synthese
190(12), 2195-2217.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
34 A. VASSILIADES,N. BASSILIADES AND T. PATKOS

Shakarian, P., Simari, G. I, Moores, G. & Parsons, S. 2015. Cyber attribution: an argumentation-based approach. In
Cyber Warfare, 151-171. Springer.

Sheh, R. K.-M. 2017. “Why did you do that?” explainable intelligent robots. In Workshops at the Thirty-First AAAI
Conference on Artificial Intelligence.

Sklar, E. I. & Azhar, M. Q. 2015. Argumentation-based dialogue games for shared control in human-robot systems.
Journal of Human-Robot Interaction 4), 120-148.

Sklar, E. I. & Azhar, M. Q. 2018. Explanation through argumentation. In Proceedings of the 6th International
Conference on Human-Agent Interaction, 277-285.

Sklar, E. I., Parsons, S., Li, Z., Salvit, J., Perumal, 8., Wall, H. & Mangels, J. 2016. Evaluation of a trust-modulated
argumentation-based interactive decision-making tool. Autonomous Agents and Multi-Agent Systems 30(1),
136-173.

Sklar, E., Parsons, S. & Singh, M. P. 2013. Towards an argumentation-based model of social interaction. In
Proceedings of the Workshop on Argumentation in Multiagent Systems (ArgMAS) at the 12th International
Conference on Autonomous Agents and Multiagent Systems (AAMAS).

Slugoski, B. R., Lalljee, M., Lamb, R. & Ginsburg, G. P. 1993. Attribution in conversational context: effect of mutual
knowledge on explanation-giving. European Journal of Social Psychology 23(3), 219-238.

Smullyan, R. M. 1995. First-Order Logic. Courier Corporation.

Snaith, M., Lawrence, J. & Reed, C. 2010. Mixed initiative argument in public deliberation. Online Deliberation, 2.

Sermo, F., Cassens, J. & Aamodt, A. 2005. Explanation in case-based reasoning—perspectives and goals. Artificial
Intelligence Review 24(2), 109-143.

Spanoudakis, G., Kloukinas, C. & Androutsopoulos, K. 2007. Towards security monitoring patterns. In Proceedings
of the 2007 ACM Symposium on Applied Computing, 1518-1525.

Spanoudakis, N. L, Constantinou, E., Koumi, A. & Kakas, A. C. 2017. Modeling data access legislation with gorgias.
In International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems,
317-327. Springer.

Spanoudakis, N. I, Kakas, A. C. & Moraitis, P. 2016a. Applications of argumentation: the soda methodology. In
Proceedings of the Twenty-second European Conference on Artificial Intelligence, 1722-1723.

Spanoudakis, N. 1, Kakas, A. C. & Moraitis, P. 2016b. Gorgias-b: argumentation in practice. In COMMA, 477-478.

Spanoudakis, N. & Moriaitis, P. 2009. Engineering an agent-based system for product pricing automation.
Engineering Intelligent Systems 17(2), 139.

Swanson, R., Ecker, B. & Walker, M. 2015. Argument mining: extracting arguments from online dialogue. In
Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 217-226.

Tang, Y., Cai, K., McBurney, P., Sklar, E. & Parsons, $8. 2012. Using argumentation to reason about trust and belief.
Journal of Logic and Computation 22(5), 979-1018.

Tang, Y., Sklar, E. & Parsons, $. 2012. An argumentation engine: argtrust. In Ninth International Workshop on
Argumentation in Multiagent Systems.

Thimm, M. & Kersting, K. 2017. Towards argumentation-based classification. In Logical Foundations of Uncertainty
and Machine Learning, Workshop at IJCAI, 17.

Tjoa, E. & Guan, C. 2019. A survey on explainable artificial intelligence (XAI): towards medical XAI. arXiv preprint
arXiv: 1907.07374.

Torres, ., Hernandez, N., Rodrguez, A., Fuentes, G. & Pineda, L. A. 2019. Reasoning with preferences in service
robots. Journal of Intelligent & Fuzzy Systems 36(5), 5105-5114.

Toulmin, S. 1958. The Uses of Argument. Cambridge University Press.

Vassiliades, A., Patkos, T., Bikakis, A., Flouris, G., Bassiliades, N. & Plexousakis, D. 2020. Preliminary notions of
arguments from commonsense knowledge. In 11th Hellenic Conference on Artificial Intelligence, 211-214.

Verheij, B. 2003. Artificial argument assistants for defeasible argumentation. Artificial Intelligence 150(1-2),
291-324.

Waltl, B. & Vogl, R. 2018. Explainable artificial intelligence the new frontier in legal informatics. Jusletter IT 4,
1-10.

Walton, D. 2005. Argumentation Methods for Artificial Intelligence in Law. Springer Science & Business Media.

Wanner, L., André, E., Blat, J., Dasiopoulou, S., Farris, M., Fraga, T., Kamateri, E., Lingenfelser, F., Llorach, G.,
Martinez, O., Meditskos, G., Mille, S., Minker, W., Pragst, L., Schiller, D., Stam, A., Stellingwerff, L., Sukno, F.,
Vieru, B. & Vrochidis, S. 2017. Kristina: a knowledge-based virtual conversation agent. In International
Conference on Practical Applications of Agents and Multi-Agent Systems, 284-295. Springer.

Wardeh, M., Wyner, A., Atkinson, K. & Bench-Capon, T. 2013. Argumentation based tools for policy-making. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law, 249-250.

Wick, M. R. & Thompson, W. B. 1992. Reconstructive expert system explanation. Artificial Intelligence 54(1-2),
33-70.

Willmott, S., Vreeswijk, G., Chesnevar, C., South, M., McGinnis, J., Modgil, S., Rahwan, I., Reed, C. & Simari,
G. 2006. Towards an argument interchange format for multiagent systems. In 3rd International Workshop on
Argumentation in Multi-Agent Systems, ArgMAS-06, 17-34.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Argumentation and explainable artificial intelligence 35

Wooldridge, M. 2009. An Introduction to Multiagent Systems. John Wiley & Sons.

Wooldridge, M. & Van Der Hoek, W. 2005. On obligations and normative ability: towards a logical analysis of the
social contract. Journal of Applied Logic 3(3-4), 396-420.

Wyner, A. Z., Atkinson, K. & Bench-Capon, T. 2012a. Model based critique of policy proposals. In International
Conference on Electronic Participation, 120-131, Springer.

Wyner, A. Z., Atkinson, K. & Bench-Capon, T. J. 2012b. Opinion gathering using a multi-agent systems approach
to policy selection. In AAMAS, 1171-1172.

Yang, S. C.-H. & Shafto, P. 2017. Explainable artificial intelligence via bayesian teaching. In NIPS 2017 Workshop
on Teaching Machines, Robots, and Humans.

Zeng, Z., Fan, X., Miao, C., Leung, C., Jih, C. J. & Soon, O. Y. 2018. Context-based and explainable decision making
with argumentation. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent
Systems, 1114-1122. International Foundation for Autonomous Agents and Multiagent Systems.

Zeng, Z., Miao, C., Leung, C. & Chin, J. J. 2018. Building more explainable artificial intelligence with argumentation.
In Thirty-Second AAAI Conference on Artificial Intelligence.

Zhang, 8., Rudinger, R., Duh, K. & Van Durme, B. 2017. Ordinal common-sense inference. Transactions of the
Association for Computational Linguistics 5, 379-395.

Zhong, Q., Fan, X., Luo, X. & Toni, F. 2019. An explainable multi-attribute decision model based on argumentation.
Expert Systems with Applications 117, 42-61.

Zhong, Q., Fan, X., Toni, F. & Luo, X. 2014. Explaining best decisions via argumentation. In ECSI, 224-237.

https://doi.org/10.1017/S0269888921 000011 Published online by Cambridge University Press
Received: 19 November 2018

® Check for updates

Revised: 26 January 2019 Accepted: 24 February 2019

 

DOI: 10.1002/widm.1312

ADVANCED REVIEW

Causability and explainability of artificial intelligence in medicine

fal
a

Andreas Holzinger’ © | Georg Langs” | Helmut Denk? | Kurt Zatloukal* | Heimo Miiller’*

lMnstitute for Medical Informatics, Statistics and
Documentation, Medical University Graz, Graz,
Austria

Department of Biomedical Imaging and Image-
guided Therapy, Computational Imaging Research
Lab, Medical University of Vienna, Vienna,
Austria

Institute of Pathology, Medical University Graz,
Graz, Austria

Correspondence

Andreas Holzinger, Institute for Medical
Informatics, Statistics and Documentation,
Medical University Graz, A-8036, Austria.
Email: andreas. holzinger@ medunigraz.at

Funding information
FeatureCloud, Grant/Award Number: 826078
H2020 EU Project; Hochschulraum-

Explainable artificial intelligence (AD is attracting much interest in medicine.
Technically, the problem of explainability is as old as AI itself and classic AI repre-
sented comprehensible retraceable approaches. However, their weakness was in
dealing with uncertainties of the real world. Through the introduction of probabilis-
tic learning, applications became increasingly successful, but increasingly opaque.
Explainable AI deals with the implementation of transparency and traceability of
statistical black-box machine learning methods, particularly deep learning (DL).
We argue that there is a need to go beyond explainable AI. To reach a level of
explainable medicine we need causability. In the same way that usability encom-
passes measurements for the quality of use, causability encompasses measurements
for the quality of explanations. In this article, we provide some necessary defini-
tions to discriminate between explainability and causability as well as a use-case of
DL interpretation and of human explanation in histopathology. The main contribu-
tion of this article is the notion of causability, which is differentiated from explain-

2 WIRES AND KNOWLEDGE DISCOVERY WI LEY

Infrastrukturmittelfonds; MEFO, Grant/Award
Number: MEFO-Graz; This work was partially
supported by the Austrian Science Fund FWF
(12714-B31) and the EU under H2020 (765148)

ability in that causability is a property of a person, while explainability is a
property of a system

This article is categorized under:
Fundamental Concepts of Data and Knowledge > Human Centricity and User
Interaction

[Correction added on 11 June 2019, after first
online publication: “explainabilty” has been
corrected to “explainability” in the article title.]

KEYWORDS

artificial intelligence, causability, explainability, explainable AI, histopathology,
medicine

 

1 | INTRODUCTION AND MOTIVATION

Artificial intelligence (AI) is perhaps the oldest field of computer science and very broad, dealing with all aspects of mimick-
ing cognitive functions for real-world problem solving and building systems that learn and think like people. Therefore, it is
often called machine intelligence (Poole, Mackworth, & Goebel, 1998) to contrast it to human intelligence (Russell & Norvig,
2010). The field revolved around the intersection of cognitive science and computer science (Tenenbaum, Kemp, Griffiths, &
Goodman, 2011). AI now raises enormous interest due to the practical successes in machine learning (ML). In AI there was
always a strong linkage to explainability, and an early example is the Advice Taker proposed by McCarthy in 1958 as a “pro-
gram with common sense” (McCarthy, 1960). It was probably the first time proposing common sense reasoning abilities as
the key to AI. Recent research emphasizes more and more that AI systems should be able to build causal models of the world
that support explanation and understanding, rather than merely solving pattern recognition problems (Lake, Ullman, Tenen-
baum, & Gershman, 2017).

This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium,
provided the original work is properly cited.
© 2019 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals, Inc.

 

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET

WIREs Data Mining Knowl Discov. 2019;9:e1312. wires.wiley.com/dmkd | 1 of 13

https://doi.org/10.1002/widm.1312
2 of 13 WI LEY-_ @ WIREs HOLZINGER Er at.

‘@ DATA MINING AND KNOWLEDGE DISCOVERY

ML is a very practical field of AI with the aim to develop software that can automatically learn from previous data to gain
knowledge from experience and to gradually improve it's learning behavior to make predictions based on new data
(Michalski, Carbonell, & Mitchell, 1984). The grand challenges are in sense-making, in context understanding, and in decision
making under uncertainty (Holzinger, 2017). ML can be seen as the workhorse of AI and the adoption of data intensive ML
methods can meanwhile be found everywhere, throughout science, engineering and business, leading to more evidence-based
decision-making (Jordan & Mitchell, 2015). The enormous progress in ML has been driven by the development of new statis-
tical learning algorithms along with the availability of large data sets and low-cost computation (Abadi et al., 2016). One now-
adays extremely popular method is deep learning (DL).

DL is a family of ML models based on deep convolutional neural networks having a long history (Schmidhuber, 2015).
DL is very popular today because they are achieving amazing results even at human level performance (LeCun, Bengio, &
Hinton, 2015). A best practice example is a recent work of the Thrun group, where they achieved with a DL approach perfor-
mance on par with medical doctors, demonstrating that such approaches are able to classify skin cancer with a level of compe-
tence comparable to human dermatologists (Esteva et al., 2017). A further example is the promising results of identifying
diabetic retinopathy and related eye diseases (Ting et al., 2017). All these are very good examples of the progress and useful-
ness of AI, but even the most prominent proponents of these (automatic) approaches recently emphasize that usable intelli-
gence is difficult to reach because we need not only to learn from prior data, to extract knowledge, to generalize, and to fight
the curse of dimensionality, but to disentangle the underlying explanatory factors of the data in order to understand the context
in an application domain (Bengio, Courville, & Vincent, 2013), where to date a doctor-in-the-loop is indispensable
(Holzinger, 2016).

Medicine as application domain is among the greatest challenges of AI/ML/DL. In medical decision support we are con-
fronted with uncertainty, with probabilistic, unknown, incomplete, imbalanced, heterogeneous, noisy, dirty, erroneous, inaccu-
rate and missing data sets in arbitrarily high-dimensional spaces (Holzinger, Dehmer, & Jurisica, 2014), (Lee & Holzinger,
2016). Often we are simply lacking of large data sets (Holzinger, 2016). A grand goal of future medicine is in modeling the
complexity of patients to tailor medical decisions, health practices and therapies to the individual patient (Holzinger, 2014).
This poses challenges particularly in the integration, fusion and mapping of various distributed and heterogeneous data up to
the visual analysis of these heterogeneous data (Turkay, Jeanquartier, Holzinger, & Hauser, 2014). Consequently, explain-
able-Al in the context of medicine must take into account that diverse data may contribute to a relevant result. This requires
that medical professionals must have a possibility to understand how and why a machine decision has been made (Holzinger,
Biemann, Pattichis, & Kell, 2017).

Explainability is at least as old as AI itself and rather a problem that has been caused by it. In the pioneering days of AI
(Newell, Shaw, & Simon, 1958), reasoning methods were logical and symbolic. These approaches were successful, but only
in a very limited domain space and with extremely limited practical applicability. A typical example is MYCIN (Shortliffe &
Buchanan, 1975), which was an expert system developed in Lisp to identify bacteria causing severe infections and to recom-
mend antibiotics. MYCIN was never used in clinical routine, maybe because of its stand-alone character and the high effort in
maintaining its knowledge base. However, these early AI systems reasoned by performing some form of logical inference on
human readable symbols, and were able to provide a trace of their inference steps. This was the basis for explanation, and
there is some early related work available, for example, (Johnson, 1994; Lacave & Diez, 2002; Swartout, Paris, & Moore,
1991). Here, we should mention that there are three types of explanations: (1) a peer-to-peer explanation as it is carried out
among physicians during medical reporting; (2) an educational explanation as it is carried out between teachers and students;
(3) A scientific explanation in the strict sense of science theory (Popper, 1935). We emphasize that in this article we mean the
first type of explanation.

In medicine there is growing demand for AI approaches, which are not only performing well, but are trustworthy, transpar-
ent, interpretable and explainable for a human expert; in medicine, for example, sentences of natural language (Hudec, Bed-
nrov, & Holzinger, 2018). Methods and models are necessary to reenact the machine decision-making process, to reproduce
and to comprehend both the learning and knowledge extraction process. This is important, because for decision support it is
necessary to understand the causality of learned representations (Gershman, Horvitz, & Tenenbaum, 2015; Pearl, 2009;
Peters, Janzing, & Schélkopf, 2017).

Moreover, explainability of AI could help to enhance trust of medical professionals in future AI systems. Research
towards building explainable-AI systems for application in medicine requires to maintain a high level of learning performance
for a range of ML and human-computer interaction techniques. There is an inherent tension between ML performance (predic-
tive accuracy) and explainability. Often the best-performing methods such as DL are the least transparent, and the ones provid-
ing a clear explanation (e.g., decision trees) are less accurate (Bologna & Hayashi, 2017).

Currently, explanations of why predictions are made, or how model parameters capture underlying biological mechanisms
are elusive. A further constraint is that humans are limited to visual assessment or review of explanations for a (large) number

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
HOLZINGER - a 3 of 13
ae nes om -WILEY12#2

DATA MINING AND KNOWLEDGE DISC

of axioms. This result in one of the main question: Can we deduce properties without experiments—directly from pure obser-
vations? (Peters et al., 2017).

Understanding, interpreting, or explaining are often used synonymously in the context of explainable-AI (Doran, Schulz, &
Besold, 2017), and various techniques of interpretation have been applied in the past. There is a helpful discussion on the
“Myth of model interpretability” by Lipton (2016). In the context of explainable-AI the term “understanding” usually means a
functional understanding of the model, in contrast to a low-level algorithmic understanding of it, that is, to seek to characterize
the model's black-box behavior, without trying to elucidate its inner workings or its internal representations. Montavon,
Samek, and Miiller (2017) discriminate in their work between interpretation, which they define as a mapping of an abstract
concept into a domain that the human expert can perceive and comprehend; and explanation, which they define as a collection
of features of the interpretable domain, that have contributed to a given example to produce a decision.

We argue that in medicine explainable AI is urgently needed for many purposes including medical education, research and
clinical decision making (Holzinger, 2018). If medical professionals are complemented by sophisticated AI systems and in
some cases future AI systems even play a huge part in the decision making process, human experts must still have the
means—on demand—to understand and to retrace the machine decision process.

At the same time, it is interesting to know that while it is often assumed that humans are always able to explain their deci-
sions, this is often not the case! Sometimes experts are not able to provide an explanation based on the various heterogeneous
and vast sources of different information. Consequently, explainable-AI calls for confidence, safety, security, privacy, ethics,
fairness and trust (Kieseberg, Weippl, & Holzinger, 2016), and brings usability (Holzinger, 2005) and Human-Al Interaction
into a new and important focus (Miller, Howe, & Sonenberg, 2017). All these aspects together are crucial for applicability in
medicine generally, and for future personalized medicine, in particular (Hamburg & Collins, 2010).

First we provide some definitions to explain what kind of explainability we mean—this will lead us to the term “Causabil-
ity” in contrast to the well-known term “Causality”; then we discuss briefly the state-of-the-art of some current explainable
models, and continue with an example and a medical use-case from histopathology. We conclude with pointing to the urgent
need of a systems causability scale to measure the quality of an explanation (Hoffman, Mueller, Klein, & Litman, 2018),
which must also include social aspects of human communication (Miller, 2019).

2 | FROM EXPLAINABILITY TO CAUSABILITY

In an ideal world both human and machine statements would be identical, and congruent with the ground truth, which is
defined for machines and humans equally. However, in the real world we face two problems:

(i) Ground truth cannot always be well defined, especially when making a medical diagnosis.

(ii) Human (scientific) models are often based on causality as an ultimate aim for understanding underlying mechanisms,
and while correlation is accepted as a basis for decisions, it is viewed as an intermediate step. In contrast today's successful
ML algorithms are typically based on probabilistic models and provide only a crude basis for further establishing causal
models. When discussing the explainability of a machine statement, we therefore propose to distinguish between:

 

Explainability _ in a technical sense highlights decision-relevant parts of the used representations of the algorithms and active parts in the algorithmic model, that
either contribute to the model accuracy on the training set, or to a specific prediction for one particular observation. It does not refer to an
explicit human model.

Causability as the extent to which an explanation of a statement to a human expert achieves a specified level of causal understanding with effectiveness,
efficiency and satisfaction in a specified context of use.

 

As causability is measured in terms of effectiveness, efficiency, satisfaction related to causal understanding and its trans-
parency for a user, it refers to a human understandable model. This is always possible for an explanation of a human state-
ment, as the explanation is per se defined related to a human model. However, to measure the causability of an explanation of
a machine statement this has to be based on a causal model, which is not the case for most ML algorithms, or a mapping
between both has to be defined.

Here, we must distinguish between an explainable model (“explainable AT’) and an explanation interface which makes the
results gained in the explainable model not only usable but also useful to the expert. As a measure for the usability of such an
Human-AI interaction interface we propose to use the term causability (see Figure 1).

The term AI itself is actually an unfortunate one for engineering, since the phenomenon of intelligence is very difficult to
define and is dependent on a wealth of different factors; Therefore, we limit ourselves here only to explicitly relevant facts for
explainability.

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
 

4 of 13 WI LEY-_ G9 WIREs HOLZINGER Er at.

DATA MINING AND KNOWLEDGE DISCOVERY

 

Why did the algorithm do that? «a4 r
Can | trust these results? o vate oer yo, xf see
< ea <—
aA

How can | correct an error? =aV,a.

 

 

 

Input data

A possible solution

 

2 ad ‘=

Explanation [i] Explainable , , , rle< See

€ interface Model aE ai) (AZ, X J} ar ro
A A =a ot

 

 

 

 

 

Input data

——

 

 

 

The domain expert can understand why...
The domain expert can learn and correct errors...
The domain expert can re-enact on demand...

FIGURE 1 The best performing statistical approaches today are black-boxes and do not foster understanding, trust and error correction (above). This implies
an urgent need not only for explainable models, but also for explanation interfaces—and as a measure for the human-Al interaction we need the concept of
causability—analogous to usability in classic human-computer interaction

Understanding is not only recognizing, perceiving and reproducing (stimulus—response on a physiological level), and not only
the content comprehension and mere representation of facts, but the intellectual understanding of the context in which these facts
appear. Rather, understanding can be seen as a bridge between perceiving and reasoning. From capturing the context, without
doubt an important indicator of intelligence, the current state-of-the-art AJ is still many miles away. On the other hand, people are
very well able to instantaneously capture the context and make very good generalizations from very few data points.

Explaining (Interpretation) means to provide causes of observed phenomena in a comprehensible manner through a lin-
guistic description of its logical and causal relationships. In the theory of science, according to the hypothetical-deductive
model of Karl Popper, causal explanations are the foundation of science in order to derive facts from laws and conditions in a
deductive way. Consequently, causality and causal reasoning is an extremely important area for explainable AI (Pearl & Mac-
kenzie, 2018). Understanding and explaining are prerequisites for retraceability. The question remains open: ““What is princi-
pally understandable for a human?”.

Directly understandable, hence explainable for humans are data, objects or any graphical representations <R?, for example,
images (arrays of pixels, glyphs, correlation functions, graphs, 2D/3D projections etc., or text (sequences of natural language).
Humans are able to perceive data as images or words and process it as information in a physiological sense, cognitively inter-
pret the extracted information with reference to their subjective previous knowledge (humans have a lot of prior knowledge)
and integrating this new knowledge into their own cognitive knowledge space. Strictly speaking, there must be made a distinc-
tion between understanding natural images (pictures), understanding text (symbols) and understanding spoken language.

Not directly understandable, thus not explainable for humans are abstract vectorspaces >R(e.g., word-embeddings) or
undocumented, that is, previously unknown input features (e.g., sequences of text with unknown symbols (e.g., Chinese for
an English speaker). An example shall illustrate it: in the so-called word embedding (Mikolov, Chen, Corrado, & Dean,
2013), words and/or phrases are assigned to vectors. Conceptually, this is a mathematical embedding of a space with one
dimension per word into a continuous vector space with a reduced dimension. Methods to generate such a “mapping” include,
for example, deep neural nets and probabilistic models with an explicit representation in relation to the context in which the
words appear.

For more details on the theory behind scientific explainability we refer to the principles of abductive reasoning (Ma et al.,
2010) and point to some current work (Babiker & Goebel, 2017; Goebel et al., 2018).

3 | GENERAL APPROACHES OF EXPLAINABLE AI MODELS

We can distinguish two types of explainable AI, which can be denominated with Latin names used in law (Fellmeth & Hor-
witz, 2009): posthoc explainability = “(lat.) after this”, occurring after the event in question; for example, explaining what the
model predicts in terms of what is readily interpretable; ante-hoc explainability = “(lat.) before this”, occurring before the

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
6 5 of 13
HOLZINGER er AL GD WIREs WI LEY_|2*8

DATA MINING AND KNOWLEDGE DISCOVERY —

event in question; for example, incorporating explainability directly into the structure of an Al-model, explainability by
design.

Posthoc systems aim to provide local explanations for a specific decision and make it reproducible on demand (instead of
explaining the whole systems behavior). A representative example is local interpretable model-agnostic explanations (LIME)
developed by Ribeiro, Singh, and Guestrin (2016b), which is a model-agnostic system, where x € R? is the original represen-
tation of an instance being explained, and x € R? is used to denote a vector for its interpretable representation (e.g., x may be
a feature vector containing word embeddings, with x being the bag of words). The goal is to identify an interpretable model
over the interpretable representation that is locally faithful to the classifier. The explanation model is g : R? —R, g eG,
where G is a class of potentially interpretable models, such as linear models, decision trees, or rule lists; given a model g € G,
it can be visualized as an explanation to the human expert (for details please refer to (Ribeiro, Singh, & Guestrin, 2016a)).
Another example for a posthoc system is black box explanations through transparent approximations (BETA), a model-
agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the
original model and interpretability of the explanation introduced by Lakkaraju, Kamar, Caruana, and Leskovec (2017).

Bach et al. (2015) presented a general solution to the problem of understanding classification decisions by pixel-wise
decomposition of nonlinear classifiers which allows visualization of the contributions of single pixels to predictions for
kernel-based classifiers over bag of words features and for multilayered neural networks.

Ante-hoc systems are interpretable by design towards glass-box approaches (Holzinger et al., 2017, 2018); typical exam-
ples include linear regression, decision trees and fuzzy inference systems. The latter have a long tradition and can be designed
from expert knowledge or from data and provides—from the viewpoint of Human-Al interaction—a good framework for the
interaction between human expert knowledge and hidden knowledge in the data (Guillaume, 2001). A further example was
presented by Caruana et al. (2015), where high-performance generalized additive models with pairwise interactions (GAMs)
were applied to problems from the medical domain yielding intelligible models, which uncovered surprising patterns in the
data that previously had prevented complex learned models from being fielded in this domain; of importance is that they dem-
onstrated scalability of such methods to large data sets containing hundreds of thousands of patients and thousands of attri-
butes while remaining intelligible and providing accuracy comparable to the best (unintelligible) ML methods. A further
example for ante-hoc methods can be seen in Poulin et al. (2006), where they described a framework for visually explaining
the decisions of any classifier that is formulated as an additive model and showed how to implement this framework in the
context of three models: naive Bayes, linear support vector machines and logistic regression, which they implemented success-
fully into a bioinformatics application (Szafron et al., 2004).

3.1 | Example: interpreting a deep neural network

Deep neural networks (DNN), particularly convolutional neural networks (CNN) and recurrent neural networks (RNN) have
been demonstrated to be applicable to a wide range of practical problems, from image recognition (Simonyan & Zisserman,
2014) and image classification (Esteva et al., 2017) to movement recognition (Singh et al., 2017). At the same time these
approaches are also remarkable from a scientific point of view, since they reflect human processes. For instance, humans orga-
nize their ideas hierarchically (Bengio, 2009; Schmidhuber, 2015), and recent work has observed evidence about how learned
models in CNNs are similar to those found in the human visual ventral pathway (Khaligh-Razavi & Kriegeskorte, 2014).
Since the early phases of research on artificial neural networks, people have tried to make them explainable. One of the early
approaches was the approach of gradients in the form of sensitivity analysis (Simonyan & Zisserman, 2014).

An artificial neural network (NN) is a collection of neurons organized in a sequence of multiple layers, where neurons
receive as input the neuron activations from the previous layer, and perform a simple computation (e.g., a weighted sum of the
input followed by a nonlinear activation). The neurons of the network jointly implement a complex nonlinear mapping from
the input to the output. This mapping is learned from the data by adapting the weights of each individual neuron using back-
propagation, which repeatedly adjusts the weights of the connections in the network in order to minimize the difference
between the current output vector and the desired output vector. As a result of the weight adjustments, internal hidden units
which are not part of the input or output come to represent important features of the task domain, and the regularities in the
task are captured by the interactions of these units (refer to the original paper of Rumelhart, Hinton, and Williams (1986) and
the review by Widrow and Lehr (1990) for an overview).

Typically, deep neural networks are trained using supervised learning on large and carefully annotated data sets. However,
the need for such data sets restricts the space of problems that can be addressed. On one hand, this has led to a proliferation of
deep learning results on the same tasks using the same well-known data sets (Rolnick, Veit, Belongie, & Shavit, 2017). On
the other hand, to the emerging relevance of weakly- and un-supervised approaches that aim at reducing the need for annota-
tions (Schlegl, Seebéck, Waldstein, Schmidt-Erfurth, & Langs, 2017; Seebéck et al., 2018).

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
6 of 13 WI LEY-_ G9 WIREs HOLZINGER Er at.

DATA MINING AND KNOWLEDGE DISCOVERY

Several approaches to probe and interpret deep neural networks exist (Kendall & Gal, 2017). Uncertainty provides a mea-
sure of how small perturbations of training data would change model parameters, the so-called model uncertainty or epistemic
uncertainty, or how input parameter changes would affect the prediction for one particular example, the predictive uncertainty,
or aleatoric variability (Gal, 2016). In a Bayesian Deep Learning approach, Pawlowski, Brock, Lee, Rajchl, and Glocker
(2017) approximate model parameters through variational methods, resulting in uncertainty information of model weights,
and a means to derive predictive uncertainty from the model outputs. Providing uncertainty facilitates the appropriate use of
model predictions in scenarios where different sources of information are combined as typically the case in medicine. We can
further differentiate aleatoric uncertainty, into homoscedatic uncertainty independent of a particular input, and heteroscedatic
uncertainty possibly changing with different inputs to the system.

Methods for attribution seek to link a particular output of the deep neural network to input variables. Sundararajan, Taly,
and Yan (2017) analyze the gradients of the output when changing individual input variables. In a sense this traces the predic-
tion uncertainty back to the components of a multivariate input. Zhou, Khosla, Lapedriza, Oliva, and Torralba (2016) use acti-
vation maps to identify parts of images relevant for a network prediction. Recently attribution approaches for generative
models have been introduced. Baumgartner, Koch, Tezcan, Ang, and Konukoglu (2017) demonstrate how image areas that
are specific to the foreground class in Wasserstein Generative Adversarial Networks (WGAN) can be identified and high-
lighted in the data. Biffi et al. (2018) learn interpretable features for variational auto encoders (VAE) by learning gradients in
the latent embedding space that it linked to the classification result.

Activation maximization (Montavon et al., 2017) identifies input patterns that lead to maximal activations relating to spe-
cific classes in the output layer (Berkes & Wiskott, 2006; Simonyan & Zisserman, 2014). This makes the visualization of pro-
totypes of classes possible, and assesses which properties the model captures for classes! (Erhan, Bengio, Courville, &
Vincent, 2009). For a neural network classifier mapping data points x to a set of classes (@,),, the approach identifies highly
probable regions in the input space, that create high output probabilities for a particular class. These positions can be found by
introducing a data density model in the standard objective function logp(@, 1x) — Al|x||? that is maximized during model
training. Instead of the @>-norm regularizer that implements a preference for inputs that are close to the origin, the density
model or “expert” (Montavon et al., 2017) results in the term logp(w,| x) + logp(x) that is to be maximized. Here, the proto-
type is encouraged to simultaneously produce strong class response and to resemble the data. By application of Bayes' rule,
the newly defined objective can be identified, up to modeling errors and a constant term, as the class-conditioned data density
p(xl w,). The learned prototype thus corresponds to the most likely input x for the class w, (Figure 2).

A possible choice for the expert is a Gaussian restricted Boltzmann machine (RBM). The RBM is a two-layer, bipartite,
undirected graphical model with a set of binary hidden units p(/), a set of (binary or real-valued) visible units p(v), with sym-
metric connections between the two layers represented by a weight matrix W. The probabilistic semantics for an RBM is
defined by its energy function (for details see the chapter by Hinton (2012). Its probability function can be written as:

log p(x) = Difj(x) - sx'Lolx + cst, where f(x) = log (1 + exp (w)x + bj)) are factors with parameters learned from the
i

data. When interpreting more complex concepts such as natural images classes, other density models such as convolutional
RBM's (Lee, Grosse, Ranganath, & Ng, 2009) or pixel RNN's (Oord, Kalchbrenner, & Kavukcuoglu, 2016) are suitable.

The selection of the so-called expert p(x) plays an important role. Basically, there are four different cases: In the case
where “the expert” is absent, that is, the optimization problem reduces to the maximization of the class probability function p

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

mz
—- ou | ig —_.
om
Input pattern Prediction Attribution Classification model Prediction
oz a
ee ——— 0
ons s
Input pattern Prediction uncertainty Generative model Attribution
zz
—— e
@
Input pattern Model uncertain ®
put pi ty :
@
e
e
—

 

 

 

Prototypes maximizing activation

FIGURE2 An overview of how deep learning models can be probed for information regarding uncertainty, attribution, and prototypes

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
6 Tof 13
HOLZINGER er AL GD WIREs WI LEY_7"8

DATA MINING AND KNOWLEDGE DISCOVERY —

(@,| x). In the case where we see the other extreme, that is, the expert is overfitted on some data distribution, and thus, the
optimization problem becomes essentially the maximization of the expert p(x) itself.

When using activation maximization for the purpose of model validation, an overfitted expert must be especially avoided,
as the latter could hide interesting failure modes of the model p(@,l x). A slightly underfitted expert (case b), for example, that
simply favors images with natural colors, can already be sufficient. On the other hand, when using AM to gain knowledge on
a correctly predicted concept @,, the focus should be to prevent underfitting. Indeed, an underfitted expert would expose
optima of p(a,| x) potentially distant from the data, and therefore, the prototype x* would not be truly representative of @,.

Unsupervised learning and generative models. In certain applications, it is helpful to not only predict based on input data,
but learn the structure of a set of training examples, to either provide a parametric representation of its density p(x), or at least
be able to sample from this density generating examples of the same type as the training examples. Examples are Boltzmann
machines, autoencoders, or generative adversarial networks) which do not provide the density function directly, but are able
to sample from it, usually via the following two steps:

1. Sample from a simple distribution g(z) ~ N(0,/) which is defined in an abstract code space Z;
2. Apply to the sample a decoding function g : Z — X, that maps it back to the original input domain.

There are two aspects of models learned by unsupervised learning that are relevant in the context of explainability. First,
the latent representations learned in these models can hold structure that reflects relatively complex relationship patterns in the
data. For instance, in Mikolov et al. (2013) the authors show that word embeddings can reflect semantic similarity. Second,
being able to generate instances, or even instances that are as close as possible to an observation, provides means to study the
difference of examples to a class. This is relevant in medicine, where the discovery and study of anomalies that are potentially
linked to disease is relevant Schlegl et al. (2017).

One example is the generative adversarial network (GAN) introduced by Goodfellow et al. (2014). It consists of two
models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability
that a sample came from the training data rather than from G. The training procedure for G is to maximize the probability of
D making an error—which works like a minimax (minimizing a possible loss for a worst case maximum loss) two-player
game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution
and D equal to 5 everywhere; in the case where G and D are defined by multilayer perceptrons, the entire system can be
trained with backpropagation.

To learn the generator's distribution p, over data x, a prior must be defined on the input noise variables p,(z), and then a
mapping to the data space as G(z; 0,), where G is a differentiable function represented by a multilayer perceptron with param-
eters #,. The second multilayer perceptron D(x; 6,) outputs a single scalar. D(x) represents the probability that x came from
the data rather than p,. D can be trained to maximize the probability of assigning the correct label to both training examples
and samples from G. Simultaneously G can be trained to minimize log(1 — D(G(z))); in other words, D and G play the follow-
ing two-player minimax game with value function V(G, D):

min max V(D, G) = Exxp,,,(x) [log D(x)] + Bx~p,q)[ log (1 -— D(G)))).- (1)

Nguyen, Dosovitskiy, Yosinski, Brox, and Clune (2016) proposed building a prototype for w, by incorporating such a
generative model in the activation maximization framework. The optimization problem is redefined as:

max log p(«-|g(z)) — Allzll’, (2)
ZEL

where the first term is a composition of the newly introduced decoder and the original classifier, and where the second term is
an @>-norm regularizer in the code space. Once a solution z* to the optimization problem is found, the prototype for a, is
obtained by decoding the solution, that is, x* = g(z*).

The @-norm regularizer in the input space can be understood in the context of image data as favoring gray-looking
images. The effect of the 7-norm regularizer in the code space can instead be understood as encouraging codes that have high
probability. High probability codes do not necessarily map to high density regions of the input space; for more details refer to
the excellent tutorial given by Montavon et al. (2017).

3.2 | Example use-case histopathology

This section demonstrates the complexity of explanations of a human pathologist. For the following diagnosis

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
8 of 13 WI LEY-_ G9 WIREs HOLZINGER Er at.

DATA MINING AND KNOWLEDGE DISCOVERY

 

Kupffer cell Erythrocyte
Sinusoid Hepatocyte
endothelial cell nucleus nucleus

Hepatocyte
large lipid droplet

FIGURE 3 Features in a histology slide annotated by a human expert pathologist

Steatohepatitis with mild portal and incomplete-septal fibrosis and mild centrilobular fibrosis of chicken wire
type. The morphological picture corresponds to alcoholic steatohepatitis. History of alcohol abuse?

3.2.1 | Example for human posthoc explanation

We asked an experienced pathologist to explain what he considered relevant in the histology slides. A very small portion of
the histological sections are shown in Figure 3 as illustration. For this specific diagnosis the pathologist gave the following
facts as posthoc explanation:

Liver biopsy with 10 evaluable portal fields.

Lobule architecture preserved.

Liver cells arranged in regular plates one cell layer thick.

Portal fields slightly widened and slightly fibrotic.

Isolated incomplete porto-portal and porto-central septa.

Portal fields slightly inflamed with mixed-cell (lymphocytes, sporadic neutrophil granulocytes) inflammatory inflitrates.

Inflammation restricted to portal field.
Parenchymatous border plate intact, liver cells with low anisocaryosis, moderately large droplet fatty liver (estimated

parenchyma fatty degeneration at 30% of parenchymal area).

Lobular central hepatic cells balloonized, light cytoplasmic with incorporation of Mallory-Denk bodies.

Most of these liver cells are surrounded by neutrophil granulocytes and some of them are interspersed (satelliteosis).
Minor perivenular fibrosis (=central sclerosis).

Kupffer cells slightly diffusely increased, isolated Kupffer cell nodules detectable.

In the Berliner blue stain minor parenchymatous and Kupffer cell siderosis.

3.2.2. | Example for the ante-hoc structure of explanations

We also asked the pathologist to explain the process and most relevant concepts in liver pathology. Please note, that the fol-
lowing description just demonstrates the structure and complexity and is far away from a textbook on liver pathology. The
pathologist described the following procedure and observation as an ante-hoc explanation for liver pathology:

e Describe in the macroscopic evaluation of the histological section the following features:
—type, number and size of specimens (surgical specimen or biopsy)
—tissue cohesion of biopsies
—staining quality (H & E/CAB/Sirius red/iron/ev. PAS, immunohsistochemistry)
—already visible exogenous tissue (tumor)
e Describe in microscopic evaluation at low magnification the following features:
—lobular architecture preserved/disturbed in the sense of fibrosis or necrosis/destroyed in the context of cirrhosis or tumors
—number of assessable portal fields (optimal 10-15 per slide)
—liver cell (hepatocyte) plates regular-one cell layer thick/several cell layers thick

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
6 9 of 13
HOLZINGER er AL GD WIREs WI LEY_|2*8

DATA MINING AND KNOWLEDGE DISCOVERY —

—inflammatory changes portal/lobular/combined; necrosis lobular peripheral/lobular central)
—presence or absence of tissue
e Describe in microscopic evaluation at higher magnification the following features:

—portal tracts: regular/extended/fibrotic/rounded/edematous

—connective tissue parenchyma border: sharp/unsharp

—parenchymatous border plate: preserved/partially destroyed/mostly destroyed/nonexistent inflammatory infiltrates portal/-
periportal-interface, sparse/tight/localized-follicular/ lymphocytic/ lymphohistiocytic/neutrophil-granulocytic/ stressed-eosino-
phil-granulocytic/granulomatous;

—abnormal content of the portal field not present (tumor cells/foreign bodies/parasites)

—portal vessels (arteria hepatica, vena portae, and lymphatic vessels) present/expanded/narrowed/inflammatory;

—Bile ducts: present/elongated/absent/single-layer epithelium/multilayer epithelium/polymorphic epithelium/inflammatory
changes/partially destructed/scarred/content (bile thrombus/porphyrinthrombus);

—ductal reaction absent/low/pronounced /ductal cholestasis.

—lobules (lobulus, liver parenchyma): Liver cells large/balloonized/small-atrophic/anisocytosis/apoptosis

—cytoplasm: granular/net-like/light cytoplasmic-glycogen-rich/diffuse homogenized/focally homogenized

—cytoplasmic inclusions fat large droplet/fat small droplet/ lipofuscin granules/siderin granules/AAT inclusions/Fibrinogen
inclusions/Mallory Denk bodies (MDB), Hyaline bodies/bilirubin

—canalicular bilirubinostasis

—Necroses disseminated/confluent/lobular central/lobular periphery/bridging porto-central/bridging centro-central/massive;
—liver cell nuclei anisocaryosis/pycnosis/punch cores/“sand cores“/core inclusions;

—Kupffer cells focally increased (nodular)/diffus increased/enlarged/inclusions (siderin, pigment, erythrocytes, pathogen,
foreign material);

—star cells (stellate cells) increased

—sinusoidal dilated/abnormal content (e.g., blood, fibrin, and tumor cells)

—central vein lumen open/narrowed/ obliterated/inflamed/wall fibrosis.

-fibrosis: portal/perisinusoidal/pericellular/perivenular/septal/porto-portal/porto-central/centro-central/meshed wire fibrosi-
s/incomplete cirrhosis/cirrhosis.

—foreign tissue (tumor tissue to be characterized morphologically, primary/secondary-metastatic/unclear).

For a specific case values of all above features contribute to the diagnosis with different weights and causal relations pre-
sent in the human model on liver pathology, which an expert acquired by training and experience.

4 | FUTURE OUTLOOK

4.1 | Weakly supervised learning

Supervised learning is very expensive in the medical domain because it is cumbersome to get strong supervision information and
fully ground-truth labels. Particularly, labeling a histopathological image is not only time-consuming but also a critical task for
cancer diagnosis, as it is clinically important to segment the cancer tissues and cluster them into various classes (Xu, Zhu, Chang,
Lai, & Tu, 2014). Digital pathological images generally have some issues to be considered, including the very large image size
(and the involved problems for DL), insufficiently labeled images (the small training data available), the time needed from the
pathologist (expensive labeling), insufficient labels (region of interest), different levels of magnification (resulting in different
levels of information), color variation and artifacts (sliced and placed on glass slides) etc. (Komura & Ishikawa, 2018).

Weakly supervised learning (Xu et al., 2014) is an umbrella term for a variety of methods to construct predictive models
by learning with weak supervision; weak because of either incomplete, inexact or inaccurate supervision. In a strong supervi-
sion task we want to learn f: X — Y from the training data set D = (xy, y1), --- m; Ym), Wherein X is the feature space and
(x;, y) are always assumed to be identically and independently distributed data (which is not the case in real-world problems!).

In the context of weakly supervised learning, we propose classifying whole slide images according to widely used scoring
systems based on association with histomorphological characteristics and an overall predictive score, and to provide in addi-
tion a relevance map generated by observing the human expert during diagnosis making. By the combination of well-known
human features and new multiscale morphological classifiers the human causal model can be on the one hand extended and
on the other hand the CNN model can be explained with known histomorphological features. We propose to extract from both
benign and malign single cell nuclei and to classify chromatin organization (Araujo et al., 2017) within the nuclei to correlate
these to histopathological features and molecular markers.

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
10 of 13 WI LEY-_ G9 WIREs HOLZINGER Er at.

DATA MINING AND KNOWLEDGE DISCOVERY

4.2 | Structural causal models

A very important direction is research towards structural causal models (Pearl, 2009; Pearl & Mackenzie, 2018; Peters et al.,
2017). Current AI work on either a statistical or model-free mode. This entails severe limits on both effectiveness and perfor-
mance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve for strong AI (Pearl &
Mackenzie, 2018). To achieve human level intelligence, AI need the guidance of a model of reality, similar to the ones used
in causal inference tasks. Consequently, we propose to: (1) develop new visualization techniques that can be trained by medi-
cal experts, as they can explore the underlying explanatory factors of the data and (2) formalize a structural causal model of
human decision making and mapping features in these to DL approaches. In digital pathology such mechanistic models can be
used to analyze and predict the response of a functional network behavior to features in histology slides, molecular data and
family history.

4.3 | Develop causability as a new scientific field

The human-computer interaction community has established a range of usability methods (Holzinger, 2005). Similar to these
usability methodologies, methods and tests, we need the development of causability methodologies, methods and tests, which
are based on clear scientific principles and theories of causality in order to establish causability as a scientific field which will
become necessary with increased use of AI. The same as usability measures ensures the “quality of use” (Bevan, 1995), causa-
bility measures must ensure the “quality of explanations”.

According to the three Layer Causal Hierarchy by Pearl (2018):

Level 1: Association P(yl x) with the typical activity of “seeing” and questions including “How would seeing X change
my belief in Y?”, in our use-case above this was the question of “what does a feature in a histology slide the pathologist about
a disease?”

Level 2: Intervention P(yl do(x), z) with the typical activity of “doing” and questions including “What if I do X?”, in our
use-case above this was the question of “what if the medical professional recommends treatment X—will the patient be
cured?”

Level 3: Counterfactuals P(y, Xx, y) with the typical activity of “retrospection” and questions including “Was Y the cause
for X?”, in our use-case above this was the question of “was it the treatment that cured the patient?”

For each of these levels we have to develop methods to measure effectiveness (does an explanation describe a statement
with an adequate level of detail), efficiency (is this done with a minimum of time and effort) and user satisfaction (how satis-
factory was the explanation for the decision making process). Again we should mention that there are three types of explana-
tions: (1) a peer-to-peer explanation as it is carried out among physicians during medical reporting; (2) an educational
explanation as it is carried out between teachers and students; (3) A scientific explanation in the strict sense of science theory
(Popper, 1935). We emphasize that in this article we always refer to the first type of explanation.

5 | CONCLUSION

Al is already one of the key technologies in our economy. It will bring changes similar to the introduction of the steam engine
or electricity. However, concerns about potential loss of control in the Human-AI relationship are growing. Issues such as
autonomous driving and the unclear decision making of the vehicle, for example, in extreme cases shortly before an accident
collision, have long been the subject of public debate. The same goes for the question of the extent to which AI can or should
support medical decisions or even make them itself. In many cases it will be necessary to understand how a machine decision
was made and to assess the quality of the explanation.

While rule-based solutions of the early AI in the 1950s represented comprehensible “glass box” approaches, their weak-
ness lay in dealing with uncertainties of the real world. Many problems from our everyday lives cannot be represented by for-
mal, mathematical rules of logic. The failure of such algorithms to solve problems that are relatively simple for humans, such
as natural language, recognizing faces, or understanding a joke, ultimately led to the “AI winter” in the 1980s. Only through
the triumph of probabilistic and statistical learning methods in connection with the success of artificial neural networks (“deep
learning”) did AI applications become increasingly successful.

Today, DL algorithms are very useful in our daily lives: autonomous driving, face recognition, speech understanding, rec-
ommendation systems, etc. already work very well. However, it is very difficult for people to understand how these algorithms
come to a decision. Ultimately, these are so-called “black box” models. The problem is that even if we understand the underly-
ing mathematical principles and theories, such models lack an explicit declarative representation of knowledge. Early AI solu-
tions (at that time called expert systems) had the goal from the beginning of making solutions comprehensible, understandable

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
HOLZINGER - a 11 of 13
ae nes oe -WILEYL22

DATA MINING AND KNOWLEDGE DISC

and thus explainable, which was also possible in very narrowly defined problems. Of course, we should mention that many
problems do possibly not need explanations for everything at any time.

Here, the area of explainable AI is not only useful and necessary, but also represents a huge opportunity for AI solutions
in general. The generally accused opacity of AI can thus be reduced and necessary trust built up. Exactly this can promote the
acceptance with future users lastingly.

The main problem of the most successful current ML systems, recently emphasized by Pearl (2018), is that they work on a
statistical, or model-free mode, which entails severe limitations on their performance. Such systems are not able to understand
the context, hence cannot reason about interventions and retrospection. However, such approaches needs the guidance of a
human model similar to the ones used in causality research (Pearl, 2009; Pearl & Mackenzie, 2018) to answer the question
“Why?”. The establishment of causability as a solid scientific field can help here.

“Data can tell you that the people who took a medicine recovered faster than those who did not take it, but they
cant tell you why. Maybe those who took the medicine did so because they could afford it and would have recov-
ered just as fast without it.”

Judea Pearl (2018), The Book of Why: The New Science of Cause and Effect

ACKNOWLEDGMENTS

We gratefully acknowledge the support of our industrial partner Kapsch, the Biobank Graz, the BBMRL.at team, the EU fea-
tureCloud project and the critical review from our colleagues at the Medical University Graz. Last but not least we want to
thank the anonymous reviewers for their critics and useful comments.

CONFLICT OF INTEREST

The authors have declared no conflicts of interest for this article.

ENDNOTE

‘Presented as a poster during the ICML 2009 workshop on Learning Feature Hierarchies, http://www.cs.toronto.edu/ rsala-
khu/deeplearning/program.html.

ORCID
Andreas Holzinger ® https://orcid.org/0000-0002-67 86-5194

REFERENCES

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., & Devin, M. (2016). Tensorflow: Large-scale machine learning
on heterogeneous distributed systems. arXiv: 1603.04467.

Araujo, T., Aresta, G., Castro, E., Rouco, J., Aguiar, P., Eloy, C., ... Campilho, A. (2017). Classification of breast cancer histology images using convolutional neural
networks. PLoS One, 12, e0177544.

Babiker, H. K. B., & Goebel, R. (2017). An introduction to deep visual explanation. arXiv:1711.09482.

Bach, S., Binder, A., Montavon, G., Klauschen, F., Miiller, K.-R., & Samek, W. (2015). On pixel-wise explanations for non-linear classifier decisions by layer-wise rel-
evance propagation. PLoS One, 10, e0130140.

Baumgartner, C. F., Koch, L. M., Tezcan, K. C., Ang, J. X., & Konukoglu, E. (2017). Visual feature attribution using wasserstein gans. Paper presented at Proceedings
of the IEEE computer society conference on computer vision and pattern recognition.

Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2, 1-127.

Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. [EEE Transactions on Pattern Analysis and Machine Intelli-
gence, 35, 1798-1828.

Berkes, P., & Wiskott, L. (2006). On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields. Neural Computation, 18, 1868-1895.

Bevan, N. (1995). Measuring usability as quality of use. Software Quality Journal, 4, 115-130.

Biffi, C., Oktay, O., Tarroni, G., Bai, W., De Marvao, A., Doumou, G., ... Rueckert, D. (2018). Learning interpretable anatomical features through deep generative
models: Application to cardiac remodeling. Paper presented at International conference on medical image computing and computer-assisted intervention
(pp. 464-471). Springer.

Bologna, G., & Hayashi, Y. (2017). Characterization of symbolic rules embedded in deep dimlp networks: A challenge to transparency of deep learning. Journal of Arti-
ficial Intelligence and Soft Computing Research, 7, 265-286.

Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., & Elhadad, N. (2015). Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmis-
sion. Paper presented at 21th ACM SIGKDD international conference on knowledge discovery and data mining (KDD °15) (pp. 1721-1730). ACM.

Doran, D., Schulz, S., & Besold, T. R. (2017). What does explainable AI really mean? A new conceptualization of perspectives. arXiv: 1710.00794.

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
12 of 13 WI LEY-_ G9 WIREs HOLZINGER Er at.

DATA MINING AND KNOWLEDGE DISCOVERY

Erhan, D., Bengio, Y., Courville, A., & Vincent, P. (2009). Visualizing higher-layer features of a deep network. University of Monetreal Technical Report Nr. 1341.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural net-
works. Nature, 542, 115-118.

Fellmeth, A. X., & Horwitz, M. (2009). Guide to Latin in international law. Oxford, England: Oxford University Press.

Gal, Y. (2016). Uncertainty in deep learning. Cambridge, England: University of Cambridge.

Gershman, S.J., Horvitz, E. J., & Tenenbaum, J. B. (2015). Computational rationality: A converging paradigm for intelligence in brains, minds, and machines. Science,
349, 273-278.

Goebel, R., Chander, A., Holzinger, K., Lecue, F., Akata, Z., Stumpf, S., Kieseberg, P., & Holzinger, A. (2018). Explainable Ai: The new 42?. Paper presented at
Springer lecture notes in computer science LNCS 11015 (pp. 295-303). Springer.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... Bengio, Y. (2014). Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, & K. Q. Weinberger (Eds.), Advances in neural information processing systems (NIPS} (pp. 2672-2680). Montreal, Canada: Neural
Information Processing Systems Foundation.

Guillaume, S. (2001). Designing fuzzy inference systems from data: An interpretability-oriented review. IEEE Transactions on Fuzzy Systems, 9, 426-443.

Hamburg, M. A., & Collins, F. S. (2010). The path to personalized medicine. New England Journal of Medicine, 363, 301-304.

Hinton, G. E. (2012). A practical guide to training restricted boltzmann machines. In Neural networks: Tricks of the trade, lecture notes in computer science LNCS
(Vol. 7700, pp. 599-619). Heidelberg: Springer.

Hoffman, R. R., Mueller, S. T., Klein, G., & Litman, J. (2018). Metrics for explainable AI: Challenges and prospects. arXiv:1812.04608.

Holzinger, A. (2005). Usability engineering methods for software developers. Communications of the ACM, 48, 71-74.

Holzinger, A. (2014). Trends in interactive knowledge discovery for personalized medicine: Cognitive science meets machine learning. JEEE Intelligent Informatics
Bulletin, 15, 6-14.

Holzinger, A. (2016). Interactive machine learning for health informatics: When do we need the human-in-the-loop? Brain Informatics, 3, 119-131.

Holzinger, A. (2017). Introduction to machine learning and knowledge extraction (MAKE). Machine Learning and Knowledge Extraction, 1, 1-20 http://www.mdpi.
com/2504-4990/1/1/1

Holzinger, A (2018). From machine learning to explainable AI. Paper presented at 2018 World symposium on digital intelligence for systems and machines (DISA).

Holzinger, A., Biemann, C., Pattichis, C. S., & Kell, D. B. (2017). What do we need to build explainable AI systems for the medical domain? arXiv:1712.09923.

Holzinger, A., Dehmer, M., & Jurisica, I. (2014). Knowledge discovery and interactive data mining in bioinformatics—State-of-the-art, future challenges and research
directions. BMC Bioinformatics, 15,11.

Holzinger, A., Plass, M., Holzinger, K., Crisan, G. C., Pintea, C.-M., & Palade, V. (2017). A glass-box interactive machine learning approach for solving np-hard prob-
Jems with the human-in-the-loop. arXiv: 1708.01104.

Holzinger, A., Plass, M., Kickmeier-Rust, M., Holzinger, K., Crian, G. C., Pintea, C.-M., & Palade, V. (2018). Interactive machine learning: Experimental evidence for
the human in the algorithmic loop. Applied Intelligence, 1-14.

Hudec, M., Bednrov, E., & Holzinger, A. (2018). Augmenting statistical data dissemination by short quantified sentences of natural language. Journal of Official Statis-
tics (JOS), 34, 981-1010.

Johnson, W. L. (1994). Agents that learn to explain themselves. Paper presented at Twelfth national conference on artificial intelligence (AAAI °94) (pp. 1257-1263).
AAATL

Jordan, M. 1, & Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. Science, 349, 255-260.

Kendall, A., & Gal, Y. (2017). What uncertainties do we need in bayesian deep learning for computer vision? In Advances in neural information processing systems
(pp. 5574-5584). Long Beach, CA: Neural Information Processing Systems Foundation.

Khaligh-Razavi, S.-M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS Computational Biology,
10, e1003915.

Kieseberg, P., Weippl, E., & Holzinger, A. (2016). Trust for the doctor-in-the-loop. In European Research Consortium for Informatics and Mathematics (ERCIM} news:
Tackling big data in the life sciences (Vol. 104, pp. 32-33). Sophia Antipolis, France: European Research Consortium for Informatics and Mathematics.

Komura, D., & Ishikawa, S. (2018). Machine learning methods for histopathological image analysis. Computational and Structural Biotechnology Journal, 16, 34-42.

Lacave, C., & Diez, F. J. (2002). A review of explanation methods for Bayesian networks. The Knowledge Engineering Review, 17, 107-127.

Lake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, e253.

Lakkaraju, H., Kamar, E., Caruana, R., & Leskovec, J. (2017). Interpretable and explorable approximations of black box models. arXiv:1707.01154.

LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 527, 436-444.

Lee, H., Grosse, R., Ranganath, R., & Ng, A. Y. (2009). Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. Paper
presented at 26th Annual international conference on machine learning (CML ’09) (pp. 609-616). ACM.

Lee, S., & Holzinger, A. (2016). Knowledge discovery from complex high dimensional data. In S. Michaelis, N. Piatkowski, & M. Stolpe (Eds.), Solving large scale
learning tasks. Challenges and algorithms, lecture notes in artificial intelligence, LNAI 9580 (pp. 148-167). Heidelberg, Germany: Springer.

Lipton, Z. C. (2016). The mythos of model interpretability. arXiv: 1606.03490.

Ma, J., Broda, K., Goebel, R., Hosobe, H., Russo, A., & Satoh, K. (2010) Speculative abductive reasoning for hierarchical agent systems. Paper presented at Interna-
tional workshop on computational logic in multi-agent systems (pp. 49-64). Springer.

McCarthy, J. (1960). Programs with common sense (pp. 75-91). RLE and MIT Computation Center.

Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (1984). Machine learning: An artificial intelligence approach. Heidelberg, Germany: Springer.

Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv:1301.3781.

Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267, 1-38.

Miller, T., Howe, P., & Sonenberg, L. (2017) Explainable AI: Beware of inmates running the asylum or: How i learnt to stop worrying and love the social and beha-
vioural sciences. arXiv: 1712.00547.

Montavon, G., Samek, W., & Miiller, K.-R. (2017). Methods for interpreting and understanding deep neural networks. arXiv: 1706.07979.

Newell, A., Shaw, J. C., & Simon, H. A. (1958). Chess-playing programs and the problem of complexity. JBM Journal of Research and Development, 2, 320-335.

Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., & Clune, J. (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.
In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, & R. Garnett (Eds.), Advances in neural information processing systems 29 (NIPS 2016) (pp. 3387-3395).
Barcelona, Spain: Neural Information Processing Systems Foundation.

Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu, K. (2016). Pixel recurrent neural networks. arXiv: 1601.06759.

Pawlowski, N., Brock, A., Lee, M. C., Rajchl, M., & Glocker, B. (2017). Implicit weight uncertainty in neural networks. arXiv preprint arXiv:1711.01297.

Pearl, J. (2009). Causality: Models, reasoning, and inference (2nd ed.). Cambridge, England: Cambridge University Press.

Pearl, J. (2018). Theoretical impediments to machine learning with seven sparks from the causal revolution. arXiv: 1801.04016.

Pearl, J., & Mackenzie, D. (2018). The book of why. New York, NY: Basic Books.

Peters, J., Janzing, D., & Schélkopf, B. (2017). Elements of causal inference: Foundations and learning algorithms. Cambridge, MA: MIT-Press.

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
HOLZINGER - a 13 of 13
ae nes oe -WILEY1222

DATA MINING AND KNOWLEDGE DISC

Poole, D. L., Mackworth, A. K., & Goebel, R. (1998). Computational intelligence: A logical approach. New York, NY: Oxford University Press.

Popper, K. (1935). Die Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwissenschaft. Wien, Austria: Springer-Verlag.

Poulin, B., Eisner, R., Szafron, D., Lu, P., Greiner, R., Wishart, D. S., ... Anvik, J. (2006). Visual explanation of evidence with additive classifiers. In National confer-
ence on artificial intelligence (pp. 1822-1829). Cambridge, MA: MIT Press.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016a). Model-agnostic interpretability of machine learning. arXiv: 1606.05386.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016b). Why should I trust you?: Explaining the predictions of any classifier. Paper presented at Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144). ACM.

Rolnick, D., Veit, A., Belongie, S., & Shavit, N. (2017). Deep learning is robust to massive label noise. arXiv:1705.10694.

Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533-536.

Russell, S. J., & Norvig, P. (2010). Artificial intelligence: A modern approach (3rd ed.). Upper Saddle River: Prentice Hall.

Schlegl, T., Seebdck, P., Waldstein, S. M., Schmidt-Erfurth, U., & Langs, G. (2017). Unsupervised anomaly detection with generative adversarial networks to guide
marker discovery. In International conference on information processing in medical imaging (pp. 146-157). Heidelberg, Germany: Springer.

Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85-117.

Seebsock, P., Waldstein, S. M., Klimscha, S., Bogunovic, H., Schlegl, T., Gerendas, B. S., ... Langs, G. (2018). Unsupervised identification of disease marker candidates
in retinal oct imaging data. IEEE Transactions on Medical Imaging., 1.

Shortliffe, E. H., & Buchanan, B. G. (1975). A model of inexact reasoning in medicine. Mathematical Biosciences, 23, 351-379.

Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv: 1409.1556.

Singh, D., Merdivan, E., Psychoula, I., Kropf, J., Hanke, S., Geist, M., & Holzinger, A. (2017). Human activity recognition using recurrent neural networks. In
A. Holzinger, P. Kieseberg, A. M. Tjoa, & E. Weippl (Eds.), Machine learning and knowledge extraction: Lecture notes in computer science LNCS 10410
(pp. 267-274). Cham: Springer.

Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365.

Swartout, W., Paris, C., & Moore, J. (1991). Explanations in knowledge systems: Design for explainable expert systems. IEEE Expert, 6, 58-64.

Szafron, D., Lu, P., Greiner, R., Wishart, D. S., Poulin, B., Eisner, R., ... Fyshe, A. (2004). Proteome analyst: Custom predictions with explanations in a web-based tool
for high-throughput proteome annotations. Nucleic Acids Research, 32, W365-W371.

Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman, N. D. (2011). How to grow a mind: Statistics, structure, and abstraction. Science, 331, 1279-1285.

Ting, D. S. W., Cheung, C. Y.-L., Lim, G., Tan, G. S. W., Quang, N. D., Gan, A., ... Lee, S. ¥. (2017). Development and validation of a deep learning system for dia-
betic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. Jama, 378, 2211-2223.

Turkay, C., Jeanquartier, F., Holzinger, A., & Hauser, H. (2014). On computationally-enhanced visual analysis of heterogeneous data and its application in biomedical
informatics. In A. Holzinger & I. Jurisica (Eds.), Interactive knowledge discovery and data mining: State-of-the-art and future challenges in biomedical informatics.
Lecture notes in computer science LNCS 8401 (pp. 117-140). Heidelberg, Germany: Springer.

Widrow, B., & Lehr, M. A. (1990). 30 years of adaptive neural networks: Perceptron, madaline, and backpropagation. Proceedings of the IEEE, 78, 1415-1442.

Xu, Y., Zhu, J.-Y., Chang, E. I. C., Lai, M., & Tu, Z. (2014). Weakly supervised histopathology cancer image segmentation and classification. Medical Image Analysis,
18, 591-604.

Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. (2016). Learning deep features for discriminative localization. Paper presented at Proceedings of the
IEEE conference on computer vision and pattern recognition (2921-2929).

How to cite this article: Holzinger A, Langs G, Denk H, Zatloukal K, Miiller H. Causability and explainability of arti-
ficial intelligence in medicine. WIREs Data Mining Knowl Discov. 2019;9:e1312. https://doi.org/10.1002/widm.1312

asuaary] SUOLLLOS aANRAID o[qeoT{dde wp £q pouroaa’ a sole WO ‘asn Jo Sol Joy Kreger] OUL[UG AaTtAA UO (SUONIPUOS-pUR-sULI/UKOS Ao[LM A rEIqifoUT|UO//:sdnY) SUOTTPUCD PUE SUEY, Up 20g ‘[ZZOT/T T/L] UO KreAQrT oUTTUD Aart A “ToeUST eUEIYDO; Aq TIE T UEPLA/ZOOTOL/Loppuios Kolm Areiqrauruo'sourmyysdny Woy popeotumog “p “610Z ‘S6LPZPET
Better Metrics for Evaluating Explainable Artificial Intelligence
Blue Sky Ideas Track

Avi Rosenfeld
Department of Computer Science
Jerusalem College of Technology, Jerusalem, Israel, 91160
rosenfa@jet.ac.il

ABSTRACT

This paper presents objective metrics for how explainable artifi-
cial intelligence (XA) can be quantified. Through an overview of
current trends, we show that many explanations are generated
post-hoc and independent of the agent’s logical process, which
in turn creates explanations with limited meaning as they lack
transparency and fidelity. While user studies are a known basis for
evaluating XAI, studies that do not consider objective metrics for
evaluating XAI may have limited meaning and may suffer from
confirmation bias, particularly if they use low fidelity explanations
unnecessarily. To avoid this issue, this paper suggests a paradigm
shift in evaluating XAI that focuses on metrics that quantify the
explanation itself and its appropriateness given the XAI goal. We
suggest four such metrics based on performance differences, D, be-
tween the explanation’s logic and the agent’s actual performance,
the number of rules, R, outputted by the explanation, the number
of features, F, used to generate that explanation, and the stability,
S, of the explanation. We believe that user studies that focus on
these metrics in their evaluations are inherently more valid and
should be integrated in future XAI research.

KEYWORDS

Explainable Artificial Intelligence; Interpretable Machine Learning;
Human-Agent Systems; System Evaluation

ACM Reference Format:

Avi Rosenfeld. 2021. Better Metrics for Evaluating Explainable Artificial In-
telligence: Blue Sky Ideas Track. In Proe. of the 21th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2021), Online, May
3-7, 2021, AAMAS, 6 pages.

1 OVERVIEW

As the field of Artificial Intelligence matures and becomes ubiqui-
tous, there is a growing emergence of systems where people and
agents work together. These systems, often called Human-Agent
Systems or Human-Agent Cooperatives, have moved from theory
to reality in the many forms, including digital personal assistants,
recommendation systems, training and tutoring systems, service
robots, chatbots, planning systems, self-driving cars and medical
diagnostic systems [2-4, 6, 14, 21-25, 27, 29, 34, 36, 37, 40-42, 42-
45, 45, 47, 48, 50, 52, 53, 55, 56, 58-60, 63, 64, 66, 67, 69, 72, 73]. In this
paper we focus on how well agents in these systems explain their
logic to the people they interact with - the challenge of quantifying
the effectiveness of explainable artificial intelligence (XAI).

 

Proc. of the 21th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2021), U. Endriss, A. Nowé, F. Dignum, A. Lomuscio (eds.), May 3-7, 2021, Online.
© 2021 International Foundation for Autonomous Agents and Multiagent Systems
(www.ifaamas.org). All rights reserved.

Please consider the following scenario to better motivate sur-
rounding challenges in quantifying XAIL The XYZ company has just
developed a new agent to automate the analysis of medical imaging
to diagnose a deadly disease such as cancer. Currently, radiolo-
gists are 97% successful in finding the disease using state-of-the-art
imaging techniques, but the agent is accurate with 99.5% accuracy.
However, both the agent and the human experts miss different
types of cancer. As such, 0.5% of the cancers that are missed by the
agent are found by the experts, but the agent is overall significantly
better than experts at finding cancer. One would ideally hope that
the agent and human work in tandem and thus experts will find the
cancers the agent missed and the agent will inform the experts of
cancers they didn’t find to create an 100% overall accuracy. Unfor-
tunately, XYZ’s agent provides no explanation and they have also
noted that the experts that use their system often trust it when they
shouldn’t. As a result they have formulated the following questions:
Would the experts learn how to find cancer better had the system
explained its logic better? Given a set of explanations, how can
XYZ quantify the effectiveness of each explanation and identify
the best one? Is the agent safe and should it be trusted? How can
XYZ quantify bias if the agent mistakes only one ethnic minority’s
cancer? Should society hold XYZ legally liable for this agent bias
or might it be the expert’s responsibility to avoid this problem?

The goal of this paper is to highlight how ambiguities in XAI
definition and goals impact how agent designers quantify XAIL
We argue that many current XAI methods are based on a wrong
assumption that agents must maximize their performance using
certain machine learning techniques even if they are not readily and
fully understood by the intended user. This disconnect highlights a
potentially poor fit between the motivation for why explanations
are needed and how those algorithms are currently being evalu-
ated by XAI researchers. As we will see in the following sections,
many such explanations are not capable of instilling trust in the
system [55] and others even hurt the user’s ability to understand
the agent’s decisions [38]. Also, user studies to date typically mea-
sure XAI based on the user’s performance and how it’s impacted
by explanations [12, 22, 37, 38, 51, 65]. However, not only are user
studies relatively hard to run, they may be of limited value [38]
and may suffer from confirmation bias [68]. Instead, we present
four objective XAI measures to quantify XAI effectiveness either
on their own or in conjunction with user studies:

e D, the performance difference between the agent’s model
and the performance of the logic presented as an explanation
R, the number of rules in the agent’s explanation

F, the number of features used to construct the explanation
e S, the stability of the agent’s explanation
Elements of these metrics exist in other papers [28, 46, 47, 49, 51, 55],
and throughout the paper we point out similarities and differences
to previous works. In order to better understand the novelty of
this work, we first briefly overview state-of-the-art approaches for
generating explanations.

2 HOW EXPLANATIONS ARE GENERATED

Unfortunately, no consensus currently exists about the meaning of
various terms related to explainability including interpretability and
transparency. Part of the confusion is likely complicated by the fact
that the terms, “explainability, interpretability and transparency"
are often used synonymously while others implicitly define these
terms differently [11, 12, 16, 17, 30, 51, 55, 57]. Previous work by
Rosenfeld and Richardson defined interpretability as a technical
term focusing on the clarity of the system’s internal logic and ex-
plainability as the ability of human user to understand that logic
[51]. In contrast, Rudin defined explanations as agent attempts to
explain its logic in a post-hoc fashion without necessarily being
tied to the agent’s true decision model, while interpretations are
inherently tied to the agent’s logic [55]. Both works agree that
the XAI goal is to completely, accurately and clearly quantify the
agent’s logic, something that Rosenfeld and Richardson refer to as
transparency [51] and Rudin terms fidelity [55]. To avoid termi-
nology confusion, we will use these terms synonymously as both
focus on the same paramount XAI goal.

The level of agent transparency depends on which of three basic
approaches are used to generate XAI: directly from a transparent
machine learning algorithm, through feature selection and/or anal-
ysis of the inputs, or by using an algorithm to create a post-hoc
modeling, outcome, or visualization tool. The first approach is to
only use certain types of machine learning methods, such as deci-
sion trees or other rule-based approaches, that transparently output
a model that can readily be understood by the user. For example, if
a decision tree outputs a relatively small set of rules, this output
can then be directly implemented as the agent’s logic and serves
as the explanation presented to the system’s user [51, 55, 58].

A second approach is to use feature selection and analysis to
establish which data elements should be focused upon. Even if
transparent models are then not used, using a limited set of features
can help clarify the agent’s logic [19, 26, 51, 55]. The advantage
of this approach is that the information presented to the user is
generated directly from the mathematical relationship between
a small set of features and the target being learned. Additionally,
even if the agent uses more complex models machine learning mod-
els, this approach helps the user better understand the underlying
relationships between the input and output of the system even if
she does not fully grasp the full interplay of all input possibilities
and the resultant model. This in turn allows the agent to use more
accurate models without sacrificing significant fidelity levels [51].

The last, and possibly most prevalent, approach uses mechanisms
external to the system’s logic to help describe the inner working
of a black-box system that is not inherently understood [37, 55].
This approach is often used in conjunction with state-of-the-art
prediction models obtained from neural networks and ensemble
methods that are not transparent [63]. One group of approaches
within this category create proxy models secondary to the agent’s

logic to approximate the agent’s logic via transparent models such
as decision trees [9, 20, 62, 71]. Other approaches, such as saliency
maps, highlight which portion of the input features, such as areas of
a picture, are important based of the structure of model being used-
typically in a neural network [1, 61, 70]. A third approach high-
lights which inputs are important based on model perturbations to
query the system for how the agent’s performance would be im-
pacted without those inputs [10, 13, 31, 32, 39]. Popular examples of
XAT algorithms within this approach are LIME (Local Interpretable
Model-Agnostic Explanations) [39] and SHAP (SHapley Additive
exPlanation) [31, 32]. While the post-hoc explanation is not a full
representation of the system’s logic, they do enable people to better
understand the system’s logic [51] - even in black-box systems.

3 MATCHING XAI WITH ITS NEED

An effective evaluation metric must quantify the benefit of XAI to-
wards achieving the system’s goals. These goals stem from various
needs including legal, ethics, safety, trust, and knowledge discovery
considerations [3, 12, 51, 55]. Itis important to differentiate between
different types of entities requiring the explanations - whether it
is a user interacting with the system or an outside societal or legal
body. While XAI research is typically geared for individual users,
there is a growing need to address legal and governmental con-
cerns. Both the EU and UK governments have adopted guidelines
requiring agent designers to provide users information about com-
puter decisions. In the words of the EU’s “General Data Protection
Regulation" (GDPR), users are legally entitled to obtain “mean-
ingful explanation of the logic involved" of these decisions and
additional legislation exists to ensure that automated decisions are
not biased against any ethnic or gender groups [12, 17]. However,
demonstrating that a system is generally unbiased or even provides
“meaningful’ explanations is not the same as providing transparency
and full fidelity about the logical process of the system for every
possible situation.

Explainability has also been suggested to help the system de-
signer evaluate the system or to confirm that the system is func-
tioning properly and safely. This requirement is particularly acute
within life-and-death human-agent systems including the medi-
cal application built by the XYZ company. Without XAI, both the
medical practitioner and the patient might fear that the agent’s
recommendations might be adopted wrongly at times and thus put
people’s lives in danger.

In contrast, certain explanation goals are less critical- such as
using explanations for knowledge discovery to help researchers
gain understanding of various medical phenomena. Explainability
can similarly be useful in building trust between the user and system
especially when mistakes were made [8]. XAT is not critical in these
cases but could help improve the total utility of the human-agent
system. Assuming the agent effectively conveys its logic in the XYZ
application, the user could potentially understand when to accept
the agent’s recommendation and when to ignore them. This would
create an ideal decision support system (DSS) by leveraging agent
and user strengths.

Some explanations goals are relatively easy to evaluate. We gen-
erally believe that explanations built to address legal, ethics, safety,
and knowledge discovery considerations can be typically evaluated
with a simple or binary score- either the system addresses these
considerations or not. If the explanation lacks fidelity, and thus
doesn’t truly quantify the agent’s logic, then there is no basis to
consider the system safe or non-biased. Similarly, if the knowledge
discovery is rooted in explanations based on simplified logic that is
not being used, then the XAI is independent of the agent’s logic,
making any gain from this knowledge minimal [55].

More commonly, explanations are important to create better
human-agent interactions - either to develop trust or to foster
better performance in a DSS [51]. In these cases, evaluating the
explanation is more difficult as it must quantify the complex rela-
tionship between the agent and human components of the system.
As is the case in the XYZ system, the agent’s ability to create ac-
curate models is critical for the overall success of the system. As
such, any evaluation metric should reason about the joint agent
and human performance with classic metrics such accuracy, ROC
and/or precision. Assuming the agent performance achieved from
black-box models is higher, these models could then be used- even
at a potential loss of transparency. Many XAI researchers assume
that these black-box models are inherently better due to their su-
perior performance and explainability must be created given this
constraint [10, 13, 17, 31, 32, 37, 39]. Accordingly, it is tempting to
suggest that transparent models and their explanations be used for
life-or-death decisions such as the ones made by the XYZ system,
or when legally required, but explanations with less fidelity might
be acceptable in other situations [51, 55]. User studies with classic
performance and satisfaction metrics can then be used to weigh the
effectiveness of various explanations [12, 22, 37, 38, 51, 65]. How-
ever, for reasons we now detail, we instead suggest using the D, R,
F, S metrics to quantify XAL

4 BETTER METRICS FOR XAI EVALUATIONS

Tn this section we argue that the “gold-standard" for evaluating XAI
system through user studies may at times be inherently flawed due
to several reasons. First, there is an assumption that user studies can
properly capture the complex dynamics between user performance
and explanations. This has already been shown to not always be
correct. Second, XAI research has assumed that providing better
explanations aid group behavior. However, one large-scale study
involving 3800 participants did not find this to be the case and
providing more detailed explanations hurt performance [38]. Even
studies seeming to support the benefit from a given explanation
algorithm may suffer from confirmation bias, where user studies
are constructed to confirm the effectiveness of a wrong hypothesis,
here a poor explanation given the XAI’s goals [68].

We generally claim that existing user studies that evaluate expla-
nations generated post-hoc with a separate logic and low fidelity to
validate legal, ethics or safety concerns are inherently flawed. As
Rudin points out, if the post-hoc explanation is based on a funda-
mentally different logic than the one used by the agent, then what
is being evaluated? If they contain the same logic, then why not use
that model instead of the black-box model [55]? This critique is par-
ticularly an issue regarding explanations created post-hoc via proxy
methods as these explanations are a known oversimplification of
the agent’s logic [9, 20, 28, 62, 71]. Saliency maps have also been
found to be equally problematic as these visualization tools are
often the same regardless of the specific input used- making their

general worth questionable [1]. Thus, even ifa user is satisfied with
explanations of these types, the positive result is potentially based
on confirmation bias. Even when trust and performance needs to be
evaluated, and the complex interplay between XAI and performance
must be considered, one should question the validity of explana-
tions generated with low fidelity. Given the logical gap between
the agent’s logic and the explanation, classic user metrics cannot
necessarily quantify if a positive result is due to the explanation or
confirmation bias. Objective metrics are critical for quantifying the
tradeoff between agent fidelity and performance.

To address this challenge, we present four XAI evaluation met-
rics, D, R, F, S, that are not dependant on the task being performed
or the XAI algorithm developed. As a result, these metrics can-
not suffer from any confirmation bias. Consequently, D, R, F, and
S can be used to quantify XAI similarly to how the NASA-TLX
[54] and the System Usability Scale [5] quantify elements of user
performance.

The first metric, D, is predicated upon the assumption that
human-agent system designers used black-box models for the agent
because they provided a significant improvement in agent perfor-
mance [18, 51]. It is still unclear if this tradeoffis typically necessary
for most applications or even for specialized tasks such as image
processing where neural networks are typically used [55]. To eval-
uate this tradeoff, D quantifies the change of agent performance, 6,
between the black box model, and the best observed transparent
model. This measure is similar to the disagreement metric previ-
ously developed [28] but uses 6 to quantify performance differences
between models to facilitate comparing different types of expla-
nations and the potential improvement in performance versus the
loss of fidelity. Even if a user is happy with an explanation, 5 helps
measure if the tradeoff between this explanation and one with
transparency was warranted by comparing the performance, P; of
the transparent model and the performance, P, — 6, of the black
box model. For example, assuming XYZ’s black box model is 99.5%
accurate, but a transparent model is 95.5% accurate, 5 would have
to be less than 0.04 to justify using the black box model. Similarly,
if a model based on feature construction is 99% accurate, 6 would
have to be less than 0.005 to justify using the black box model. User
studies could then focus on what value for 6 is most justified for a
given XAI goal and specific task.

The D metric can be useful both in cases where binary and non-
binary evaluation of XAI is warranted, but will typically be more
helpful in the later case. Assuming binary evaluation is needed due
to legal, ethics or safety issues, any value for 6 greater than a trivial
value € shows that the explanation and agent are not synonymous,
and thus any benefit from the explanation is likely nil. For example,
if an explanation is used to show lack of bias in XYZ’s system, but
the logic based on the explanation is different for certain cases,
then close inspection is needed to evaluate if these cases represent
a bias or can be ignored. Conversely, if 5 is zero then the two
models are equivalent and the more transparent model should be
used regardless. Thus, we believe D is more useful when XAT is
beneficial to the system, but not necessary.

While D focuses on performance differences between models,
the second metric, R, focuses on the size of the agent’s explanation
without comparison to other models. R quantifies explanations
based on their simplicity - the fewer rules in the explanation, the
better. This metric is built upon an assumption that simpler expla-
nations should be preferred as per Occam’s Razor [51] and work
by Gigerenzer and Brighton about the bias-variance tradeoff [15].
These works, among others, assume that the world is inherently or-
derly and understandable by relatively simple rules. Thus, complex
rules should be penalized.

While many utility functions are possible for R, similar to Rudin
[55] we suggest using a parameter A to quantify the number of rules
within the agent’s model. In contrast to their work, we suggest that
a penalty of A « £ be used to penalize the performance metric
where £ = size(m) — c. We define size(m) as the number of rules
in the explanation. We set c=1 by default but this value can be
set to larger values to show that explanations with this number of
rules are fully explainable and should not be penalized. Formally, we
define £ = size(m)—c where (size(m)-c)=0, and zero otherwise. For
example, assuming the size of the model, size(m), is 1, no penalty
is added regardless of the value for A. If A = 0.005, the default
value for c is used and 5 rules exist in the explanation, then a 0.02
performance penalty is added for the four rules above the base size
of 1. Alternatively, no penalty may be desired for any model less
with fewer than 5 rules, and c=5 can be set. As such, a penalty will
only exist when 6 or more rules are in the explanation as per £’s
definition. In all cases the A « £ penalty could be optimized and
evaluated based on theoretical or user studies.

We assume that the R metric is most useful for transparent meth-
ods, but it can also be in conjunction with other XAI algorithms as
well. While decision trees are generally assumed to be transparent
models [51], one would be pressed to consider a decision tree with
thousands of rules as being explainable. Thus, A * £ can quantify
the impact of this complexity on the system’s performance. Con-
versely, while neural networks and ensemble methods are typically
assumed to be non-transparent, given a small enough model size
they may be understood by the intended user. We suggest user
studies based on R be performed to further analyze these claims.

While the R metric focuses on quantifying the number of rules
in the agent’s output, the F metric focuses on the number of fea-
tures inputted by agent to create its explanation. This metric was
particularly built for those explanations based on feature analysis.
Even when the agent’s model is based on a complex learning model
with lower fidelity, the assumption is that explainability will be
higher if the user can focus on a smaller number of features, thus
making the XAI clearer. To quantify this relationship, we again
suggest that a penalty of A « £ be defined similarly to R, but here
we define size(m) as the number of inputs to the model instead of
the rules outputted. The threshold c can again be used to quantify
a maximum number of feature inputs where no penalty should be
applied. It is possible that a magical number of around 7 [33] be
used for c in both R and F, but further studies are needed given a
specific XAI goal/task.

We posit that the size(m) value in the R and F metrics need not
based on a single feature or field, and more complex constructed
features be considered as one unit for purposes of size(m). For exam-
ple, image processing typically currently focuses on pixels inputs,
but some constructed image features such as edges are inherently
more interpretability and might be considered as a single feature
for purposes of these metrics. Similarly, a complex driving style
feature was previously found to be useful in quantifying people’s

use of adaptive cruise control [45] and the maximum cancer core
length feature was found to be an important feature in quantifying
the existence of prostate cancer [7]. As a general rule, we suggest
that derived features be treated the same as non-derived ones as
long as they are equally understood by the intended user.

The last evaluation metric we present, S, quantifies the stability
of the agent’s explanation. Feature stability is a metric central to
feature selection analysis and refers to its ability to robustly handle
small noise perturbations. Finding stable features is important as it
indicates that the feature selection is unlikely to be overfitted to the
specific data being considered. Unstable feature inputs have been
linked to poor explainability [35]. We suggest boostrapping the
data and then observing its impact on the outputted agent explana-
tions. The similarity between the bootstraps’ explanations can be
quantified using Jaccard and/or Tanimoto similarity measures. We
stress that only small perturbations should be used by the bootraps
such that the class labels are not changed and small amounts of
resampling noise are used to check that the explanations are stable
and thus general. At the other extreme, “the data randomization
test", was created that randomly permutes all labels within the
training classes [1]. This check also uses similarity to evaluate if
an explanation is useful, but here a lack of similarity shows that
explanations are not dependent on the data being used, rendering
them without value. In all cases, and as was the case for the R
and F metrics, a penalty cost, A * £ can be used to penalize the
performance metric. As similarity metrics are typically between
0-1, with 1 being complete similarity, we suggest directly defining
S=A*(1-similarity) such that performance-(A*(1-similarity)) could
be used as a penalty. Once again, user studies could be used to help
compare different methods and appropriately set the values for A.

While we have presented each of these metrics individually, we
believe that they are complimentary in many cases. It is likely desir-
able that a transparent explanation be both stable, perform similarly
to agent, and contain relatively few rules. As such one might expect
it to have high scores for the D, R, S metrics. Conversely, any agent
using black box methods might score lower for the D, F metrics
especially if a proxy model is used for generating explanations,
but if this explanation is relatively simple and stable it would still
achieve higher scores for the R, S metrics. Thus, composite scores
could be constructed and focused user studies should be conducted.

5 CONCLUSION

In this paper we argue that many XAI studies wrongly assume
low fidelity explanations should be accepted for certain tasks. We
also argue that user studies may also have confirmation bias in
their evaluation of XAI. To remove these concerns, we advocate
using four general metrics, D, R, F, S, to quantify XAI explainability
based on the difference in the agent’s performance using models
with higher fidelity versus lower fidelity, the number of rules in
the outputted explanation, the number of features used by the
agent to generate the explanation, and the stability of the agent’s
explanation. The advantage of these measures is that they make no
a-priori assumption about the relatively advantage of using an XAI
algorithm with higher or lower fidelity, yet facilitate comparison
without any potential confirmation bias from user studies. We hope
that these metrics will be considered in the future for what we
consider to be more significant evaluations of XAL
REFERENCES

(4)

[2]

[3]

[4]

[5]

[6]

(7

[3]

[9]

10]

f4]

(12]

(13]

(14)
(15]

[16]

[17]

[18]

{19]

(20]

(21]

(22]

[23]

[24]

Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt,
and Been Kim. 2018. Sanity checks for saliency maps. In Advances in Neural
Information Processing Systems. 9505-9515.

Ofra Amir and Kobi Gal. 2013. Plan recognition and visualization in exploratory
learning environments. ACM Transactions on Interactive Intelligent Systems (TiiS)
3, 3 (2013), 16.

Alejandro Barredo Arrieta, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Ben-
netot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel
Molina, Richard Benjamins, et al. 2020. Explainable Artificial Intelligence (XAD):
Concepts, taxonomies, opportunities and challenges toward responsible AI. In-
formation Fusion 58 (2020), 82-115.

Amos Azaria, Zinovi Rabinovich, Claudia V Goldman, and Sarit Kraus. 2015.
Strategic information disclosure to people with multiple alternatives. ACM
Transactions on Intelligent Systems and Technology (TIST) 5, 4 (2015), 64.

Aaron Bangor, Philip T Kortum, and James T Miller. 2008. An empirical evaluation
of the system usability scale. Intl. Journal of Human-Computer Interaction 24, 6
(2008), 574-594,

Samuel Barrett, Avi Rosenfeld, Sarit Kraus, and Peter Stone. 2017. Making friends
on the fly: Cooperating with new teammates. Artificial Intelligence 242 (2017),
132-171.

Echeverria LM Carmona, A Haider, A Freeman, U Stopka-Farooqui, A Rosenfeld,
BS Simpson, Y Hu, D Hawkes, H Pye, S Heavey, V Stavrinides, JM Norris, AE-S
Bosaily, Barrena C Cardona, S Bott, L Brown, N Burns-Cox, T Dudderidge, A
Henderson, R Hindley, R Kaplan, A Kirkham, R Oldroyd, M Ghei, R Persad, S
Punwani, D Rosario, I Shergill, M Winkler, HU Ahmed, M Emberton, and HC
Whitaker. 2020. A critical evaluation of visual proportion of Gleason 4 and
maximum cancer core length quantified by histopathologists. Sci Rep 10 (2020).
Jessie ¥Y Chen, Katelyn Procci, Michael Boyce, Julia Wright, Andre Garcia, and
Michael Barnes. 2014. Situation awareness-based agent transparency. Technical
Report. Army Research Lab Aberdeen Proving Ground MD Human Research and
Engineering Directorate.

Houtao Deng. 2014. Interpreting tree ensembles with intrees. arXiv preprint
arXiv:1408.5456 (2014).

Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting,
Karthikeyan Shanmugam, and Payel Das. 2018. Explanations based on the
missing: Towards contrastive explanations with pertinent negatives. In Advances
in Neural Information Processing Systems. 592-603.

Derek Doran, Sarah Schulz, and Tarek R. Besold. 2017. What Does Explainable
Al Really Mean? A New Conceptualization of Perspectives. In Proceedings of the
First International Workshop on Comprehensibility and Explanation in AI and ML.
Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter-
pretable machine learning. arXiv preprint arXiv:1702.08608 (2017).

Ruth C Fong and Andrea Vedaldi. 2017. Interpretable explanations of black boxes
by meaningful perturbation. In 2017 IEEE international conference on computer
vision (ICCV). 3449-3457.

Maria Fox, Derek Long, and Daniele Magazzeni. 2017. Explainable Planning.
CoRR abs/1709.10256 (2017).

Gerd Gigerenzer and Henry Brighton. 2009. Homo heuristicus: Why biased
minds make better inferences. Topics in cognitive science 1, 1 (2009), 107-143.
Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, and
Lalana Kagal. 2018. Explaining Explanations: An Approach to Evaluating Inter-
pretability of Machine Learning. CoRR abs/1806.00069 (2018).

Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca
Giannotti, and Dino Pedreschi. 2018. A Survey of Methods for Explaining Black
Box Models. ACM Comput. Surv. 51, 5 (Aug. 2018), 93:1-93:42.

David Gunning. 2017. Explainable artificial intelligence (xai). Defense Advanced
Research Projects Agency (DARPA), nd Web 2, 2 (2017).

Isabelle Guyon and André Elisseeff. 2003. An introduction to variable and feature
selection. Journal of machine learning research 3 (2003), 1157-1182.

Satoshi Hara and Kohei Hayashi. 2016. Making tree ensembles interpretable.
arXiv preprint arXiv:1606.05390 (2016).

Nicholas Hoernle, Kobi Gal, Barbara J. Grosz, Leilah Lyons, Ada Ren, and Andee
Rubin. 2020. Interpretable Models for Understanding Immersive Simulations.
In Proceedings of the Twenty-Ninth International Joint Conference on Artificial
Intelligence, I}CAI 2020, Christian Bessiere (Ed.). ijcai-org, 2319-2325.

Robert R Hoffman, Shane T Mueller, Gary Klein, and Jordan Litman. 2018. Metrics
for explainable Al: Challenges and prospects. arXiv preprint arXiv:1812.04608
(2018).

Yoshimasa Horie, Toshiyuki Yoshio, Kazuharu Aoyama, Shoichi Yoshimizu,
Yusuke Horiuchi, Akiyoshi Ishiyama, Toshiaki Hirasawa, Tomohiro Tsuchida,
Tsuyoshi Ozawa, Soichiro Ishihara, et al. 2019. Diagnostic outcomes of esophageal
cancer by artificial intelligence using convolutional neural networks. Gastroin-
testinal endoscopy 89, 1 (2019), 25-32.

Nicholas R Jennings, Luc Moreau, David Nicholson, Sarvapali Ramchurn, Stephen
Roberts, Tom Rodden, and Alex Rogers. 2014. Human-agent collectives. Commun.
ACM 57, 12 (2014), 80-88.

[29]
[30]

[31]

[32]

[33]

[34]

[35]

[36]

Akiva Kleinerman, Ariel Rosenfeld, and Sarit Kraus. 2018. Providing explanations
for recommendations in reciprocal environments. In Proceedings of the 12th ACM
Conference on Recommender Systems. ACM, 22-30.

Igor Kononenko. 1999. Explaining classifications for individual instances. In In
Proceedings of IJCAI’99. 722-726.

Sarit Kraus, Amos Azaria, Jelena Fiosina, Maike Greve, Noam Hazon, Lutz Kolbe,
Tim-Benjamin Lembcke, Jorg P Muller, Soren Schleibaum, and Mark Vollrath.
2020. Al for explaining decisions in multi-agent environments. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 34. 13534-13538.

Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2017. In-
terpretable & explorable approximations of black box models. arXiv preprint
arXiv:1707.01154 (2017).

Pat Langley, Ben Meadows, Mohan Sridharan, and Dongkyu Choi. 2017. Explain-
able Agency for Intelligent Autonomous Systems.. In AAAL 4762-4764.
Zachary Chase Lipton. 2016. The Mythos of Model Interpretability. arXiv preprint
arXiv: 1606.05390 (2016).

Scott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin,
Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2020.
From local explanations to global understanding with explainable AI for trees.
Nature machine intelligence 2, 1 (2020), 2522-5839.

Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. In Advances in Neural Information Processing Systems. 4765-4774.
George A Miller. 1956. The magical number seven, plus or minus two: Some
limits on our capacity for processing information. Psychological review 63, 2
(1956), 81.

Shamim Nemati, Andre Holder, Fereshteh Razmi, Matthew D Stanley, Gari D
Clifford, and Timothy G Buchman. 2018. An interpretable machine learning
model for accurate prediction of sepsis in the ICU. Critical care medicine 46, 4
(2018), 547.

Sarah Nogueira, Konstantinos Sechidis, and Gavin Brown. 2017. On the stability
of feature selection algorithms. The Journal of Machine Learning Research 18, 1
(2017), 6345-6398.

Erfan Pakdamanian, Shili Sheng, Sonia Baee, Seongkook Heo, Sarit Kraus, and Lu
Feng. 2021. DeepTake: Prediction of Driver Takeover Behavior using Multimodal
Data. In Proc. of CHI-21.

Dino Pedreschi, Foscea Giannotti, Riccardo Guidotti, Anna Monreale, Salvatore
Ruggieri, and Franco Turini. 2019. Meaningful Explanations of Black Box AI
Decision Systems. Proceedings of the AAAI Conference on Artificial Intelligence 33
(jul. 2019), 9780-9784,

Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wort-
man Vaughan, and Hanna Wallach. 2018. Manipulating and measuring model
interpretability. arXiv preprint arXiv:1802.07810 (2018).

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why Should I
Trust You?: Explaining the Predictions of Any Classifier. In Proceedings of the
22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. 1135-1144.

Ariella Richardson and Avi Rosenfeld. 2018. A Survey of Interpretability and
Explainability in Human-Agent Systems. XAI 2018 (2018), 137-143.

Hanan Rosemarin, Ariel Rosenfeld, and Sarit Kraus. 2019. Emergency department
online patient-caregiver scheduling. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 33. 695-701.

Ariel Rosenfeld, Noa Agmon, Oleg Maksimov, and Sarit Kraus. 2017. Intelligent
agent supporting human-multi-robot team collaboration. Artificial Intelligence
252 (2017), 211-231.

Avi Rosenfeld, Zevi Bareket, Claudia V Goldman, Sarit Kraus, David J LeBlanc,
and Omer Tsimhoni. 2012. Learning Driver’s Behavior to Improve the Acceptance
of Adaptive Cruise Control. In IAAL

Avi Rosenfeld, Zevi Bareket, Claudia V Goldman, Sarit Kraus, David J LeBlanc,
and Omer Tsimhoni. 2012. Towards adapting cars to their drivers. AI Magazine
33, 4 (2012), 46-46.

Avi Rosenfeld, Zevi Bareket, Claudia V Goldman, David J LeBlanc, and Omer
Tsimhoni. 2015. Learning drivers’ behavior to improve adaptive cruise control.
Journal of Intelligent Transportation Systems 19, 1 (2015), 18-31.

Avi Rosenfeld and Matanya Freiman. 2020. Explainable Feature Ensembles
through Homogeneous and Heterogeneous Intersections. In IJCAI-PRICAI 2020
Workshop on XAL

Avi Rosenfeld, David G. Graham, Rifat Hamoudi, Rommel Butawan, Victor Eneh,
Saif Khan, Haroon Miah, Mahesan Niranjan, and Laurence B. Lovat. 2015. MIAT:
A Novel Attribute Selection Approach to Better Predict Upper Gastrointestinal
Cancer. In International Conference on Data Science and Advanced Analytics.
Avi Rosenfeld, David G Graham, Sarah Jevons, Jose Ariza, Daryl Hagan, Ash
Wilson, Samuel J Lovat, Sarmed S Sami, Omer F Ahmad, Marco Novelli, et al.
2020. Development and validation of a risk prediction model to diagnose Barrett’s
oesophagus (MARK-BE): a case-control machine learning approach. The Lancet
Digital Health 2, 1 (2020), e37-e48.

Avi Rosenfeld, Ron Illuz, Dovid Gottesman, and Mark Last. 2018. Using discretiza-
tion for extending the set of predictive features. EURASIP Journal on Advances in
Signal Processing 7, 1 (2018), 1-11.
[50]

(51]

[52]

[53]

[54]

[55]

[56]

(57]

[58]

[59]

[60]

(61]

Ariel Rosenfeld and Sarit Kraus. 2016. Providing Arguments in Discussions
on the Basis of the Prediction of Human Argumentative Behavior. ACM Trans.
Interact. Intell. Syst. 6, 4 (2016), 30:1-30:33.

Avi Rosenfeld and Ariella Richardson. 2019. Explainability in human-agent
systems. Auton. Agents Multi Agent Syst. 33, 6 (2019), 673-705.

Avi Rosenfeld, Vinay Sehgal, David G. Graham, Matthew R. Banks, Rehan J.
Haidry, and Laurence B. Lovat. 2014. Using Data Mining to Help Detect Dysplasia:
Extended Abstract. In 2014 IEEE International Conference on Software Science,
Technology and Engineering. IEEE, 65-66.

Avi Rosenfeld, Inon Zuckerman, Erel Segal-Halevi, Osnat Drein, and Sarit Kraus.
2016. NegoChat-A: a chat-based negotiation agent with bounded rationality.
Autonomous Agents and Multi-Agent Systems 30, 1 (2016), 60-81.

Susana Rubio, Eva Diaz, Jesus Martin, and José M Puente. 2004. Evaluation of
subjective mental workload: A comparison of SWAT, NASA-TLX, and workload
profile methods. Applied Psychology 53, 1 (2004), 61-86.

Cynthia Rudin. 2019. Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature Machine
Intelligence 1, 5 (2019), 206-215.

Maha Salem, Gabriella Lakatos, Farshid Amirabdollahian, and Kerstin Dauten-
hahn. 2015. Would you trust a (faulty) robot?: Effects of error, task type and
personality on human-robot cooperation and trust. In Proceedings of the Tenth
Annual ACM/IEEE International Conference on Human-Robot Interaction. 141-148.

Wojciech Samek, Thomas Wiegand, and Klaus-Robert Miller. 2017. Explainable
artificial intelligence: Understanding, visualizing and interpreting deep learning
models. arXiv preprint arXiv:1708.08296 (2017).

Vinay Sehgal, Avi Rosenfeld, David G Graham, Gideon Lipman, Raf Bisschops,
Krish Ragunath, Manuel Rodriguez-Justo, Marco Novelli, Matthew R Banks,
Rehan J Haidry, et al. 2018. Machine learning creates a simple endoscopic
classification system that improves dysplasia detection in Barrett’s oesophagus
amongst non-expert endoscopists. Gastroenterology Research and Practice (2018).

Raymond Sheh. 2017. why did you do that?” explainable intelligent robots. In
AAAI Workshop on Human-Aware Artificial Intelligence.

Maarten Sierhuis, Jeffrey M Bradshaw, Alessandro Acquisti, Ron Van Hoof, Re-
nia Jeffers, and Andrzej Uszok. 2003. Human-agent teamwork and adjustable
autonomy in practice. In Proceedings of the seventh international symposium on
artificial intelligence, robotics and automation in space (I-SAIRAS).

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside
convolutional networks: Visualising image classification models and saliency

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

(71]

[72]

[73]

maps. arXiv preprint arXiv:1312.6034 (2013).

Hui Fen Tan, Giles Hooker, and Martin T Wells. 2016. Tree Space Proto-
types: Another Look at Making Tree Ensembles Interpretable. arXiv preprint
arXiv:1611.07115 (2016).

Erico Tjoa and Cuntai Guan. 2020. A Survey on Explainable Artificial Intelligence
(XAI): Toward Medical XAI. IEEE Transactions on Neural Networks and Learning
Systems (2020).

David Traum, Jeff Rickel, Jonathan Gratch, and Stacy Marsella. 2003. Negotia-
tion over tasks in hybrid human-agent teams for simulation-based training. In
Proceedings of the second international joint conference on Aufonomous agents and
multiagent systems. ACM, 441-448.

Jasper van der Waa, Elisabeth Nieuwburg, Anita Cremers, and Mark Neerincx.
2021. Evaluating XAI: A comparison of rule-based and example-based explana-
tions. Artificial Intelligence (2021).

Kurt VanLehn. 2011. The relative effectiveness of human tutoring, intelligent
tutoring systems, and other tutoring systems. Educational Psychologist 46, 4
(2011), 197-221.

Alfredo Vellido. 2019. The importance of interpretability and visualization in
machine learning for applications in medicine and health care. Neural Computing
and Applications (2019), 1-15.

Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019. Designing
theory-driven user-centric explainable AI. In Proceedings of the 2019 CHI confer-
ence on human factors in computing systems. 1-15.

Bo Xiao and Izak Benbasat. 2007. E-commerce product recommendation agents:
use, characteristics, and impact. MIS quarterly 31, 1 (2007), 137-209.

Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.
2016. Learning deep features for discriminative localization. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. 2921-2929.
Yichen Zhou and Giles Hooker. 2016. Interpreting Models via Single Tree Ap-
proximation. arXiv preprint arXiv:1610.09036 (2016).

Guangming Zhu, Bin Jiang, Hui Chen, Elizabeth Tong, Yuan Xie, Tobias D Faizy,
Jeremy J Heit, Greg Zaharchuk, and Max Wintermark. 2020. Artificial Intelligence
and Stroke Imaging: A West Coast Perspective. Neuroimaging Clinics 30, 4 (2020),
479-492.

Inon Zuckerman, A Rosenfeld, Sarit Kraus, and E Segal-Halevi. 2013. Towards
automated negotiation agents that use chat interfaces. In The sixth international
workshop on agent-based complex automated negotiations (ACAN).
1911.04542v2 [eess.SP] 19 Nov 2019

arXiv

Explainable Artificial Intelligence (XAI) for 6G:
Improving Trust between Human and Machine

Weisi Guo IEEE Senior Member, RSS Fellow

Abstract—As the 5th Generation (5G) mobile networks are
bringing about global societal benefits, the design phase for the
6th Generation (6G) has started. 6G will need to enable greater
levels of autonomy, improve human machine interfacing, and
achieve deep connectivity in more diverse environments. The
need for increased explainability to enable trust is critical for
6G as it manages a wide range of mission critical services (e.g.
autonomous driving) to safety critical tasks (e.g. remote surgery).
As we migrate from traditional model-based optimisation to deep
learning, the trust we have in our optimisation modules decrease.
This loss of trust means we cannot understand the impact of:
1) poor/bias/malicious data, and 2) neural network design on
decisions; nor can we explain to the engineer or the public the
network’s actions. In this review, we outline the core concepts of
Explainable Artificial Intelligence (XAI) for 6G, including: public
and legal motivations, definitions of explainability, performance
vs. explainability trade-offs, methods to improve explainability,
and frameworks to incorporate XAI into future wireless systems.
Our review is grounded in cases studies for both PHY and MAC
layer optimisation, and provide the community with an important
research area to embark upon.

Index Terms—machine learning; deep learning; deep reinforce-
ment learning; XAI; 6G;

I. INTRODUCTION

An essential fabric of modern civilization is the digital
economy, which is underpinned by wireless communication
networks. We are on the cusp of entering a new era of
mass digital connectivity, where increasingly more people, ma-
chines, and things are being connected to automate and digitise
traditional services. Wireless networking has transitioned from
its traditional role as an information channel (1G to 3G) to a
critical leaver in the new industrial revolution (SG and beyond
to 6G [1]). This has caused not only up to 1000x growth
in the communication data rate demand, but also an increase
in diverse service requirements, such as massive URLLC for
tactile control of autonomous entities across transport to preci-
sion manufacturing in 6G. Orchestrating co-existence via spec-
trum aggregation between different radio access technologies
(RATs) is essential to meeting this demand. As such, real-time
radio resource management (RRM) is critically important, but
has become too complex for conventional optimisation. This
brings the need to evolve towards an Artificial Intelligence
(AI) driven ecosystem [2] to support more fine-grained user-
centric service provision (see 3GPP Release 16 TR37.816).
Research on the application of machine learning in 5G PHY

Weisi Guo is with the Alan Turing Institute, London, United Kingdom,
and Cranfield University, Bedford, United Kingdom. *Corresponding Author:
wguo @turing.ac.uk.

and MAC layers can be found in IEEE ComSoc Best Readings
in Machine Learning in Communications !.

A. Al and Trust

An open challenge with Deep Learning (DL) is the lack
of transparency and trust compared to traditional model-based
optimisation. Neural networks (NN), especially when coupled
with reinforcement learning (e.g. deep reinforcement learning
- DRL [3]) cannot explain the essential features that influence
actions, nor the impact of bias on the uncertainty of rewards.
This is made harder in high-dimensional mobility scenarios,
such as joint airborne UAV and ground vehicle environments
[4]. Even in a relatively trusted area of Bayesian inference,
recent research have shown that they are extremely brittle to
poor data. As such, there is the need to develop statistical AT
algorithms that can quantify uncertainty, especially mapping
big data inputs, algorithm design, to the projected wireless
key performance indicators (KPI). A trustworthy AI should
be able to explain its decisions in some way that human
experts can understand (e.g. the underlying data evidence and
causal reasoning). Understanding both our opportunity and
vulnerability to AI and big data is essential to the success
of future customised wireless services.

B. Novelty & Organisation

In this review, we outline the core concepts of Explainable
Artificial Intelligence (XAI) for 6G, including the key novel-
ties in their corresponding sections:

1) Section II-A: Public and legal motivations for improving
the transparency and trust in AI algorithms;

2) Section II-B: Definitions of explainability from specific
quantitative indicators, to general qualitative outputs;

3) Section III: Review of current deep learning techniques
in PHY and MAC layer and their level of performance
vs. explainability trade-off;

4) Section IV: Technical methods to improve explainability
in deep and deep reinforcement learning methods;

5) Section V: Propose an encompassing framework to in-
corporate XAI into future 6G wireless systems;

Our review is grounded in cases studies for both PHY and
MAC layer optimisation, including examples of explainability
in existing algorithms. Together, the author hope this article
provide the community with an important research area to
embark upon.

‘https://www.comsoc.org/publications/best-readings/machine-learning-
communications
II. MOTIVATION AND DEFINITIONS OF XAI
A. Public Trust & Legal Frameworks

At the heart of our need to add_ explainabil-
ity/interpretability/openness to deep learning is the need
to build trust in a quantifiable way. Traditional model based
techniques have reasonably high clarity in how an assumed
model and the input data leads to output decisions, i.e.,
Bayesian inference gives a statistically sound framework for
mapping the confidence in our data to the model outcomes.
However, deep learning (DL), at least in its naive form,
has none of the above. It’s infamous ”black box” approach
yields strong performance due to the automated discovery of
high-dimensional nonlinear mappings, but human operators
cannot understand the following:

e What data features are contributing to decisions and
where do we need more or better data

« How to improve the algorithm design, when more and
better data is not helping

e Uncover hidden bias in both the input data and the
algorithm

e Reverse teach human experts to uncover new insights

Not every XAI method will address all of the above, and this
paper sets out the methodology for addressing some of the
above challenges.

The legal framework for AT is still in its infancy, and there
are several explicit cases for XAI in different geographic
regions:

e EU: 2018 General Data Protection Regulation (GDPR)
in EU requires machine learning algorithms to be able to
explain their decisions (see Recital 71).

e USA: 2017 Equal Credit Opportunity Act update (Reg B,
art 1002.9) requires agencies to provide an official set of
reasons on the main factors that affect the credit score.

e National: 2016 French Digital Republic Act requires
the degree and mode of algorithms that contribute to
decisions, the data used and its provenance, the weight
of different data features, and the resulting actions.

There is ongoing debate on whether there is a negative bias
towards machine decisions (when humans do not always need
to explain their actions). The key is that rightly or wrongly,
humans can attempt to explain if prompted to, and we need
machines to have that equal capability in order to ensure
trust and a concrete pathway towards improving safety and
reliability.

B. Definitions and Modes of Explainability

The essential functional role of that machine learning plays
in wireless systems has not changed compared to classic model
based techniques. In one way or another, the mathematical
representation is:

9 = f(z), ()

where inputs x map to an estimated output of true y (y) via a
model f(-). In classic statistics, we apply Bayesian inference
to estimate the parameter values of a known function, e.g. @
in the example linear mapping of 7 = 0x +n. In DL, we

automate the discovery of nonlinear mapping between input
and output via the training process.

An intuitive and good starting point for explainability is for
it to meet two conditions:

1) Prediction is correct, e.g. 7 = y, and

2) Prediction is based on the correct data features and logic,

e.g. aspects of x combined with the form of f(-) are

agreeable to human reasoning/experience.
The latter is much harder to define, let alone articulate in a
DL framework. This is particularly challenging when we are
dealing with DRL, large input data sets, and multiple hidden
layers — we will discuss these aspects later in the paper. For
now, we discuss the different modes of explainability that we
may wish for or can only have.

1) Visualisation with Case Studies: The simplest form are
visual outputs from the DL algorithm highlighting features in
raw data that causally lead to the output choice. This may or
may not map to the human perceptions of key features which
also contribute to our cognitive reasoning. When combined
with well known case studies, whereby the input and output
mapping is established, we can both satisfy that predictions are
correct and it is likely the human operator can easily accept
or reject the key visual features.

2) Hypothesis Testing: A more rigorous form of the afore-
mentioned is hypothesis testing, whereby a well formulated
argument is tested based on the input data and output decision.
Here, we can test if: i) certain key features are important in
the mapping, ii) the mapping function behaves as we expect
(monotonic, nonlinear, ...etc.), and iii) we can accept or reject
the hypothesis.

3) Didactic Statements: Perhaps the ultimate form of ex-
plainability would use natural language to communicate to the
human operator, explaining what data features and algorithmic
functions led it to reach a decision/output. This requires very
strong explainability, as well as a machine-human interface to
explain the learning and decision process.

Now that we have established our motivation for under-
standing deep learning from a human reasoning perspective,
and the ways in which this might be measured and manifested,
we jump deeper into the wireless context to see to what degree
this can be accomplished.

II. DEEP LEARNING IN WIRELESS: EXPLAINABILITY VS.
PERFORMANCE

A. Review of Deep Learning & Wireless Applications

1) PHY Layer: Supervised DL has a wide range of applica-
tions in the PHY layer. In signal detection, it can equalise non-
linear distortions by feeding the received signals corresponding
to transmit data and pilots [5], outperforming classic MMSE
approaches - see example in Fig. la. When channels have
memory, a bidirectional recurrent neural network (RNN) is
more suitable and does not require channel state information
(CSD, out performing Viterbi detection [6]. Similar approaches
for block code decoding, channel estimation for mm-Wave
Massive MIMO, and end-to-end channel estimation have also
been performed — a summary of their performances is given in
Table I, along with their reported performances and potential
level of explainability.
[» ]

Deep Learning
(Nonlinear Equalization)

 

Received Signal
(Nonlinear Distortion)

Transmission

Wireless
Channel

in PHY Layer

Supervised Leaming

 

 

 

[e]

 

Old Actions

( ) Wireless Network
Environment
Continue |»
Service

  

Reinforcement Learning
in MAC Layer

 

 

 

Old States &
Rewards

Next Time Step

——"»> )

Deep Learning
(Reward Estimation)

Detection
Performance

Why Did This
Error Occur?

~—_——>

 

 

No Obvious
Mechanism to
Understand and
Improve

 

 

 

Reward
Distribution

New Actions
e=-
Continue

6 Service < >
e-
Delay ,
a Why This
Se
@ mice Action?

 

—
Offload to
Wi-Fi

o) DS

 

 

 

 

 

Fig. 1. Examples of deep learning applications in PHY and MAC layers: a) supervised equalisation of nonlinear symbol distortion; b) reinforcement learning

of action choice in offloading users.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

TABLE I
AI EXAMPLES IN WIRELESS COMMUNICATION
Problem Domain l Representative Paper | Classic Approach | AI or DL Approach Improvement at BER | Explainability
Signal Detection Yel8 (WCL) DFT with LS or MMSE DNN with 3 hidden >15dB at 107! Low
Channel with Memory Farsad18 (TSP) Viterbi Detector (VD) SBRNN with 1 hidden | 20 VD mem. at 1071 Low
Decoding of LDPC Nachmanil& (JSTSP) Belief Propagation (BP) RNN with 5 hidden 1dB at 1078 V. Low
Channel Estimation Neumann18 (TSP) Orth. Matching Pursuit CNN with 1 hidden 2dB at 10~1 Low
NOMA SCMA Detection Kim18 (CL) Message Passing DNN with 4 hidden 2dB at 10-3 V. Low
Channel Est. mm-M-MIMO Hel8 (WCL) Support Detection CNN and 3 layers 17dB at 5dB SNR Vv. Low
Cognitive Radio Tsakmalis18 (JSTSP) Expectation Prop. Bayesian MCMC 25 flops at 10~! error Medium
Power Allocation Nasirl9 (ISAC) Frac. Prog. & WMMSE DQN‘with 3 hidden lbps/Hz. None
Cross RAT Channel Access Yul9 (ISAC) RL DQN with 6 hidden 5% rate None
Interf. Align with Cache Hel7 (TVT) RL DQN with 4 hidden 20% rate None
Antenna Sel. Joung16 (CL) MaxMinNorm SVM 5% at 10-1 Low
WSN Diagnostics Liu10 (TON) Clustering Bayesian Belief Net. 5% Medium
User Behaviour Recog. Wang10 (TMC) SVM Random Forest 2-6% Low
QoE of Multimedia Hameed16 (TM) Fixed Decision Tree 50% overhead High

 

 

 

2) MAC Layer: In MAC layer RRM, classic reinforcement
learning (RL) based solutions do not rely on accurate system
models and is able to run in a model-free manner. Whilst
this overcame the challenges faced by traditional model de-
pendent optimisation (e.g. dynamic programming and convex
optimisation), the Q-table used in RL cannot scale to more
complex problem sets such as coordinated BS offloading to
heterogeneous devices, and will lead to non-convergence and
high cost. Deep RL (DRL [3]) relies on the powerful function
approximation and representation learning properties of DNN
to empower traditional RL with robust and high efficient
learning. In Fig. 1b, we demonstrate an example of offloading

user traffic based on observed state (e.g. interference, load,
signal strength,...etc.), and reward (e.g. spectrum efficiency,
energy efficiency) inputs. This in turn is translated into a
reward distribution over possible actions (e.g. continue service,
offload to WiFi,...etc.) and an action is selected. In the next
time iteration, the consequence of those actions are observed.
There has been a number of papers [7] that have examined the
use of DRL in cellular communications, including in relatively
complex mobility settings [4]. We will not exhaustively list
them here, but we will review their performance and explain-
ability trade-off below. A summary of their performances is
given in Table I, along with their reported performances and
QoE
A Classification

Bayesian
inference

Diagnostics

SVM
Behaviour

Classification

Cognitive
Radio

Antenna
Selection

Explainability

Channel Signal

A ac A Aa”
res DNN A

Radio Resource
Management

Random
Forest

Decoding ‘ @
DQN

 

a >

 

Performance

Fig. 2. Trade-off between AI performance and explainability with a variety
of PHY and MAC layer examples.

potential level of explainability. Currently, most existing DRL
solutions applied in RRM use off-the-shelf algorithms with
little consideration on the RRM feature set and DRL design.
This means that the resulting benefit and penalties incurred
(e.g. latency and energy consumption) cannot be understood
by the radio engineers monitoring and configuring the network.
In order to achieve a trusted autonomy, the DRL agents have
to be able to explain its actions for transparent human-machine
interrogation.

B. Trade-off Mapping and Interpretation Bias

In Fig. 2 we show a generalised mapping of AI algorithms
reviewed in Table I. Here, we can see that Bayesian techniques
(of which decision trees can also fit into) have a high degree
of explainability, mapping data evidence to model form to
parameter estimation and output confidence.

Even when Bayesian inference is problematic, we tend to
understand why [8], e.g. when:

1) the number of outcomes is large, e.g. higher order
modulation (64-QAM) or continuous actions (power
control levels even when discretized)

2) a large number of marginals of the data-generating
distribution are unknown (e.g. unknown mobility speed
distribution amongst a range of autonomous vehicles)

We also know how this affects AI decisions: (i) two sets of data
from the same situation may appear completely different and
lead to different decisions, or (ii) small changes in the model
or data (its prior) can cause a different posterior conclusion.
We detail more on data and algorithm bias below.

As we move away from the Bayesian framework, non-linear
classification techniques such as Support Vector Machine
(SVM) and random forest (RF) quickly lose explainability and

Machine
Perception of
Probability

Human
Perception of
Probability

    

Causes Risk Seeking
in Rewards and Risk
Averse in Penalties

°

     
    
  

Under-Estimate
High Probability
Events

Over-Estimate
Low Probability
Events

Cognitive Bias
(Weighted Probability)

Causes Risk Averse
in Rewards and Risk
Seeking in Penalties

¢

 

 

Actual Probability

Fig. 3. Cognitive bias in interpreting probability: human bias under-estimate
high probability and over-estimate low probability events.

there is no clear reason why data leads to one type of classifi-
cation nor do we understand how over-fitted it is. For example,
RF finds the optimal decision tree, but is often vulnerable to
random permutation in out-of-bag (OOB) samples, otherwise
known as Mean Decreased in Accuracy (MDA). The problem
of sample bias and overfitting is further exasperated when we
use DL to resolve a wide range of signal detection and channel
estimation problems. As we wrap a RL framework around
DL, we further complicate the explainability model, reaching
almost zero explainability in the DRL naive form.

Whilst its classification performance in complex problems is
superior to the aforementioned Bayesian and classic non-linear
techniques, it doesn’t perform well for simple problems nor
when there is clear bias. Bias in DL is not as well documented.
First, it maybe intuitive to think that the weights connecting
units may reveal insight (partial explainability) to its high
performance — indeed we show this is the case for many
problems below. However, in some experiments it has been
shown that random linear combinations of high level units also
perform well. This leads to the second well known observation,
which is that DNNs learn mapping f(-) in a discontinuous
way. As such, adding purposefully designed input data noise
(with no explainable features) into a well established classifier
can lead to severe mis-classification [9]. This remains an open
challenge which we discuss more at the end of the paper.

Even if machine intelligence can explain the probability
of rewards and penalties in reinforcement learning, there is
a risk that humans will not perceive it in the same way as the
machine. It is well known in Prospect Theory (2002 Nobel
Memorial Prize in Economics) that we have a cognitive bias
in interpreting probability for rewards and penalties. Whilst
machine utility functions are based on logic, human cognitive
bias tend to under-estimate high probability events and over-
 

[2]

Next Time Step
Deep Le

Old Actions

(a
Wireless Network

Environment

Continue Ly *
Service

o

Old States &
Rewards

     
  

(Reward Estimation)

arning Reward

Distribution
Delay

Ome
e@ (MM Continue
Service

AS
A

Why This
Action?

 

1

v

—_

—
Offload to
Wi-Fi

 

 

 
 
 
 
 
 
    
  

Reward of
Users 8 &9

[e]

ii) Local Perturbation
/ Linear Model

 

Load of

 

 

 

 

 

———_——- iii) Weight Informed Reduced NN

 

rot @ ©

Continue
Service

    

 

Users 4&5

   

Offload to

&

 

 

 

CSI, Load, Mobility, Reward

 

[e |

VISUAL: “Spectral
Efficiency Reward is an
important Feature”

Fig. 4. Explainability examples in DRL: a) DRL without explainability, b) a rai
(NN), and c) human reasoning based on explainability.

estimate low probability events. Reliability aside, this leads us
to prefer to avoid risks in high probability reward situations
(e.g. 100% chance of 100 Mbps > 90% of 120 Mbps), and
risk seeking in high probability penalty situations. Conversely,
it also leads us to prefer to seek risks in low probability reward
situations, and risk adverse in low probability penalty situa-
tions. As such, not only should machines reduce bias, but must
account for this human-machine difference in interpretability.
This area of human psychology is a subject of intense research
in DARPA’s XAI program.

IV. METHODS TO IMPROVE EXPLAINABILITY

Here, we give a review of recent attempts to improve
explainability, especially in deep learning (including DRL).
We map specific methods to the main explainability capabil-
ities we wish for in SectionII, which were: 1) Visualisation
with Case Studies, 2) Hypothesis Testing, and 3) Didactic
Statements.

A. Physics Informed Design

Designing DL algorithms that are physics based can negate
many of the concerns, as they have direct explainability.
For example, equalising the nonlinear channel loss (e.g. a
multitude of dispersion and phase noise in NLSE channels)

 

 

 

Wi-Fi
Gradient
mT

| VISUAL / ANALYSIS:
¥ “Demand in URLLC vs.

eMBB is an Important

Feature”

oe

 

EXPERIMENT: “Rewards and KPis
Lead Mosily to 2 Actions: Continue
Service or Offload to Wi-Fi”

nge of explainability options using data features and compressed neural network

is traditionally achieved via digital back propagation methods
such as Split-Step Fourier Method (SSFM). Designing DNN
that approximates this process in the form of a Learned Digital
Back Propagation (LDBP) is achieved by unrolling the SSFM
iterations and approximating each span inversion with 2 layers
[10]. However, in many cases, this is not possible because we
lack a workable traditional model or that it has unsatisfactory
performance.

B. Visualisation Techniques

At the perhaps most intuitive level of explainability, one
can visualise the features that are important based on their
weights or gradients of local nodes in the NN. In a gradient
based approach, we calculate the gradient of each input feature
with respect to an output:

(a + “) x (2)
x
where a small change in the input data feature leads to the
level of outcome change can be visualised. An example of
visual outputs in Fig. 4b-i include the spectral efficiency (SE)
reward of users 8 & 9 and its high impact on the output actions.
Local features in hidden layers are non-linear and therefore
the interpretation maybe not trivial. This explainability pro-
cess can be further enhanced by yielding didactic statement

f'(z) = lim

Azr—0 >
explanations by layer-wise relevance propagation (reversing
the NN by weight importance).

C. Local Data and Local Model Reduction

Instead of reducing the global DL model, we can also create
simpler surrogate models of selected partial data. For example,
we can select only the load demand data (see states in Fig. 4b-
ii) to see how this input feature affects the output. In general,
let the model being explained be f, then one attempts to
identify one or a set of interpretable model g € G (such
as the interpretable linear models, decision trees, rule tables
discussed previously) that is locally faithful to the classifier in
question: § = g(x*), where x* is a subset of x [11]. We can
also create local explainable models (e.g. local linear model
y = @x* + n) to understand better what DL is doing. In
Fig. 4b-ii, we can see that the load of users 4 & 5 influence
action choice and can be local linearly divided between the
URLLC and eMBB load demand - and this output can be either
visual or quantitative analysis. One popular approach based on
the above logic is called Local interpretable model-agnostic
explanations (LIME) [12]. LIME introduces a measure of
complexity for g: Q(g); such that one solves the following
to obtain the minimum explanation &{(z):

E(x) = arg max L(f, 9, U2) + Q(g), (3)
géG

where £(f,g, Uz) is a measure of how unfaithful reduced

model g is in approximating g in the locality of ©. As such,

LIME quantifies the simplest explanation by minimizing the

error of local model reduction and its complexity.

D. Global Model Reduction Techniques

Since we know that simpler models are more likely to
be explainable, e.g. fewer parts to link mathematically, more
likely to be in a form we recognise, ...etc., and as such model
reduction makes sense. There are a multitude of ways in which
this can be achieved with varying results and we detail some,
but not all approaches below.

1) Problem Reduction: In reinforcement learning, the
framework is often formulated from a Markov Decision Pro-
cess (MDP). The size of MDP is directly determined by
the state and action spaces, which grow super-polynomially
with the number of variables that characterise the domain. To
support fine-grained RRM, we have to adopt high-resolution
communication context to accommodate context-aware opti-
mization, which often results in a large-scale Partially Ob-
servable MDP (POMDP). The worst-case complexity is de-
termined by the model, ranging from POMDP with PSPACE-
complete (polynomial to input) to PO Stochastic Games with
NEXP-complete (non-deterministic Turing machine using time
grey complexity. In general, one can compress MDP model
in two stages:

e MDP model construction: one can appropriately choose
the definitions of state and/or action to adjust their reso-
lution. For example, when the transmit power constitutes
the action space, we could use a limited number of
discretised levels to approximate their dynamic range

 

Network
Environment

a
i
»)

 

 

») +. DES

FAY

»)

 

 

 

 

a} User has No

Service

 

h) Malicious
Designed
Noise

Spoils
Reasoning

 

 

 

2c) TEXT: “Network is
I too Congested Due to.
High Demand”

¥

Users

3

+ Low Expertise

+ No Timete Investigate

No

 
 
   

d) ANALYSIS: “High
Demand in URLLC has
Denied eMBS Services.”

Engineers

b) Engineer Aware

© High Expertise
e) Mental i

State

b
qe
Lo | emes

+ Time to Investigate

ut

d) XAl
Reasoning

Yes

+ Improve Wireless

jystem
+ Improve Al

 

\

g) Comprehension

= Complaints ee
2 Switch Service
+ Compensation f) Empathy

Fig. 5. XAI Integration Framework in Future Wireless Networks: XAI pro-
vides diverse explanations to consumer users and radio engineers with varying
levels of expertise and demand, their mental state affects their interpretation
and this feeds back to the ecosystem. Challenges include malicious attacks
using designer noise.

with controlled performance loss. Example: hierarchical
action space methods can be used to approximate the
POMDP problem, achieving a scalable compression.

e During the learning process: the size of MDP model can
be further reduced by aggregating identical or similar
states, allowing us to reduce learning complexity with a
bounded loss of optimality [13]. The similarity of states
can be measured in terms of optimal Q function, reward
and state transitions, Boltzmann distributions on Q values,
elc..

2) Neural Network Reduction: Previous studies have re-
vealed that NNs are typically over parameterised [14], and
one can achieve similar function approximation by removing
components (e.g. pruning the network as shown in Fig. 4b-
ui) and only retaining useful parts with greatly reduced model
size. There are several typical ways on compressing DNN by
exploiting sparsity in NN:

e Reducing the number of parameters: removing the num-
ber of connections/weights, or pruning filters.

e Architectural reform: replacing fully-connected layers
with more compact convolutional layers.

e Quantization: reduce the bit width integer to store
weights.

In general, selecting appropriate local data or reducing the
global model also gives extra explainability power by devel-
oping experiential and example based explanations, including
testing hypothesis (e.g. does a selected feature set cause the
outcome we expect from traditional wisdom?).
V. XAI INTEGRATION INTO 6G: FRAMEWORK AND
FUTURE CHALLENGES

A. Framework and Open Challenges

In the context of Beyond 5G and 6G, the main areas
that require improved trust are mainly in automation: 1)
transport, 2) precision manufacturing, 3) healthcare, and 4)
human machine brain interface. The framework we propose
in Fig. 5 is a general one which explains to users and
radio engineers the current actions of the network. In our
example scenario, a) service is denied to users that b) the
engineer is aware of through classical mechanisms of network
monitoring and diagnostics. XAI provides differing levels of
explanations to them in order to achieve different goals: c)
user is given a simple didactic statements via APP/Social
Media/brain interface, and d) engineer is given quantitative
analysis via local linear model classification of a key feature
set. Their e) mental states affect their understanding level and
cognitive bias. For f) users - empathy is more important to
maintain high customer satisfaction, and failure to do so can
lead to complaints, compensation, and churn for the network.
For g) engineers - comprehension is critical to affect change
in the network/AI if needed.

The open challenges are numerous and we list the following
two multi-disciplinary areas:

1) Human Machine (Brain) Interface: developing ra-
tional and intuitive interfaces (proprietary or existing)
that communicate (e.g. didactic statements, interactive
visual, brain wave) to users and engineers - without
disrupting their lives and fit into existing workflows and
processes. In particular, it needs to tackle the cognitive
biases in human minds, their mental states, which all
impact on their degree of comprehension and empathy.
The recent advances in human-brain interfacing [1]
for tactile control and shared intelligence presents a
futuristic framework for XAI.

2) XAI Twin: develop an explainable twin AI system to
work in parallel to the deep learning systems that are
designed for optimisation performance. The XAI twin
enables us not to sacrifice performance trade-off (as seen
in Fig. 2, whilst offering intuitive explanations. Recent
work to develop a Neuro-Symbolic Concept Learner
(NS-CL) agent that mimics human concept learning,
able to translate back to the language description of the
features [15].

3) Defence against Attacks: We saw in the example given
in Fig 5h [9] that a targeted noise input (with no clear
features) can lead to catastrophic errors to a well trained
DL engine. How can we develop defence mechanisms
that can recognise targeted attacks against DL and XAI
engines?

B. Conclusions

As 6G will need to enable greater levels of autonomy
across a wide range of industries, building trust between
human end users and the enabling AI algorithms is critical.
At the moment, we simply don’t understand a wide range
of deep learning modules that contribute to PHY and MAC

layer roles, ranging from channel estimation to cross-RAT
access optimisation. The need for increased explainability to
enable trust is critical for 6G as it manages a wide range of
mission/safety critical services as well as interfacing human
brain and machines directly. In this review, we outlined the
core concepts of Explainable Artificial Intelligence (XAD
for 6G, including: public and legal motivations, definitions
of explainability, performance vs. explainability trade-offs,
methods to improve explainability, and proposed a framework
to incorporate XAI into future wireless systems. Our review
has been grounded in cases studies for both PHY and MAC
layer optimisation, and provide the community with an
important research area to embark upon.

Acknowledgements: The author wishes to acknowledge
EC H2020 grant 778305: DAWNA4IoE - Data Aware Wireless
Network for Internet-of-Everything, and The Alan Turing
Institute under the EPSRC grant EP/N510129/1.

REFERENCES

[1] W. Saad, M. Bennis, and M. Chen, “A Vision of 6G Wireless Systems:
Applications, Trends, Technologies, and Open Research Problems,”
IEEE Network, 2020.

[2] R. Li, Z. Zhao, X. Zhou, G. Ding, Y. Chen, Z. Wang, and H. Zhang,
“Intelligent 5G: When Cellular Networks Meet Artificial Intelligence,”
IEEE Wireless Communications, vol. 24, no. 5, pp. 175-183, October
2017.

[3] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y. Liang,
and D. L Kim, “Applications of Deep Reinforcement Learning in
Communications and Networking: A Survey,” JEEE Communications
Surveys Tutorials, pp. 1-1, 2019.

[4] U. Challita, W. Saad, and C. Bettstetter, “Interference Management for
Cellular-Connected UAVs: A Deep Reinforcement Learning Approach,”
IEEE Transactions on Wireless Communications, vol. 18, no. 4, pp.
2125-2140, April 2019.

[5] H. Ye, G. Y. Li, and B. Juang, “Power of Deep Learning for Channel
Estimation and Signal Detection in OFDM Systems,” JEEE Wireless
Communications Letters, vol. 7, no. 1, pp. 114-117, Feb 2018.

[6] N. Farsad and A. Goldsmith, “Neural network detection of data se-
quences in communication systems,” JEEE Transactions on Signal
Processing, vol. 66, no. 21, pp. 5663-5678, Nov 2018.

[7] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath,
“Deep reinforcement learning: A brief survey,” JEEE Signal Processing
Magazine, vol. 34, no. 6, pp. 26-38, Nov 2017.

[8] H. Owhadi, C. Scovel, and T. Sullivan, “On the brittleness of bayesian

inference,” SIAM Review, vol. 57, no. 4, p. 566-582, April 2015.

C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,

and R. Fergus, “Intriguing properties of neural networks,” in Jnterna-

tional Conference on Learning Representations (ICLR), 2014, pp. 1-10.

C. Hager and H. D. Pfister, “Nonlinear interference mitigation via deep

neural networks,” in 20/8 Optical Fiber Communications Conference

and Exposition (OFC), March 2018, pp. 1-3.

M. T. Ribeiro, S. Singh, and C. Guestrin, “’why should i trust you?”:

Explaining the predictions of any classifier,” in ACM SIGKDD Interna-

tional Conference on Knowledge Discovery and Data Mining. New

York, NY, USA: ACM, 2016, pp. 1135-1144.

S. Chakraborty, R. Tomsett, R. Raghavendra, D. Harborne, M. Alzantot,

F. Cerutti, M. Srivastava, A. Preece, S. Julier, R. M. Rao, T. D. Kelley,

D. Braines, M. Sensoy, C. J. Willis, and P. Gurram, “‘Interpretability of

deep leaming models: A survey of results,” in JEEE SmartWorld, Aug

2017, pp. 1-6.

D. Abel, D. Hershkowitz, and M. Littman, “Near optimal behavior via

approximate state abstraction,” in JCML, ser. Proceedings of Machine

Learning Research, vol. 48, Jun 2016, pp. 2915-2923.

W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning structured

sparsity in deep neural networks,” in N/PS, Dec 2016, pp. 2082-2090.

J. Mao, C. Gan, P. Kohli, J. Tenenbaum, and J. Wu, “The neuro-symbolic

concept learner: Interpreting scenes, words, and sentences from natural

supervision,” in Jnternational Conference on Learning Representations

(ICLR), 2019, pp. 1-10.

[9

[10]

Qi

[12]

[13]

[14]

[15]
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/pu blication/336641970

Blockchain for explainable and trustworthy artificial intelligence

Preprint in Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery - October 2019

DOL: 10.1002/widm.1340

CITATIONS
6

4 authors:

 

My

gi" ® ~=Mohamed Nassar
ane a .
Tas American University of Beirut

76 PUBLICATIONS 400 CITATIONS

SEE PROFILE

Fr Muhammad Habib ur Rehman
r King's College London

57 PUBLICATIONS 1,296 CITATIONS

SEE PROFILE

Some of the authors of this publication are also working on these related projects:

Project Cloud Computing Security View project

Project Arabic reCAPTCHA for digitizing Arabic manuscripts View project

All content following this page was uploaded by Muhammad Habib ur Rehman on 18 October 2019.

The user has requested enhancement of the downloaded file.

READS
1,781

 

Khaled Salah
Khalifa University

300 PUBLICATIONS 4,270 CITATIONS

SEE PROFILE

Davor Svetinovic
Khalifa University

99 PUBLICATIONS 1,525 CITATIONS

SEE PROFILE

ResearchGate
Received: 14 April 2019 Revised: 22 August 2019 Accepted: 4 September 2019

 

DOI: 10.1002/widm.1340

ED

RE:
FOCUS ARTICLE oe WIRE ss Ano knownsnse vscovey WI LEY

Blockchain for explainable and trustworthy artificial intelligence

Mohamed Nassar! | Khaled Salah?® | Muhammad Habib ur Rehman?© |

Davor Svetinovic” ©

‘Department of Computer Science,

American University of Beirut, Beirut, Abstract
Lebanon The increasing computational power and proliferation of big data are now
*Center for Cyber Physical Systems, empowering Artificial Intelligence (AI) to achieve massive adoption and applica-

Electrical Engineering and Computer bility in many fields. The lack of explanation when it comes to the decisions made

Science, Khalifa University of Science and . . . we .. .
Technology, Abu Dhabi, UAE by today's AI algorithms is a major drawback in critical decision-making systems.

For example, deep learning does not offer control or reasoning over its internal pro-

Correspondence tputs. More important! t black-box AI implementati

Davor Svetinovic, Center for Cyber Physical cesses or outputs. More importantly, current black-box implementations are
Systems, Khalifa University of Science and subject to bias and adversarial attacks that may poison the learning or the inference
Technology, Abu Dhabi, UAE. processes. Explainable AI (XAI) is a new trend of AI algorithms that provide

Email: davor.svetinovic@ku.ac.ae . . 4 .
explanations of their AI decisions. In this paper, we propose a framework for

achieving a more trustworthy and XAI by leveraging features of blockchain, smart
contracts, trusted oracles, and decentralized storage. We specify a framework for
complex AI systems in which the decision outcomes are reached based on
decentralized consensuses of multiple AI and XAI predictors. The paper discusses
how our proposed framework can be utilized in key application areas with practical

uSE Cases.

This article is categorized under:
Technologies > Machine Learning
Technologies > Computer Architectures for Data Mining
Fundamental Concepts of Data and Knowledge > Key Design Issues in Data
Mining
KEYWORDS

Blockchain, Consensus, Prediction models, Reputation, Smart contract

 

1 | INTRODUCTION

Artificial Intelligence (AI) is currently showing transformative impact on a number of fields and industries. Over 70 years of
research and development has resulted in complex deployed AI systems that are starting to face complicated external
issues ranging from security to ethics (Awad et al., 2018; Frank, Wang, Cebrian, & Rahwan, 2019). In particular, AI systems
are facing two major limitations: susceptibility to biases and adversarial attacks (Chen et al., 2019; Ng, 2019). The susceptibil-
ity to biases increases when AI systems correctly perform decisions on a subset of data but they do not perform well across
the whole population. While statistical in nature, these biases may entail societal biases at the detriment of a certain commu-
nity. Embedding AI in socioinstitutional mechanisms is challenging at many levels as discussed in Sileno, Boer, and van

 

WIREs Data Mining Know! Discov. 2019;e1340. wires. wiley.com/dmkd © 2019 Wiley Periodicals, Inc. 1 of 13
https://doi.org/10.1002/widm. 1340
2 of 13 WI LEY_— fg) WIREs NASSAR ET AL.

DATA MINING AND KNOWLEDGE DISCOVERY

Engers (2018). From another side, the adversarial attacks on AI systems occur when some malicious actors (e.g., a software,
or a person) try to manipulate the data which results in wrong decisions (such as misclassification or bad clusters). In addition
to this, current AI implementations represent black-box solutions, therefore, and as such, the need for trustworthy explanations
about potential biases and adversarial attacks is increasing rapidly.

Furthermore, the lack of explanation regarding the internal data representations and the decisions made by AI systems is
currently one of the most important challenges in making AI systems even more widely accepted, especially in mission-critical
domains. For example, we tend to blindly accept a decision recommended by a deep learning system in domains such as
object recognition, game playing, and chatbots, therefore tolerating the possibility of a wrong prediction or a false positive,
but this blind acceptance should not be tolerable when it comes to critical decision-making systems such as security,
healthcare, or finance, where human lives or significant assets are at stake. In response, significant efforts are being made to
make deep learning much more trustworthy and controllable by humans, for example, an explainable AI (XAD) initiative was
launched by the United State Defense Advanced Research Projects Agency (DARPA) (Gunning, 2016). One of the reasons
XAI is required is the increasing number of published adversarial attacks against machine learning and deep neural networks
in particular. These attacks come in different flavors such as dataset poisoning, adversarial examples, internal network manip-
ulation, and side-channel attacks (Papernot et al., 2016).

Malicious actors may cause random or targeted misclassifications by manipulating the environment around the system, the
data acquisition block (e.g., camera or microphone), or the input samples. The attack can be as simple as adding adversarial
noise to the input samples and as stealthy as incrementally shifting the decision boundaries during the training process. A pre-
diction system that lacks comprehensive explanations imposes a take-or-leave response policy. If we know that a system may
be subject to adversarial attacks, our trust in its output fades away since no reasoning, proof or explanation is accompanying
the output.

According to Thompson (1984), it is more important to trust the people who wrote the software than trust the software
itself. But nowadays the AI ecosystem is much more complex with many more stakeholders: the philosopher, the AI
researcher, the data scientist, the data provider, the developer, the library author, the hardware manufacturer, the OS provider,
and so on. Since it is practically infeasible to build trust relationships with and between all stakeholders, put aside decide lia-
bility when things go wrong, we take trustworthiness at a technical level in our scope.

We define technical trustworthiness as the qualitative measure of confidence one can objectively assign to the output of an
AI system. To give a concrete example, let us consider the set of binary classifiers for images of dogs and cats. A system with
98% training accuracy and 97% testing accuracy is more trustworthy than a system with 100% training accuracy and 90% test-
ing accuracy. A system which explains its decisions based on features of heads and tails is more trustworthy than a system that
predicts “dog” whenever the background is just white. The latter is clearly due to overfitting to the training set. To increase its
trustworthiness, an AI system must structurally have elements of explainability, consensus, and historical robustness
reputation.

A broader definition of trustworthiness, which is based on ethics and morality, is required much more than ever (Dignum,
2017). Nevertheless, technical AI and XAI trustworthiness enables the implementation of moral and philosophical algorithms,
that is, algorithms that are based on ethical principles and philosophical solutions of the so-called “trolley problems”
(Keeling, 2019).

In this paper, we specify a framework based on the premise that the critical decisions in complex AI systems must be sub-
ject to a consensus among distributed AI and XAI agents or predictors hosted by trusted oracles with the assumption that the
majority of these agents are honest. Trustworthy AI requirements for resilience to biases and adversarial attacks can be ful-
filled to a large extent by blockchain technologies (Salah, Rehman, Nizamuddin, & Al-Fuqaha, 2019). Blockchain can provide
the following key features for an XAI system (Suliman, Husain, Abououf, Alblooshi, & Salah, 2018):

e Transparency and visibility: All the transactions are stored in a publicly auditable, append-only and transparent ledger.
The ledger state and logs of transactions and function calls are stored in a secure, tamper-proof, decentralized manner that
is accessible by all participating stakeholders.

e Immutability: The blockchain ledger is comprised of timestamped blocks, where each block is secured by a cryptographic
hash. Each block contains a group of transactions and references the hash of the block preceding it. Any change in one of
the blocks will invalidate the entire chain of blocks.

e Traceability and nonrepudiation: Each participating node or user in the blockchain must cryptographically sign each trans-
action or function call; and each signed item must get verified and validated by the mining nodes of the blockchain.
 

NASSAR . 3 of 13
— @ > WIRE Ne AND KNOWLEDGE DISCOVERY —_WI LEY?“

Transactions become a part of the immutable ledger, and transacting users or nodes cannot deny or repudiate invocation of
function calls or transactions.

e@ Smart contracts: A smatt contract (SC) is code that governs the interactions among different participants and allows for the
execution of contractual and business logic in an automated, trusted and decentralized manner, where the execution out-
come of the SC code is validated and verified by all mining nodes, and agreed on by the majority of these nodes.

Therefore, a promising solution to address black-box AI is by shifting our trust from a single prediction system to a set of
distributed predictors, providing predictions and explanations, in which AI predictions and decision outcomes are recorded,
stored, aggregated, and managed in a decentralized, secure, unbiased, and trusted manner by using blockchain, SCs, and
decentralized storage.

The contributions of this paper can be summarized as follows:

e We briefly review the state-of-the-art and discuss key issues pertaining to XAI and adversarial AI.

e We specify a general blockchain framework for providing a more trustworthy and XAI and we highlight its key compo-
nents, functions, and support services that include decentralized registration and reputation.

e We illustrate basic workflow outlining the roles of the four main components used to provide decentralized governance,
reputation, and decision-making.

e We discuss different practical use cases to show how our framework can be used.

2 | STATE-OF-THE-ART

The European Union's new General Data Protection Regulation (GDPR) presents many implications for Al-based decision
systems (Regulation, 2016). Among others, it requires the right of explanation. In case a person is subject to a decision such
as automatic refusal of an online credit application or e-recruiting practices, she has the right to obtain an explanation of the
decision reached after such assessment and to challenge the decision. GDPR may highlight opportunities for computer scien-
tists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.
Therefore, it is necessary to be able to understand the behavior of AI systems and to limit adversarial situations. Based on
related works, we suggest an initial taxonomy of XAI methods as shown in Figure 1.

 

 

 

 

 

Methods
Local pe
----- = -------~ approximation
ypes Gradient based
Partial Global

 

 
 
   
    
  
 

 

dependence plot approximation

 

 

Model-agnostic

 

 

 

 

 

Interpretable Al

   

Post-hoc

SS

 

 

 

 

 

 

Accumulated
local effects plot

 

SS

 

 

 

 

i
I
I
I
I
I
I
I
{ M-plot
I
I
I
I
I
I
I
I
\

   
 
 

Visual

  

Bt

 

 

 
   

 

  

 

 
 
  

 

 

  
  

 

   
 

 

 

 

Noo------------ “ ; Surrogate models Global
representations

4 Tools ‘ Explainable Nenana nnn nn nnn 7
1 !
1 1
I CNN-Viz | Al
1 1
i I (
{ ! Counterfactual -
RNN-VIZ explanations Adversarial shteration
i J machine learning

 

 
 
  

 

Black box attacks
Anchor

explanations

wee

 

 

  
   

 

FIGURE 1 Types and methods of explainable AI (XAI) techniques
4 of 13 WI LEY_— fg) WIREs NASSAR ET AL.

DATA MINING AND KNOWLEDGE DISCOVERY

2.1 | Interpretable AI

Many machine learning models are inherently interpretable if the training features are primarily meaningful (e.g., the surface
of an apartment or the age of an employee). Humans can understand the cause of decisions made by shallow decision trees
and sparse linear models since these models can be easily translated into if-else rules. Still, the most successful learning
models nowadays are not interpretable since they are mostly based on ensembles of voting submodels such as random forests
or are very complex such as boosted trees and deep neural networks. Several interpretation methods are proposed to deal with
these cases.

Most of these methods are post-hoc (i.e., they consider a trained classification or regression model), and a few are intrinsic
(ie., they are trained to solve a decision task and provide explanations at the same time). Some methods consider global or
modular approximations of the learned model while others provide explanations on a local scale for a single prediction or a
group of predictions. The interpretation methods can also be divided into two main approaches: gradient-based approaches
which are specific to neural networks, and more general, model-agnostic approaches based on local perturbations and counter-
factual examples (Robnik-Sikonja & Bohanec, 2018).

Model-agnostic methods treat the machine learning models as black-box functions (Ribeiro, Singh, & Guestrin, 2016a).
XAI also considers the question of devising viable criteria for evaluating the quality of explanations. Even though the usual
consumer of explanations is the human end-user, fidelity metrics are required when it is difficult to have a human in the loop
to judge good from bad explanations. This is particularly important when the same prediction is given inconsistent explana-
tions, which is also known as the Rashomon effect.

2.2 | Visual interpretation

Visual interpretation makes use of plots to show the importance of features with respect to the final decisions of the model.
The partial dependence plot (PDP) works by marginalizing the machine learning model output to show the relationship
between the predicted outcome and a subset of the features, usually only one or two. A better alternative is the M-plot which
takes conditional probabilities in the dataset in consideration, therefore avoiding uncommon combinations of features. The
accumulated local effects (ALE) plot accumulates the differences in prediction for the data points per interval of the examined
feature, by predicting for the extremities of the interval instead of predicting for the feature value itself. This cancels the effect
of correlation with other features which remain unchanged.

Individual conditional expectation (ICE) and centered ICE plots are similar to PDP but these plots consider the data
instances one by one and track the change in prediction with respect to the change in only one feature. Recent advances in
visual interpretation for deep learning are discussed in Choo and Liu (2018).

2.3 | Feature contribution

Methods that are not necessarily visual are more important in the context where machines have to take the decision rather than
humans. Actual metrics for feature importance and feature interaction were also investigated (Datta, Sen, & Zick, 2016). An
interesting idea is to consider that features are players in a coalition game, the prediction value as the total reward of this game
and to represent the individual feature contribution by its Shapley value (Strumbelj & Kononenko, 2014).

2.4 | Surrogate models

A model-agnostic approach is to take a machine learning box and try to simulate it using a surrogate model which happens to
be interpretable (Ribeiro, Singh, & Guestrin, 2016b). The goal is to find a simple function g among a set of interpretable func-
tions G that best emulates the original function f. For example, g is a shallow decision tree while fis a convolutional neural
network. While this task seems intractable at the global level, it suddenly becomes viable at a small, local level. LIME is a
method to find local surrogate models by minimizing a loss function. It takes an instance x and looks for a function g which is
very similar to f in the neighborhood of x. The exact definition of the neighborhood depends on the data type whether it is
images, text, or relational data.
NASSAR . 5 of 13
— @ > WIRE Ne AND KNOWLEDGE DISCOVERY —_WI LEY:

2.5 | Counterfactual examples

 

Most important to our context of critical AJ-based decision-making is the ability to explain a single decision (Wachter,
Mittelstadt, & Russell, 2017). A counterfactual explanation is a statement of how the world would have to be different for a
desirable outcome to occur. It describes a causal argument of the form: “if A has not occurred than B would not have occurred.”

In particular, we look up the smallest change to the feature values that would flip the outcome of the prediction. For exam-
ple, if turning color from black to white would change the AI decision for a loan, the counterfactual would be: “If the skin
color was not black, then the loan would not be rejected!” Technically, counterfactuals are computed by minimizing a loss
function which is composed of the norm of change in the feature vector (e.g., d(x,x’)) and the norm of difference between the
perturbed instance prediction and the targeted prediction (e.g., If(x’) -— y’l).

In contrast, an anchor explanation is the subset of features that are sufficient to anchor a prediction regardless of the values
of the other features.

We summarize the XAI techniques based on our taxonomy in Table 1. The limitations of the different methods invite dis-
tributed consensus protocols and trust models which we aim to provide through a blockchain framework.

2.6 | Adversarial machine learning

Counterfactual explanations can be used as adversarial samples to deceive the system (Biggio & Roli, 2018). It was shown
that slightly changing the pixel values of an image can lead a convolutional neural network to make wrong predictions
(Szegedy et al., 2013). The process is even simpler if one has access to the gradients of the learning model (Kurakin,
Goodfellow, & Bengio, 2016). Changing only one pixel is sometimes sufficient to fool the system (Su, Vargas, & Kouichi,
2017). Using such approaches, the changes remain unperceived to the human eye. Moreover, three-dimensional (3D) printed
artifacts are shown to deceive a camera-equipped detection system (Athalye & Sutskever, 2017). An adversarial printable
patch that can be stuck next to objects was also designed by researchers (Brown, Mane, Roy, Abadi, & Gilmer, 2017). Black-
box attacks that do not require internal knowledge about model or training data were also proposed by numerous researchers
(Papernot et al., 2017).

2.7 | Integrating Blockchain with AI systems

Blockchain augments decentralized AI systems by enabling an open-source and publicly accessible digital ledger which is dis-
tributed among AI agents across peer to peer networks (Nebula AI, 2018). It enables AI agents to collaboratively perform con-
sensus and save new decisions on the blocks which could be traced back and difficult to alter. Blockchain provides
transparency and visibility of AI decisions to all participating AI agents on the network hence it becomes difficult for AI
agents to alter or refuse the decisions (Hasan & Salah, 2019). In addition, the programmable blockchain platforms enable
SCs-based programming models for decentralized AI applications which ensure self-execution of AI agents based on
predefined terms and conditions (Marwala & Xing, 2018).

Blockchain provides decentralization, determinism, immutability, data integrity, and resilience against several security
attacks on AI agents and their data. Alternately, AI systems are normally orchestrated around centralized computing and data
storage infrastructures and these systems need to handle continuously evolving data which results in probabilistic and volatile
decision-making. However, the integration of blockchain and conventional AI systems ensures improved data security and

TABLE 1 Comparative table of the different explainable AI XAD methods

Method type Examples Strengths Limitations

Intrinsic Decision tree, regression, rules Direct explanation Limited performance

Visual PDP, ALE, t-SNE User-friendly plots Limited scope of features (1 or 2)

Surrogate LIME, SHAP Blackbox. It works for complex models/ It assumes feature independence.
deep learning Simple local model

Examples Counterfactuals, anchors Blackbox. One-shot explanation Rashomon effect

Features set Feature importance, Shapley Wide scope of features Computational limits

Gradient based Structure occlusion, saliency maps _It works for deep learning and CNN Whitebox, zero gradient problem

Notes: ALE, accumulated local effects; CNN, convolutional neural network; PDP, partial dependence plot.
6 of 13 WI LEY_— fg) WIREs NASSAR ET AL.

DATA MINING AND KNOWLEDGE DISCOVERY

collective intelligence due to consensus-based decentralized data and decision storage mechanisms (Salah et al., 2019). It also
ensures improved trust and high efficiency brought by multiparty/multiagent decision-making systems that follow various con-
sensus protocols. In previous work, we presented a detailed taxonomic discussion of key concepts to enable blockchain for
decentralized AI applications. Considering the scope of this article, we will limit this discussion in this paper and we will urge
the interested readers to follow (Salah et al., 2019) for further understanding. Considering the limitations in recent XAJ-related
literature and integration benefits of blockchain technologies, we propose a blockchain-based framework to deal with the sub-
tleties of explainable and adversarial AI decision-making.

3 | BLOCKCHAIN-BASED FRAMEWORK FOR TRUSTWORTHY AI

While XAI looks very promising, explaining complex models can eventually lead to a dilemma. Providing an explanation
intrinsically means that a simpler model can be devised. In an interview with one of the pioneers of deep learning, Geoffry
Hinton warned regulators that insisting on making AI systems explain how they work would be a complete disaster (Simonte,
2018). In analogy to humans, most people have no idea how they preform and do work. If people are asked to explain their
decision, most of them are forced to make up a story. Neural nets are similar, they learn billions of numbers that represent the
knowledge they have extracted from the training data and use these numbers to make a prediction, say whether a pedestrian is
in an image or not. If there were any simple rules for deciding whether an image contains a pedestrian or not, it would have
been a solved problem long time ago. Nevertheless, having many audited systems to decide on the prediction outcome and
produce consistent explanations can make such systems more trustworthy.

Our approach follows this direction of polling multiple nodes or predictors that run AI computational models, and provide
explanation to AI outcomes, and gradually build a reputation for each of these predictors. In our approach, we allow the sys-
tems to provide explanations of their predictions either directly or through other model-agnostic explanation systems. A plau-
sible explanation of a prediction would definitely contribute to the reliability of the decision made. The interpretation of
explanation systems, including the decision outcomes, need to be audited in an immutable, tamper-proof, and decentralized
way, and in a way that can be traced and tracked with high reliability and resiliency. Resiliency is robustness against tamper-
ing attacks. In case a party changes one bit of information in a block, the hash of the block, which must lead to the previous
block in the chain, is broken. Subsequently the whole chain of hashes is broken since it will not lead to the genesis block, even
when all previous blocks have been tampered with. Reliability comes from the fact that each node has a full copy of the led-
ger, and hence if any node fails or goes off, the blockchain remains unaffected. These latter features can be facilitated and pro-
vided by blockchain platforms and decentralized storage systems.

3.1 | Honesty and incentives

The ecosystem of our proposed solution includes AI and XAI nodes or predictors that act as trusted oracles, perform computa-
tion and interact with blockchain SCs which record and log execution outcomes and decisions in the blockchain immutable
ledger. The fact that execution and decision outcomes by predictors get validated and verified, and then logged in the ledger
in an immutable manner, and accessible by all participants is a strong incentive for an AI system to act honestly in carrying
out computation. Moreover, penalties can be imposed in case of reporting false outcomes. SCs can reach final decision out-
comes by vote of the majority of the reported outcomes.

In this context, the predictors act as oracles that report results to the SCs. Prior to the use, these oracles have to be regis-
tered with public or private owner, and each oracle has a reputation that gets built over time by users through the use of
blockchain SCs. Reputation and registration of these oracles are done in a decentralized manner through the use of blockchain
SCs, as we discuss later in our proposed framework shown in Figure 2. A good reputation would thrive the business of the
oracle owner in the AI market and therefore is a very viable incentive for accurate and correct conduct.

3.2 | Participants
The participants or stakeholders in our proposed solution primary include the following three types:
e Frontend users who are interested in performing a task requiring AI prediction or computation on data (e.g., classification,

clustering, regression, etc.). The user may also require explanations about some decisions and provide positive or negative
feedback about the decisions.
NASSAR . 7 of 13
— @ > WIRE Ne AND KNOWLEDGE DISCOVERY —_WI LEY 78

 

 

Frontend Decentralized Applications (DApps) and Dashboards for access, control, and management

: - :

Backend Decentralized Services and Platforms

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Access Layer
Rest JSON
Web3 HTTP RPC JMS SOAP
Al Layer Support Services
Al Predictors XAI Predictors negistration : — Repu tation a
Par,||P. P. Py ar,|| Px Px Service ame, me my
Al; Alg eee) LAT, X AI; XATz Joes} XAT, co
__ -_
A . Report reputation
Register, make Record registry and
Register, make Report to smart and explain Report to smart profile information scores to frontend
decisions and contracts hashes decisions and contacts hashes or about predictors apps about predictors
store metadata decision metadata explained decision
store metadat metadata
S ra X 7 A
Blockchain Platform \ a 7
Smart Contracts
= a
v— v= v=
v—| | eS
v— T= V— ‘=
A = taka
Decentralized File System or BLOB Sty SC,
(IPFS/FileCoin, Eth Swarm, Storj,
BigChainDB+IPDB, IOTA, etc.)

 

 

 

 

 

FIGURE 2 _ Blockchain framework for trustworthy artificial intelligence (AD

e A Predictor (Pa; which has a trained AI model for a given task.
e A Predictor with Explanations (Pxa)) which has a trained AI model and a corresponding model-specific interpretation
model.

3.3 | Blockchain design and framework

In this section, we present our blockchain-based design and framework to achieve explainable and trustworthy AI. The frame-
work is leveraging blockchain SCs to record, govern interactions, and provide consensus for AI predictions and outcomes
among AI and XAI oracles. The framework includes also decentralized storage, registration, and reputation support services.
Figure 2 presents the overall system architecture and design of our proposed framework. The framework components are
grouped into two distinct subsystems: (1) fronted decentralized applications (DApps) and (2) backend decentralized services
and platforms.

3.3.1 | Frontend DApps

Our proposed system for running explainable and trustworthy AI applications supports a variety of decentralized frontend
applications that can be used by the different users, stakeholders, or interested parties. The DApps can come in different forms
ranging from CLI-based interface applications to interactive mobile or web-based dashboard applications. The interface for
the frontend DApps allows configuration, various parameterization, selection of the number of AI or XAI predictors/oracles
and their types, accessibility of data, decision outcomes, reputation and registration services, interpretability, and traceability
of decision outcomes.

3.3.2 | Backend services and platforms

The system components of our framework backend are categorized and grouped into four modules namely: (1) Access layer,
(2) AT layer, (3) support services, and (4) blockchain platform.
8 of 13 WI LEY_— fg) WIREs NASSAR ET AL.

DATA MINING AND KNOWLEDGE DISCOVERY

Al access layer

The Access Layer enables interfaces for variety of data transfer protocols. It enables Web3 interface for direct communication
between DApps and blockchain platform. It uses JEON-RPC API which facilitates in data transfer between web-enabled appli-
cations and Ethereum blockchain network using remote procedure calls (RPC). The JSON-RPC is a light-weight, state-less
RPC protocol which enables multiple levels of communication via sockets, processes, http, and a variety of other message
passing environments using JSON's RFC 4627 data format. Moreover, the layer includes conventional communication proto-
cols and APIs, such as REST Http to communicate with cloud data centers, JSON-RPC for client-server communications with
centralized repositories, Java message service (JMS) API for communication within physical and virtual application compo-
nents, and simple object message protocol (SOAP) for data communication between sensors-based data sources and backend
blockchain platform.

Al layer

Al layer is the primary layer of our proposed framework whereby all data processing and knowledge discovery operations to
produce trustworthy, collaborated, and agree-upon decisions are performed. The layer is composed of two different types of
predictors; namely: AI predictors (Pay ; to Pa; ,) and XAI predictors (Px az; to Pxaz m), aS Shown in Figure 2. Depending on
the configuration received from frontend DApps, AI and XAI nodes either run on raw data and perform all prepossessing
operations (such as data cleaning, noise removal, outliers' detection, feature extraction, dimensionality reduction, etc.), or the
nodes directly perform decisions on already processed data by inputting into learning models and generating decisions
accordingly.

The AI predictors operate on the data using conventional black-box AI algorithms, and produce decisions. On the other
hand, the XAI predictors differ in processing mechanisms whereby they produce summary information in addition to decision
outcomes to assist in explaining these outcomes. XAI predictors may also communicate with other AI predictors to provide
them with explanation capabilities and build a summary of explainable decisions.

It is worth mentioning that there is a cost-trust trade off in selecting the number of predictors. Increasing the number of pre-
dictors can obviously increase majority reporting and the mitigation of collusion and dishonesty among predictors, but at the
same time, there will be an obvious increase in cost of leasing more prediction nodes to perform the AI task.

Support services

This component provides two types of support services; namely: (1) registration service and (2) reputation service. The regis-
tration service enables registering and managing actors and participants in the ecosystem. These participants include users, AI
and XAI prediction service providers, data hosting services and repositories, decentralized storage services, as well as reputa-
tion services. The reputation service computes and maintains (in a decentralized manner using SCs) the reputation of AI and
XAI predictors. Prior to running AI applications, users through DApps can select predictor nodes that are highly reputable.
SCs responsible to run AI applications on predictors can be set up to automatically report a reputation score to a reputation SC
at the completion of running each AI task. AI and XAI predictors that will not report decision outcomes matching the majority
of the predictors will get a negative score, and be penalized in terms of payment rewards which will be paid automatically in
cryptocurrency. This forces predictor nodes to act honestly.

Blockchain platform

The Blockchain Platforms include primarily: (1) the blockchain network to run different SCs, and (2) a decentralized storage
to store results and metadata reported by the AI and XAI predictors. Blockchain is not suitable and highly expensive for stor-
ing large size data and content. Also traditional centralized cloud storage or local storage cannot be trusted and can be single
point of manipulation, tampering, compromise, and failure. Decentralized storage systems (such as IPFS, Eth SWARM, or
Storj, etc.) are stronger and viable alternative. IPFS (interplanetary file system) is a content-addressable, peer-to-peer file sys-
tem in which the file content is stored on multiple IPFS nodes, and the content hash is the actual address of the file (Benet,
2014). The hashes are used by the SC code for comparison and matching.

The SCs are of different types as depicted in Figure 3. The Registration SC enables decentralized registration and identifi-
cation of AI/XAI predictors on the blockchain network. This SC allows for associating attributes related to ML/AI compute
capabilities, performance, URL, and so on. Al-task SC is responsible for collecting the final decision outcome from the
Aggregator SC and reporting the results back to the frontend DApp. The Aggregator SC is responsible for receiving and com-
paring the outputs (in form of hashes stored on IPFS) from the registered predictors which were previously selected by the
user through frontend DApps, and subsequently reporting the final decision to the AJ-task SC based on the majority.
NASSAR . 9 of 13
— @ > WIRE Ne AND KNOWLEDGE DISCOVERY —_WI LEyY1°“

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Support Services Frontend DApp
Registration Reputation > Data Access, Control,
Service Service and Management
A A

A >

A, : a
og ' Retrieve average Upload Al task SC with SLA requirements R
— S reputation scores ee eee 8
g 38 r I 3
o 3 3
6 2 | | =
2 s Reputation SC I Al-task SC I g
° 2 I | °
VO 2 Update reputation | A | g
e 2 scores I | 3
—E 3 I |g
Cr. 2 I | =
= T . . I | =
= Registration SC Aggregator SC 3
2 | 1 |&8
5 Ite tL J at
a

Request to register Report hashes of outcomes
with XAI metadata
ot i Store metadata
of outcome
XAl

Al Predictors Predictors

 

 

 

 

 

Al/XAlI Oracles

 

 

IPFS

 

Predictor Registration

FIGURE 3 Basic operational workflow of the framework

Moreover, the Aggregator SC is responsible for dispersing payments to the oracles as well as reporting a reputation score for
each predictor to the Reputation SC. Dishonest predictors will be penalized in terms of payment reward and reputation score.
Reputation SC will receive scoring from the different Aggregator SCs and compute the temporal accumulative score for
predictors.

3.4 | Basic workflow

The frontend DApp will interface directly to the backend decentralized platform and support services through standards web
and RPC interface protocols that may include JSON-RPC, JMS, SOAP, and so on. The DApp will have the ability to select
registered predictors and define the execution SLA (service-level agreement) parameters including latency, pricing, penalties,
and so on. The DApp will also upload the AI task and Aggregator SCs which will govern the SLA agreement, and will notify
selected predictors of the sources of data that will be used by predictors to carry out computation and analytics, and report
their results to the Aggregator SC. At this stage, communication between DApp and predictors is carried out off the chain
(Le., with no transactions being sent to the SCs through blockchain network). If predictors accept the SLA agreement (or AI
task/prediction proposal), they send a commitment transaction of acceptance to the Aggregator SC. The predictor can show
10 of 13 WI LEY_— fg) WIREs NASSAR ET AL.

DATA MINING AND KNOWLEDGE DISCOVERY

seriousness of such commitment by sending a deposit along with commitment transaction in Eth (Ethereum cryptocurrency),
as implemented in Hasan and Salah (2018). This Eth deposit can be the same price of computation reward, and it can be lost
in case the predictor fails to act honestly or reports results not in agreement with the majority of results reported by other pre-
dictors. If the predictor acts honestly, it will get back twice the deposit amount, which consists of one for the reward price and
the other for the commitment deposit.

3.4.1 | Oracle registration and predictor selection

Our proposed blockchain-based design and framework will enable trusted oracles, which are publicly available prediction ser-
vice providers, to register through registration SC. The oracle node can host multiple AI and XAI predictors, and provide reg-
istry attributes about prediction and decision-making types and capabilities, pricing model, and its abilities to satisfy different
SLA objectives in terms of latency and speed, and to handle various input data types, and so on. The frontend DApps will use
such attributes, as well as information obtained from registration and reputation services to select the most reputable, afford-
able, and well performing predictors to carry out the required prediction.

3.4.2 | Decision-making

The proposed framework is perceived to be designed as an open public platform, therefore, different oracles can enable differ-
ent types of AI/XAI predictors to solve the same problem. For example, a few predictors can use tree-based algorithms and
others can use neural network based or probabilistic model-based algorithms for classification decisions. However, the varia-
tion between decisions among different AI and XAI predictors will be obvious due to multiple reasons such as the computa-
tional structure of AI algorithms, types of inputs and outputs, types of decisions, and the quality of training data used for
learning models or optimization algorithms. Therefore, we can broadly categorize AI/XAI predictors into following two types
(Bohanec, Borstnar, & Robnik-Sikonja, 2017).

e Deterministic predictors: Deterministic predictors produce exact decisions which are purely based on the input and training
data. These predictors produce Yes/No, True/False, Positive/Negative, Present/Not-present type of decisions. However,
only a few predictors such as supervised binary classifiers (e.g., logistic regression, decision trees, support vector machines,
etc.) belong to this category.

e Nondeterministic predictors: The nondeterministic or probabilistic predictors produce inexact decisions. Most of the exis-
ting AI/XAI predictors belong to this category. The inexact predictors perform all types of AI decisions such as classifica-
tion, clustering, frequent pattern mining, and optimization on input data.

By design, blockchain SCs produce outcomes that have to be exactly the same in order for all blockchain mining nodes to
reach consensus. This entails a transformation of inexact and probabilistic decision outcomes to exact and deterministic out-
comes. The transformation of decisions through frontend DApp will be a possible option whereby the frontend DApp will
specify the interpretation rules (e.g., accuracy ranges or intervals) as part of SLA proposal, and the predictors will produce
exact specified value against each range of inexact decisions.

3.4.3 | Aggregated decisions

AI/XAI predictors hash and store the metadata of decision outcomes on the IPFS. These decision outcomes include types of
decisions, values of evaluation metrics (e.g., levels of accuracy of classifier), confidence values, explanations about decisions,
and types of explanations. The aggregator SC will compare the reported hashes from predictors and it will determine the cor-
rect decisions based on the majority. The aggregator SC will also ensure that the predictors are satisfying the SLA require-
ments set by the user DApp, and it will report the final result to AI-task SC which will report back to frontend DApp for final
feedback or approval. The AI-task SC will feedback the data and hashes of correct decision outcomes to all participating ora-
cles to produce correct decisions on future input data.
NASSAR STA gS > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI Leyte
4 | REAL-WORLD USE CASES

 

Our blockchain framework can facilitate and support diverse types of real-world AI systems and applications, in which AI/ML
decision outcomes and their explanations can be more trusted and presented in a manner that is decentralized, tampered-proof,
and undisputed with traceability and immutable logs that are accessible by all stakeholders. We list a few use cases to illustrate
such wide applicability.

4.1 | Medical image diagnosis

Recent research works show the emergence of deep medical image analytics for preventive and precision medicines (Lu &
Harrison, 2018). Our proposed framework can advance this research by enabling a decentralized disease diagnose system for
radiologists who can deploy DApps for consensus-based disease detection. The frontend DApps will input the radiology
images whereby AI and XAI predictors from different radiology labs can produce decisions and explanations to help radiolo-
gists to reach a more trustworthy, explainable, traceable, and unbiased conclusion. All interested parties including physicians,
radiologists, insurance companies, patients, and their care takers can run DApps to perform decentralized predictions and
access the same results and explanations.

4.2 | Customer profiling

Banking and insurance companies can also benefit from the proposed framework by analyzing massively generated big cus-
tomer data on social networks and online web portals. Financial institutions perform a background check of potential bor-
rowers or policyholders to minimize the risks of investments. It is envisioned that predictors will be able to determine the
credit-worthy customers by detecting anomalies in a customer's previous credit histories, medical profiles, and demographic
information. Regulators, financial institutions, credit bureaus, as well as customers, through frontend DApps, will all have
access to undisputed, unbiased, trusted decision outcomes and explanations.

4.3 | Tax auditing and fraud detection

Recent regulations by Financial Action Task Force and International Monetary Fund are compelling governments to take strict
actions against money launderers and tax defaulters in order to stop illegal funding and streamline tax collections (FATF,
2018). Governments can harness citizen's big data and analyze their banking transactions, income sources, and tax-payment
histories to curb money-laundering and tax frauds. Hence, governments can benefit from the proposed framework to detect
tax evasions and frauds. Our blockchain-based framework allows interested parties including judges, government, attorneys,
and citizens to access trustworthy and undisputed decisions and explanations in which money-laundering and tax-evasion pat-
terns are detected.

4.4 | Voting and election predictions

The proposed framework can aid government investigation agencies and interested parties to detect and explain fraud in cou-
nting votes, or in illegal activities or financing in election campaigns. Different stakeholders will be able to access traceable,
trustworthy, explainable, and undisputed prediction outcomes as a result of performing analytics on different aggregated
datasets that may include financial records, social media content, emails, and so on.

4.5 | Use cases for real-time AI applications

The examples above involve the applicability of our framework for nonreal-time situation, in which decisions are made on
nonreal time data. For trustworthy and explainable real-time prediction decisions, which may involve for example a fatal acci-
dent by an autonomous vehicle, our framework can be utilized in analyzing and explaining the decisions made by the autono-
mous vehicle prior to the accident, based on the collected datasets gathered by the different sensing devices deployed on the
vehicle. However, it is envisioned that in the future, as blockchain technology matures and overcomes issues related to perfor-
mance, speed, and scalability, our framework can be leveraged for real-time predictions that are more trustworthy,
12 of 13 WI LEY_— G) WIREs NASSAR ET AL.

DATA MINING AND KNOWLEDGE DISCOVERY

decentralized, unbiased, and resilient to adversarial attacks. This realization also entails minimal network latency and higher
prediction speed on the part of the predictors.

5 | CONCLUSION

Today's regulations increasingly require the interpretation of AI models and explanations of individual predictions. This has
led to a new research domain of XAI. At the same time, the deep learning community is skeptic about the real merits of XAI
systems, arguing that analogously to the human brain, the best AI systems do not know how to explain their work. In this
paper, we have proposed a blockchain-based framework to create a more resilient, trustworthy, XAI system that can reduce
biases and adversarial attacks. We presented several use cases to show how blockchain SCs combined with decentralized stor-
age can be leveraged to achieve a trustworthy XAI. Our framework can be used as a reference model to develop a more trust-
worthy decentralized AI and XAI systems and applications. Technical trustworthiness as a measure of acceptance of decisions
by DApp users have requirements of consensus, economic models and incentives for honesty, explainability, and robustness
of predictors. In addition, many more infrastructure requirements are needed such as security, privacy, reliability, usability,
dependability, performance, and governance. The emerging blockchain technology seems the most adequate, if not the only
one, to fulfill these requirements. Still, many challenges must be tackled, the most important ones are minimizing human in
the loop for validating explanations and real timeliness for certain applications.

CONFLICT OF INTEREST

The authors have declared no conflicts of interest for this article.

AUTHOR CONTRIBUTIONS

Mohamed Nassar: conceptualization, writing-original draft. Khaled Salah: conceptualization, resources, supervision, writing-
teview, editing. Muhammad Habib Ur Rehman: writing-review and editing-lead. Davor Svetinovic: resources, supervision,
editing.

ORCID

Khaled Salah ® https://orcid.org/0000-0002-23 10-2558
Muhammad Habib ur Rehman © https://orcid.org/0000-000 1-742 8-2272
Davor Svetinovic © https://orcid.org/0000-0002-3020-9556

RELATED WIREs ARTICLES

Tunnel crack detection using coarse-to-fine region localization and edge detection
Hierarchical third-order tensor decomposition through inverse difference pyramid based on the three-dimensional Walsh-

 

 

Hadamard transform with applications in data mining

 

REFERENCES

Athalye, A., & Sutskever, I. (2017). Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397.

Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shari, A., ... Rahwan, I. (2018). The moral machine experiment. Nature, 563(7729), 59-64.

Benet, J. (2014). IPFS-content addressed, versioned, p2p file system. arXiv preprint arXiv:1407.3561.

Biggio, B., & Roli, F. (2018). Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 84, 317-331.

Bohanec, M., Borstnar, M. K., & Robnik-Sikonja, M. (2017). Explaining machine learning models in sales predictions. Expert Systems with Applica-
tions, 71, 416-428.

Brown, T. B., Mane, D., Roy, A., Abadi, M., & Gilmer, J. (2017). Adversarial patch. arXiv preprint arXiv:1712.09665.

Chen, T., Liu, J., Xiang, Y., Niu, W., Tong, E., & Han, Z. (2019). Adversarial attack and defense in reinforcement learning-from AI security view.
Cybersecurity, 2(1), 11.

Choo, J., & Liu, S. (2018). Visual analytics for explainable deep learning. arXiv preprint arXiv:1804.02527.
NASSAR - 13 of 13
— @ > WIRE Mn AND KNOWLEDGE DISCOVERY —_WI LEY_| 28

Datta, A., Sen, S., & Zick, Y. (2016). Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In
2016 IEEE symposium on security and privacy (pp. 598-617), San Jose, CA.

Dignum, V. (2017). Responsible autonomy. arXiv preprint arXiv:1706.02513.

FATF. (2018). FATF fintech & regtech initiatives. [Online]. Retrieved from https://www. fatf- gafi.org/fintech-regtech/fatfonfintechregtech/?hf=10&
b=0&s=desc(fatf_releasedate)

Frank, M. R., Wang, D., Cebrian, M., & Rahwan, I. (2019). The evolution of citation graphs in artificial intelligence research. Nature Machine Intel-
ligence, 1(2), 79.

Gunning, T. (2016). Explainable artificial intelligence (XAI). [Online]. Retrieved from https://www.darpa.mil/program/explainable-artificial-
intelligence

Hasan, H. R., & Salah, K. (2018). Blockchain-based proof of delivery of physical assets with single and multiple transporters. JEEE Access, 6,
46781-46793.

Hasan, H. R., & Salah, K. (2019). Combating deepfake videos using blockchain and smart contracts. IEEE Access, 7, 41596-41606.

Keeling, G. (2019). Why trolley problems matter for the ethics of automated vehicles. Science and Engineering Ethics, 1-15. https://doi.org/10.
1007/s11948-019-00096-1

Kurakin, A., Goodfellow, L, & Bengio, S. (2016). Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533.

Lu, L., & Harrison, A. P. (2018, July). Deep medical image computing in preventive and precision medicine. EEE Multimedia, 25(3), 109-113.
https://doi.org/10.1109/MMUL.2018.2875861

Marwala, T., & Xing, B. (2018). Blockchain and artificial intelligence. arXiv preprint arXiv:1802.04451.

Nebula AI, T. (2018). Nebula ai (nbai)decentralized ai blockchain whitepaper. Retrieved from https://neironix.io/documents/whitepaper/4082/
NBAL _whitepaper_EN. pdf

Ng, A. (2019). Ai for everyone. Retrieved from https://www.coursera.org/learn/ai-for-everyone/home/welcome

Papernot, N., McDaniel, P., Goodfellow, L, Jha, S., Celik, Z. B., & Swami, A. (2017). Practical black-box attacks against machine learning. In Pro-
ceedings of the 2017 ACM on Asia conference on computer and communications security (pp. 506-519), Saarbrucken, Germany.

Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., & Swami, A. (2016). The limitations of deep learning in adversarial settings. In
Security and privacy (Euros&p), 2016 [EEE European symposium on (pp. 372-387).

Regulation, G. D. P. (2016). Regulation (EU) 2016/679 of the European parliament and of the council of 27 April 2016 on the protection of natural
persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46. Official Journal of
the European Union (OJ), 591-88), 294.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016a). Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016b). Why should I trust you? Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SigKDD international conference on knowledge discovery and data mining (pp. 1135-1144).

Robnik-Sikonja, M., & Bohanec, M. (2018). Perturbation-based explanations of prediction models. In Human and machine learning (pp. 159-175).
Cham, Switzerland: Springer.

Salah, K., Rehman, M. H. U., Nizamuddin, N., & Al-Fugaha, A. (2019). Blockchain for AI: Review and open research challenges. IEEE Access, 7,
10127-10149. https://doi.org/10.1109/ACCESS.2018.2890507

Sileno, G., Boer, A., & van Engers, T. (2018). The role of normware in trustworthy and explainable AI. arXiv preprint arXiv:1812.02471.

Simonte, T. (2018). Googles ai guru wants computers to think more like brains. [On-line]. Retrieved from https:/Avww.wired.com/story/googles-ai-
guru-computers-think-more-like-brains/

Strumbelj, E., & Kononenko, L. (2014). Explaining prediction models and individual predictions with feature contributions. Knowledge and Informa-
tion Systems, 41(3), 647-665.

Su, J., Vargas, D. V., & Kouichi, S. (2017). One pixel attack for fooling deep neural networks. arXiv preprint arXiv:1710.08864.

Suliman, A., Husain, Z., Abououf, M., Alblooshi, M., & Salah, K. (2018). Monetization of IoT data using smart contracts. JET Networks, 8(1),
32-37.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, L, & Fergus, R. (2013). Intriguing properties of neural networks. arXiv
preprint arXiv:1312.6199.

Thompson, K. (1984). Reflections on trusting trust. Communications of ACM, 27(8), 761-763.

Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual explanations without opening the black box: Automated decisions and the GDPR.
Harvard Journal of Law & Technology, 31(2), 2018.

 

How to cite this article: Nassar M, Salah K, ur Rehman MH, Svetinovic D. Blockchain for explainable and
trustworthy artificial intelligence. WIREs Data Mining Knowl Discov. 2019;e1340. https://doi.org/10.1002/widm.1340
Expert Systems With Applications xxx (xxxx) Xxx

 

    

ELSEVIER

Contents lists available at ScienceDirect
Expert Systems With Applications

journal homepage: www.elsevier.com/locate/eswa

 

 

Shapley-Lorenz eXplainable Artificial Intelligence

Paolo Giudici®’, Emanuela Raffinetti b

* Department of Economics and Management, University of Pavia, Via San Felice 5, 27100 Pavia, Italy
> Department of Economics, Management and Quantitative Methods, University of Milan, Via Conservatorio 7, 20122 Milano, Italy

 

ARTICLE INFO ABSTRACT

 

Keywords:

Shapley values
Lorenz Zonoids
Predictive accuracy

Explainability of artificial intelligence methods has become a crucial issue, especially in the most regulated fields,
such as health and finance. In this paper, we provide a global explainable AI method which is based on Lorenz
decompositions, thus extending previous contributions based on variance decompositions. This allows the
resulting Shapley-Lorenz decomposition to be more generally applicable, and provides a unifying variable

importance criterion that combines predictive accuracy with explainability, using a normalised and easy to
interpret metric. The proposed decomposition is illustrated within the context of a real financial problem: the

prediction of bitcoin prices.

 

1. Introduction

The growing availability of data and computational power allows to
develop machine learning models that are highly predictive. On the
other hand, the consideration of the possible adverse consequences on
activities that have a high societal impact has led policy makers and
regulators to a degree of suspicion towards AI applications. To foster
innovations while protecting the society, consensus is emerging on the
development of eXplainable AI (XAI) methods, that is, methodologies
able to make machine learning models interpretable and, therefore,
understood, particularly in terms of causal discovery.

Indeed, in the recent years, the increasing diffusion of artificial in-
telligence applications and products has led policy makers and regula-
tors to demand the underlying machine learning models to be
explainable, so that human users could understand them: see, for
example, the recent paper by European Commission (2020). This
requirement is particularly evident in highly regulated economic sec-
tors, such as health and finance.

In line with the policy requirements, researchers have recently
addressed the issue of how a machine learning model can be made
explainable. Existing papers address the contents to different explana-
tion classes. A detailed review of these methods can be found in Guidotti
et al. (2018). In this paper, the focus is only on two approaches: global
explanations and local explanations. This because our proposal is the
result of the combination of local and global explanations. While global
explanations describe the model as a whole, in terms of which explan-
atory variables most determine its predictions, for all the statistical

* Corresponding author.

units, local explanations aim at interpreting individual predictions, at
the single statistical unit level (for a recent review and comparison, see
eg. Aas, Jullum, & Loland, 2020; Joseph, 2019; Molnar, 2020). Among
the local explanation methods, the Shapley value approach, originally
introduced in Shapley (1953) and implemented by Lundberg and Lee
(2017) and Strumbelj and Kononenko (2010), is gaining a remarkable
relevance due to its attractive characteristics. According to the Shapley
value procedure, the total change in prediction is divided among the
features in a way which is fair to their contributions across all possible
sets of features. Note that to obtain reliable explanations, the Shapley
value method resorts to all the features. The advantage of Shapley
values, over alternative XAI methods, is that they can be used to measure
the contribution of each explanatory variable for each point prediction
of a machine learning model, regardless of the underlying model itself
(see e.g. Lundberg & Lee, 2017; Strumbelj & Kononenko, 2010). In other
words, Shapley based XAI are model agnostic so that, differently from
the model specific approaches, their interpretation tools are not limited
to their respective model classes or data, allowing generality of appli-
cation and personalisation of their results (they can explain any single
point prediction) to be achieved.

Our purpose is to combine the interpretability power of the local
Shapley value approach with a more robust global approach, as in Owen
and Prieur (2017) and Song, Nelson, and Staum (2016). To this aim, we
apply the Shapley value game theoretic approach to Lorenz Zonoid
model accuracy tool, recently proposed by Giudici and Raffinetti (2020).
In such a way, the advantages associated with the local approach based
on the Shapley values are exploited together with the properties of the

E-mail addresses: paolo.giudici@unipv.it (P. Giudici), emanuela.raffinetti@unimi.it (E. Raffinetti).

https://doi.org/10.1016/j.eswa.2020.114104

Received 10 May 2020; Received in revised form 29 July 2020; Accepted 6 October 2020

Available online 16 October 2020
0957-4174/© 2020 The Authors.

Chttp://ereativecommons.org/licenses/by-ne-nd/4.0/).

Published by Elsevier Ltd.

This is an open access article under the CC BY-NC-ND license

 

 

Please cite this article as: Paolo Giudici, Emanuela Raffinetti, Expert Systems With Applications, https://doi.org/10.1016/j.eswa.2020.114104

 

 
P. Giudici and E. Raffinetti

Lorenz Zonoids, giving rise to a global approach which fulfills the
interpretability requirement.

On the graphical view point, the Lorenz Zonoids can be seen as a
generalisation of the ROC curve in a multidimensional setting. More-
over, in one-dimensional setting, the Lorenz Zonoid is related to the
AUROC (Area Under the ROC curve) measure. Therefore, our proposal
has the advantage of combining predictive accuracy and explainability
performance into one single diagnostics, as highlighted in Giudici and
Raffinetti (2020). Furthermore, the nature of Lorenz Zonoids allows
them to be easily replicated to any subset of the available units, allowing
the diagnostics to be easily applied at any desired local level.

The main contributions of our work are, in summary: (a) the intro-
duction of a novel global explainable AI framework, based on the
combination of Lorenz Zonoids with the Shapley value approach; (b) the
mathematical derivation of the exact expression of a novel Shapley-
Lorenz decomposition, that can explain any machine learning model
in terms of the contribution of each explanatory variable to the Lorenz
Zonoid goodness of fit.

Our proposal lies within the field of explainable AI methods. It ex-
tends the global decompositions of Owen and Prieur (2017) and Song
et al. (2016), based on the (euclidean) variance decomposition, to a
decomposition based on Lorenz Zonoids. The Lorenz Zonoid decompo-
sition presents similarities with the classical variance decomposition.
Both the approaches aim to detect the variables which mainly impact the
phenomenon of interest. Nevertheless, differently from the classical
variance decomposition, the Lorenz Zonoid decomposition is based on
the mutual distance between all observations, rather than deviations
from the mean and, therefore, is more robust to outlying observations.
These features make the Lorenz Zonoid decomposition a promising tool
for further extensions in the AI framework, addressed to the assessment
of the contribution related to each explanatory variable in terms of the
explained mutual variability. As discussed by Giudici and Raffinetti
(2020), this methodology appears more generally applicable and
directly interpretable within a predictive accuracy context, differently
from the approach of Joseph (2019), based on a linear regression
approximation.

The expression of our obtained Shapley-Lorenz decomposition also
shows that it can be considered as a natural extension of the standard
Shapley approach, as it can be calculated not only at the global but also
at the local level, providing, in both cases, a normalised measure that
can be interpreted within the ROC framework.

The paper is organised as follows. In Section 2 we provide some
background on Shapley values. In Section 3 we present our proposal. In
Section 4 we exemplify our proposal in the context of a real application
that concerns the prediction of bitcoin prices. Section 5 concludes with
some final remarks.

2. Background

Shapley values were originally proposed as a pay-off concept from
cooperative game theory (Shapley, 1953). Note that the concept of “pay-
off” in XAI corresponds to the model prediction, as well described in the
papers by Joseph (2019) and Lundberg and Lee (2017).

Shapley values represent the average of the marginal contributions
of the players associated with all their possible orders, where, for
“order”, we intend all the possible orders of players’ arrivals to the
coalition. The orders are equally likely and, in each order, each player
gets his marginal contribution from the coalition he joins to. As dis-
cussed by Joseph (2019), Shapley values play a crucial role in improving
machine learning model explainability. They allow to evaluate the
learned functional forms of a model without having to specify them ex
ante.

More generally, Shapley values fulfill a number of useful properties
that allow to better understand how the model uses its features to pro-
vide a reliable response in a complex decision making process. For
example, the sum of the Shapley values is the model accuracy; they are

Expert Systems With Applications xxx (xxxx)} xxx

equal for features with the same importance; in a linear model, the
Shapley value of a feature is expressed as the linear combination of its
Shapley values across the model.

Formally, let i=1,...,n be a statistical unit, whose (multivariate)
characteristics Y; are to be predicted (on a “test set”) with a machine
learning model (educated on a “training set”), so that an automated
action (say, a(Y;)) is taken.

Let #7 = Fix) indicate the predicted value for the response vector
Y¥;, based on an explanatory vector of characteristics X;, obtained with
the machine learning model l. For ease of notation, we drop the suffix 1
henceforth.

As discussed by Bussmann, Giudici, Marinelli, and Papenbrock
(2020), the Shapley value based approach can be developed by using the
SHAP (SHapley Additive exPlanations) computational framework (see,
eg. Lundberg & Lee, 2017). This approach differs from the GAM
(Generalized Additive Models) approach described by Lou, Caruana, and
Gehrke (2012), While the GAM method explicitly decomposes the model
into linear combinations of simple models trained by a single explana-
tory variable, the Shapley value approach decompose the overall model
into linear combinations of all the model configurations trained by all
the possible combination of the available explanatory variables.

A machine learning model can be decomposed into functions of the
additional individual components of x; (the feature variables) according
to a function ¢ as follows:

x
o(7(x)} = y+ a(x), Vi = Lyn, (1)
k=1

where: k indicates a single feature variable; K denotes the total number
of available explanatory variables; n is the total number of units to be
predicted; @ ¢ RX; ¢, € R. The local functions ¢,(X;) are the Shapley
values.

Note that linear machine learning models (such as regression
models) fulfill this requirement. As shown by Joseph (2019), a linear
model satisfies the following:

K
off (x)) =).+ TAX, @)

k=1

in which $y = Ao and 14%) = Vea Xe.

Starting from the previous observation, Joseph (2019) proposed to
regress the response values on the individual Shapley values to obtain a
linear approximation to a machine learning model. While this proposal
is tempting, as it provides local explanations which can be statistically
tested, it may lead to a highly parameterised model, driven by a
computationally expensive procedure. This because the expression in (2)
has to be considered for possible subsets of the K available variables, as
in a regular model selection procedure.

When referring to a machine learning model, the players of a coop-
erative game, aimed at generating a pay-off, are the K explanatory
variables that can be included in the model and each model is a com-
bination of several variables, which thus “cooperate” towards the pre-

dictions F(x). Following Lundberg and Lee (2017) and Strumbelj and
Kononenko (2010), and using a notation coherent with that considered
for the construction of our proposal, the marginal contribution of a
variable X;, (k =1,...,K) can be expressed in the form of Shapley values
as

|X |!(K — |X| —1)!
|

o(7%) ) ~ Kl

[Fux UX); -F(X)]. (3)
XC BN)\N
In Eq. (3): @(X)\X; is the set of all the possible model configurations

which can be obtained with K—1 variables, excluding variable Xj; |X |
denotes the number of variables included in each possible model;

f(x UX), and f(x ); are the predictions associated with all the possible
P. Giudici and E. Raffinetti

model configurations including variable X, and excluding variable X;,
both calculated for the unit i The quantity within the squared paren-
theses defines the contribution of variable X;, to the model prediction,
for any single unit.

Given the challenging computational efforts needed to calculate the
marginal contribution of each variable, especially when K is large,
Lundberg and Lee (2017) and Strumbelj and Kononenko (2010) have
proposed computational methods to approximate Shapley values with
similarly additive feature methods which possess a specified set of
properties, such as local accuracy, missingness and consistency.

A remarkable characteristic of the obtained Shapley values approach
is that they provide the explanation of the additional importance of each
variable for each individual unit. This helps to explain the nature of the
contribution of each variable but, on the other hand, it does not explain
whether the same variable should be maintained in the model, in a more
parsimonious version which, according to Occam’s razor principle, im-
proves goodness of fit and interpretation.

Indeed, the drawback of Shapley based XAI methods lies in their very
power: being designed to understand point predictions, they may be
highly unstable, in the presence of data anomalies, such as fake data,
missing data or outliers. In relation with this, they are not suited to
understand which variables are important, at the overall level. Although
Shapley values can be summed over point predictions, to give an
“overall” measure of importance of a single variable, this simple mea-
sure leads to compensation, excessive leverage of single observations
and, above all, the lack of a normalised measure to assess the relative
importance of each variable contribution.

This explains why the tasks of establishing model predictive accuracy
for explainable machine learning models based on Shapley values are
left to more classic model comparison tools, such as pairwise statistical
tests, when possible or, in the more general machine learning context, to
cross-validation tools, such as the Receiver Operating Characteristics
(ROC) Curve and the corresponding AUROC or Gini value (see e.g.,
Guégan & Hassani, 2018).

It may be the case that a variable which is highly explainable for
most individual predictions is not included into the “best” model that
corresponds to the highest Area Under the ROC Curve (AUROC).
Conversely, a model selected in terms of best AUROC may contain
variables that do not differentiate between individual predictions and,
therefore, are not explainable at the local level.

To reconcile the two views (predictive accuracy and local explain-
ability) we propose to develop a Shapley based framework that de-
composes predictive accuracy, rather than individual predictions. And
that could, possibly, be localised. This is our main contribution.

3. Proposal

To achieve our aim we exploit a model selection measure, recently
introduced by Giudici and Raffinetti (2020), which is based on the
employment of Lorenz Zonoids and on a mutual notion of variability.
The Lorenz Zonoid-based measure fulfills some attractive properties: it is
akin to the well known Receiver Operating Curve (ROC), robust to the
presence of outlying observations and independent on the nature of the
response variable.

Lorenz Zonoids were introduced by Koshevoy and Mosler (1996) asa
generalization of the Lorenz curve in d dimensions. The same authors
showed that, when d = 1, the Lorenz Zonoid corresponds with the well
known Gini coefficient which, in turn, is related to the Area Under the
ROC Curve.

Suppose to consider a response variable Y and a set of explanatory
variables Xj, ...,Xj,...,.Xn, withj = 1,...,h. To evaluate the relationships
between Y and the X,,...,X; explanatory variables, a machine learning
model can be applied, and the associated predicted values, denoted with

¥x, Kao are obtained. The Lorenz Zonoid of Y and Py, ae can be
defined by (see, e.g. Giudici and Raffinetti, 2020):

Expert Systems With Applications xxx (xxxx)} xxx

2WCovl¥,r(¥ 5
Zan (”) = 20M) and IZ 1 (F..3]

mye

(4)
nyt

where n is the total number of observations, y is the response variable Y
mean value, r(Y) and r( ¥x, .-oXq) ate the rank scores corresponding to the

Y and Vx, Xs variables. Given a sample data of size n, formulas in (4)
can be reformulated as:

2Cov(y, r' ~
[Las (») = 2607) and EZ 4-4 (5, a)

ny
2004." Giassa)) o
ny
whereyandy,,___., are the vectors of the observed and predicted values,

r(y) and r(Y,, _»,) are the ranks of the observed and predicted values,
and y is the sample mean.

In Giudici and Raffinetti (2020), the Lorenz Zonoids were exploited
giving rise to new dependence measures suitable in assessing the
contribution of each explanatory variable to the predictive power of a
model. Specifically, a Marginal Gini Contribution (MGC) measure,
allowing to measure the absolute explanatory power of any single co-
variate,! and a Partial Gini Contribution measure (PGC), allowing to
measure the additional contribution of a new covariate to an existing
model, were developed as follows.

Let X; be one of the h explanatory variables (j = 1,...,A). The mar-
ginal contribution provided by a single covariate X; is given by:

MGCypy _ EZaa1 9) _ Cov(¥x,,r(¥x)) ) (6)

LZa1(¥) Cov(Y,r(Y) )

Let Vx... x, and Vx... x, be the predicted values provided by a full
model, including all the covariates, and a reduced model, excluding
covariate X;. The additional contribution related to the inclusion of
covariate X;, can be determined as

PGCYX% 1 =—— > 7)

We remark that, when the Y response variable is continuous, and the
machine learning model is linear, the marginal contribution provided by
a single covariate X to an existing model, in Eq. (7), simplifies to the well
known variance decomposition of the multiple correlation coefficient,
R:

h
2 _ 2 2
Rup, yee X,) Rela ( ~ Roly, yee »] , (8)
j=l

where: Rivx, ...X,) denotes the multiple correlation coefficient (the ¥

2 d
lenotes
YX [Ries

the partial correlation coefficient (the variability of Y, additionally
explained by the j-th explanatory variable, after the previous i <j vari-

“Wition is of 2
ables, whose contribution is given by Reo”

When the covariates are independent, we obtain, as a further special
case, that:

variability explained by all the involved h covariates); R

1 Note that to the term “covariate” is employed as a synonym of the term
“explanatory variable”. The two words will be then used interchangeably.
P. Giudici and E. Raffinetti

ir yes Xn) ~ Rx: 1)

The previous remark suggests that, using a Lorenz Zonoid decom-
position, we can extend the global explanations of machine learning
models proposed by Owen and Prieur (2017) and Song et al. (2016).
While the latter Authors employed a variance decomposition approach
to explainable machine learning, dealing with the issue of dependent
covariates, we extend the approach from variance decomposition to
Lorenz Zonoid decomposition, obtaining a simpler and more versatile
approach.

In line with the need of diagnosing both predictive accuracy and
explainability, we now combine the Lorenz Zonoid, aimed at evaluating
predictive accuracy in a rather general context, with the Shapley value
approach, aimed at obtaining individual unit explanations.

The main intuition of our proposal is the following. Shapley pro-
posed to employ game theory with pay-offs that are given by:

Pog (X¥) = F(X UXe), — F(X),s (10)

for any statistical unit i.
We propose to apply game theory with pay-offs that are given by the
numerator of the PGC measure:

Poff (x*) = LZy-1 (Fs Xi) — EZ a= (Fs peep Xp ) , (11)

for a set of statistical units (i = 1,...,n).

The resulting expression, that we call Shapley-Lorenz decomposi-
tion, allows to identify the contribution of each explanatory variable,
not in terms of the differential contribution to the locally predicted
values (as with standard Shapley values), but in terms of the differential
contribution to the global predictive accuracy.

We now proceed with the mathematical derivation of the Shapley-
Lorenz decomposition.

First, we replace LZ4_, (-) in place of f(-) in the Shapley expression in
(3), and obtain that the marginal contribution associated with the
additional variable X* is equal to

128,(?) =

[X'[!(K — |x" — a
1
X CBX) Ne x

IZa=1 (Prux) — LZaa1 (?x) |
(12)

where 1Za4 (Vy ux,) and 1Za4(¥y) describe the (mutual) variability
explained by the models including the X' UX, variables and the X
variables, respectively. Note that LZj_; (Yeux) and 1Za4(¥y) in Eq.
(12) can be expressed as function of the covariance operator, as reported
in Appendix Section Al.

As what we observe is indeed a sample of n observations, we need to
estimate the population mean y, with the sample mean, ¥. Then,
denoting with Vy _y, and ¥y the predicted values provided by the model

including and excluding the X,; covariate, ordered in non-decreasing
sense, the formula in Eq. (12) becomes

~ X'|I(K — |xX’| — 1)!
i,(s)= yo MRS
XC @(X)\Ne :

 

(13)

Through some mathematical manipulations, whose details are con-
tained in Appendix Section A2, Eq. (13) can be re-written as

12, (5) - S A PL | iran

X CRN ml

59 “| \ a4)

LZ a= (5a) — EZ e21 (5+) .

Expert Systems With Applications xxx (xxxx)} xxx

where Vy x, (@) and Vy (i) are the predicted values for the i-th statistical
unit obtained by the model including and excluding the X;, covariate.
Comparing Eq. (14) with (3) note, part from the different notation, the
similarities between the two expressions. While the standard Shapley
decomposition “explains” the covariate contributions at the individual
level, the Shapley-Lorenz decomposition “explains” the same contribu-
tions at the global level. Indeed, through Eq. (14), a description of the
model as a whole, in terms of the explanatory variables mostly deter-
mining its prediction, is provided.

Shapley-Lorenz decomposition, differently from standard ones, al-
lows to detect which variables could be eliminated, as unnecessary for
model predictions, leading to a more parsimonious structure. Indeed
standard Shapley values can be summed across units, leading to “global”
variable importance measures which, however, are not normalised
within a model accuracy context, as Shapley-Lorenz ones.

In addition, looking at expression (14), note that Shapley-Lorenz
decomposition can always be calculated, without loss of generality, to
subsets of the n units to be predicted. This leads to a natural “local-
isation” of the measure, without altering its predictive meaning.

4. Application

In line with our initial discussion, to illustrate our proposal we
consider the application of machine learning models in the highly
regulated field of finance.

In finance, the notion of XAI is increasingly discussed by public and
private institutions, to provide transparent and effective machine
learning methods (see, e.g. Arras, Horn, Montavon, Miiller, & Samek,
2017; Arrieta et al., 2019). The idea is to introduce a suite of techniques
that allows to improve the interpretability of the models while preser-
ving an adequate level of prediction accuracy. This idea has recently led
some scholars to promote XAI methods aimed at making both the
financial technology risk measurement models interpretable and trans-
parent, and the risks of financial innovations, enabled by the application
of AI, sustainable (see, e.g. Bracke, Datta, Jung, & Shayak, 2019; Buss-
mann et al., 2020). In particular, in Bussmann et al. (2020) an
explainable AI model based on similarity networks (Mantegna & Stan-
ley, 1999) and Shapley values is proposed to measure the credit risks
associated to the use of AI based credit scoring platforms.

To exemplify our proposal, we apply it to a dataset that has been used
to predict bitcoin prices, and their up or downtrends. As illustrated in
Giudici and Raffinetti (2020), the available data provide information on
the daily bitcoin prices in eight different crypto exchanges, from 18 May,
2016 to 30 April, 2018. For the sake of brevity, we refer to the time
series observations on Coinbase prices, which represent the response
variable to be predicted by the available financial explanatory variables.
Specifically, as candidate financial explanatory variables the time series
for Oil, Gold and SP500 prices are taken into account. The choice of such
set of variables is related with their economic importance, and with the
need to explain our proposal with a model simple enough so that cal-
culations can be clearly understood.

In this application, we will select, as our candidate machine learning
model, a linear regression model and we will calculate the Shapley-
Lorenz marginal contributions, associated with the inclusion of SP500,
Gold and Oil, according to the formula (13). When considering SP500,
Gold and Oil as additional explanatory variable, the corresponding
marginal contributions can be written in full as follows:

LZ (Coinbase = (1/3) (iz (Fes causon —1Z (Scan) )
+(1/6) (LZ (Fspso0 cota) _ IZ (¥eota)) + (1/6) (LZ (Fspso0,ou) _ IZ (Fou)
+(1/3) (LZ sps00))
P. Giudici and E. Raffinetti

et ( Coinbase ) = (1 / 3) (iz (Fass) —LZ (ses) )
+(1/6) (LZ (Feta sps00) — LZ (Fsps00)) + (1/6) (LZ Feta,ou) — LZ Vou) )
+(1/3)(LZeoia))

La, (Coinbase = ( / 3) (iz (Fousesoncau) LZ (Sseso0cau) )
+(1/6) (EZ (Fou,spsco) - LZ(¥spsoo)) + (1/6) (EZ (Foucoua) - LZ (Feu) )
+(1/3)(LZFoi)).

For the sake of comparison, we will also consider the variance
decomposition associated with the same variables, which holds under
the assumption of a linear model. Applying the Shapley formula as
before, but replacing Lorenz Zonoid with Partial correlation coefficients,
we obtain the following marginal contributions:

RSpso9 = =(1 /3) (Ri ‘SP500,Gold,Oil Reota ow) + ( /6) (Ri ‘SP500,Gold Res)
+(1/6) (Resoo.ou — Roa) + (1 /3)Rs SP500

Reoia = 3 Gold,SP500,0iL Ry ps00, OIL
= (1/3) (% )
+(1 /6) (Re Gold SP500 Rpso0) + ( /6) (Re Gold,Oil Rou) + (1/3) Roota

Reoia = =(1 /3) (R Oil,SP500,Gold Rs psoo ja) + (1 /6) (R: Oil, SP500 Reps)
+(1/6) (Réxscou — Rese) + ( /3)R OIL

We can also calculate the “standard” global Shapley value for each
variable summing, for each variable, its contribution to any single unit
prediction. We remark that the result is a measure that, differently from
before, is not normalised and, therefore, not easily interpretable.

The results of all the previous calculations are displayed in Table 1.

Table 1 shows that, employing the Lorenz-Shapley approach, vari-
able SP500 provides the highest marginal contribution in the prediction
of the Coinbase prices (as in Giudici & Abu-Hashish, 2019), while the
other two give a minimal contribution. This conclusion is in line with the
economic literature, which shows that the bitcoin has reached the status
of a speculative asset, that is used to diversify portfolios, being signifi-
cantly negatively correlated with classic assets such as stock prices
summarised by the SP500 index.

The conclusions from the Shapley-Lorenz approach are also quite
similar to those obtained with the linear R?-based Shapley approach.
This shows that a non linear machine learning model does not lead to a
substantial change of the interpretability that could be drawn from a
linear model, applied to the same data. Note that, in general, the
Shapley-Lorenz approach has to be preferred to the linear R?-based
Shapley approach, especially in the presence of outlying observations.

Finally, the Global Shapley values, obtained summing the Shapley
variable contributions across all units are, as expected, non normalised,
and with a sign. While the Global Shapley value fails to tell which var-
iable is most important in terms of the explained variability, the sign is
consistent with the previously commented economic finding: the SP500
index is negatively correlated with the Coinbase prices.

The Shapley value approach appears as more intuitive than further
typically used AI methods, as shown by the use case about explainability
of risk management models described and developed by Bussmann et al.
(2020) within the European FINTECH Project (https://www.fintech-h
02020.eu). The use case was indeed presented, in a “human-centric”
study, to the regulators of most European countries, and one of the main
feedback was that the approach is nice and promising but should be
compared to what obtained with ’classical” model assessment methods,

Expert Systems With Applications xxx (xxxx)} xxx

Table 1

Marginal contribution of each explanatory variable in terms of the linear
Shapley-Lorenz approach, in terms of the R® coefficient and the standard
Shapley approach.

 

 

Additional covariate (Xx) 1Zz% 1 (Coinbase) Ry, Global Shapley
SPSOO 0.336 0.631 —96377.28
Gold 0.097 0.072 59811.19

Oil 0.075 0.049 —43428.39

 

which is what the Shapley-Lorenz approach provides.

5. Conclusions

In this paper we have introduced a novel global explainable AI
model, based on the application of the Shapley approach to Lorenz
Zonoid.

The proposed decomposition extends those recently proposed in
terms of variance decomposition, leading to a variable contribution
measure that is more generally applicable, and easier to interpret. In
addition, the expression of the marginal contribution shows how global
explanations can be mapped to local ones and viceversa.

We believe that our proposal could be quite useful, as it provides a
unified criterion to assess both predictive accuracy and explainability of
the explanatory variables contained in a machine learning model. In
addition, the metric in which the measure is expressed is a normalised
one, related to the AUROC and Gini index and, therefore, easier to
interpret.

The application of the measure to a financial problem that concerns
bitcoin price prediction shows its ease of application, consistency and
versatility.

The potential users of our model are, besides academic researchers,
Al developers, for compliance and regtech purposes; and policy makers
and regulators, for AI certification, monitoring, and suptech purposes.

Future extensions of the research concern, on one hand, the devel-
opment of a statistical testing procedure, which could add to variable
contributions a significance measure. On the other hand, the extensive
application to several other application fields.

CRediT authorship contribution statement

Paolo Giudici: Conceptualization, Funding acquisition, Project
administration, Resources, Supervision, Writing - review & editing.
Emanuela Raffinetti: Data curation, Formal analysis, Investigation,
Methodology, Software, Validation, Visualization, Writing - original
draft.

Declaration of Competing Interest

The authors declare that they have no known competing financial
interests or personal relationships that could have appeared to influence
the work reported in this paper.

Acknowledgments

This research has received funding from the European Union’s Ho-
rizon 2020 research and innovation program “FIN-TECH: A Financial
supervision and Technology compliance training programme” under the
grant agreement No 825215 (Topic: ICT-35-2018, Type of action: CSA).

Acknowledges go to the three anonymous reviewers for their valu-
able comments and suggestions which allowed to improve the paper.
P. Giudici and E. Raffinetti Expert Systems With Applications xxx (xxxx)} xxx
Appendix A

Al. Covariance formulation of Lorenz Zonoids

As shown in Eq. (4), LZq-1 (¥y ux,) and £Z4_1 (¥y) in Eq. (12) can be written through the covariance formulation leading to

~ 2 ~ ~
Zs (Frun) == cov(Peunsr( Prin) ) (15)

and

~ 2 ~ ~
21(Fv) == cov(Pear(Fv)). (16)

A2. Derivation of equation in (14)

Given a sample of n observations, formulas in Eqs. (15) and (16) become

~ 2 ~ ~ 2/147 . a(n +1)
LZaa1 (5. a) = we (Fran (5. ) 5 E Ss Iy'ux, (‘) - ny (17)
i=]

and

1a (5. ~Zem(sr(5e) == E Sisy (') ot (18)
i=l

Inserting the expressions (15) and (16) into (12), we obtain that the marginal contribution of an explanatory variable X), is a function of:

~ ~ 2 ~ ~ 2 ~ ~
LZ (Pyux) — EZ a (Fy) == cov Persar(Frus)) - = cov Fe.r(Fy))

(19)
2 ~ ~ ~ ~
a [Cov(Feums7(Prum)) _ cov( Py .r(¥v))],
whose sample version, by resorting to Eqs. (17) and (18), can be obtained as:
a a 2 a a a
EZ (Frux,) — Za (Fy) = Ww [Cov Feu, 7 (Frux,) ) _ Covidy (Fx) )]
214. ., Issn np] 2[lfS. ~~ Gaz
27. _ (20)
=a Ss iGyvux, @) —¥x ()) .
VY at

The previous quantity defines the contribution of variable X;, to a particular model configuration, with X the considered explanatory variables. It is
the analog of the quantity in squared parentheses in the Shapley Eq. (3). Comparing the two quantities, note that the Shapley-Lorenz decomposition is
indeed a function of the individual Shapley differences. A function that, differently from the pure sum of the individual Shapley values, considers a
normalised sum of their cumulative intensities.

Completing (20) with the remaining part of Eq. (13), that takes into account all possible model configurations, the Shapley-Lorenz marginal
contribution of a covariate X;, is finally obtained as:

in, (5) _y¥ ae a Sir $y 0] \

XCPN)\N ml

which corresponds to expression in (14).

References Arrieta, A. B., Driaz-Redriguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A.,
Garcia, $., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R., & Herrera, F. (2019).
Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and
challenges toward responsible Al. arXiv preprint arXiv:1910.10045.

Bracke, P., Datta, A., Jung, C., & Shayak, $. (2019). Machine learning explainability in
finance: An application to default risk analysis. Staff Working Paper No. 816, Bank of
England.

Aas, K., Jullum, M., & Loland, A. (2020). Explaining individual predictions when features
are dependent: More accurate approximations to Shapley values. arXiv preprint
arXiv: 1903. 10464.

Arras, L., Horn, F., Montavon, G., Miiller, K.-R., & Samek, W. (2017). “What is relevant in
a text document?”: An interpretable machine learning approach. PLoS One, 12(8),
1-23. https://doi.org/10.1371/journal. pone.0181142
P. Giudici and E. Raffinetti

Bussmann, N., Giudici, P., Marinelli, D., & Papenbrock, J. (2020). Explainable Al in
credit risk management. Frontiers in Artificial Intelligence, 3(26), 1-5. https://doi.org/
10.3389/frai.2020.00026

European Commission. (2020). On artificial intelligence - A European approach to
excellence and trust. White Paper, European Commission, Brussels, 19-02-2020.

Giudici, P., & Abu-Hashish, I. (2019). What determines bitcoin exchange prices? A
network VAR approach. Finance Research Letters, 28, 309-318. https://doi.org/
10.1016/j.frl.2018.05.013

Giudici, P., & Raffinetti, E. (2020). Lorenz model selection. Journal of Classification.
https://doi.org/10.1007/s00357-019-09358-w

Guégan, D., & Hassani, B. (2018). Regulatory learning: How to supervise machine
learning models? An application to credit scoring. The Journal of Finance and Data
Science, 4(3), 157-171. https://doi.org/10.1016/j.jfds.2018.04.001

Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018).
A survey of methods for explaining black-box models. ACM Computing Surveys
(CSUR), 51(5), 1-42. https://doi.org/10.1145/3236009

Joseph, A. (2019). Shapley regressions: A framework for statistical inference in machine
learning models. Staff Working Paper No. 784, Bank of England.

Koshevoy, G., & Mosler, K. (1996). The Lorenz Zonoid of a multivariate distribution.
Journal of the American Statistical Association, 91(434), 873-882. https://doi.org/
10.2307/2291682

Expert Systems With Applications xxx (xxxx)} xxx

Lou, Y., Caruana, R., & Gehrke, J. (2012). Intelligible models for classification and
regression. In Proceedings of the 18th ACM SIGKDD international conference on
knowledge discovery and data mining (pp. 150-158).

Lundberg, S. M., & Lee, S. (2017). A unified approach to interpreting model predictions.
arXiv preprint arXiv:1705.07874.

Mantegna, R. N., & Stanley, H. E. (1999). Introduction to econophysics: Correlations and
complexity in finance. Cambridge University Press.

Molnar, C. (2020). interpretable machine learning — A guide for making black box models
explainable. Available at URL: https://cristophm.github.io/interpretable-ml-book.

Owen, A. B., & Prieur, C. (2017). On Shapley value for measuring importance of
dependent inputs. SIAM/ASA Journal of Uncertainty Quantification, 5, 986-1002.
https://doi.org/10.1137/16M1097717

Shapley, L. S. (1953). A value for n-person games. Contributions to the Theory of Games,
307-317.

Song, E., Nelson, B., & Staum, J. (2016). Shapley effects for global sensitivity analysis:
Theory and computation. SIAM/ASA Journal of Uncertainty Quantification, 4,
1060-1083. https: //doi.org/10.1137/15M1048070

Strumbelj, E., & Kononenko, I. (2010). An efficient explanation of individual
classifications using game theory. Journal of Machine Learning Research, 11, 1-18.
https://doi.org/10.1145/1756006.1756007
