6.

FAIRNESS THROUGH REGULARIZATION FOR LEARNING TO RANK

 

6.4.1 Group fairness in learning to rank

Group fairness notions in classification are typically based on an underlying probabilistic
framework that allows statements about conditional independence relations [BHN19]. Sim-
ilarly, we assume that D € P(Q x Z x {0,1}) is an unknown fixed distribution over
query /document/relevance triplets. For the rest of our work, all statements about prob-
abilities of events will be with respect to samples (q,d,r(q,d)) ~ D.

Analogously to the situation of classification, we assume that any item d € Z has a protected
attribute, A(d), which denotes the group membership for which fairness should be ensured.
For example, A(d) can correspond to gender, when the retrieved items are images of people,
or to the country of origin of an Amazon product. For simplicity, we assume binary-valued
protected attributes, but extensions are easily possible.

A plausible notion of fairness in the context of ranking is: For any relevant item the
probability of being included in the ranker’s output should be independent of its
protected attribute. This intuition is easy to formulate in our formalism, resulting in a direct
analog of the equality of opportunity principle from fair classification [HPS16].

Definition 16 (Equality of opportunity for LTR). An item selection function f : Qx I —
{0, 1} fulfills the equality of opportunity condition, if

P(F(a,@) = 1AM) = 0,r(4,4) = 1) = PG) = YAM) =1,7r@,d)=1). (65)

In practice, a ranker will rarely achieve perfect fairness, so we also introduce a quantitative
version of Definition |16| in the form of a fairness deviation measure WM19],

that reports a ranking procedure’s amount of unfairness:

Definition 17. The equality of opportunity (EOp) violation of f : Q x Z — {0,1} is

1" f) =|P(F (qd) =1A(d) =0, r(q, d) =1) — P(F(q, 4) = 1A) = 1, rg, 4) = 1).

Clearly, f is fair in the sense of Definition 16 if and only if it fulfills TPOP( f) = 0.

Other fairness measures As discussed extensively in the literature, different notions of
fairness are appropriate under different circumstances. For example, to check the equality of
opportunity condition one needs to know which items are relevant for a query, and this can be
problematic, e.g., if the available data itself exhibits a bias in this respect. A major advantage
of our formalism compared to prior fair ranking methods is that it is not partial to a specific
fairness measure. Besides equality of opportunity, many other notions of group fairness can
be expressed by simply translating the corresponding expressions from classification.

For example, one can avoid the problem of a data bias by demanding: The probability of
any item to be selected should be independent of its protected attribute (disregarding
its relevance to the query). In our formalism, this condition is a direct analog of demographic

parity [CKPO09].

Definition 18 (Demographic Parity for LTR). An item selection function f : Qx I >
{0,1} fulfills the demographic parity condition, if

P(f(4, 4) =1A(d) =0) =P(FG, 4) =A) =). (6.6)

76

2d log (222) + 2 log(48/6
+P4, { sup |y(h) — 72(h, $?)| > 8 2dlog("7) + 2log(48/8)
heH

 

(1—a)Pon
+P ( sup (Ao(h) + Ai(h)) > ar")
nen
24948
~4 4° 2

 

 

 

 

Concentration for equality of opportunity

We introduce similar notation as in Section |C.2.2| but tailored to the equality of opportunity
conditional probabilities.

We use the notation Cy, = 7, l{i: a? = a,y? = 1,7 ¢ B}| for the number of points in
S? that were not marked (are clean) and contain a point from protected group a and label
y =1and By = D2, Li: a? =a,y? = 1,i € B}| for the number of points in S? that were
marked (are potentially bad) and contain a point from protected group a and label y = 1.
Note that By + By, is the total number of poisoned points for which y = 1 and so is at most
Bin(n, a). Similarly, denote by C,(h) = 3, Hi: h(a?) = 1,a? =a, yf = 1,1 € B}| and
BL (h) = D2 Li: h(x?) = La? =a,y? = Li € PH.

Denote

My U{A(a?) =1,a2 =a,y) = 1}

7

Vier La? = a, y? = 1}

 

and
yia(h) = P(h(X) = 1|A =a, = 1),

so that P2P(h) = |y%(h) — 72, (h)| and F29?(h) = |y9(h) — yu1(h)|. Note that 72, (h) is
an estimate of a conditional probability based on the corrupted data. We now introduce the
corresponding estimate that only uses the clean (but unknown) subset of the training set S?:

cin) = Ch) _ Eka Mala?) = 1a? = ayy? = Lig B}
Tall) = Oy "i@=ay=ligh

 

 

 

Similarly to before, we first bound how far the corrupted estimates 7},(h) of y1a(h) are from
the clean estimates {,,(/), uniformly over the hypothesis space 1:

 

 

Proposition 3. /fn > max { ae eres) we have
P® ( sup (lyfo(h) — rfo(h)| + lati (h) — rf(A)I) = 0 _) <5. (C.11)
heH ~ Pyo/3+a

Proof. Similarly to the proof of Proposition |2\ we first show that certain bounds on By,
and C{, hold with high probability. Then we show that the supremum in (C.11) is bounded
whenever these bounds hold.

142

3.4. On the sample complexity of adversarial multi-source learning

 

In particular, the same learner £ should work against any adversary and for any a or set G.
At the same time, the adversary is arbitrary once CL is fixed, so in particular it can depend on
the learning algorithm.

Note that the robust learner should achieve optimal error as m — oo, while N can stay
constant. This reflects that we want to study adversarial multi-source learning in the context
of a constant and potentially not very large number of sources. In fact, our lower bound
results in Section [3.5|show that the adversary can always prevent the learner from approaching
optimal risk in the opposite regime of constant m and N — oo.

3.4 On the sample complexity of adversarial
multi-source learning

In this section, we present our main result in this chapter, a theorem that states that whenever
H. has the uniform convergence property, there exists an algorithm that guarantees a bounded
excess risk against both the fixed-set and the flexible-set adversary. We then derive and discuss
some instantiations of the general result that shed light on the sample complexity of PAC
learning in the adversarial multi-source learning setting. Finally, we provide a high-level sketch
of the theorem’s proof.

3.4.1 Main result

Theorem 4. Let N,m,k € N be integers, such that k € (N/2,N]. Leta = X=* < £ be
the proportion of corrupted sources. Assume that H has the uniform convergence property
with rate function s. Then there exists a learner L : (X xy)\*™ + H with the following
two properties.

(a) Let G be a fixed subset of [N] of size |G] =k. For S’ ={S1,..., Sy} 'S" D, with
probability at least 1 — 5 over the sampling of S’:

R(L(A(S'))) — min R(h) < < as(km, 5 e Se) + Ga mars s(m, ae Si) (3.5)

uniformly against all fixed-set adversaries with preserved set G', where S = {S),...,Sv} =
A(S') is the dataset modified the adversary and Sg = UiecS; is the set of all uncorrupted
data.

(b) For S’ ={S\,..., Sy} °*" D, with probability at least 1 — 6 over the sampling of 3’:

i 6 6
— mi 1) < 2s(km, —~, ax s(m, —, S; i
R(L(A(S ))) min R(h) < 2s(km, a) , Sa) + 6a ae s(m, ON? Si) (3.6)

uniformly against all flexible-set adversaries with preserved size k, where S = {S),..., Sw} =
A(S') is the dataset returned by the adversary, G is the set of sources not modified by
the adversary and Sg = UiecS; is the set of all uncorrupted data.

The learner L is in fact explicit, we define and discuss it in the proof sketch that we provide in
Section [3.4.3] The complete proof is provided in Appendix |A.1]

As an immediate consequence we obtain:

29

4. ADVERSARIAL MULTI-SOURCE LEARNING IN PRACTICE

 

0.33

—— Ours

-— Reference only

       

0.32); —+— All data
—-— Median of probs
0.311 —— Feng etal.

 

Batch norm
0.30

0.29

Average classification error

0.27

0 2 4 6 8 10
Number of corrupted sources

Figure 4.1: Results from the experiments on 20 books and 20 other products from the
"Multitask dataset of product reviews". The x-axis gives the number n of non-books in an
experiment and the y-axis - the mean classification error. Error bars give the standard error of
the estimates.

for testing. An additional set of 100 labeled reviews were available from every other product.
The algorithms were trained on all available data and evaluated on the test set. The average
classification errors achieved by the algorithms are presented in Table 4.1) together with the
standard errors of those estimates. We see in particular that our algorithm successfully uses
the information from multiple sources to achieve the best overall performance.

4.4.3. Animals with Attributes 2

The Animals with Attributes 2 dataset contains 37322 images of 50 animal classes.
The classes are aligned to 85 binary attributes, e.g. color, habitat and others, via a class-
attribute binary matrix, indicating whether an animal possesses each feature. This results in a
total of 85 different binary prediction tasks of identifying whether an animal on a given image
possesses a certain attribute or not.

Feature representations of the images are obtained via the following procedure. We use a
ResNet50 network [HZRS16], pretrained 3]on ImageNet [RDS*15], to obtain feature represen-
tations of the ImageNet data and reduce their dimension to 100 by PCA. Finally, for each image
in the Animals with Attributes 2 dataset, we compute the ResNet50 feature representation
and apply the PCA projection pre-learned on ImageNet.

3We use a pretrained model from the TensorNets package, |https://github.com/taehoonlee/tensornets
46

6.4. Fairness in learning-to-rank

 

As a corresponding quantitative measure we define the demographic parity (DP) violation of
f as

r?"(f) =|P(F (4, 4) = 1A(@) = 0) — P(f(a, 4) = YA(d) = 1).
Another meaningful notion of fairness in ranking is: The probability of any item to be
selected should be independent of its protected attribute, individually for all relevant

and for all irrelevant items. This condition yields the ranking analog of equality odds
HPS16}.

Definition 19 (Equalized Odds for LTR). An item selection function f : Q x I — {0,1}
fulfills the equalized odds condition, if for all r € {0,1}:

P(F(4,4) = AM =0,r(9,4d) =r) =P(FG4) =NAM=Lrad=r) (67)
The equalized odds (EOd) violation of f is

reo“ f) =» P(F(q, 4) =1A(d) =0, r(q,d)=r) — PF(a, 2) =1A(d)=1, rg, d)=r)).

re{O,1

 

 

6.4.2 Training fair rankers

In order to enforce the fairness of a LTR system during the training phase, we create empirical
variants of the fairness violation measures and add them as a regularizer during the training step
[KASI1]. For this construction to make sense, we have to answer two questions:
Can we solve the resulting optimization efficiently? and Does the inclusion of a regularizer
generalize, i.e. ensure fairness also on future predictions? |n rest of this section, we will answer
the first question. The second question we will address in Section |6.4.3|

To allow for gradient-based optimization, we parametrize the binary-valued item selection
function in a differentiable way using a real-valued score function s : Q x Z — [0,1], similarly
to the discussion in Section [6.3] Our inspiration, however, comes from the classification
setting, such as logistic regression, and we assume that s is not arbitrary real-valued, but that
it parameterizes the probability that d is selected for g, i.e. s(q,d) = P(f(q,d) = 1).

Empirical fairness measures For a given training set, S, in the format discussed in
Section |6.3] we obtain empirical estimates of the previously introduced fairness violation
measures. For any a € {0,1}, r € {0,1}, denote by S, the subset of data points (q, d,r(q, d))
in S with A(d) =a, and by S,,, the subset of data points in S with A(d) = a and r(q,d) =r.

Definition 20 (Empirical fairness violation measures). For a function s : Q x I — (0, 1],
its empirical equality of opportunity violation on a dataset S' is

[P20P(s: S) -|5— otg,dl (q,d)|.
(8; Boal > s(q, 4) — Eat > s(q, 4)

(q,d)ESo4 (q,4)€S1,1

 

The empirical demographic parity violation of s on S' is
1

 

 

r"(s:8) =| eo — Tay Ysla-d))
Ol (q,d)ESo U(qdyesi
and the empirical equalized odds violation of s on S is
1 1 1
PeW(s; 8) == Yad - ig DY stad)
2 fo. 1} [Sorl; (q,d)€So,1 ISirlaaesia

7

4. ADVERSARIAL MULTI-SOURCE LEARNING IN PRACTICE

 

« Label bias: The labels of all (corrupted) samples are switched to class 1.

« Shuffled labels: The labels of all samples are shuffled randomly, separately in each
corrupted source.

= Shuffled features: Given a permutation of the indexes between 1 and 100, the features
of all samples are shuffled according to it.

= Blurred images: Each image is blurred by filtering with a Gaussian kernel with standard
deviation o = 6.

» Dead pixels: In each image a random 30% of the pixels are set to pure black or white.

= RGB channels swapped: The values in the red and the blue color channels of each image
are swapped.

Given an attribute, a type of corruption and a value of n € {0, 10, 20, 30, 40, 50,55, 59}, the
data is split randomly, as described above, and the samples of n randomly chosen sources are
corrupted. Our algorithm, as well as all baselines, then learn a model based on the resulting
data and the performance of the obtained predictors is evaluated on the test data. For any
combination of target attribute, corruption strategy and value of n, the experiment is repeated
100 times with a different random seed to obtain error estimates.

The results for the first attribute from the Animals with Attributes 2 data ("black") are given
in Figure 4.2] Each plot corresponds to a different type of contamination. The x-axis gives
the number of sources providing corrupted data and the y-axis corresponds to the average
error that an algorithm achieved on the test set, over the 100 runs for each experimental setup.
The error bars give the standard deviation around this average.

Our algorithm (green) performs at least as well as or strictly better than all baselines, for
any type of corruption and any proportion of corrupted sources. When all sources provide
clean data, the performance of our method matches the one of the classic regularized logistic
regression approach on i.i.d. data (blue). As the number of corrupted sources increases,
the performance of all baselines gradually degrades, while our algorithm is able to leverage
the remaining clean data and suppress the effect of the corruptions. The median-based
baselines perform reasonably when less than half of the sources are corrupted, but fail for
larger proportions. The robust logistic regression baseline performs poorly, again likely due to
the non-convexity of the loss function. As all sources become unreliable, our method performs
as well as the approach of learning from the reference dataset only, which is indeed optimal
since all other data is corrupted.

We summarize the results from all attributes in Table {4.2| For any number of corrupted
sources n (columns), we compare our method to the performance of each baseline (rows). We
report values in the form A/B/C, where A is the number of times that our method performed
significantly better than the corresponding baseline, B is the number of times it performed
equally well and C is the number of times it performed significantly worse, summed over the
various types of corruptions and all attributes. For a fixed type of corruption and attribute, we
say that one method performs significantly better than another over the set of 100 runs with
this setup, if the difference in the average performance of the two models is larger than the
sum of the standard deviations around those means (that is, if the error bars, as in Figure (4.2|
do not intersect).

48

CHAPTER

On the Sample Complexity of
Adversarial Multi-Source PAC Learning

We now move to the first topic covered in this thesis, namely that of robust learning from
corrupted data sources.

As discussed in Chapter |2| the problem of learning from adversarially corrupted data is in
general very hard: the classic result of states that when a fixed fraction of a training
dataset is corrupted within the malicious adversary model, successful learning in the PAC sense
is not possible anymore. In other words, there exists no robust learning algorithm that could
overcome the effects of adversarial corruptions in a constant fraction of the training dataset
and approach the optimal model, even in the limit of infinite data.

In this and the next chapter we will study learning from untrusted data in a different setup and
show that by tailoring our adversarial model to a specific scenario, learning against a malicious
opponent becomes possible, both in theory and in practice. Specifically, we will consider a
scenario where a number of datasets, coming from different sources, are given for training. In
addition, we will assume that while an unknown subset of these sets may contain corrupted
data, the other datasets will contain data sampled i.i.d. from the target distribution. We refer
to this problem as “adversarial multi-source learning”. In the next section we argue why such a
setup is highly relevant from a practical perspective. Then, for the rest of this chapter, we will
study the limits of adversarial multi-source learning and provide PAC-style upper and lower
bounds on the performance that a learner can achieve under two strong adversarial models. In
Chapter |4| we will also study the problem from a practical perspective and provide an algorithm
for multi-source learning that is designed to work against more moderate data corruptions,
but exhibits a strong performance in a broad range of experiments.

3.1 Motivation and outline

Due to the outstanding performance of modern machine learning algorithms on various
real-world tasks, there is an increasing amount of interest by practitioners in producing
predictive models, specific to their purposes. In many application domains, however, it may
be prohibitively expensive for a single expert to produce a high-quality labeled dataset, that
is large enough for training a good model. Therefore, it has become a common practice to
obtain data from various external data sources. Examples range from the use of crowdsourcing

23

5.4. Lower bounds

 

malicious adversary A of power a and a hypothesis h* € H, such that with probability at least

0.5
a

 

R(L(S?), P) — R(h*,P) > min { i 2PyP:}

-—a’

and

DP _ pPPip* pp) > mi ao
r* (£(S?), P) —P (FP) > min ef.

The proof of this theorem (as well as of the other hardness results presented in this section) is
based on the so-called method of induced distributions, pioneered by [KL93]. The idea is to
construct two distributions that are sufficiently different, so that different classifiers perform
well on each, yet can be made indistinguishable after the modifications of the adversary. Then
no fixed learner with access only to the corrupted data can be “correct” with high probability on
both distributions and so any learner will incur an excessively high loss and exhibit excessively
high unfairness on at least one of them, regardless of the amount of available data.

Here we provide a sketch proof of Theorem )| to illustrate the type of construction used. A
complete proof can be found in Appendix |C.1|

Proof. (Sketch) Let 1 = ;*., so that a = Ti We assume here that 7 = 7°, < 2Po(1—Po),
with the other case following from a similar construction, but with an adversary that uses a

smaller value of a (so that it leaves some of the data points at its disposal untouched).

Take four distinct points {x1, 72, 3,4} € VY. We consider two distributions Pp and Py, where
each P; is defined as

1—-P—-n/2 ife=xmja=l1y=1

Py -— 7/2 if 2 =2,a=0,y=0

Pi(a,a,y) = 47/2 ife=2%3,a=i,y =i
n/2 ife@=ar,a=-1,y =i
0 otherwise

Note that these are valid distributions, since 7 < 2P)(1— Py) < 2P) < 2(1 — Py) by
assumption and also that Pp = P;(A = 0) for both i € {0,1}. Consider the hypothesis space
H= {ho, hi}, with

ho(#1) = 1 ho(x2) =0 ho(x3) =k, ho(aa) = 0

and
hi(x1) = 1 hi (a2) = 0 hi (a3) = 0 hi(x4) = 1.

The point of this construction is as follows: there are only two points, x73 and x4, where the
two distributions differ. This is also where the classifiers differ and, in fact, each classifier
h; is better performing on the distribution P;, in both accuracy and fairness, than the other
classifier.

Indeed, it is easy to verify that
L(hi,P;) — L(hi,P;) =n, for both i = 0,1. (5.8)

59

Proof. Let Sf, = {i: a? =a,y? = 1,1 ¢ P}. For any a {0,1} we have
PA (5, > (1+ m)F1a + Ata) = PR (Fa > (1+ 17a + Ata
Si,

<P (c, < OS An)

+ YY PRS + 0)A1a + Ate

Se :C1a>F5™ Pian

<8 (Cy < C59) Pn)

 

$5) P(5¢)

 

 

Sq) P#(S¢,)

 

 

Ci, , Ch
p? la la >
* » (7 fe Cha * Cia 7

Se :C1g> U5 Pian

 

 

 

(1+7)%16 + Ata

<8 (cy, < OS An)

C. a
+ Ss ps ( >(1+)F10

S$y:C1a> 5% Pian

< exp (Sore)

 

Se.) P#(S¢,)

 

 

& ) PA(S¢,)

 

8
> exp (—7CeTu un) pa ia)

SP:Cyg> 5 Pran
1—a)Pia a.
< exp (Sgr) + exp (- a — 0) Pisa) :

A similar argument, with the other direction of the Chernoff bounds, gives the other bound.
(b) Similarly to the argument in the proof of Proposition 3)

Ch
Cy a

Bra

Ara = — Pra
' ~ Cia + Bra

 

ia — (C.21)

 

 

Using the inequalities and (C.15),

 

 

 

2a a Bio By 2a °
P* {Aim +A <P | > <o
( ” a) ~ (aes 'Cu+ Bu >a)
(C.22)
Since also
a Cla a Ce A ( a
P G “UF P = SOP (Bin(|S{,|,0) > 0) P*(S{,) =
te Sha

 

we have that 0 < 7, = Ai, almost surely, for both a € {0,1}. Therefore, 0 < 7% + 7h =
Ajo + Ay, and the result follows.

 

 

 

 

146

The thesis of Nikola Konstantinov, titled Robustness and Fairness in Machine Learning, is
approved by:

Supervisor: Prof. Christoph H. Lampert, IST Austria, Klosterneuburg, Austria

Signature:

 

Committee Member: Prof. Dan Alistarh, IST Austria, Klosterneuburg, Austria

Signature:

 

Committee Member: Prof. Ingo Steinwart, University of Stuttgart, Stuttgart, Germany

Signature:

 

Defense Chair: Prof. Edouard Hannezo, IST Austria, Klosterneuburg, Austria

 

Signature:

Signed page is on file

4.3. Robust learning from untrusted sources

 

irrelevant or low-quality data provided by some sources. In this section, we design a more
robust algorithm that instead minimizes a weighted empirical loss.

4.3.1 Theory

Setup. Let ¥ be an input space and Y be an output space. Our theoretical setup covers
both the case of classification (V = {1,2,..., A}) and regression (V = R). We assume that
the learner has access to a small reference dataset Sp := {(@71, yra),---;(*rmp;Yrymr)} of
my samples drawn i.i.d. from a target distribution Dr over X x Y. In addition, training data
is available from N untrusted data sources, each of them characterized by its own distribution,
D;, over XY x Y, possibly different from Dr. We denote the number of samples from source i
by m;. Let the oe! iid. datasets be S; := {(i1, yi) ,---, (®ims, Yimi) tae D;
for each i=1,...,N.

Let 2: V x Y > R, bea loss function, bounded by some M > 0. For any distribution D on
xX x Y and any function h: X — Y, denote by

 

 

 

 

Ro (h) = Ewy~p (E(h(), y))

the expected loss of the predictor h with respect to the distribution P. Let R; (h) = Ro, (h)
be the expected loss of a predictor h on the distribution of the i-th source. Denote by R; the
corresponding empirical counterparts.

Given a hypothesis class H C {h: ¥ — J}, our goal is to use all samples from the sources
to construct a hypothesis with low expected loss on the target distribution Dy. Note that if
we also want to use the reference data at training time, we can simply include it as one of the
data sources.

Source-specific weights. For a vector of weights a = (a1,..., ay), such that S%,a;=1
and a; > 0 for all 7, we define the a-weighted expected risk of a predictor h as:

 

 

 

 

Ral) =Y-aiRi(h) => aBeeyywv, (U(A(2),9)) (4.1)

and its empirical counterpart as:

N N Q; Mm,
= VaR; =D Soe (A(xi,3), Ying) (4.2)
1. = ‘t j=1

With 1 as our hypothesis class, let ha = argminyew Rea (A).

We aim to find weights a, such that the predictor ha performs well on the target task, i.e.
such that Rr(hq) is small.

Evaluating the quality of a source. Intuitively, a good learning algorithm will assign more
weight to sources, whose distribution is similar to the target one, and less weight to those
that provide different or low-quality data. Although any standard distance measure on the
space of distributions could in theory be used to measure such differences, most of them would
not provide any guarantees on the performance of the learned classifier. Furthermore, most
similarity measures between distributions, e.g. the Kullback-Leibler divergence, are hard to
estimate from finite data and overly strict, as they are independent of the learning setup.

39

[SNT*20]

[SS15]

[SSBD14]

[Ste18]

[SZER* 19]

[SZS+14]

[TNKB20]

[TPT 19]

[TRO*19]

[Tuk60]

[TYFT20]

[UAGO5]

[Val84]

[Val85]

[Vap13]

Muhammad Shafique, Mahum Naseer, Theocharis Theocharides, Christos
Kyrkou, Onur Mutlu, Lois Orosa, and Jungwook Choi. Robust machine learning
systems: Challenges, current trends, perspectives, and the road ahead. /EEE
Design & Test, 2020.

Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In ACM
SIGSAC conference on computer and communications security, 2015.

Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning:
From Theory to Algorithms. Cambridge university press, 2014.

Harald Steck. Calibrated recommendations. In Conference on Recommender
Systems (RecSys), 2018.

Piotr Sapiezynski, Wesley Zeng, Ronald E Robertson, Alan Mislove, and Christo
Wilson. Quantifying the impact of user attentionon fair group representation
in ranked lists. In International World Wide Web Conference (WWW), 2019.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, lan Goodfellow, and Rob Fergus. Intriguing properties of neural networks.
International Conference on Learning Representations (ICLR), 2014.

Bahar Taskesen, Viet Anh Nguyen, Daniel Kuhn, and Jose Blanchet. A distribu-
tionally robust approach to fair classification. arXiv preprint arXiv:2007.09530,
2020.

Virginia Tsintzou, Evaggelia Pitoura, and Panayiotis Tsaparas. Bias disparity
in recommendation systems. In Workshop on Recommendation in Multi-
stakeholder Environments at RecSys, 2019.

Ki Hyun Tae, Yuji Roh, Young Hun Oh, Hyunsu Kim, and Steven Euijong
Whang. Data cleaning for accurate, fair, and robust models: A big data-Al
integration approach. In International Workshop on Data Management for
End-to-End Machine Learning (DEEM), 2019.

John W Tukey. A survey of sampling from contaminated distributions. Contri-
butions to probability and statistics, pages 448-485, 1960.

Zilong Tan, Samuel Yeom, Matt Fredrikson, and Ameet Talwalkar. Learning
fair representations for kernel models. In Conference on Uncertainty in Artificial
Intelligence (AISTATS), 2020.

Nicolas Usunier, Massih R Amini, and Patrick Gallinari. Generalization error
bounds for classifiers trained with interdependent data. Conference on Neural
Information Processing Systems (NIPS), 2005.

Leslie G Valiant. A theory of the learnable. Communications of the ACM, 1984.

Leslie G Valiant. Learning disjunction of conjunctions. In /nternational Joint
Conference on Artificial Intelligence (LJCAI), 1985.

Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer, 2013.

103

2.

PRELIMINARIES

 

and do so with high probability with respect to the sampling of the data. If such a learner
exists, then a hypothesis space is in a sense “learnable” - there is a learner that performs well
on it given enough data. This is formalized in the following definition.

Definition 1 (Agnostic PAC Learnability). A hypothesis space H. is agnostic PAC learnable with
respect to a loss function 0: Y x Y + R, if there exists a learner L: US, (¥ x Y)" +H
and a function my : (0,1) x (0,1) +N, such that for any €,6 € (0,1) and any distribution
D, if the learner takes as input a set S of at least my(e,5) points sampled i.i.d. from ‘D, then
with probability at least 1 — 6 with respect to the sampling of the points in S:

R(L(S)) < inf R(h) +6. (2.3)

Whenever H. is agnostic PAC learnable, the (point-wise) smallest possible function my
determines the so-called sample complexity of H, that is, how many samples are indeed so
that an e-good hypothesis can be recovered with probability at least 1 — 6, regardless of the
underlying probability distribution.

Realizable PAC learning An important special case of the PAC learning problem is the
one where the label space is binary and the distribution D under consideration is such that
there exists a perfectly accurate hypothesis in H, that is a hypothesis h* € H, such that
P(ay)~p(h* (a) = y) = 1. Note that this means in particular that the labels are deterministic
given the inputs (up to a set of measure 0). This scenario is referred to in the literature as
the realizable PAC learning scenario. In such a context we have the following definition of
PAC learnability (without the term “agnostic” included, since we are now making a specific
assumption about the type of distributions D that are allowed).

Definition 2 (PAC Learnability (realizable case)). A hypothesis space H. is PAC learnable
with respect to the 0 — 1 loss function, if there exists a learner L : US, (X& x Y)" + H and
a function my : (0,1) x (0,1) +N, such that for any €,6 € (0,1) and any distribution D for
which the realizability assumption holds with respect to H, if the learner takes as input a set
S' of at least my(€,5) points sampled i.i.d. from D, then with probability at least 1 — 6 with

respect to the sampling of the points in S:

Pixyyv(L£(S)(X) #Y) <e. (2.4)

Uniform convergence Since both the training and test samples are assumed to be drawn
from D in an i.i.d. manner, one may hope that the performance of a hypothesis at prediction
time can be estimated by looking at its performance on the data points in S. Therefore, it is
natural to consider the empirical loss of any h € H, given by

Rh) = = ¥e(h(es), uw). (25)

This concept leads to a natural learning algorithm that simply selects a hypothesis by minimizing
the empirical risk, giving rise to the empirical risk minimization principle (ERM principle).
Formally, the ERM learner Liru : US, (X x Y)” > H is defined as

Leu (S) =argminR(h) VS €U%,(¥ xy)". (2.6)
heH

One may hope that if R(Lipas(S)) is small, then this guarantees that the same is true
for R(Lerm(S)) and therefore that the ERM rule necessarily recovers a good hypothesis.

8

6.6. Summary

 

Intuitively, the max values quantify how much an algorithm can improve fairness without
decreasing the ranking quality, while the mean values report the average improvement of
fairness over the values of the trade-off parameters, as a more robust measure. The results
confirm that in all cases our proposed training method is able to greatly reduce the unfairness
in the test time ranking without majorly damaging ranking quality. In comparison, the baselines
behave inconsistently between experiments and are less robust to the choice of the trade-off
parameter, indicating that training with the right regularization, as integrated in our method,
is indeed beneficial for test time fairness.

Table 6.1: Maximal and mean relative fairness increase, achievable without a significant
decrease of ranking quality, for our algorithm and the baselines. See main text for details.

 

Ours DELTR FA*IR Per-query
Max | Mean || Max | Mean || Max | Mean |} Max | Mean
equality of '= 2 48% | 34% |] 39% | 23% |] 56%] 18% |] 19%] -5%

, t=4]|46%| 37% || 2% | -8% || 56%] 11% |] 14%] -4%
opportunity + — 5 || 46% | 32% || 18%| 7% || 6% | -17% || 8% | -13%
demographic’ = ° 27% | 17% || 55% | 36% || 83%] 24% |) 15%] -1%

. t =4]] 44% | 32% |) 12%] 6% ||56%| 10% || 27%] 5%

Parity t=5]]57%| 40% |] 26% | 14% |] 11%] -35% |) 24%] 1%
. t=3 |] 20% | 13% |/ 48% | 31% |[57%| 16% || 14%] -3%
equalized + — 4|/30%| 21% || 9% | 4% |].40%| 3% |/13%| -1%

TREC

 

 

 

 

 

 

 

odds t=5|]29%| 21% |] 21% | 12% || 0% | -35% |] 7% | -4%
average 39% | 27% |) 26% | 14% | 41%] -1% |/ 16%] -3%

* =
MSMARCO Ours DELTR FA*IR Per-query

Max | Mean |) Max | Mean || Max] Mean |} Max | Mean
equality of — com |]/55%| 36% || NA | NA |/64%]| 11% || 24%] 6%
opportunity ext || 19%] 10% || NA | NA || 0% |-112% || 2% | 0%
demographic com |] 42% | 27% || NA | NA |/ 39%] -50% || 0% | 0%

 

 

 

 

 

 

 

 

 

 

 

 

 

parity ext || 20% | 13% || NA | NA || 0% |-168% || 7% | 3%
equalized com || 61% | 41% || NA] NA |] 45% | -12% |/ 14%] 4%
odds ext ||28%| 17% || NA] NA || 0% |-142% || 1% | 0%

average 37% | 24% || NA | NA |/ 25%] -79% || 8% | 2%

 

 

 

 

 

 

 

6.6 Summary

We introduced a rigorous framework for transferring classification fairness notions to the context
of LTR, by rephrasing ranking as a collection of query-dependent classification problems. This
simple viewpoint allows for expanding the optimization methods and proof techniques from
fair classification to ranking and multi-label learning. We report the first, to our knowledge,
generalization bounds for group fairness in ranking and show that including a suitable regularizer
during training can greatly improve the fairness of rankings with no or minor reduction in
model quality. This effect seems even more pronounced than in classification tasks, especially
if the number of relevant items per query is large. We hypothesize that the multi-label nature
of the ranking task naturally allows for more fairness without adverse effects on accuracy.

85

6.4. Fairness in learning-to-rank

 

dependencies complicates the analysis, we believe that it is necessary, so as to make sure that
real-world ranking data, which typically is far from i.i.d., is covered.

We now characterize the generalization properties of the fairness regularizers. Let F Cc {f :
QO x I > {0,1}} be a set of item selection functions that make independent deterministic
decisions per item (e.g., by thresholding a learned score function). Then, the following holds:

Theorem 17. Let S be a dataset sampled as described above with 2Nm > v for v =
VCdim(F). Let 7 = min,« (P(r(@,d) =r A(d) = a)) and v = ming (P(A(a) i a).
Then, for any 6 > 0, each of the following inequalities holds with probability at least 1 — 6
over the sampling of S, uniformly for all f © F:

     
  
   

  
  
 

 

   

 

y | 2eNm) |
re(f) < reH(y,8) + ayf2-e
Nr?
y | 2eNm) |
BEd) << TP f.8) +8 5 og( v )
Nr?

v1 2eNm l 24

POP(f) < POP(y, 8) + 8y/2Uteet =e“) + lout) or eas).

Proof sketch. The proof consists of two parts. First, for any fixed item selection function a
bound is shown on the gap between the conditional probabilities contributing to a fairness
measure and their empirical estimates. For this, we build on the same technique of
for showing concentration of fairness quantities as the one we used in Chapter pI We combine
this with the large deviations bounds for sums of dependent random variables in terms of the
chromatic number of their dependence graph of [Jan04], see below. Secondly, the bounds
are extended to hold uniformly over the full hypothesis space by evoking a variant of the
classic symmetrization argument (e.g. [Vap13}), while carefully accounting for the dependence
between the samples. A complete proof can be found in the supplementary material.

Dealing with the between-sample dependence To deal with the dependence between
the samples, we use the following framework from [Jan04]. Let Y, be a set of random variables,
with a ranging over some index set A. Let X = )0,¢4 Yo. To derive concentration bounds
for X, the following notions are useful:

Definition 21 ([Jan04]). Given A and {Ya}aea:

= A subset A’ C A is independent if the random variables {Y.} aes’ are (jointly) indepen-
dent.

« A family {Aj}; is a cover of A if U;A; = A. A cover is proper if each set A; is
independent.

= (A) is the size of the smallest proper cover of A, that is the smallest integer m, such
that A can be written as the union of m independent subsets.

Then the following result holds, similar to the Hoeffding inequality, but accounting for the
amount of dependence between the random variables {Ya}aca:

79

< 25y4(2n)PS, 51 ([7G(h, $1) — 78(h)] > t/4V [ng(h, $”) — 76(h)| > t/4)
< 45y4(2n)P$, (76h, 5”) — y6(h)| > t/4)

ei on

 

< 1254,(2n) e =
< 125y,(2n) exo ( Bg

Using Py < P, and Sauer’s lemma, whenever 2n > d we have

2en\ 4 Pil—a)PR
Pa, sup |Ya(h) — 7¢(h, S?)| >t) < 12 (=) exp _arie “
hen d 128

Using inversion, we get that

2d log (24) + pal) < 5 (C.10)
—ti_#ns (=2 .

pt, sup |ya(h) — 76(h, S?)| > 8
: (sap s(h, S| > ie

whenever

 

 

dlog(*) + 2log(12/6) 8 log(6)
; 52 (1 —a)Pon > (1—a)Pon’

It's easy to see that the right inequality holds whenever 6 < 1 and 2n > d. In addition,
inequality (C.10) trivially holds if the left inequality is not fulfilled. Therefore, (C.10) holds
whenever 2n > d.

Step 3 Finally, we use (C.7) and (C.10) to proof the lemma. Recall from the proof of
Lemma 3) that

[PPP (h) —PPP(h)| = [IG (HS?) — af (h, 5?) — nol) — 12(7)I
S |n0(h, S”) — o(h)| + l7i(2, 5?) — a(A)| + Aol) + Ar(h).

Therefore,

sup |P??(h) — PPP(h)| < sup |79(h, 5”) — yo(h)

 

heH
+ sup |yi(h, S?) — yi(h)
heH
+ sup(Ao(h) + Ai(h)).
heH

Now, using the union bound and inequalities (C.7) and (C.10), whenever

8log(8/d) 12log(6/5) s

be ;
n> max { FEC. 0 15

we get

. 2d log(22) + 2 log (48/5)
p&, | sup |F2?(h) — PP (h)| > APP + 16,| ——S 4
5 [sp (h) (h)| 2 +16 (1—a)Pyn

2d log (2) + 2 log(48/6)
af. af eB 8 (=F g
< PS s -—4 ro
< PS (sn lyo(h) — 7(h, S?)| = 8 i — abn

141

[ZZY12] Chao Zhang, Lei Zhang, and Jieping Ye. Generalization bounds for domain
adaptation. In Conference on Neural Information Processing Systems (NIPS),
2012.

[ZZY13] Chao Zhang, Lei Zhang, and Jieping Ye. Generalization bounds for domain
adaptation. arXiv preprint arXiv:1304.1574, 2013.

106

[YCKB19]

[YCRB18]

[YDJ19]

[YGS19]

[YS17]

[ZBC*17]

[ZC20]

[ZHC18]

[ZIK17]

[ZL17]

[ZLM18]

[ZSCK20]

[ZVRG17]

[ZXXS17]

Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Defend-
ing against saddle point attack in Byzantine-robust distributed learning. In
International Conference on Machine Learing (ICML), 2019.

Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. Byzantine-
robust distributed learning: Towards optimal statistical rates. /nternational
Conference on Machine Learing (ICML), 2018.

Himank Yadav, Zhengxiao Du, and Thorsten Joachims. Fair learning-to-rank
from implicit feedback. arXiv preprint arXiv:1911.08054, 2019.

Ke Yang, Vasilis Gkatzelis, and Julia Stoyanovich. Balanced ranking with
diversity constraints. In International Joint Conference on Artificial Intelligence
(IJCAI), 2019.

Ke Yang and Julia Stoyanovich. Measuring fairness in ranked outputs. In
Scientific and Statistical Database Management Conference (SSDBM), 2017.

Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed
Megahed, and Ricardo Baeza-Yates. FA*IR: A fair top-k ranking algorithm. In
Conference on Information and Knowledge Management (CIKM), 2017.

Meike Zehlike and Carlos Castillo. Reducing disparate exposure in ranking:
A learning to rank approach. In International World Wide Web Conference
(WWW), 2020.

Ziwei Zhu, Xia Hu, and James Caverlee. Fairness-aware tensor-based recom-
mendation. In Conference on Information and Knowledge Management (CIKM),
2018.

Guoxi Zhang, Tomoharu Iwata, and Hisashi Kashima. Robust multi-view topic
modeling by incorporating detecting anomalies. In Joint European Conference
on Machine Learning and Knowledge Discovery in Databases, 2017.

Alexander Zimin and Christoph H. Lampert. Learning theory for conditional risk
minimization. In Conference on Uncertainty in Artificial Intelligence (AISTATS),
2017.

Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted
biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM
Conference on Al, Ethics, and Society, 2018.

Meike Zehlike, Tom Sthr, Carlos Castillo, and Ivan Kitanovski. Fairsearch: A
tool for fairness in ranked search results. In /nternational World Wide Web
Conference (WWW), 2020.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P
Gummadi. Fairness constraints: Mechanisms for fair classification. In Conference
on Uncertainty in Artificial Intelligence (AISTATS), 2017.

Jing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. Multi-view learning overview:
Recent progress and new challenges. Information Fusion, 38:43-54, 2017.

105

2.

PRELIMINARIES

 

adversary can ensure that this subpopulation is never present in the corrupted dataset at all.
In contrast, the malicious adversary can not in general ensure that, since the subset of the
data that it can manipulate is random.

In particular, the hardness results implying the impossibility of learning against a malicious
adversary transfer also to the case of the nasty adversary. In fact, for binary classification one
can show that a nasty adversary of power a can always ensure that the error incurred by a
learning algorithm is at least 2a [BEKO2}.

2.3. Fairness in machine learning

We now turn to another topic in trustworthy machine learning that is studied in this thesis,
namely the one of fairness. As argued in the introduction, with machine learning models
penetrating modern automated decision-making systems, it is becoming increasingly important
to ensure that ML systems do not discriminate individuals or organizations based on various
characteristics protected by law or by certain ethical standards. Such considerations are
especially relevant for tasks such as automated screening of job or loan applications, but also
for medical data, where it is desirable that a disease recognition model works well for people
of all races, say.

Here we discuss what types of fairness notions are commonly considered in the machine
learning literature. Then we focus on one of these types, namely on group fairness notions. We
discuss how these can be studied alongside with accuracy in the context of binary classification.
We also present several group fairness notions that will be studied in this thesis. The short
discussion in this section necessarily touches only on a few topics in fair machine learning. For

a more thorough introduction we refer to [BHN19] |MMS*19}.

2.3.1 Types of machine learning fairness

Fairness being a rather general and sophisticated philosophical concept, there are many ways
to rigorously define it in the context of supervised machine learning. Focusing on the context
of classification, there are three commonly considered types of fairness, namely individual,
group and counterfactual fairness.

Individual fairness Individual fairness, first discussed in a machine learning context by
[DHP*12}, is a constraint on machine learning systems that aims to ensure that, informally,
similar instances (e.g. individuals) are treated similarly by the classifier. The fairness notion
is individual in the sense that it is designed to protect every instance in the population, as
it guarantees that each instance is treated similarly to others like it. Formally, the notion
of individual fairness depends on two distance metrics - one on the space of the inputs
(say, the space of applicants for a job) and one on the space of classifier outputs (e.g. the
square distance between the two scores given by the classifier to two applicants). Individual
fairness is then understood as a Lipschitz property of the classifier with respect to these two
distance metrics [DHP*12]. Choosing the distance notions is therefore the key to incorporating
application-specific fairness considerations.

Group fairness Groups fairness, e.g. IHPS16}, refers to the concept that the
decisions of a classier should, on average over the population, be taken without exhibiting a
discriminative behavior with respect to a certain protected attribute (e.g. race or gender) of

18

6.4. Fairness in learning-to-rank

 

For any k&, the P@k value reflects which items appear in the top-k list, but not their ordering.
Moreover, P@k is automatically small for datasets in which queries have few relevant documents.
To mitigate these shortcomings, one can add position-dependent weights and normalize by
the score of a best-possible ranking.

Definition 15. /n the same setting as for Definition |14| the normalized discounted cumulative
gain at k is defined as NDCG@k = + oN, NDCG@k(q) for

min(k,A;) 1

NDCG@k(q) = (> we) / ( yy

J loge +1 5a

) (6.3)

log,(j +1

where IK; = |{d € Zg, : r(qi,d) = 1}| is the number of relevant items for query q;. Queries
with no relevant items are excluded from the average, as the measure is not well-defined for
these.

The role of k in these definitions is twofold. Firstly, in most applications only a fixed number
of items, k, can be retrieved in total and so one is only interested in the performance of the
ranker up to the k-th document. Secondly, multiple values of k can be considered for the
quality of the ranking system to be better assessed.

6.4 Fairness in learning-to-rank

We now introduce our framework for group fairness in ranking. The main step is to exploit a
correspondence between ranking and multi-label learning, a view that has previously been em-
ployed for practical tasks, e.g., in extreme classification [BDJ*19}, but not —to our knowledge—
to make LTR benefit from prior work on classification fairness.

Specifically, we study how the fairness of the subset selection function (6.1) can be evaluated.
The objects for which we want to impose fairness, the items, occur as outputs of the learned
function. This makes it hard to leverage fairness notions from classification, which are defined
with respect to the inputs.

We advocate an orthogonal viewpoint: for any fixed query g, we treat the items not as elements
of the predictor’s output, but as the inputs to a query-dependent classifier: f, : Z — {0,1},
where f,(d) = 1, if item d is should be returned for query g, and f,(d) = 0 otherwise. As the
query is a priori unknown, this means one ultimately has to find an item selection function

f:QxT{0,u. (6.4)

While, of course, both views are equivalent, the latter one allows us to readily integrate notions
of classification fairness into the LTR paradigm.

Note that even though the item selection function f(q,d) and the relevance label r(q, d) have
the same signature, their roles are different. 1 specifies if an item is relevant for a query or
not. f indicates if the item should be returned as a result and hence characterizes the impact
of the decisions made by the ranking system. While f will in general be an approximation of
r, as learned by the LTR model, f will also likely depend on other factors. For example, if at
most k items can be retrieved, but more than & are relevant, some relevant items will end
up not being included. Additionally, the choice of f may be influenced by diversity, or, as we
argue, fairness considerations.

G5

>t

1
+ yo Pa ((@ — Ya

S8:Ca> U5 Pan

 

 

s) Ps

< exp (-— + Ss 2exp (-20?C,) P# (5°)

SP:Ca>U5 Pan
<=+2exp (-?( = a) Pan) :
S38 ;

where the second inequality follows from Hoeffding's inequality. Note that this step crucially
uses that the marked indexes are independent of the data. The triangle law gives

Ing — 4] — lo — nl < yo — HY —% +1 < bo — Yl 4

 

 

Hla ml
< | — Wl + 1% - ol + 1 -— V+ lt - Hl
= |99 — ol + yi — Mi] + Ao + Ai.

 

Combining the previous two results (recall that we assume Py < P,)

PA (198 — 21 — ly — nl] > 2t+ Ao + Ar)
< Pa (96 — yol + lyf — i] + Ao + Ai > 2t + Ao + Ar)
< P* ((I9§ — ol > t) V (laf — nil > #))
< PF (lo§ — yl > #) + PA (nf — ul > 4)

5
< at 4exp(—t?n(1 — a) Pp).

Setting t = to = ae gives

 

log(16/0) } nr)
P# | |i? — | — lo - Ao + A1+2,/ —2 | < = 44525. C4
(ib l= ho —nll > Bot A +24] OES | ct aa. (CA)
In addition Proposition [2| gives
P(A) +A; > AP?) < > (C.5)
Using (C.4) and (C.5) we obtain that:
log(16/65)
P3 [42 — | — lo — vil] < AP? +.2,| —B
(i 41 — |v - all < +2, Nd —a)Py
log(16/6)
2 Ee ((ir >t ~ ay — nl < Ao + At + 2, NI -a)B A (Ao t+ Ar Ss APP)
6.6

>1-=-==1-6.
- 2 2

 

 

 

 

Finally, we show how to extend the previous result to hold uniformly over the whole hypothesis
space, provided that H has a finite VC-dimension d := VC(H)

138

About the Author

Nikola Konstantinov graduated from the Sofia High School of Mathematics in 2013 and
went on to complete a Master's degree in “Mathematics and Statistics” at the University of
Oxford. His master thesis, titled “Kernel Dependence Measures for Unsupervised Learning”,
was conducted under the supervision of Prof. Dino Sejdinovic. In 2017 Nikola joined IST
Austria as a PhD Candidate, doing rotations on multi-task learning (with Prof. Christoph H.
Lampert), distributed optimization (with Prof. Dan Alistarh) and evolutionary game theory
(with Prof. Krishnendu Chatterjee) and doing coursework in data science and computer science.
Nikola then joined the group of Prof. Christoph H. Lampert, where he worked on topics
related to robustness and fairness in machine learning. In 2019 he did an internship in the
“Intelligent Cloud Control” group of Amazon.com, where he developed evaluation and model
comparison methods for predictive models in cloud computing. In 2020 Nikola became an
ELLIS PhD student and as part of this program he visited Prof. Nicolé Cesa-Bianchi at the
University of Milan.

the following distribution on ¥:

Pp(a1) =1—4e and Pp(x2) = 4e, (A.28)

where € = ar: Assume that the points are labelled by a function f € H (to be chosen later

as either /; or hz). Denote the initial uncorrupted collection of datasets by S’ = (S{,..., Si),
with Si = {(2j 1, f(t) 1))s +++ 5 (@hins f(@im))} and ai; being i.i.d. samples from D.

First we show that with constant probability the point x) appears at most am times in G.
Indeed, let C' be this number of appearances. Then C is a binomial random variable with
probability of success 4e and number of trials (1 — a) Nm. Therefore, by the Chernoff bound:

 

ainvin 1
Pg(C > aNm) = Py (C > (1+ 1)4e(1 —a)Nm) < eo" < eV < a (A.29)

and so: 3
Ps(C <aNm) > 50" (A.30)

Now consider the following policy for the fixed-set adversary @* : S’ + S. For any index
i © [N] the adversary replaces Si = {(2x/1, f(2j1)),---5 (hms f(Cim))} with a dataset
Si = {(@i1, Yi) -- ++ (Lim; Yim) }, such that:

(ij, f(ais))) fie @=[1,2,...,k]
(2.5, Yi3) = \ (t2,—-f(v2)), fie [k+1,...,N] and (@-—k-1)m4+j<C  (A31)
(a1, f(x1)), otherwise

Then the adversary returns S' = (S},..., Sy). That is, the adversary keeps the datasets in G
untouched, and fills the datasets in [N’]\G with as many 2's as there are in G, but wrongly
labelled.

Crucially, whenever C' < aNm, the union of the data in all N sets will look the same no
matter if the original labelling function was hy or hg. In particular, £(@*(S’)) will be identical
in both cases.

Finally, we argue that under the event C’ < aNm and the chosen adversary, the learner would
incur high loss and show that this implies the result in (A.27). Let S be the set of all datasets
in (X x Y)**™, such that C < aNm holds. We just showed that Pg/(S’ € S) > 2 and
that whenever S’ € S, £(A*(S’)) is independent of whether the original labelling function was

hy or ho.

Consider a fixed set S’ € S and let S = @*(S') and hg = L(S). Denote by R(hs, f) =
Pp(hs(x) 4 f(x) Na # x) and note that R(hs, f) < Po(hs(x) 4 f(x)) = R(L(A*(S’))).
Notice that:

R(hg, hi) + R(hs, ha) = D2 Uns (ei¢m(e) Loar P(@i) + D7 Lng (ei)¢hale)Lae¢e, Pai)

i=1,2 i=1,2
= Lng (v2) ¢hi (#2) 4€ ste ns (v2) ¢ho(a2) 4€
= 4e,
where we used that hi(x2) = 1 = —ho(x2) and that hg is independent of the underlying

labelling function.

114

 

 

 

5:3: Preliminaries|'s ¢::a meee BRE Sa WER Bw aes ww eee sw eae 54
4. Towerbounds| 2: :3 @ 4:35 84832585) @G ota Mw eee mB es 58
5.5 Upper bounds ....... 2... 0000000000200 00 000000. 62
5.6 Summary and discussion... . 2... 0.00000. 020 00000000. 68

6 Fairness through Regularization for Learning to Rank 71
6.1 Motivation and outline. 2... ee 71
6.2 Related work. 2 2. ee 72
6.3. Preliminaries... 2... ee 74
6.4 Fairness in learning-to-rank 2... ee 75
6.5 Experiments .. 2... 2... 0.020000 0000000000000 000. 81
6.6: Summaty|: 2s cv ea mk kb me RE me BM A Mi SE aoe See Rw 85
{Discussion and future work 87
Bibliography 91
A Proofs from Chapter |3 107
A.1_ Proof of Theorem |4/and its corollaries)... 2.2... ee 107
A.2 Proof of Theorem[b]). 2: sia eee caw eee ee 113
A: Proofof [heoreml6]. «2 ::a% eae ee es BM EEE MS PGR wee: 115

B_ Proofs from Chapter |4| 119
C_ Proofs from Chapter|5 123
.1 Lower bounds proofs... 2.2... 0... ee 123

.2 Upper bounds proofs... 2.2... 0.000.000.0000 00000000. 134

D Proofs from Chapter }6 155
D.1 Non-uniform bounds)... 2... 0.000.002.0000 0002200000. 155
D.2 Uniform bounds on proof of Theorem 17)... 2... ........200.. 157

List of Figures

give the standard error of the estimates.| .. 2.2... ......-00... 4
4.2 Results for the attribute "black" from the Animals with Attributes 2 dataset. Each
plot corresponds to a different contamination type. The x-axis gives the number

algorithms, achieved over 100 different runs. Error bars correspond to the standard
deviation around those means.|. .. 2... 2 4

 

a

 

 

NX

5.5. Upper bounds

 

We note that the algorithms studied in this section require the knowledge of a and of Py and
Pi for demographic parity and equality of opportunity respectively, since they explicitly use
these quantities when selecting a hypothesis. Estimates of these two quantities can often
be obtained in practice, for example by having the quality of a small random subset of the
data S? verified by a trusted authority, or via conducting an additional survey /crowdsourcing
experiment.

Bound for demographic parity Given a corrupted dataset S? = {(x?,a?,y?)}, let he
argminyes, R(h) and h?? € argmin, 4 12?(h). Further, we define the sets

8dlog($) + 2log(16/6)
n

Hi = {" EH: Rh) — Rh") < 3a +4

H2 = She H: TPP (h) —TPP (APP) < 2APP 4 32 adlog(?y") + 215g (96/5)
(1—a)Pon

That is, 1 and Hz are the sets of classifiers that are not far from optimal on the train data,

in terms of their risk and their fairness respectively. The upper bound terms are selected

according to the concentration properties of the two measures and describe the amount of

expected variability of those, due to the randomness of the training data. Now define the

component-wise learner:

anyhEHiNHe, ifHiNHe 40
any h eH, otherwise,

£2P (50) =

that returns a classifier that is good in both metrics, if such exists, or an arbitrary classifier
otherwise. Then the following result holds.

Theorem 14. Let H. be any hypothesis space with d = VC(H) < oo. Let PE P(X x Axy)
be a fixed distribution and let A be any malicious adversary of power a < 0.5. Suppose that
there exists a hypothesis h* € H, such that U(h*) < Wh) for allh © H. Then for any
6 € (0,1) andn > max { paces) , tesG2/6) a}, with probability at least 1 — 6:

(1-a)Po ? a

L?? (LPP (9?)) = (0+ oO (\<) AAPP 4.6 (Ea) .

Since AP? = O (4), in the large data limit we obtain that

 

LPP (£2?) < (01a),0 (F)) (5.20)

Note that this bound is order-optimal for the class of finite hypothesis spaces, and hence
also for the class of hypothesis spaces with finite VC dimension, according to Theorem [8|and

Inequality (5.13).

Bound for equality of opportunity Similarly, let 2°? € argmin,.,,%°?(h). Further,
we define the set

2d log (78) + 2 log(96/5)

a . PEOp __ PEOp/7 EOp < EOp | «
Hs = 4h eH : BEOP(h) — BEOP(RBOP) < 2APOP 4 39 a En

65

Proof. Fix any i,j and any 211,.-.,2ZN,my; Ze Denote the a-weighted empirical average of
the loss with respect to the sample z1,1,... Bias . ++) ZNymy_ by Ri. Then we have that:

1 A A

JO (5-5 28g) — OC Zgee DI = [sup (Ra (f) — Ra (f)) — sup(Ra (f) — Ra (F))

SEH fEeH

 

< | sup(R, (f) — Ra (f))|

SoH
= in, [UP (4(f(tig), Yas) — 2 (Fig), ws)
< “vy

M3,

 

 

 

Note: the inequality we used above holds for bounded functions inside the supremum.

 

Let S denote a random sample of size m drawn from a distribution as the one generating out
data (i.e. m; samples from D; for each i). Now, using Lemma|2| McDiarmid's inequality gives:

 

F(5)- B19) 29 <0 n Jeo (-z - z|

2
N wan @2 470
ial 2a me 7

i=1 m,

For any 6 > 0, setting the right-hand side above to be 6/4 and using (B.3), we obtain that
with probability at least 1 — 6/4:

Ra (h) < Re (h) + Es (sup (Ralf) - Ralf)

 

To deal with the expected loss inside the second term, introduce a ghost sample (denoted by
S"), drawn from the same distributions as our original sample (denoted by S). Denoting the
weighted empirical loss with respect to the ghost sample by R,,, 8; = mj/m for all i, and
using the convexity of the supremum, we obtain:

 

 

 

 

 

 

 

 

(sup (Ra(f)— Ro(f)) = Es (sup (Be (a(0)) ~ Ra)

< Ess (sp
feH

 

 

PN
a
ont
—~
Sy
WV
|
a
2
—~
SS
WV
SX
nN

 

 

 

 

1 Nir Qj; 1 , ¥
=> (in (EES (ur —eoeanns))
Introducing m independent Rademacher random variables and noting that ¢(f(2’),y') —
(f(x), y) and o (ef(2),¥) - e(f(2),y)) have the same distribution, as long as (x,y) and

120

Therefore
a - =P EO: a - =P EO
P ax 7, (h) < AP?) <P < 2APO? — Ey) — E
(gain max (ht) < \< (sin max, < 10 u)
+ P# (Ey) + Ey, > A?)

cs (C31)

|

Finally, using (C.30) and (C.31),

P# (H" A (Hi U He) £0) = PB (nin Re(h) < =) V (sain max Yi, (h) < A")
5d 3d
arts:
26
3B

Step 3 Combining steps 1 and 2, we have that with probability at least 1— 0, h* € H* (and
so H* is non-empty) and for any h € H, R(h,P) < i and [2P(h) < rd which
completes the proof.

 

 

 

 

154

3.3. Preliminaries

 

= given m samples S = {(21,41),.--;(2msYm)} ‘” D, with probability at least 1 — 6
over the data :
sup |R(h) — R(h)| < sue (m,6,S), (3.3)

heH

where R(h) is the empirical risk of the hypothesis h.

= sue(m,6,Sm) + 0 asm — oo, for any sequence (Sin)men with Si, € (& xy)”.

It is easy to see that the this version is equivalent to the one in Definition 3) We only state
the property with the sample complexity used as an explicit bound on the gap between the
empirical and the true risk, as this simplifies the layout of our analysis later on.

Throughout this chapter we drop the dependence of 57. on H and ¢ and simply write s.

3.3.2. Multi-source learning

Our focus in this chapter is on learning from multiple data sources. For simplicity of exposition,
we assume that they all provide the same number of data points, i.e. the training data consists
of NV groups of m samples each, where m, N € N are fixed integers.

Formally, we denote by (4 x )"*”" the set of all possible collections (i.e. unordered sequences)
of N groups of m datapoints each. A (statistical) multi-source learner is a function L :
U3?_, US_, (x Y)"*”" — H that takes such a collection of datasets and returns a predictor
from H.

3.3.3 Robust Multi-Source Learning

Informally, one considers a learning system robust if it is able to learn a good hypothesis,
even when the training data is not perfectly i.i.d., but contains some artifacts, e.g. annotation
errors, a selection bias or even malicious manipulations.

In lines with the discussion in Section [2.2.2] we model this by assuming the presence of an
adversary, that observes the original datasets and outputs potentially manipulated versions.
The learner then has to operate on the manipulated data without knowledge of what the
original one had been or what manipulations have been made.

The multi-source analogue to the definition of a single-source adversary is as follows:

Definition 9 (Adversary). An adversary is any function @: (&xy)X*™ > (Xx yyN*™.

Throughout the chapter, we denote by S’ = {S},S5,...,S\} the original, uncorrupted
datasets, drawn i.i.d. from D, and by S = {S), S2,..., Sx} = A(S’) the datasets returned by
the adversary.

Different scenarios are obtained by giving the adversary different amounts of power. For
example, a weak adversary might only be able to randomly flip labels, i.e. simulate the presence
of label noise. A much stronger adversary would be one that can potentially manipulate all
data and do so with knowledge not only of all of the datasets but also of the underlying data
distribution and the learning algorithm to be used later.

Here we adopt the latter view, as it leads to much stronger robustness guarantees. We define
two adversary types that can make arbitrary manipulations to data sources, but only influence

27

[KLRS17]

[KMR17]

[KMZ20]

[KTK12]

[KVR19]

[KZ18]

[KZ19]

[LBC*20]

[LBSS21]

[Liu11]

[Lo15]

[LZMv19]

[MAA 16]

Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual
fairness. In Conference on Neural Information Processing Systems (NIPS),
2017.

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs
in the fair determination of risk scores. Innovations in Theoretical Computer
Science Conference (ITCS), 2017.

Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness
with unobserved protected class using data combination. In Conference on
Fairness, Accountability and Transparency (FAccT), 2020.

Hiroshi Kajino, Yuta Tsuboi, and Hisashi Kashima. A convex formulation
for learning from crowds. Transactions of the Japanese Society for Artificial
Intelligence, 27(3):133-142, 2012.

Caitlin Kuhlman, MaryAnn VanValkenburg, and Elke Rundensteiner. FARE:
Diagnostics for fair ranking using pairwise error metrics. In /nternational World
Wide Web Conference (WWW), 2019.

Nathan Kallus and Angela Zhou. Residual unfairness in fair machine learning
from prejudiced data. In International Conference on Machine Learing (ICML),
2018.

Nathan Kallus and Angela Zhou. The fairness of risk scores beyond classification:
Bipartite ranking and the xAUC metric. In Conference on Neural Information
Processing Systems (NeurIPS), 2019.

Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain,
Xuezhi Wang, and Ed H Chi. Fairness without demographics through adver-
sarially reweighted learning. In Conference on Neural Information Processing
Systems (NeurIPS), 2020.

Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical
risk minimization. In International Conference on Learning Representations
(ICLR), 2021.

Tie-Yan Liu. Learning to Rank for Information Retrieval. Springer Science &
Business Media, 2011.

Danica Lo. When you Google image CEO, the first female photo on

the results page is Barbie. |https://www.glamour.com/story/
2015. Accessed: 2021-05-26.

Alex Lamy, Ziyuan Zhong, Aditya K Menon, and Nakul Verma. Noise-tolerant
fair classification. In Conference on Neural Information Processing Systems
(NeurIPS), 2019.

Frank McKeen, Ilya Alexandrovich, Ittai Anati, Dror Caspi, Simon Johnson,
Rebekah Leslie-Hurd, and Carlos Rozas. Intel® software guard extensions
(intel® sgx) support for dynamic memory management inside an enclave. In
Proceedings of the Hardware and Architectural Support for Security and Privacy
2016, page 10, 2016.

99

Acknowledgements

First and foremost | would like to express my sincere gratitude to my supervisor Christoph
Lampert, who has continuously supported me throughout my PhD journey. Under his
supervision | had the privilege to be able to fully commit to curiosity-driven, independent
research, with sufficient time and resources given to me to allow for deep and rigorous
exploration of the topics that interested me most. Thank you, Christoph, for teaching me so
much not only about machine learning, but also about research in general.

| would also like to thank all members of my thesis committee for their invaluable guidance
throughout my PhD studies. In particular, special thanks goes to Dan Alistarh for all the
support, exciting discussions and interesting work we did together, especially during the early
phases of my PhD, and for teaching me a great deal about optimization and lower bounds. |
would also like to thank my external member, Ingo Steinwart, for his very helpful feedback
during the qualifying exam, the progress reviews and the defense. Finally, many thanks to
Edouard Hannezo for kindly stepping in as an exam chair for both the qual and the defense.

lam very grateful to Nicold Cesa-Bianchi and his group for warmly welcoming me at the
University of Milan, during my ELLIS exchange, and for many exciting discussions we had about
online learning and multi-armed bandits. | would also like to thank Krishnendu Chatterjee and
Christian Hilbe for the interesting rotation project on evolutionary game theory, as well as the
members of the ICC group at Amazon for welcoming me during my internship there.

Special thanks goes to Dino Sejdinovic, my master thesis supervisor, who first showed me the
exciting side of science and ML theory and fully supported me in my intention to pursue a
PhD; as well as to Veliko Kolev, my high school teacher, whose classes and methods taught
me that in mathematics, and life, the only person that can solve one’s problems is oneself.

An important part of my PhD have been the fun and inspiring times spent with the rest of
the MLCV group members. Many thanks to Alex K., Alex P., Alex Z., Amelie, Bernd, Jonny,
Mary, Paul and all our interns and rotation students! The same holds for all members of the
ML community at IST, especially everyone who took part in the MLCV tea talks and reading
group. Thank you all, | have learned a lot from you. Special thanks goes to my co-authors
Elias and Eugenia - for the interesting research we did together and for sharing my excitement
about lower bounds and discrepancies.

My PhD times would not have been as much fun without the friendly, open, dynamic and
international environment at IST. Thank you to all my friends and colleagues for the great times
and especially to my 2017 cohort-mates for the many enjoyable outings we had together. Special
thanks goes to Bernd, Harald, Heloisa, Nika, Vlad and Wojciech for the many memorable
experiences we have shared. A warm thank you is also due to my non-IST friends, who have
(hopefully) helped me to preserve my sanity, especially to Alastair, Tsvetomir and Venci.

Last, but definitely not least, | would like to thank my family, and especially my parents and
sister, whose love and endless support made this thesis possible.

viii

4.5. Summary

 

The results in Table 4.2) show that our method performs significantly better than all baselines
for many types of corruption and many values of n, especially for high levels of contamination,
while essentially never performing significantly worse than any baseline.

In addition, we performed experiments in which a proportion p of the samples in the n
corrupted sources are modified (instead of all of them). Apart from p = 1, we experimented
with p = 0.5 and p = 0.2. We present the same table of results for these cases in Table 4.3)
and |4.4| respectively.

Table 4.3: Summary of the results for p = 0.5, over all 85 prediction tasks and all corruptions.

 

"ll n=0 n= 10 n= 20 n= 30 n= 40 n= 50 n= 55 n= 59

 

 

 

Baseline
Reference only 84/1/0 508/2/0 | 501/9/0 | 488/22/0 | 471/39/0 | 424/86/0 | 303/207/0 |] 156/354/0
All data 0/85/0 || 0/510/0 | 82/428/0 | 158/352/0 | 215/295/0 | 241/269/0 | 223/287/0 || 168/342/0
Median of probs. || 9/76/0 || 30/480/0 | 53/457/0 | 93/417/0 | 189/321/0 | 272/238/0 | 252/258/0 || 216/294/0
8/77/0 || 28/482/0 | 19/491/0 | 84/426/0 | 172/338/0 | 254/256/0 | 253/257/0 || 217/293/0
[YCKB18) 14/71/0 |] 123/387/0 | 227/283/0 | 155/355/0 | 247/259/4 | 295/215/0 | 282/228/0 || 224/286/0
55/30/0 || 287/223/0 | 282/228/0 | 329/181/0 | 350/160/0 | 358/152/0 | 374/136/0 || 367/143/0

Batch norm 0/85/0 || 2/508/0 | 78/432/0 | 139/370/1| 183/326/1 | 186/323/1 | 155/354/1 || 97/412/1

 

Table 4.4: Summary of the results for p = 0.2, over all 85 prediction tasks and all corruptions.

 

n

 

 

 

Baseli n=0 n= 10 n= 20 n= 30 n= 40 n= 50 n=55 n= 59
aseline

Reference only 84/1/0 507/3/0 | 505/5/0 | 504/6/0 | 492/18/0 | 459/51/0 | 429/81/0 |] 404/106/0
All data 0/85/0 || 0/510/0 | 0/510/0 | 0/510/0 | 0/510/0 1/509/0 | 2/508/0 1/509/0
Median of probs. |} 9/76/0 || 28/482/0 | 21/489/0 | 16/494/0 | 17/493/0 | 28/482/0 | 30/478/2 || 31/479/0
|FXM 14] 8/77/0 || 30/480/0 | 24/486/0 | 16/494/0 | 16/494/0 | 20/489/1 | 23/485/2 || 26/484/0
14/71/0 || 95/415/0 | 146/364/0| 34/476/0 | 42/468/0 | 39/471/0 | 36/474/0 || 40/470/0
[Pre82| 55/30/0 || 282/228/0 | 282/228/0 | 275/235/0 | 287/223/0 | 264/246/0 | 281/229/0 || 267/243/0

 

 

 

 

 

 

 

 

 

 

 

 

 

Batch norm 0/85/0 || 0/510/0 | 0/510/0 | 0/510/0 | 0/510/0 | 0/509/1 | 0/509/1 || 1/508/1

 

4.5 Summary

We introduced an algorithm for learning from data provided by multiple untrusted sources.
It incorporates information from all of them, while being robust to arbitrary corruptions and
manipulations of the data. By making use of the grouped structure of the task and a reference
dataset, the method is able to successfully learn even if more than half of the available data is
corrupted or uninformative. Our method is theoretically justified and easy to implement, even
in cases when the data is decentralized and/or private. We demonstrated its effectiveness
through two sets of extensive experiments, showing its superior performance to all baselines,
for various levels and types of corruption.

49

where

 

APO? — 3ar+ 2KAPOP + O (\é + fs)
b 10

2a a
APOp 0 _ -o(+).
Pyo/3 +a Pro

and

Proof. Similarly to the proof of Theorem {12| we combine

pa (wp (R?(h) — R(h)| > ea + 24 Sdlog() + “ests/) < a (C.25)
hEH 2 n 2

 

and Lemma |5|
a 2dlog(2") + 21 6
pa, sup |P20"(h) — T#9P(h)| > A#OP + 16 2dlog(“7) + 2log(96/4) < 8 (C.26)
heH (1—a)Pion 2

Now let hy = areminyey(RP(h) + AT? (h)) and let

(en s 2en .
Abr Sat /8dlog($) + 2log(16/6) | 2ABOP 4 39 2d log( #5") + 2 log(96/9)_
n (1 — a)Pion

Then we have that with probability at least 1 — 6

 

 

LXOP(LNOP(S)) = R(LAOP(S?)) + ADPOP(LSO"(S"))
< RP(LE°(S")) + PPOM(LPP (SH) + SAFOP
= a 1
=mi EO. EO;
= min (RP(h) + APPOP(h)) + 5x”

< min LY°?(h) + AX??.
S min Ly'"(h) + Ay

 

 

 

 

Component-wise upper bounds

We now prove the component-wise upper bound results.

Bound for demographic parity Recall our notation h” € argminyey R?(h) and h?? €
argmin,c,, ?"(h). Further, we define the sets

_ _ dlog(@) + 210g (16/6
1 = {he HR) RO) < 044 Stee) iets}

2en
H. = SheH: TPP (h) —TPP (APP) < 2APP 4 32 2d log(“r) + 2108(96/9) .
(1—a)Pon

149

2.

PRELIMINARIES

 

convergence property is more likely to be violated or at least occur at a slower rate. Therefore,
because of the finite sample size, the ERM hypothesis might not generalize and perform poorly
at test time.

It turns out that this reasoning applies to learning more generally, resulting in the so-called
bias-variance trade-off. For hypothesis spaces that are too “simple”, there might be no
hypothesis that performs well on a given distribution, resulting in a large, irreducible bias
term. At the same time, whenever a hypothesis space is too “rich”, there might be multiple
hypothesis that appear as good candidates based on the train data. However, the limited
sample size and the large number of potential candidates result in a corresponding variance
term that describes the hardness of accurately estimating the properties of the true data
distribution given the training set only. In-between there is a sweet spot, where a hypothesis
space whose “complexity” is the right one for the task at hand hits the best possible trade-off
between bias and variance. Note that this reasoning can be seen as an instance of the Occam
razor: out of various possible statistical models for a given task (that is, various choices of
H), one should select the one that works (has small bias) and is as simple as possible (has
small variance).

These considerations emphasize on the importance of studying various notions of the complexity
(i.e. richness) of hypothesis spaces in the context of learning. One may hope that an appropriate
notion of complexity can then inform machine learning practitioners on how to select a
hypothesis space given a particular learning task, through the lens of the bias-variance trade-off.
Moreover, it is intuitive that good notions of complexity may provide sufficient and necessary
conditions for PAC learnability, as hypothesis spaces that are “too rich” may induce learning
problems that are impossible to solve even with infinite data.

We now discuss two popular complexity measures, the VC dimension and the Rademacher
complexity, which are central concepts in learning theory and which allow for quantifying the
bias-variance trade-off and proving PAC-style guarantees for learning algorithms.

VC dimension Consider the context of binary classification with the 0 — 1 loss. Given n
points 21,...,2, € V, denote

Hor,..a, = {(h(a1),-..,h(an)) > h © H}.

Note that |H,,,...x,,| is the number of possible label assignments that the hypotheses in H.
can give to the n data points. Clearly |H.,,..,,| < 2” and if |H.,,.,,| = 2” we will say that

H shatters the set of points {21,...,%n}. Next define the growth function of . as

 

 

 

 

Sy(m) = sup [Haya
DAs
Again, Sx,(n) < 2” and if Sy(n) = 2” then there exists a set of n input points that H can
shatter. Intuitively, if # can shatter a large number of input points, then H constitutes a
very rich set of models. This motivates the following definition of what is known as the VC
dimension of a binary hypothesis space H.

Definition 4. The VC dimension of a hypothesis space H Cc {0,1}* is the maximum number
of points that H. can shatter, that is the largest n € N, such that S3,(n) = 2”. If no such n
exists, then we say that H. is of infinite VC dimension.

It turns out that the VC dimension being finite is a necessary and sufficient condition for
agnostic PAC learnability.

10

4.4. Experiments

 

Here we focus on ways to compute the discrepancies under such constraints. Once this
is done, the vector a can be computed easily and then any standard distributed training
procedure, e.g. [MMR*17], can be used to obtain the a-weighted empirical loss
minimizer. Standard approaches in distributed learning only require the exchange of gradients
of minibatches with respect to the current state of the model between the data sources and the
central server, so in particular the actual local datasets are never observed by the learner. In
cases when the gradients may reveal sensitive information about the data, secure aggregation
or other privacy-preserving distributed learning methods [SS15] can be used on top
to ensure privacy.

We distinguish two cases, depending on whether the reference dataset can be shared with the
sources.

Case 1: the reference dataset is available to all nodes. If the reference dataset can be
shared with the sources without privacy and communication complications, the discrepancies
can be estimated /ocally on every source, in parallel. \f necessary, the computational protocol
can be executed via a trusted computation method [CMMO09], for example by using Software
Guard Extensions (SGX extensions) [MAA*16], to ensure the correctness of the procedure.
The discrepancies alone can then be sent to the learner and the algorithm proceeds as described
above. This approach ensures the privacy of the local datasets and allows for all discrepancies
to be computed in parallel.

Case 2: the reference dataset can not be shared. In this case the learner can still
compute the empirical discrepancies without observing the data from the sources directly,
by using a gradient-based optimization procedure. This is because the function inside the
supremum in (4.5) decomposes into a term depending only on the reference dataset and a
term depending only on the data of the source. Therefore, each discrepancy can be estimated
by using a sequence of queries to the source about the gradient of a minibatch from its data
with respect to a current candidate for the predictor achieving the supremum.

4.4 Experiments

4.4.1 Method and baselines

We perform two large sets of experiments, following the setup considered in this chapter.
We train our algorithm on the data from all sources, including the reference dataset. The
hyperparameter A is selected by 5-fold cross-validation on the trusted data. The prediction
tasks we consider here are binary classification problems with the 0/1-loss, so we compute the
empirical discrepancies by approximately solving the optimization problem (4.5) as follows.
Given the two datasets S; and S'y, the binary labels of one of them are flipped. The optimization
can then be reduced to an empirical risk minimization problem that we solve using a standard
convex relaxation approach. We refer to the supplementary material for a more formal
description.

We compare the performance of our algorithm to the two naive approaches: training on the
reference dataset only (corresponding to A = 0 in our algorithm; denoted as "Reference only"
in the plots and tables) and merging the sources and training on all the data (corresponding
to \ — oo; referred to as "All data" in the plots and tables). All three methods use linear
predictors and are trained by regularized logistic regression. The regularization parameter is
always selected by 5-fold cross-validation on the reference data. The learned models are then
evaluated on held out test data.

43

2.

PRELIMINARIES

 

from past human behavior [MMS*19]. For example, when creating models for evaluating job
applications, the training data used will likely be based on previous rounds of applications,
where human decision makers may not have been completely objective in their evaluations.

Adversarial manipulations of the data In some cases one may expect worst-case manipu-
lations of the data that specifically target the model’s accuracy or other desirable properties,
such as the model’s fairness. For example, in 2016 Microsoft released its own Twitter bot,
called Tay, that was meant to learn from real-world Twitter data in an online manner |Hun16].
This resulted in an immediate surge of tweets containing inappropriate content, published by
users seeking to sabotage the performance of the bot. Consequently Tay was taken down by
its creators soon after it started mimicking the malicious data content that it was trained
on. Additionally, multiple studies from the academic community, e.g. ICLL*17],
have shown that even a small fraction of adversarially perturbed training data points can
significantly impact the performance of the resulting model.

These examples suggest that the presence of data corruption is a common phenomenon in
real-world data. Moreover, these and others data quality issues can have a direct negative
impact on model performance. It is therefore important to study various models that go
beyond the classic PAC learning setup by allowing for a certain fraction of the data to deviate
from the clean distribution D. The underlying intention is that such models can inform
machine learning researchers about ways of designing learning algorithms that are robust to
data corruption.

2.2.2 Learning against an adversary

Models of data corruption A common approach towards addressing the issue of poor data
quality is to design learning models under specific assumptions about the types of corruptions
that are present. For example, consider the case of creating a supervised learning dataset
S' = {(«;,y;)}*_, based on crowdsourcing, where the inputs are collected centrally and the
crowdsourcing workers are only asked to provide labels. In such a scenario it is natural to
assume that the input variables x; are clean and indeed sampled i.i.d. from the marginal
distribution of the inputs. The corruption model is then restricted to label manipulations.
If it is reasonable to assume that labeling mistakes happen randomly and independently of
the input values, then an appropriate model is that of label noise. In such a scenario, each
data point (x;, y;) is assumed to be generated as follows. First, a clean data point (2%, y%)
is drawn. Then with some (small) probability a, the label yf is flipped and hence the final
data point is (2;, y;) = (xf, 1 — yf). Otherwise, with probability 1 — a, the returned point is
(2;,y;) = (#,48).

There are two main disadvantages of such application-specific data corruption models. Firstly,
the types of corruptions present in the data vary greatly between different applications and
therefore separate models might have to be developed and studied for different scenarios.
Secondly, even within a single application it is often hard to foresee all possible types of data
problems that might be present. Therefore, using a corruption model that is too specific
may lead to flawed guarantees and therefore unexpected performance of the learned model
at prediction time. This is undesirable, especially in security-critical applications of machine
learning, such as autonomous driving and fraud detection. Moreover, the inability to provide
worst-case guarantees for machine learning models may in the long run hinder their adoption
for real-world decision-making tasks.

14

Upper bound with fast rates Recall our notation:

EN L{A(a?) = 0,a? = a,y? = 1}
Mal =a,yP = 1}

 

Va(h)

 

(C.27)

as the empirical estimate of 7,,,(h) = P(h(X) = 0|A=a,Y =1) =0 for a€ {0,1}. Given
a (corrupted) training set S?, denote by

He = {he Hlngx,(h) < AP? A RPh) < =I (c.28)

 

the set of all classifiers that have a small loss and small values of 7, for both a € {0,1} on
S$”. Consider the learner L/* defined by

anyheH*, if H* #0

. (C.29)
anyh eH, otherwise.

Lfast( sp) =

We then have the following:

Theorem 16. Let 1. be finite and P € P(X x Ax ) be such that for some h* € H,
P(h*(X) = Y) = 1. Denote by Pj, = P(Y = 1,A =a) fora € {0,1}. Let A be any
malicious adversary of power a < 0.5. Then for any 6,7 € (0,1) and any

 

° 16l74)
n> max {seca 12log(12/8) 2log(8|#|/6) _ 2log 74) |

 

 

(l-a)Pio ’ a , 3n?a 37? (1 — a) Poa
_ og {oxill/s)
. 1? Pioa
with probability at least 1 — 6
oO;
LPP (chase) ~ 3a . 2A” p .
~\l-n 1-7

Proof. Throughout the proof we will drop the dependence of H* (and other subsets of H)
of the data S?. We will be interested in the probability of certain events involving * under
all randomness in the generation of S?: the random sampling of the clean data, the marked
point and the adversary (denoted by P4 as elsewhere).

 

Step 1 First note that by Lemma /7(b), whenever n > max { Seal s/0), eee

PA ((-o(h*) > APO) v (af, (ht) > AP) < PH (aR(he) + aH (ht) > AP) <9

In addition, since |98| ~ Bin(n, a)

152

© by Nikola Konstantinov, February, 2022
All Rights Reserved

IST Austria Thesis, ISSN: 2663-337X

ISBN: 978-3-99078-015-2

| hereby declare that this thesis is my own work and that it does not contain other people's
work without this being so stated; this thesis does not contain my previous work without
this being stated, and the bibliography contains all the literature that | used in writing the
dissertation.

| declare that this is a true copy of my thesis, including any final revisions, as approved by my
thesis committee, and that this thesis has not been submitted for a higher degree to any other
university or institution.

| certify that any republication of materials presented in this thesis has been approved by the
relevant publishers and co-authors.

Signature:

Nikola Konstantinov
February, 2022

Signed page is on file

0.5
L(L(S?),P) — L(h*,P) > min {*..2F0(1 - Py)

and

DP p _ PPP (h* P) > mi a ibs mi {om \.
r’* (£(S?),P) —P (WP) > min | Se — Bt > tn ap,’

Proof. Let 7 = 7“, so that a = a

Case 1 Assume that 1 = 72. < 2P)(1— Po). Take four distinct points {21, 72, 73, x4} € &.
We consider two distributions Po and P;, where each P; is defined as

1-P-n/2 ife=m,a=1,y=1

Py -— 7/2 if 7 =2%,a=0,y=0

Pi(a,a,y) = 47/2 ife =23,a=t,y =i
n/2 ifa=a,a=7i,y =i
0 otherwise

Note that these are valid distributions, since 7 < 2P9(1— Py) < 2P) < 2(1 — Py) by
assumption and also that Po = P;(A = 0) for both i € {0,1}. Consider the hypothesis space
H = {ho, hi}, with

ho(t1) =1  ho(w2)=0 hols) = 1 ho(x4) = 0
and
Note that L(h;,P;) = 0 for both i = 0,1. Moreover,

TP? (ho, Po) = |P(x,a.yy~ro (ho(X) = 1A = 0) — Pox.ayyrro(ho(X) = 1A = 1)
n/2 1—-Po-n/2

-\z —n/2+n/2 1-—Py-n/2+n/2

” ee

2P) 21-2)

 

 

-1

 

-lad 1- Po)
=1- 4

since 7 < 2P9(1 — Py) by assumption. Furthermore,
TPP (hi, Po) = [Poxayywro(ha(X) = 1A = 0) — Pex.ayynro((X) = 14 = 1)|

=|0-1
ad

Therefore, P??(h;, Po) — TP?" (ho, Po) = =): Similarly,

1
2Po(1—Po)
T?? (hi, P1) = Pocayy~r: (hi (X) = 1|A = 0) _ P(x, A,Y)0P1 (hi (X) = 1|A => 1)|

124

~ . : 8dlog(&) + 2log(16/5
< Rin) 33% 4g og(F) + 2log(16/d)

 

 

 

 

 

2 n
< R(h") 33 ay 8dlog($) + 2log(16/6)
2 n
dlog(@) + 2log(16/6
< R(h*) +6a+8 Biles al + See)

Similarly,

2d log (78) + 2log(96/5)
(1 —a)Pon

 

PPP (h) < PPP (h*) + 4A?? + 64

 

and the result follows.

 

 

 

Bound for equality of opportunity Similarly, let h2°? € argmin,.,,%°?(h). Further,
we define the set

2en
hs = f CH: 20P(h) _ POP (7,207) < 2A + 39 2dlog(*7") + ae

(1 —a)Pion

That is, 3 is the set of classifiers that are not far from optimal on the train data, in terms
of equality of opportunity fairness. Now define the component-wise learner for equality of
opportunity:

anyhEHiNHs, if HiNH3 40

EOP Sp _—
ew (S") any hE H, otherwise,

that returns a classifier that is good in both metrics, if such exists, or an arbitrary classifier
otherwise. Then we have the following:

Theorem 15. Let H. be any hypothesis space with d = VC(H) < oo. Let PE P(X x Ax)
be a fixed distribution and let @ be any malicious adversary of power a < 0.5. Suppose that
there exists a hypothesis h* € H, such that U(h*) < Wh) for allh © H. Then for any

6 € (0,1) and n > max CESS Blog 2/9) 4h, with probability at least 1 — 6

LPO? (£2OP(SP)) X (o0-+0 (\2) 4APOP 4 © (V5)

Proof. From the proof of Theorem [13] we have that with probability at least 1 — 0:

_ 8d log(@ 2 log(16/6
sup [R?(h) — R(h)| < oa +4 9, Sloat) + 21og(16/5)
heH 2 n

. 2d log (222) + 2 log(96/6)
sup |P“0?(h) — r22?(h)| < AZO? + 16,| ——S\ a? A
eu (h) (h)| < (1—a)Pion

and Lemma |5|

 

 

 

 

The proof proceeds in an identical manner to that of Theorem [14]

151

6.

FAIRNESS THROUGH REGULARIZATION FOR LEARNING TO RANK

 

Theorem 18 ({Jan04]). Let Y, and X be as above, with aq < Ya < ba for every a € A, for
some real numbers a, and b,. Then, for every t > 0:

P(X > E(X) +1) < exp (yee) . (6.9)

The same upper bound holds for P(X < E(X) —t).

 

 

 

 

If instead one considers the mean of {Ya}aca, namely X= wl Daca Yo: then the following
holds:

 

os BK PAP )
P(X > E(X)+t con (oy hap . 6.10
ane (A) Snealba = da)? (6.10)
Specifically, if the Y, are Bernoulli random variables:
P(X > E(X) +t) < exp (2741) . (6.11)
x(A)

For the case of the ranking dataset S = { (dis di, 75) biel) jefe] and a fixed function f :
Q xT — {0,1}, we study the set of random variables Y(;,;, = f(qi,d;), for the indexes
A= {(i,9) :t © [N], 7 € [m]}. Examining the assumptions we made about the way that the
data is sampled, it is easy to see that y(A) = m. Therefore, Theorem (18) can be used to
understand the concentration properties of the random variables Y(;,;) = f(a, 4G).

Discussion Theorem {17| bounds the fairness violation on future data by the fairness on
the training set plus an explicit complexity term, uniformly over all item selection functions.
Consequently, any item selection function with low fairness violation on the training set will
have a similarly low fairness violation on new data, provided that enough data was used for
training. Indeed, the complexity term decreases like ,/log N/N as N — oo, which is the
expected behavior for a VC-based bound.

The same scaling behavior does not hold with respect to the number of items per query,
m. This is unfortunate, but unavoidable, given the weak assumptions we make on the data
generation process: because we do not restrict how the per-query item sets are created, each
of them could simply consist of many copies of a single item. In that case, even arbitrary large
m would provide only as much information as m = 1. In the current form, m appears even
logarithmically in the numerator of the complexity term. We believe this to be an artifact
of our proof technique, and expect that a more refined analysis will allow us to remove this
dependence in the future.

Note that for real data, we do expect larger m to be have a beneficial effect on generalization.
This is the reason that we prefer to present the bound as it is in the theorem, i.e. with the
empirical fairness estimated from all available data, rather than any alternative formulation,
e.g. subsampling the training set to m = 1, which would recover an i.i.d. setting. Finding an
assumption on the generating process of real-world LTR data that does allow bounds that
decrease with respect to m is an interesting topic for future work.

In addition, we expect that more advanced techniques from learning theory, e.g. analysis
based on Rademacher complexities [BM02], can be applied to obtain sharper, data-dependent
guarantees. Indeed, there has been work on extending the classic Rademacher complexity
generalization bounds to the case of dependent data, e.g. [UAGO5], and we deem the application
of such techniques in the context of fair LTR an interesting direction for future work.

80

4. ADVERSARIAL MULTI-SOURCE LEARNING IN PRACTICE

 

Our aim is to test whether the proposed algorithm successfully leverages information from the
sources, while being robust to various perturbations in the distributions of the local datasets,
and whether exploiting the multi-source structure of the data gives any improvement over the
two standard learning procedures. We also compare the performance of our algorithm to the
following robust learning baselines.

Robust aggregation of local models. We consider two recently proposed approaches
for robust distributed learning. Following [EXM14], one baseline learns a separate linear
model based on each of the source datasets. The final linear predictor is then constructed as
the geometric median of these locally learned weight vectors. Another baseline, inspired by
[YCKBI8], takes the component-wise median instead. Thirdly, based on the locally learned
models all NV estimates for the probability that a test point belongs to a certain class are
computed and the final prediction for the label of that point is obtained by taking the median
of these probabilities and thresholding it (referred to as "Median of probs" in the plots and
tables). All these baselines aim at learning a robust ensemble of local models.

Robust logistic regression. We use the method of [Pre82], based on the minimization of a
Huber-type modification of the logistic loss. Specifically, the method minimizes the following
robust loss function, instead of the classic logistic loss:

log(1 + e~¥"*), if log(1 + eM") <c

(w,z,y) =
(w, 2,4) ne + e~ve™®) — ©, otherwise

In our experiments, we use the recommended threshold value of c = 1.3452, under which the
estimate of the linear predictor has been shown to achieve a 95% asymptotic relative efficiency
[Pre82]. We also include a regularization term here and learn the regularization parameter by
5-fold cross-validation on the reference data. This baseline is an example of learning robustly
on the whole dataset.

Batch normalization. Inspired by the success of batch normalization in deep learning [IS15],
we compute the mean and standard deviation of the data at each source separately. We
then subtract from each data point the mean and divide by the standard deviation of its
corresponding dataset. We do the same for the reference data. We then merge all data
together and train a logistic regression model with a regularization term. Finally, at test time
every input is preprocessed by subtracting the mean and dividing by the standard deviation
of the reference dataset, before applying the classifier. This approach aims at increasing
robustness to source-specific biases.

4.4.2. Amazon Products data

Our first set of experiments is on the "Multitask dataset of product reviews'?| [PLI7],
containing customer reviews for 957 Amazon products from the "Amazon product data"
[MTSVDH15], together with a binary label indicating whether each review is positive
or negative. All reviews in the data set are represented via 25-dimensional feature vectors,
obtained by computing a GloVe word embedding and applying the sentence embed-
ding procedure of [ALMI7]. We treat the classification of a review as positive or negative as
a separate prediction task for each of the products, resulting in a total of 957 input-output
distributions.

“http: //cvml.ist.ac.at/productreviews/

44

2.

PRELIMINARIES

 

Definition 5. Let H be a hypothesis space and ¢ be a loss function. Let S = {(x;, yi) }?_y
be a set of n i.i.d. data points from a distribution D. Let 0 = (01,...0) be a set of n i.i.d.
Rademacher random variables, that is, random variables uniformly distributed on {—1, 1}.
Then the empirical Rademacher complexity of H. with respect to € and the sample S' is defined
as

 

 

 

 

Rs(CoH) = E, (sup + > o;€(h(ai), w) . (2.11)

heH N jy

The (distributional) Rademacher complexity of H. with respect to ¢ and the distribution D is
defined as

(00H) = Espn (=. (sw 1S oit(h( a), »)) = Ego (Ss(CoH)). (212)

heH N jy

 

 

 

 

Intuitively, the Rademacher complexity captures the degree to which the hypothesis space can
capture random noise and is therefore a useful measures of the expressiveness of 1.
As a result, one can also use the Rademacher complexity to bound the gap between the test
time and the train time loss of a classifier, uniformly over the hypothesis space.

Theorem 3 (Theorem 3.3 in [MRT18]). Let H be a hypothesis space and ¢ be a loss function
such that €(y1,y2) < M for some M > 0 and for all y,, yo € Y. Then for any 6 € (0,1), with
probability at least 1 — 0 over the i.i.d. sampling of a training set S ~ D” from a distribution
D, each of the following holds uniformly over all h © H:

R(h) < Rh) +2, (CoH) + My/ se) (2.13)
R(h) < Rh) + Ws (L0H) +3M4/ meee 8) (2.14)

In the special case of binary classification, the Rademacher complexity can be upper-bounded
using the VC dimension d = VC(H.). The Rademacher complexity can first be bounded in
terms of the growth function of H. as follows:

and

 

R,(CoH) < seit) (2.15)

Next, Sauer’s lemma can be used to show that whenever n > d,

Su(n) < ()’.

Therefore, for any n > d,

HR, (00H) < | Lele! <6 ( ‘) (2.16)

2n ~ n

Note that this upper bound can be used together with Theorem 3) to show a bound on the
excess risk with the same asymptotic behavior as the one in Theorem 2 In particular, the
bound based on the VC dimension is looser. This is essentially because the VC dimension is a
distribution-agnostic measure of complexity and generalization arguments that are based on it
are necessarily worst-case over the properties of D. In contrast, the Rademacher complexity
is a tool for providing distribution or data-dependent guarantees for learning, which can be
tighter for certain “easier” learning problems.

12

3. ON THE SAMPLE COMPLEXITY OF ADVERSARIAL MULTI-SOURCE PAC LEARNING

 

Algorithm 3.1: Dataset filtering for robust multi-source learning

Inputs: S),...,Sy
Initialize T = {} // trusted sources
fori=1,...,N do

if dy (Si, S;) <s (m, sy: Si) +s (m, a, 5;) 5

for at least || values of j 4 i, then
T=TUu {i}

end if
end for
Return: U,er 5; // all data of trusted sources

 

 

Rates for the flexible-set adversary

An analogous result to Corollary |2| holds also for flexible-set adversaries:

Corollary 3. In the setup of Theorem|4| against any flexible-set adversary, it holds that

ve) , (3.13)

4,

 

R(L(A(S ))) — min Rh) < 4Re + 12a mane KR, +0 (

The proof is provided in the supplemental material.

Making the same assumptions as above, we obtain a sample complexity rate

x 1 V7
O +=}. 3.14
( Vkm we) ( )
which differs from (6.9) only in the rate of dependence on a, which, if at all, matters only
for very small (but non-zero) a. Despite the difference, most of our discussion above still

applies. In particular, even for the flexible-set adversary the same learning algorithm exhibits
robustness for a > 0 and achieves optimal rates for a = 0.

 

 

Moreover, an explicit upper bound on the sample complexity against a flexible-set adversary is
given by:

~(1 1 vie °
m(e,d) << O 2 (oo + va) : (3.15)

3.4.3. Proof Sketch for Theorem |4|

The proof of Theorem 4| consists of two parts. First, we introduce a filtering algorithm, that
attempts to determine which of the data sources can be trusted, meaning that it should be
safe to use them for training a hypothesis. Note that this can be because they were not
manipulated, or because the manipulations are too small to have negative consequences. The
output of the algorithm is a new filtered training set, consisting of all data from the trusted
sources only. Second, we show that training a standard single-source learner on the filtered
training set yields the desired results.

Step 1. Pseudo-code for the filtering algorithm is provided in Algorithm 3.1, The crucial
component is a carefully chosen notion of distance between the datasets, called discrepancy,

32

 

properties can be useful not only for studying the limits of learning in terms of fairness, but
also for quantifying the fairness-accuracy trade-off in the context of binary classification.

Fairness-accuracy trade-off in ranking and multi-label learning The fairness-accuracy
trade-off is important to understand not only for classification, but also in the context of
ranking. As discussed in Chapter |6| our experimental findings suggest that achieving fairness
does not necessarily need to come at the price of utility in LTR settings. We hypothesize that
this is due to the multi-label nature of ranking problems: for a given query, it is often the case
that many items are relevant and so the exact set of returned items, as well as their order,
can often be adapted in multiple ways without a substantial drop in utility.

An interesting open question is whether this intuition can be formalized and whether our
experimental findings will also hold for real-world recommender systems applications.

89

xvi

3.

ON THE SAMPLE COMPLEXITY OF ADVERSARIAL MULTI-SOURCE PAC LEARNING

 

Corollary 1. Assume that H has the uniform convergence property. Then H. is multi-source
PAC-learnable against the class of fixed-set and the class of flexible-set adversaries.

Proof. It suffices to show that for any 6 € (0,1), the right hand sides of (3.5) and (3.6)
converge to 0) for m —> oo. This it true, since s(m, 5, S) — 0 as m — oo for any 6 and S, by
the definition of uniform convergence. Since the same learner works regardless of the choice
of G and/or a, the result follows.

 

 

 

 

Discussion. Corollary [Ijis in sharp contrast with the situation of single dataset PAC robustness.
As discussed in the Preliminaries section, in the single dataset case, where an adversary controls
an a-fraction of the individual data points, a malicious adversary can ensure that no learner
can recover a hypothesis with accuracy better than a/(1 — a) [KL93]. Similarly, a nasty
adversary can ensure an irreducible error of 2a [BEK02]. Both results hold regardless of the
value of m, thus showing that PAC-learnability is not fulfilled.

3.4.2 Rates of convergence

While Theorem {4] is most general, it does not yet provide much insight into the actual sample
complexity of the adversarial multi-source PAC learning problem, because the rate function s
might behave in different ways. In this section we give more explicit upper bounds in terms a
standard complexity measure of hypothesis spaces — the Rademacher complexity. Let

Rs (CoH) = (sup 5 o,l(h(xi), uN): (3.7)

heH N Gr

be the (empirical) Rademacher complexity of #1 with respect to the loss function @ on a
training set S = {(21,y1),..-,(@n, Yn) }. Here {a;}%, are i.id. Rademacher random variables.
Let Sg = Uieg Si, Ri = Rs, (C0 H) and Re = Rs, (0H). Assume also that the loss
function ¢ is bounded, so that for some constant M > 0, €(y1, y2) < M for all y1, yo € Y.

Rates for the fixed-set adversary.

An application of Theorem [4| together with a standard uniform concentration result gives:

Corollary 2. In the setup of Theorem 4, against any fixed-set adversary, it holds that

log(3)

18M e8 () 12
akm + a( 8M — at max).

R(L(A(S’))) — min R(h) < 4Re + 6M max
(3.8)

The full proof is included in the supplementary material.

In many common learning settings, the Rademacher complexity scales as O(1/,/n) with the
sample size n (see e.g. [BBL04]). Thereby, we obtain the following rates against the fixed-set
adversary:

(== +07), (3.9)

where the ©-notation hides constant and logarithmic factors.

30

3.4. On the sample complexity of adversarial multi-source learning

 

The results in Corollary |2| and Equation (3.9) allow us to reason about the type of guarantees
that can be achieved given a certain amount of data. However, they also imply an explicit
upper bound on the sample complexity of adversarial multi-source learning (i.e. an upper
bound on the smallest possible m(¢, 5) in Definition (12) of the form:

log(F) 1
e (1—a)N

2
+a . (3.10)

 

m(e,d) << O

Discussion. We can make a number of observations from Equation (3.9). The \/1/km-term
is the rate one expects when learning from & (uncorrupted) sources of m samples each, that
is from all the available uncorrupted data. The ,/1/m-term reflects the rate when learning
from any single source of m samples, i.e. without the benefit of sharing information between
sources. The latter enters weighted by a, i.e. it is directly proportional to the power of the
adversary. In the limit of a — 0 (i.e. all NV sources are uncorrupted, k —> N), the bound
becomes O(,/1/Nm). Thus, we recover the classic convergence rate for learning from Nm
samples in the non-realizable case. This fact is interesting, as the robust learner of Theorem |4|
actually does not need to know the value of a for its operation. Consequently, the same
algorithm will work robustly if the data contains manipulations but without an unnecessary
overhead (i.e. with optimal rate), if all data sources are in fact uncorrupted.

Another insight follows from the fact that for reasonably small a, we have:

 

x({ 1 x({ 1

O( Geter) < 0(=a): (3.11)
so learning from multiple, even potentially manipulated, datasets converges to a good hypothesis
faster than learning from a single uncorrupted dataset. This fact can be interpreted as
encouraging cooperation: any of the honest parties in the multi-source setting with fixed-set
adversary will benefit from making their data available for multi-source learning, even if some
of the other parties are malicious.

Comparison to Byzantine-robust optimization. Our obtained rates for the fixed-set
adversary can also be compared to the state-of-art convergence results for Byzantine-robust
distributed optimization, where the compromised nodes are also fixed, but unknown.
and develop robust algorithms for gradient descent and stochastic gradient descent
respectively, achieving convergence rates of order

6(=— — ak ! + ~) (3.12)
— + a—— :
Ji
for w < 1/2 unknown. Clearly, these rates resemble ours, except for the additional 1/m-term,
which matters when a is 0 or very small. As shown in RB18], this term can also be made
to disappear if an upper bound { > a is assumed to be known a priori.

Overall, these similarities should not be over-interpreted, as the results for Byzantine-robust
optimization describe practical gradient-based algorithms for distributed optimization under
various technical assumptions, such as convexity, smoothness of the loss function and bounded
variance of the gradients. In contrast, our results are purely statistical, not taking computational
cost into account, but holds in a much broader context, for any hypothesis space that has the
uniform convergence property of suitable rate and without constraints on the optimization
method to be used. Additionally, our rates improve automatically in situations where uniform
convergence is faster.

31

7. DISCUSSION AND FUTURE WORK

 

At the same time, there are many real-world applications where the presence of strategic
entities altering test time instances to their own advantage is a significant practical concern
HMPW106) |AEIK18]. Therefore, dealing with potentially worst-case data perturbations is

crucial also in the context of test time manipulations.

Beyond the scope of this thesis are also the issues of privacy and interpretability. Indeed, as
machine learning models are increasingly trained on private data, for example via federated
learning [MMR*17], it is important to ensure that knowledge about a model or its training
process cannot serve as a basis for reconstructing sensitive information . In addition,
with the increasing amounts of available data and computing power, larger and larger models,
e.g. deep neural networks, are being adopted for solving various real-world prediction tasks.
While excelling in performance, these models often act as black boxes, as they are too large to
be analyzed by standard mathematical techniques. Methods for interpreting and explaining
the decisions made by such massive models are highly desirable |GBY*18], in particular for
the sake of ensuring fairness, detecting spurious correlations or providing feedback to entities
that receive an unfavorable classification decision.

Within the context of robust and fair machine learning, there are a number of possible
extensions of the work presented in this thesis that we see as interesting directions for future
work. Below we present these, in an order that aligns with the chapters of the thesis.

Adversarial multi-source learning from heterogeneous data _ In Chapters [3 and |4| we
have studied the problem of learning from multiple unrealiable data sources, where a subset of
the sources can contribute arbitrary, even adversarially perturbed, data. However, a recurring
assumption is that the clean sources contain data that is sampled from the same distribution,
or at least from distributions that are very similar to each other. In contrast, there are many
situations where this assumption may not hold. In fact, in setups such as federated learning
and collaborative learning [BHPQI7], the point of using multiple data sources is
exactly to obtain data from different modes of an underlying meta-distribution and some data
heterogeneity is not only expected, but also desirable.

In such contexts, an additional challenge towards achieving robustness is that the natural
variability in the distributions of the clean sources makes it difficult to distinguish between-
source differences that are benign (due to the sources containing different types of clean data)
from those that are due to data corruptions and other issues with the datasets. For this
distinction to be possible, one would need to have an appropriate model of what “natural”
changes of the distribution can be expected between the clean sources.

To our awareness, robustness in the heterogeneous multi-source setting is a problem that has
largely remained unaddressed in the literature and we deem this an exciting direction for future
work.

Learning theory for fairness-aware learning In Chapter |5| we showed that PAC learning
guarantees for fairness can be obtained as long as the VC dimension of the hypothesis class
that is used is finite. However, we also argued that in many situations fairness can be certified
trivially (and provably), for example in cases where the hypothesis class contains a constant
classifier. This means that while the VC dimension being finite is a sufficient condition for
“fairness learnability”, it is certainly not a necessary one.

Therefore, a natural question is what are the properties of a hypothesis space that precisely
characterize learnability in terms of various popular fairness measures. Understanding these

88

(2’,y') have the same distribution:
E A 3 1 Nm Gy ay t ,
s (sup (Ralf) — Ra(F)) ) <Essre {sup | Fois (CF (eis) ths)

fen
Lf (#ig), vig) ))

_ mi a;
<E Sa (w (= S Y= Big ): Yi “4)
fEH i=1 j=1 Bi
N mj a
+Eso up Ly. LR —0:,) Cf (@i3), Vig)

2 j=1 j= I

N mm;
= 2E so ae [ S ye RB %, jf f (xi) a)
feH \m

fai ja Pi

 

 

 

 

 

We can now link the last term to the empirical analog of the Rademacher complexity, by using
the McDiarmid Inequality (with an observation similar to Lemma 1). Putting this together,
we obtain that for any 6 > 0 with probability at least 1 — 5/2:

. N mi log (4) M2 [X 42
Ro (h) < Ra (h) + 2E, Sup os vs 7, Toll f (tig), Yj) | | +8 —s > —
i=1 j=1! B i= %
(B.6)
Finally, note that:

N m wre

e- (y (2 > i s0;i,l( Ff (@i3), Yig \) <E “(Se & Sup (= Se oi glf (wis) a)
Hig= 1 Bi i=l MG j=l
N mi

= Lok, (2 YS oi jlCf (wig) 1.)

"4 Fal

Bounding #..(h) — Ra(h) with the same quantity and with probability at least 1 — 5/2 follows
by a similar argument. The result then follows by applying the union bound.

 

 

 

 

Now we show:

Theorem 7. Given the setup above, let ha = argminneyRa(h) and h% = argminneyRr(h).

For any 6 > 0, with probability at least 1 — 6 over the data:
log ($) a?
am

N N
Rr(ho) < Rr(hp) +4- aM; (H) +2 ady (Dj, Dr) +6

i=1 i=l

; eis)

 

 

 

 

Ri (H) =E, | su * gi 50( Sf (i,5), Yay
(H) =: (oy (FS ut J hn)

and o;,; are independent Rademacher random variables.

121

CHAPTER

Discussion and future work

In this thesis we explored several topics in the field of trustworthy machine learning. In
particular, we studied adversarial multi-source learning as a model for learning from unreliable
data sources. We also explored the topic of fairness, in particular in the presence of data
corruption and in the challenging setup of learning to rank. A major focus of this work was on
designing machine learning algorithms that come with theoretical guarantees, even under severe
problems with the data, i.e. worst-case data corruption, and even in contexts beyond binary
classification, such as ranking. We believe that establishing such performance guarantees is a
necessary step towards increasing the trust in real-world machine learning algorithms among
the general public.

A distinctive feature of the results presented in this thesis, as compared to other works in
the field of trustworthy machine learning, is the use of PAC learning techniques for obtaining
guarantees for machine learning models. In the context of robustness to training data corruption,
a substantial amount of recent research effort has been focused
on designing practical gradient-based attacks and defenses for learned systems, resulting in
a cat-and-mouse game between the learner and the adversary. In contrast, we studied the
fundamental limits of PAC learning in the presence of worst-case data corruption, similarly
to [KL93]. In the case of fairness-aware learning, the vast majority of works addressing the
problem from a theoretical perspective have focused on understanding fair decision-making
when the set of a// possible binary classifiers is used and when full information about the data
distribution is available, disregarding learnability considerations, e.g. [MWi8a]. In
contrast, we accounted for the finite-sample effects when studying fairness-aware learning,

similarly to ABD*18}.

We hope that our analysis of the learning-theoretic aspects of robust and fair machine learning
serves as a useful complement to the on-going effort in making learned models more trustworthy
and reliable.

While this thesis has focused on robustness to training data corruption and on fairness, there
are many other desirable properties of machine learning algorithms that are important to satisfy
in order to ensure their positive impact in real-world applications. Already in the context of
robustness, an orthogonal aspect to the one studied in this thesis is that of protecting learned
models from test-time data manipulations. This topic has received a lot of recent attention,
for example in the context of image recognition, where it is known that deep neural network
models can easily be fooled by inconceivable, but carefully chosen pixel perturbations [SZS*14].

87

Fix an arbitrary learner £ : Unen(¥ x Ax Y)" — {ho, hi}. Note that, if the clean
distribution is Po, the events (in the probability space defined by the sampling of the poisoned
train data)

{L(L(S?), Po) — L(ho, Po) = n} = {£(S") = hi}

- {F#0P(£(S"),Po) — TP (hy, By) > a}
2Prio

are all the same. Similarly, if the clean distribution is P,
{L(L(S?), Pi) — L(hi, Pi) = n} = {£(S?) = ho}

= {r""(c(S”),P,) -TP%(h,,P1) > se}.
2Pio

Therefore, depending on whether we choose Pp or P; as a clean distribution, we have
Psraw, (L(C(S®),Po) — L(ho, Po) = 9 AT#0P(L(S”), Po) — F#P(ho, Po) > a)
= Pspapy, (£(S?) = hi)
and
Pspap' (Z(c(5"),Pi) — L(hi, Pi) > n AT”? (L(9?), Py) — 12°?(hy, Pr) > Pe)
= Pse.p, (£(S”) = ho)

Finally, note that P, = P4, so that either Psp. (£(S?) = hy) > 1/2 or Psp. (L(S?) = ho) =
1/2. Therefore, for at least one of = 0,1, both

a

 

L(L(S?), P;) — L(hi, Pi) > 7 = -_

and

T¥0°(¢(5?), P;) — P(h,,P,) > —E- = —_S
(£(S?), Pi) (hisPi) 2 op” = aaa)

both hold with probability at least 1/2. This concludes the proof of the first case.
Case 2 Now assume that ;°. > 2min { Pio, 1 — Pio — Pir}. We distinguish two cases:

Case 2.1 Suppose that Pip < 1 — Pio — Pu. We have that 7°. > 2Pio. Then, denote by
ay, the unique number between (0, 0.5), such that a = 2Pi9 = 2min { Pio, 1 — Pio — Pu},
and note that a; < a. Then repeat the same construction as in Case 1, but with 7) = a
and an adversary that with probability a,/a does the same as in Case 1 and leaves a marked

point untouched otherwise.

 

 

Then the same argument as in Case 1 gives that for some i € {0,1}, with probability at least
0.5, both of the following hold

 

L(L(S?),P;) — L(h;,P;) > “1 = 2Pio
1- Qa,
and ‘
P£r(¢($?), P,) — P2P(h,,P,) > = 1.

128

log (4%
t+a|18M los (*) + 12 max %;
2m ic[N]

)N log(2) 4. log (4)
-—a)Nm  2(1—a)Nm

 

H(a
<4 + 6M
<4%g +6 aI

log (4%
t+a|18M los (*) + 12 max %;
2 ic[N]

 

~ (va
< 4g + 12a max Ri; + O
= e+ Rem et (=

where for the last inequality we used H(a) < 2\/a(1— a), 1—a€ (4,1] and Ya >a.

 

 

 

 

For the case of binary classifiers, we also provide a simpler bound in terms of the VC dimension

of H.

Corollary 3. Assume that Y = {—1,1} and that H has finite VC-dimension d. Then:

(a) In the case of the fixed-set adversary there exists a universal constant C’, such that:

1 : [d [2log(3) jd 2log (4)
— < — = 46) 2? 7 |
R(L(A(S’))) min R(h) < 2C km +2 km +a] 6C nm +6 mt
)

(A.24

 

(b) In the case of the flexible-set adversary:

R(C(A(S"))) — min R(h) < O (= a + ay|4 + 0 26)  (A25)

Proof. (a) Whenever H is of finite VC-dimension d, there exists a constant C,, such that the
following generalization bound holds [BBL04]:

n 2log (2
ae |E (¢(A(x), y)) — 7 Ela) < ofé +A 2I08 (3) (A.26)

2log (2
and hence H. has the uniform convergence property with rate function s = Cfe+ fre).
Substituting into the result of Theorem 4| gives the result.

 

(b) Using the concentration result from (a) and (2) = Co.) = (vy) < 241)" where

H(p) = —plogs(p) — (1 — p) logs(1 — p) is the binary entropy function, we obtain:

— 4(7) , ( 4N
, . d 2log(-*) d 2log ( 3 )
- < — +6
R(L(A(S'))) min R(h) < acy 2 km a|6C mi 6 mt

112

 

 

Abstract

Because of the increasing popularity of machine learning methods, it is becoming important to
understand the impact of learned components on automated decision-making systems and to
guarantee that their consequences are beneficial to society. In other words, it is necessary to
ensure that machine learning is sufficiently trustworthy to be used in real-world applications.
This thesis studies two properties of machine learning models that are highly desirable for the
sake of reliability: robustness and fairness.

In the first part of the thesis we study the robustness of learning algorithms to training data
corruption. Previous work has shown that machine learning models are vulnerable to a range
of training set issues, varying from label noise through systematic biases to worst-case data
manipulations. This is an especially relevant problem from a present perspective, since modern
machine learning methods are particularly data hungry and therefore practitioners often have
to rely on data collected from various external sources, e.g. from the Internet, from app users
or via crowdsourcing. Naturally, such sources vary greatly in the quality and reliability of the
data they provide. With these considerations in mind, we study the problem of designing
machine learning algorithms that are robust to corruptions in data coming from multiple
sources. We show that, in contrast to the case of a single dataset with outliers, successful
learning within this model is possible both theoretically and practically, even under worst-case
data corruptions.

The second part of this thesis deals with fairness-aware machine learning. There are multiple
areas where machine learning models have shown promising results, but where careful considera-
tions are required, in order to avoid discrimanative decisions taken by such learned components.
Ensuring fairness can be particularly challenging, because real-world training datasets are
expected to contain various forms of historical bias that may affect the learning process. In
this thesis we show that data corruption can indeed render the problem of achieving fairness
impossible, by tightly characterizing the theoretical limits of fair learning under worst-case
data manipulations. However, assuming access to clean data, we also show how fairness-aware
learning can be made practical in contexts beyond binary classification, in particular in the
challenging learning to rank setting.

vil

2.1. Supervised machine learning

 

Next, we present a standard way to formulate these issues in statistical terms, by adopting a
standard PAC learning framework.

2.1.2 Statistical PAC learning

The objective of learning Since the learner only operates with a set of training examples,
an underlying assumption for successful learning is that the learner will be able to generalize
their knowledge based on these examples to new, unseen inputs and so to perform well at
prediction time. Intuitively, this should be the case whenever the new inputs are similar to
those that were observed by the learner at train time. To formalize this, a standard assumption
is that both the training and the test examples are sampled from a distribution D € P(X x Y)
in an independent and identically distributed (i.i.d.) manner.

This statistical treatment allows for a formalization of the learning objective. The intuitive
goal of providing a hypothesis that works well on new inputs translates to the goal of finding
a hypothesis with a small expected loss under the distribution D, that is finding an h € H
such that:

R(h) = Epon (€(h(e),y)) (2.2)

is small. In the case when Y = {0,1} and @ is the 0 — 1 loss, R(h) is also called the risk of h.

PAC learnability Given the training dataset S = {(x;,y;)}"_, € (¥& x Y)", the learner
predicts £(S') € H. Then the performance of the learner can be measured via the expected
loss of the hypothesis that it returns, that is via R(L(S)). While one may expect a perfect
learner to always return a hypothesis with expected loss equal to 0, there are several limitation
that may in general make this impossible:

= For many distributions there will be no hypothesis in H that achieves risk of 0. In such
situations, the performance of the learner, as measured by R(L(S)), should be compared
to the best possible hypothesis in the hypothesis space, that is to inf,cx1 R(h), rather
than to 0. In this sense, a good learner learner has to be agnostic to any assumptions
about the input-label distribution and work as well as possible for any distribution
instead.

» Since the learner works with a finite set of samples, there is always a certain probability
that the n samples in S will end up being unrepresentative of the underlying distribution
D. For example, if 4 is discrete, there is a non-zero probability that all inputs will end
up being the same, even if D assigns a significant mass on the other points in X as
well. Such situations, although increasingly unlikely for large values of n, imply that in
general the learner can only be expected to probably (usually) work.

= Even when the learner is presented with a useful set of samples 5, the sample size is
still finite and any estimate of the underlying distribution D will in general only be an
approximation. Therefore, we can only expect that the learner works approximately as
well as the best hypotheses in H..

These considerations naturally lead to the concept of agnostic probably approximately correct
learning (agnostic PAC learning), as introduced by Leslie Valiant [Val84]. Intuitively, a learner
will be successful if, given a sufficiently large training set from a distribution, they manage to
output a hypothesis that is approximately as good as the best ones in the hypothesis class,

7

That is, 1 and Hy are the sets of classifiers that are not far from optimal on the train data,
in terms of their risk and their fairness respectively. Define the component-wise learner:

oe any hE H, otherwise,

PPS) = a hEHiNHe, ifHiNhH. 40

that returns a classifier that is good in both metrics, if such exists, or an arbitrary classifier
otherwise. Then we have the following:

Theorem 14. Let H. be any hypothesis space with d = VC(H) < oo. Let PE P(X x Axy)
be a fixed distribution and let A be any malicious adversary of power a < 0.5. Suppose that
there exists a hypothesis h* € H, such that U(h*) < Wh) for allh © H. Then for any
6 € (0,1) and n > max (aes 12log(12/6) a}, with probability at least 1 — 6:

1l—a)Po ? a

L?? (£2? (9?)) x (%0-+0 (\2) AAPP 4.6 (am) .

Proof. From the proof of Theorem |12) we have that with probability at least 1 — 6, both of
the following hold:

& 8d log(*) + 2log(16/6
sup [R?(h) — R(h)| < 3a fs gy numetailt Heel
heH 2 n

,

. 2dlog( 22") + 2log(96/6)
sup |?" (h) — PPP(h)| < APP + 16] ——~4
sup | (h) ( ) = (1 —a)Pon

We show that under this event, H#,; MH» #4 0) and for any h € Hi N Ho,

 

en Ben
L(n) < [6a +8 8dlog(%) + 2log(16/4) |, pp 64, 2d log(**) + 2log(96/6)
n (1—a)Pon

from which the result follows. Note that

a dlog(®) + 2log(16/5
Hn") < R(nY) + 3 4 9] SOOBL + PlostO6/)
< Rh") +2242 [8d log(S) + 2log(16/6)

n

8dlog($}) + 2log(16/6)
n

 

 

 

 

< Rh") +3044
and similarly

2d log (78) + 2log(96/5)

POP (R*) < POP) + 2APP 4.30
(ht) < POP(hr) + 207? +3 Pa

Therefore, h* € H,M Hy and soH,N Ho ZO.
Now take any h € Hi Ho. We have that

s ‘Sd log(@) + 2log(16/5)
R(h) < RPh) + “ + ay eae 1 Seal

150

2.3. Fairness in machine learning

 

the inputs. The exact definition of “discriminative behavior” is typically stated in the form
of a (conditional) independence property between the classifier output and the protected
attribute and may vary between applications. As compared to individual fairness, group fairness
properties are often much easier to state, since they do not require a careful construction
of appropriate distance measures. On the other hand, group fairness ensures that a fairness
property is satisfied in a statistical sense only, that is on average, and therefore it does not
explicitly guarantee fair treatment for every individual instance.

Counterfactual fairness Counterfactual fairness provides an intuitive notion of
fair treatment with respect to a protected attribute, by aiming to ensure that the decisions of a
classifier would not change in case of an intervention on a particular instance that changes the
value of this protected attribute only. For example, in the case of loan applications decisions,
the outcome of the application of any black individual should not have been different had they
been white (all other presented information being the same). While being both intuitive and
related to every individual of the population, counterfactual fairness notions are often hard to
verify due to the inherent difficulties of counterfactual inference more generally.

2.3.2 PAC Learning and group fairness

In this thesis we will be focusing on group fairness, where the statistical nature of the fairness
constraints allows for tools from PAC learning to be transferred to fairness-aware learning as
well. Here we discuss how the fairness-aware learning problem can be formulated in statistical
terms.

The key necessary addition to the classic PAC learning framework is the notion of a protected
attribute. For any data point, we assume that, in addition to an input x € ¥ and an output
y € Y, a protected attribute value a € A is also given. This should correspond to a feature
with respect to which discriminatory behavior should be avoided, for example race or gender.
A training dataset then consists of n triplets S = {(«;,a;,yi)}" € (4 x Ax)”. We assume
that these data points are sampled from an unknown distribution D € P(X x A x Y) over the
input-attribute-label triplets space and denote the random variables corresponding to x,a and
y as X,A and Y respectively. A fairness-aware learner is a function that takes in a dataset S
of triplets and outputs a hypothesis h from a given hypothesis space H.

Different works define H either as a subset of Y**~4 or as a subset of Y*. That is, some
works, e.g. WGOST7], allow for the classifier to explicitly use the protected attribute
when making a classification decision, while others, such as [MW18a], do not. In
this thesis we opt for the latter option, so that the classifier is not explicitly using the variables
a € A. Note that while the decision for any particular instance does not use the variable a
directly, the way that the hypothesis is selected by the learner during training is fundamentally
influenced by the protected attribute values, so that, hopefully, fair treatment is ensured at
test time.

There are several reasons why we opt to study classifiers that do not use a directly for their
decisions. Firstly, the protected attribute a can always be assumed to be explicitly present
among the features of the input variable 7, so that the other case is also covered by our
analysis. Secondly, in cases when a is not part of the input 2, this might be for good reasons:
in many circumstances using a protected attribute value directly when making decisions can
be considered unethical, even if the intentions are to ensure fair treatment. Finally, in some

19

6.

FAIRNESS THROUGH REGULARIZATION FOR LEARNING TO RANK

 

be required to invite an equal number of people from disadvantaged and non-disadvantaged
backgrounds, rendering the above fairness notion inapplicable.

Secondly, few existing fair ranking methods are suited for the increasingly adopted learning to
rank (LTR) paradigm [MCI8], where a machine learning model is trained to predict
the relevances of the items for any query at test time. A LTR algorithm has to learn to be
fair based only on the training data, bringing up the problem of generalization for fairness:
a ranking method could appear fair on the training data, but turn out unfair at prediction
time. To be best of our knowledge, no previous fair ranking algorithm provides generalization
guarantees.

Outline In this chapter, we address the aforementioned challenges and develop fairness-
aware algorithms for LTR that can incorporate numerous fairness notions and provably
generalize. To this end, we exploit connections between ranking and classification, where both
the algorithmic and theoretic challenges of learning fair models are rather well understood
|WGOS17]. Importantly, many different notions of classification fairness

have been proposed, which describe different properties that are desirable in various applications

[MMs*19}.

We provide a formalism for translating such well-established and well-understood fairness
notions from classification to ranking by phrasing LTR as a binary classification problem for
every query-item pair. We exemplify our approach on three fairness notions that fit naturally in
the ranking setting and correspond the popular concepts in classification, discussed in Section
2.3} demographic parity, equalized odds, and equality of opportunity. We then formulate
corresponding fairness regularization terms, which can be incorporated with minor overhead
into many standard LTR algorithms.

Besides its flexibility, another advantage of our approach is that it makes the task of fair LTR
readily amendable to a learning-theoretic analysis. Specifically, we show generalization bounds
for the three considered fairness notions, using a chromatic concentration bound for sums of
dependent random variables to overcome the challenge that training samples for the
same query are not independent.

Finally, we demonstrate the practical usefulness of our method for training fair models.
Experiments on two ranking datasets confirm that training with our regularizers indeed yields
models with greatly improved fairness at test time, often with little to no reduction of ranking
quality. In contrast, prior fair ranking methods are unable to consistently improve our fairness
notions.

6.2 Related work

Fairness in classification Algorithmic fairness is well explored in the context of binary
classification, see Section [2.3] and [BHN19]. In this work we show how to extend the three
popular group fairness notions from Section (2.3) to the ranking setting. In principle, our
formalism is applicable to other group fairness notions, as well as to individual
and causal fairness. We defer the exploration of these to future work. On the
methodological level, we opt for a highly adaptive and scalable regularization approach,
inspired by successful similar methods for fair classification (ZVRGT7].
Our regularization technique can be integrated into any standard gradient-based LTR system
with only a minor additional computational cost. More generally other fair classification

72

Step 2 Next let H; C H be the set {h € HIR( (h,P oh. For any h € Hy
3a 3a
R°(h) < 3a) < pt (ai (". a <(l-y ay")
PA (R*(h) < 3a) iy} <9- Gp

3a
< exp (-'5 0 0)
6
< olay?
~ 8|H|

 

2log( SH) 2 log( (StH )(1=n)

as long as n > Tite Tle

 

. Taking a union bound over all h € Hy,

p4 (:nin R°(h) < 30) < :

   

heHy,
Since also P#(|98| > § 32) < 4 and R?(h) > Re(h) — |P|, we obtain
a +) PP <*)< a(( in Ro(h) < ) ( >*))< gay 6 _ 58
P* (jn Rr) < ) <P ( (qe) < 3a) v (m2) <5 +35 =
(C.30)

Similarly, let Hy = {h E H|P2Or(h) > ZAP}. Fix any h € Hy. Assume without loss
of generality that 74) > Tay > 0 (for this particular h only). Then 44) > Fy — Vu =
Yio — Yul = M10 — Yul > F agar (note that the 7, are non-negative). At the same time,
by Lemma [7a ),

5

pa (Fo <(1—7)%10 - Aw) < aie’

whenever 16|24| 16|H|
Jog ( 16l#l Aloe(26
n> max} Sloss) _ loss) |
(1—a)Pio’ 7?(1 — a) Pro 10

 

8log( 2741)

This is indeed the case since n > =) yPro by assumption and also

2log( Ft) 4log(“F")
39P (1 — a) Poa ~ 1P(1 — &)ProFo
The last inequality is obtained by observing that 7,5 > pyar > 6a, which follows by using
Pip < 0.5,a < 0.5, > 0.
Therefore, with probability at least 1 — a" MaXe Vie = Vio > (1—7)F10 — Ato = 2A"? —
Eo > 2A"? — Ey) — Ey, with Ey, = Gt where we used inequality (c.21).

Crucially, 2A“? — Ey) — Ey, does not depend on h. Therefore, taking a union bound over
all h € Ho,

pt (in max 7h, (h) < 2A%°? — By — Ex) <
heH2z @

Note also that since n > max { eet) Brel, using inequality (C.22),
)
pt (Et + Ey, > APOr) S oe

153

4.4. Experiments

 

+ ours
—+ Reference only
 Alldata

Pregibon et al

—b Median of probs
Feng et al

+ Yinet al.

+ Batch norm

 

bh Ours
+ Reference only
+ All data

Pregibon et al.

—b Median of probs
Feng et al

+ Yinetal.

—{— Batch norm

 

Average classification error

Average Classification error

 

Average classification error

+ Ours
+ Reference only
f+ All data
Pregibon et al.
—+ Median of probs
Feng et al.
as| -/ Yinetal
—{~ Batch norm

  
 
 
 
 
   

30

 

o ©

a a
Number of corrupted sources

(a) Label bias

bt ours

—}- Reference only

6  Alldata
Pregibon et al

—b Median of probs
Feng et al

tt Yinetal.

—}- Batch norm

 
   
 
 
 
 

n

Ta 8
Number of corrupted sources

(b) Shuffled labels

bh Ours
—}- Reference only
f- All data
Pregibon et al.
—b Median of probs
Feng et al
bt Yinet al.
~~ Batch norm

 
 
 
 
  
   
 

o wo

a a
Number of corrupted sources

(c) Shuffled features

be Ours
—}- Reference only
f+ All data
Pregibon et al.
—+ Median of probs
Feng et al
bt Yinet al
~~ Batch norm

   
 
 
   
 

 

Average classification error
Average classification error

Average classificatio

 

 

 

   

 

o 3 2 0 a 30 ca o To 2 0 a 3 wo o To 2 By a 3 oo

7
Number of corrupted sources Number of corrupted sources Number of corrupted sources

(d) Blurred images (e) Dead pixels (f) RGB channels swapped

Figure 4.2: Results for the attribute "black" from the Animals with Attributes 2 dataset. Each
plot corresponds to a different contamination type. The x-axis gives the number n of corrupted
sources and the y-axis gives the average classification error of the algorithms, achieved over
100 different runs. Error bars correspond to the standard deviation around those means.

We perform an independent set of experiments for each attribute and for various types and
levels of corruption of the data sources. In each run, we randomly split the data into 60
groups of 500 images, with the remaining 7322 images left out for testing. One of the
groups is selected at random as the clean reference dataset available to the learner. The
remaining 59 groups correspond to the data sources, some of which provide low-quality or
corrupted data. We consider six different types of corruptions. Three act on the labels or the
feature representations directly and the next three are synthetic modifications of the images
themselves. In the second case, the corresponding images are manipulated before the feature
representations are extracted.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

aceline " n=0] n=10 | n=20 | n=30 | n=40 | n=50 | n=55 | n=59

Reference only || 84/1/0 | 505/5/0 | 497/13/0 | 487/23/0 | 475/35/0 | 442/68/0 | 325/185/0| 0/510/0

All data 0/85/0 | 115/395/0 | 267/243/0 | 370/140/0 | 438/72/0 | 468/42/0| 479/31/0 | 484/26/0

Med of probs. |] 9/76/0 | 47/463/0 | 172/338/0 | 336/174/0 | 469/41/0| 504/6/0 | 502/8/0 | 499/11/0

[FXM14) 8/77/0 | 32/478/0 | 110/400/0 | 338/172/0 | 457/53/0| 504/6/0 | 502/8/0 | 497/13/0

[YCKB18| 14/71/0 | 179/331/0 | 390/120/0 | 432/78/0 | 472/38/0| 502/8/0 | 503/7/0 | 497/13/0

55/30/0 | 308/202/0 | 361/149/0| 416/94/0 | 437/73/0 | 455/55/0| 470/40/0 | 485/25/0

Batch norm 0/85/0 | 107/403/0 | 317/193/0| 416/94/0 | 446/63/1 | 478/32/0| 487/23/0 | 482/28/0
Table 4.2: Summary of the results from the Animals with Attributes 2 experiments, over all
85 prediction tasks and all 6 types of corruption. Given a number of corrupted sources n

(columns) and a baseline (rows), we report values in the form A/B/C, where A is the number
of times that our method performed significantly better than the corresponding baseline, B
is the number of times it performed equally well and C is the number of times it performed
significantly worse, summed over the various types of corruptions and all attributes. More
details are provided in the main body of the text.

47


3.

ON THE SAMPLE COMPLEXITY OF ADVERSARIAL MULTI-SOURCE PAC LEARNING

 

By construction, we know that for any trusted source S;, there exists an uncorrupted source
S;, such that the difference between R;(h) and R,(h) is bounded by a suitably chosen term
(that depends on the growth function s). By the uniform convergence property of H, we know
that for any uncorrupted source, the difference between R;(h) and the true risk R(h) can
also be bounded in terms of the growth function s. In combination, we obtain that Rr(h) is
a suitably good estimator of the true risk, uniformly over all h € H. Consequently, Sp can be
used for successful learning.

For the formal derivations and, in particular, the choice of thresholds, please see the supple-
mental material.

3.5 On the hardness of adversarial multi-source learning

We now take an orthogonal view compared to Section 3.4| and study where the hardness of
the multi-source PAC learning stems from and what allows us to nevertheless overcome it.
For this, we prove two additional results that describe fundamental limits of how well a learner
can perform in the multi-source adversarial setting.

For simplicity of exposition we focus on binary classification. Let VY = {—1,1} and @ be the
zero-one loss, i.e. £(y,y) = 1{y 4 y}. Following [BEK02], we define:

Definition 13. A hypothesis space H over an input set X is said to be non-trivial, if there
exist two points «1,2 € X and two hypotheses hi, h2 € H, such that hy(x1) = he(x1), but

hy (a2) x ho(x2).

3.5.1 What makes robust learning possible?

We show that if the learner does not make use of the multi-source structure of the data, i.e.
it behaves as a single-source learner on the union of all data samples, then a (multi-source)
fixed-set adversary can always prevent PAC-learnability.

Theorem 5. Let H be a non-trivial hypothesis space. Let m and N be any positive integers
and let G be a fixed subset of [N] of sizek € {1,...,N—1}. Let£L:(¥ x V)N*™ SH
be a multi-source learner that acts by merging the data from all sources and then calling a
single-source learner. Let S’ € (¥ x yys™m be drawn i.i.d. from D. Then there exists a
distribution D with minj,<y R(h) = 0 and a fixed-set adversary A with index set G, such that:

Pon (R(L(A(S")) > ia) - ap (3.19)

N-k
N

 

where a = is the power of the adversary.

The proof is provided in the supplemental material. Note that, since the theorem holds for the
fixed-set adversary, it automatically also holds for the stronger flexible-set adversary.

The theorem sheds light on why PAC-learnability is possible in the multi-source setting, while
in the single source setting it is not. The reason is not simply that the adversary is weaker,
because it is restricted to manipulating samples in a subset of datasets instead of being
able to choose freely. Inequality (3.19) implies that even against such a weaker adversary,
a single-source learner cannot be adversarially robust. Consequently, it is the additional

34

To prove part (a), we use a similar technique as in the lower bound results in and
in the classic sample complexity lower bound for binary classification, e.g. Theorem 3.20 in
[MRT1I8]. An overview is as follows. Consider a distribution on Y that has support only at
two points - the common point x; and the rare point v2. Take P(%2) = O(£). Then one can
show that with constant probability the number of datasets that contain x2 is at most aN.
We show that in this case there exists an algorithm for the strong adversary that will return
the same unordered collection of datasets, regardless of the true label of 2. Thus no learner
can guess with probability greater than 1/2 what the true label of «2 was.

Part (b) follows from part (a) and the standard lower bound for agnostic binary classification.

Proof. a) As in Theorem Bl we prove that there exists a distribution D on ¥ and a labeling
function f € H, such that the resulting joint distribution on ¥ x Y, defined by x ~ D and
y = f(a), satisfies the desired property.

Without loss of generality, let G = [1,2,...,k]. Since H is non-trivial (d > 2), there exist
hy, ho EH and 21, %2 € AX, such that hy (x1) = ho(x1), while hi(a2) = 1, but ho(a2) = —-1.
Consider the following distribution on ¥:

Pp(a1) =1—4e and Pp(x2) = 4e, (A.36)

where ¢ = ;*.. Assume that the points are labelled by a function f € H (to be chosen later as
either hy or ha). Denote the initial uncorrupted collection of datasets by S’ = (S},...,S%),
with Si = {(2i1, f(@i1))s---5 (hms f(@im))} and x; being i.i.d. samples from D.

First we show that with constant probability the point 22 is contained in no more than aN
sources. Indeed, let C;, be the number of sources that contain 22 and let C,, be the number of
points (out of the Nm in total) that are equal to x2. Clearly C, < C,. Note that C, is a
binomial random variable with probability of success 4¢ and number of trials Nm. Therefore,
by the Chernoff bound:

Py (Cy > aN) =Ps(C, > (1+ 1)4eNm) < en < eM < as (A.37)

and so: 3
Ps (Cy < aN) > Ps (C, < aN) > 20° (A.38)

Now consider the following policy for the adversary @* : S’ — S. Whenever C; < aN, let MC
G be the list of indexes 1 € G, such that S/ contains x. Let | = and note that! < Cy <
aN. For any index i € [N] the adversary replaces Si = {2/4, fla! Uea)rees (Loans f (Lim) )t
with a dataset S$; = {(2i1, Yi) ---, (im; Yim) }, such that:

 

    

vii fay), fieG@=[1,2,...,h]
2X1, f(a), fie [k+1,...,k4]) and vhyg_yy = C1
v2,—f(%2)), ifie [k+1,...,k +] and thy, yj = 22
t,f(ai)), ifie [k+1+1,...,N]

(A.39)

   

 

(:
(ig, Yig) = :
(:

 

Then the adversary returns S' = (S),...,.Sjy). That is, the adversary keeps the datasets in G
untouched, copies all of the datasets in MM into its own data, flipping the labels of the 2's,
and, in case there are additional sources at its disposal, it fills them with (correctly labelled)
x1's only.

116


6.

FAIRNESS THROUGH REGULARIZATION FOR LEARNING TO RANK

 

vs other" (denoted by com) and by ".com/.org/.gov/.edu/.net vs other"
(denoted by ext). Their minority groups are of size 32% and 5% of all passages, respectively.

We use pretrained 768-dimensional BERT feature embeddings [DCL 19] for representing the
query-passage pairs. Specifically, we follow the embedding procedure described in [NC19
HWBN20], where each query-passage pair is represented as the following token sequence:

[CLS] query text [SEP] passage text [CLS]

This sequence is then processed through a pre-trained BERT model from Tensorflow Hub
AAB~15], with maximum sequence length set to 200, and the hidden units of the first [CLS]
token are used as a representation of the query-passage pair.

6.5.2 Learning to rank models

Our algorithm We adopt a classic pointwise LTR approach with a generalized linear score
function, s(d,q) = (0,¢(q,d)), for a predefined feature function, 6: Ox I > R®. as
described in the previous section. As loss function of ranking quality, L(s,S), we use the
squared loss between the relevance labels and the predictions of s over all data. To optimize
for both ranking quality and fairness, we train with a weighted loss, as in equation (6.8). For
TREC we train all models by 1500 steps of gradient descent with a learning rate of 0.003. In
the MSMARCO experiments we train with 5 epochs of SGD with a batch size of 100 queries
and 10 passages per query and a learning rate of 0.0001.

Baselines and ablation studies Our work provides a novel way of converting well-established
fairness notions from classification to a LTR setting. While previously developed methods
for fair ranking aim to optimize for other fairness notions, it is informative to see how such
algorithms perform against our method, in order to understand the relationship between our
and previous works. Therefore, we consider two recent methods for fair ranking, DELTR

[ZC20] and FA*IR [ZBC*17], using their public implementation [ZSCK20].

DELTR is a state-of-the-art algorithm for fair LTR. At train time, a linear version of ListNet is
trained, together with a regularizer tailored to a notion of disparate exposure (SJ18}.
We use the same feature representations as for our method, as well as the same range for their
regularization parameter y, to ensure a fair comparison. Unfortunately, the implementation of

ZSCK20] does not scale to MSMARCO.

FA*IR, on the other hand, is an algorithm that changes the ranking query by query, at
prediction time, by ensuring that whenever k items are retrieved, the proportion of retrieved
items from a protected group is not smaller that the 6-th quantile of a binomial distribution
Bin(k,p), for fixed parameters p, 3 € [0,1]. We use 3 = 0.1 and p € [0.02,0.04,... , 0.98].
Since FA*IR requires access to the items relevances at prediction time, we first train via our
method without a fairness regularizer and then, at test time, use the relevances predicted by
our method as inputs to FA*IR.

We also perform an ablation study by considering a version of our algorithm that learns to
enforce fairness on the per-query level. This is inspired by [SJ17], who, however, do not
propose an algorithm for enforcing such per-query fairness notions. Within our framework this
is achieved by regularizing with a separate term for every query in a batch and then averaging
over the batch afterwards.

82

CHAPTER

Fairness-Aware PAC Learning from
Corrupted Data

We now turn to another aspect of learning from corrupted data that is especially relevant from
a present perspective - that of studying whether it is possible to ensure that in addition to
accuracy, the fairness of the learned classifier is also protected from malicious data effects.
Here we study the problem from a PAC learning perspective, in the single dataset scenario.

5.1 Motivation and outline

As explained in Section 2.3| many ways of measuring and optimizing the fairness of learned
models have been developed. The problem is perhaps best studied in the context of group
fairness in classification, where the decisions of a binary classifier have to be nondiscriminatory
with respect to a certain protected attribute, such as gender or race [BHN19}. This is typically
done by formulating a desirable fairness property for the task at hand and then optimizing for
this property, alongside with accuracy, be it via a data preprocessing step, a modification of the
training procedure, or by post-processing of a learned classifier on held-out data [MMS*19].
The underlying assumption is that by ensuring that the fairness property holds exactly or
approximately based on the available data, one obtains a classifier whose decisions will also be
fair at prediction time.

A major drawback of this framework is that for many real-world applications in which fairness
can be a concern the training and validation data available are often times unreliable and
biased [MMS*19]. For example, demographic data collected via surveys or online polls
is often difficult and expensive to verify. More generally any human-generated data is likely to
contain various historical biases. Datasets collected via crowdsourcing or web crawing are also
prone to both unwittingly created errors and conscious or even adversarially created biases.

These issues naturally raise concerns about the current practice of training and certifying fair
models on such datasets. In fact, recent work has demonstrated empirically that strong poison-
ing attacks can negatively impact the fairness of specific learners based on loss minimization.
At the same time, little is known about the fundamental limits of fairness-aware learning from
corrupted data. Previous work has only partially addressed the problem by studying weak data
corruption models, for example by making specific label /attribute noise assumptions. However,
these assumptions do not cover all possible (often unknown) problems that real-world data

51

[CGJ*19]

[CHKV21]

[CHS20]

[CKP09]

[CKWos]

[CLL*+17]

[CLM19]

[CMMo9]

[CMVv20]

[CMv21]

[CNM*20]

[CPG*19}

[S04]

Andrew Cotter, Maya Gupta, Heinrich Jiang, Nathan Srebro, Karthik Sridharan,
Serena Wang, Blake Woodworth, and Seungil You. Training well-generalizing
classifiers for fairness metrics and other data-dependent constraints. In Inter-
national Conference on Machine Learing (ICML), 2019.

L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi. Fair
classification with noisy protected attributes: A framework with provable
guarantees. In International Conference on Machine Learing (ICML), 2021.

Jaewoong Cho, Gyeongjo Hwang, and Changho Suh. A fair classifier using
kernel density estimation. Conference on Neural Information Processing Systems
(NeurIPS), 2020.

Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers
with independency constraints. In International Conference on Data Mining
Workshops (IDCMW), 2009.

Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple
sources. Journal of Machine Learning Research (JMLR), 9(Aug):1757-1774,
2008.

Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted
backdoor attacks on deep learning systems using data poisoning. arXiv preprint
arXiv:1712.05526, 2017.

Sitan Chen, Jerry Li, and Ankur Moitra. Efficiently learning structured distribu-
tions from untrusted batches. In ACM Symposium on Theory of Computing
(STOC), 2019.

Liqun Chen, Chris J. Mitchell, and Andrew P. Martin, editors. Trusted Com-
puting, Second International Conference, Trust 2009, Oxford, UK, April 6-8,
2009, Proceedings, volume 5471 of Lecture Notes in Computer Science, 2009.

L Elisa Celis, Anay Mehrotra, and Nisheeth K Vishnoi. Interventions for ranking
in the presence of implicit bias. In Conference on Fairness, Accountability and
Transparency (FAccT), 2020.

L Elisa Celis, Anay Mehrotra, and Nisheeth K Vishnoi. Fair classification with
adversarial perturbations. arXiv preprint arXiv:2106.05964, 2021.

Hongyan Chang, Ta Duy Nguyen, Sasi Kumar Murakonda, Ehsan Kazemi, and
Reza Shokri. On adversarial bias and the robustness of fair machine learning.
arXiv preprint arXiv:2006.08669, 2020.

Abhijnan Chakraborty, Gourab K Patro, Niloy Ganguly, Krishna P Gummadi,
and Patrick Loiseau. Equality of coice: Towards fair representation in crowd-
sourced top-k recommendations. In Conference on Fairness, Accountability and
Transparency (FAccT), 2019.

Andreas Christmann and Ingo Steinwart. On robustness properties of convex
risk minimization methods for pattern recognition. Journal of Machine Learning
Research, 2004.

94

CHAPTER

Adversarial Multi-Source Learning in
Practice

4.1 Motivation and outline

In this chapter we study the problem of multi-source adversarial learning from a more practical
perspective. Indeed, while Algorithm 3.1] as studied in the previous section, provides provable
guarantees against the strong adversarial models we have considered, it is by and large
impractical for real-world tasks. This is because the thresholds used for accepting or rejecting
sources are based on worst-case generalization bounds and are therefore often too large and
do not detect malicious sources in practice, unless the number of samples per source is
prohibitively large. An additional complication is that for many practical learning scenarios the
data might have to remain decentralized, because of high communication costs, or it might
not be directly available for inspection, due to privacy constraints. In contrast, the analysis
from the previous chapter only concerns the centralized data case.

In order to design a more practical learning algorithm for the general problem of learning
from unreliable data sources, we make two simplifying, but natural, assumptions. Firstly, we
assume that a small trusted reference dataset is provided to the learner, in addition to the
N untrusted data sources. This is justified in many situations where obtaining some clean
data is possible, even if expensive. For example, this is the case for medical data, where a
trusted professional can be asked to label a limited number of x-ray images manually, even
if it is impractical to obtain a dataset large enough for training only from this single expert.
Secondly, we will focus on a slightly different data corruption model: we assume that each
of the untrusted sources follows its own distribution, which may or may not be close to the
target one. This is realistic in many cases where small differences between the distributions of
the clean sources are expected, but sources with truly irrelevant or contaminated distributions
should be avoided.

In this context, we provide an alternative to the naive approaches of simply training on all
data or only on the trusted subset: we propose a method that automatically assigns weights
to each of the untrusted sources. To this end, we build up on techniques from the domain
adaptation literature and prove an upper bound on the expected loss of a predictor, learned by
minimizing any weighted version of the empirical loss. Based on these theoretical insights, our
algorithm selects the weights for the sources by approximately minimizing this upper bound.

37

5.

FAIRNESS- AWARE PAC LEARNING FROM CORRUPTED DATA

 

and against any adversary. We therefore opt to study the learnability of classes of hypothesis
spaces.

In particular, our hardness results demonstrate the existence of a finite hypothesis space, such
that a certain amount of excess inaccuracy and/or unfairness is unavoidable. Therefore, no
learner can achieve better guarantees on the class of all finite hypothesis spaces, even in the
infinite training data limit. This is contrast to, for example, classic PAC learning with clean
data, where the ERM algorithm is a PAC learner for all finite hypothesis spaces and more
generally all spaces of finite VC dimension [SSBD14].

On the other hand, the learners we construct for the upper bounds are shown to work for
any hypothesis space that is finite or of finite VC dimension, in all cases matching the lower
bounds.

Parameters of the learning problem Our bounds will depend explicitly on the corruption
ratio a and on the smaller of the protected class frequencies Py = P(A = 0) (for demographic
parity) or on Pip = P(Y =1,A =0) < P(Y =1,A = 1) (for equality of opportunity). To
understand the limits of fairness-aware learning against a malicious adversary, we will analyze
our bounds for small values of a and Po or Pio. Intuitively, the smaller the corruption rate
a is, the easier it is for the learner to recover an accurate and fair hypothesis. On the other
hand, a small value for Po or Pio implies that one of the subgroups is underrepresented in the
population, and so intuitively the adversary can hide a lot of information about this group and
thus prevent the learner from finding a fair hypothesis.

As we will see, this intuition is reflected in our bounds, which give a tool for understanding
the effect of these quantities on the hardness of the learning problem. Comparing the lower
bounds, which hold regardless of the sample size n, to the upper bounds in the limit of mn — co
allows us to reason about the absolute limits of fairness-aware learning against a malicious
adversary. Indeed, in this large data limit, we find that our upper and lower bounds match in
terms of their dependence on a and P, or Pio up to constant factors. We note that designing
algorithms that achieve sample-optimal guarantees in our context is beyond the scope of this
work. However, we will also be interested in the statistical rates of convergence of the studied
learners to the irreducible gap certified by the lower bounds. We refer to Section |5.5.2| for a
formal treatment.

 

5.4 Lower bounds

We now present a series of hardness results that demonstrate that fair learning in the presence
of a malicious adversary is provably impossible in a PAC learning sense. Complete proofs of
all results in this section can be found in Appendix |C.1,

5.4.1 Pareto lower bounds

We begin by presenting two hardness results that intuitively show that for some hypothesis
spaces H. the adversary can prevent any learner from reaching the Pareto front of the accuracy-
fairness optimization problem. We first demonstrate this for demographic parity:

Theorem 8. Let 0 < a < 0.5,0 < Py < 0.5. For any input set X with at least four
distinct points, there exists a finite hypothesis space H, such that for any learning algorithm
L: Unen(¥® x A x Y)" + H, there exists a distribution P for which P(A = 0) = Py, a

58

— Note: A short version of this work was published and presented as a contributed
talk at the NeurlPS 2021 workshop “Algorithmic Fairness through the Lens of
Causality and Robustness”.

— This paper constitutes the core of Chapter 5}

Nikola Konstantinov formulated the problem and research objectives. He proved
the theoretical results and was responsible for the paper writing.

Christoph H. Lampert advised Nikola Konstantinov throughout the project, with
important suggestions on related work and proof techniques. Christoph H. Lampert
also provided many helpful suggestions about improvements to the text.

= Nikola Konstantinov and Christoph H. Lampert. Fairness through regularization for
learning to rank. arXiv preprint arXiv:2102.05996, 2021

— This paper constitutes the core of Chapter {6|

— Nikola Konstantinov and Christoph H. Lampert jointly came up with the problem
formulation and research objectives.

— Christoph H. Lampert formulated the type of theoretical guarantees needed and
came up with the idea to use the chromatic concentration bounds for dealing with
sample dependence on a per-query level. Nikola Konstantinov formulated and
proved the corresponding theoretical results.

— Nikola Konstantinov designed and conducted the experiments included in the paper,
although Christoph H. Lampert was involved in the experiments in an earlier stage
of the project.

— Nikola Konstantinov and Christoph H. Lampert wrote and edited the paper.

During his employment as a PhD student at IST Austria, Nikola Konstantinov also co-authored
the following papers and manuscripts (not included in the thesis):

» Dan Alistarh, Christopher De Sa, and Nikola Konstantinov. The convergence of stochastic
gradient descent in asynchronous shared memory. In ACM Symposium on Principles of
Distributed Computing (PODC), 2018

= Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov,
and Cédric Renggli. The convergence of sparsified gradient methods. Conference on
Neural Information Processing Systems (NeurIPS), 2018

= Eugenia lofinova, Nikola Konstantinov, and Christoph H. Lampert. Flea: Provably fair
multisource learning from unreliable training data. arXiv preprint arXiv:2106.11732, 2021

xii

[JN20]

[JO19]

[J020]

[Joa02]

[KAAS 12a]

[KAAS12b]

[KAAS14]

[KAS11]

[KBDG04]

[KCT20]

[KFAL20]

[KL93]

[KL17]

[KL19]

[KL21a]

[KL21b]

Heinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine
llearning. In Conference on Uncertainty in Artificial Intelligence (AISTATS),
2020.

Ayush Jain and Alon Orlitsky. Robust learning of discrete distributions from
batches. arXiv preprint arXiv:1911.08532, 2019.

Ayush Jain and Alon Orlitsky. A general method for robust learning from
batches. arXiv preprint arXiv:2002.11099, 2020.

Thorsten Joachims. Optimizing search engines using clickthrough data. In
Conference on Knowledge Discovery and Data Mining (KDD), 2002.

Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Enhance-
ment of the neutrality in recommendation. In Decisions@ RecSys, 2012.

Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-
aware classifier with prejudice remover regularizer. In European Conference on
Machine Learning and Data Mining (ECML PKDD)), 2012.

Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Correcting
popularity bias by enhancing recommendation neutrality. In RecSys Posters,
2014.

Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning
through regularization approach. In 11th International Conference on Data
Mining Workshops, 2011.

Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data
streams. In VLDB, 2004.

Joon Sik Kim, Jiahao Chen, and Ameet Talwalkar. Fact: A diagnostic for group
fairness trade-offs. In International Conference on Machine Learing (ICML),
2020.

Nikola Konstantinov, Elias Frantar, Dan Alistarh, and Christoph H. Lampert. On
the sample complexity of adversarial multi-source pac learning. In /nternational
Conference on Machine Learing (ICML), 2020.

Michael Kearns and Ming Li. Learning in the presence of malicious errors.
SIAM Journal on Computing (SICOMP), 1993.

Pang Wei Koh and Percy Liang. Understanding black-box predictions via
influence functions. In International Conference on Machine Learing (ICML),
2017.

Nikola Konstantinov and Christoph H. Lampert. Robust learning from untrusted
sources. In International Conference on Machine Learing (ICML), 2019.

Nikola Konstantinov and Christoph H. Lampert. Fairness-aware learning from
corrupted data. arXiv preprint arXiv:2102.06004, 2021.

Nikola Konstantinov and Christoph H. Lampert. Fairness through regularization
for learning to rank. arXiv preprint arXiv:2102.05996, 2021.

98


6.5.

Experiments

 

 

 

 

 

 

 

 

 

 

 

 

0.70 xe 0.70
E Qualit : Ee Quality
0,12 z 0.68 Parl x 0.68
0.66 0.66
H 0.20 ma foo ”
v | Ly 0.64 @ ge 0.64@
5 0.08 i T T =
a 0.620 0.09 0.620
= a & cs
§ 0.06 0.602 5 o08 0.602
0.58 0.58
Untairness ; Unfairness
~ 0.56 oar 0.56

 

 

 

 

 

 

00 0.06 0.12 025 05 10 20 40
a

(a) Ours, equal opportunity

00 0.06 0.12 025 05 10 20 40
a

(b) Ours, demographic parity

 

 

0.16 = cai ine
0.14 oes
4
0.12
4 060%
© or0
= oss,
© 0.08 wou
S a
= 0.06 0.502
0.04 0.45
0.02] + Unfaimess
0.40

 

 

0.16 A Quality
oa

B

go

£ 0.10

s

&

E 0.08

> 0.06
0.04

 

bE Unfairness

 

 

0.02

 

 

0.0 0.06 0.12 025 05 10 20 40

¥

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0.0 0.06 0.12025 05 10 20 40

¥

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(e) DELTR, demographic parity
B20 — Quality i875,
0.75 "5650

wo.1s0} LIE 0825

g TOT TTT ot
8 os HHH ©
£ HHH Hy [ees

gow os7s 2

5 0.075 osso—
0.050

0.525
sins | Umma
0.500

 

 

 

002 017 033 048 063 079 088
p

(h) FA*IR, demographic parity

 

 

(d) DELTR, equal opportunity
0.225 =e auaity [575
0200 TT | fosso

gous i 0.625

Bors0 0600

E onas Hy [ees

oreo 09758

>? 0075 0.550
0050) 5 Untimess) «Ht 052s
0.025

0.02 0.17 0.33 0.48 0.63 0.79 0.94 500
p

(g) FA*IR, equal opportunity
ona
ne —E auality

012

Boar

2

Eo10

&

§ 0.09

> 0.08
0.07

 

 

 

+ Unfairness

0.06

 

 

 

 

0.2 —E Quality
| 0.65
wy 0.10
a 0.60%
g ®
2
£ 0.08 ag
e 55Q
€ a
E006 axoz
0.04) (1 Unfaimess 0.45

 

 

 

 

00 0.06 0.12 025 05 10 20 40
a

(j) Per-query, equal opportunity (k) Per-query, demographic parity

0.0 0.06 0.12025 05 LO 20 40
a

 

 

 

 

 

 

 

0.70
= aut
0.100 Quallty | 5 68:
0.095
4 o.6s
4 Pa
g 0.030 LLL pe ®
E 0085 TTT foe
£
5 080) 0.602
0.075
0.58
0.070) 1 Unfairness 0:56

 

 

 

00 0.06 0.12 0.25 05 10 20 40
a

 

(c) Ours, equal odds

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

—E Quality 0.70
oa
oss
“
oaz
4 0.60
E oo 9
£ 0359
$ 0.08 eg
5 50
0.06
os
0.04) —+- Unfairness
oo
WD 066 012 Os OS 10-20-40
y
(f) DELTR, equal odds
0.200. —t Quality Ser
0.175. pees
ix
Boul THAT nse
Eons Hy ee
a
$ o.oo 05758
5
oor 0880
0.525
0.050} —+— Unfaimess
0.02 0.17 0.33 048 0.63 0.79 0.94 500
p
i) FA*IR, equal odds
oat =p awaity
oss
ao
“ HH
8 m9
$0.09 0.808,
g
0.08 a
6 0.85
$ oor 3
=
0.06 0.50
unt
0.05} —+— Unfairness 0.45

 

 

 

00 0.06 0.12 025 05 10 20 40
a

(I) Per-query, equal odds

Figure 6.1: TREC: Test-time performance of fair rankers with equal opportunity, demographic
parity and equalized odds fairness, achieved by our algorithm and the baselines: unfairness (left
y-axes) and NDCG@3 ranking quality (right y-axes); after training with different regularization

strengths (x-axis).

6.5.3 Results

Figures |6.1| and 6.2) show the ranking quality and the unfairness, achieved by our method
and the three baselines, when imposing different amounts of each fairness measure in typical
settings for TREC (t = 3,k = 3) and MSMARCO (com, k = 3) respectively. As one can
see, our method is able to consistently improve fairness. For TREC, this comes at no loss
in ranking quality (here NDCG). For MSMARCO the loss is quite small for small to medium
values of a. As the figure shows, these observations are robust across the different amounts

83

5.5. Upper bounds

 

the set of all classifiers that have a small loss and small values of 7, for both a € {0,1} on
S?. Consider the learner £/2*' defined by

Liast(gp) = anyheH*, if H* #0 (5.24)
anyh eH, otherwise.

Then the following result holds.

Theorem 16. Let be finite and P © P(X x Ax Y) be such that for some h* € H,
P(h*(X) = Y) = 1. Let @ be any malicious adversary of power a < 0.5. Then for any
6,7 € (0,1) and any

 

n> max { eer 12log(12/5) 2log(8|H|/5) — 2log(*4#1)

 

 

(l-a)Pyo ’ a , 3n2a 372 (1 — a) Pio
§ 9 (loalll/s)
1? Pipa
with probability at least 1 — 6
3a 2AFOp
L£Op fast ( op ud . .
(eens) 3 l—-n 1-7»
As an immediate consequence of Theorem (16) setting 7 = 5, say, yields that for large n, with
high probability
LPOr(cfast (gp)) < (O(a), O (—)) , (5.25)
10

Again, this bound is order-optimal for finite hypothesis sets, according to Theorem 9] and
Inequality (9.15 . In addition, regarding Pio as a constant, the number of samples needed
for achieving this order-optimal element-wise error is indeed O(4), according to Theorem /16|
which is faster than the o(4) we obtained with the previous results.

5.5.3 Sketch of the upper bounds proofs

Here we present a sketch of the proofs of the upper bounds. The complete proofs can be
found in Appendix C.2|

The proofs of Theorems /12) 13) {14] [15] rely on a series of results that describe the deviations
of the corrupted fairness estimates I'(h) from the true underlying population values ['(h),
uniformly over the hypothesis space H. Key to this is bounding the effect of the data corruption,
as expressed by the maximum achievable gap between the corrupted fairness estimates and
the corresponding estimates based on the clean (but unknown) subset of the data. Then the
large deviation properties of these clean data estimates are studied instead.

Here we make this specific for the case of demographic parity, with the analysis for equality of
opportunity being similar. We denote

YL, UW A(a?) = 1, a? = a}
Dita {a7 = a}

67

qh) =

from Yar(f) := P(f(g,d) = 1|A(d) = a,r(q,d) = 1), for each pair r € {0,1},a € {0, 1}.
Since f is fixed here, we omit the dependence of 7a,(f'), y3.(f), P°P(f), PB °4(f), etc. on f
for the rest of this proof.

For any fixed I,,:
E (ile) = (i | > f( qi, d )- P(f(@, d) = — 1|A(d) = =4a, r(q, d) = — r)= Yar(f),
Lor (4,3) €lar

(D.3)

since the marginal distribution of every (qi,d;,r(qi, d)) is D. It is also easy to see that if
A = {(i,j) : i € [N],j € [m}} is the index set of the random variables Y(i;) = f (qi, di),
then x(A) =m. Therefore, for any fixed set I,, C A, we have y(lar) < x(A) =m. Now

conditional on Ig,:
t?| Lar
> vol >] < 2exp (-2=!) x
m

(D.4)

 

 

 

 

Ss
E(\yor ~~ Yar| > t Lar) = (i Tar| » f( qi, ad li) _

(4,9) €lar

 

Similarly, |Zor| = Cietwy Njetm 1(r(qi,d,) = r, A(d;) = a) is the sum of Nm Bernoulli
random variables indexed by A = {(i, 7) }ie{yj,jefmj, such that y(A) = m. Denote by
Par = Pqar)~p(A(d) = a,r(q,d) = r) and recall the notation r = ming, P,,. Then
E(\Lar|) = Par Nm. Therefore,

 

7; e
P (\Lar| < Par Nm — t) < exp (25a) ‘

Setting t = P,,Nm/2, we obtain:

Py P2N
P (der <= Nm) Z eagp (-7) (D.5)

Now assume that NV > Blea(S/6) Then for any r € {0, 1},a € {0, 1}:

 

P( yep —¥q,| Set) =P lio. — Yar| > t|Zar)P (Tar)

 

Pup .
< P(\Lor| < 2 Nm) + ys P(\7, = Yar| > tor )P Lar)

Tar:|Lar|> Fare

P?2.N
< exp ew + 1 2exp (-2

Tar:|Tar|> Pooh™

 

t | Lar |
— P Sar
m ) (Sar)
< g + 2exp (-ONP, )
= 8 ar .
The rest of the proof proceeds as in |WGOS17]. For a fixed r € {0,1} the triangle law gives:

[l1Ge — Viel — Yor — Yarll S lbp — Yor — Yor + Yael S bee — Yor! + bye — Yael

Therefore,
P(|lyGr — irl — Yor — Yell > 24) < PClyo- — orl + lat, — Yael > 24)

156

CHAPTER

Fairness through Regularization for
Learning to Rank

In this chapter we move away from the topic of data corruption and study another issue that
hinders the practicability of fairness-aware learning, namely that most existing techniques and
algorithms in this field are designed to work for binary classification only. In contrast, fairness
issues also arise in situations where machine learning models are used in far more sophisticated
domains, in particular in ranking. Here we study fairness in the context of learning to rank.
We start by motivating why fairness is important to consider in this context. We then show
how numerous fairness notions can be translated from the language of classification to learning
to rank and can be integrated in this new context with minor computational cost and with
theoretical guarantees.

6.1 Motivation and outline

Ranking problems are abundant in many contemporary subfields of machine learning and artifi-
cial intelligence, including web search, question answering, reviewer allocation, recommender
systems and bid phrase suggestions [MSR08]. Decisions taken by ranking systems affect our
everyday life, leading to concerns about the fairness of ranking algorithms.

Indeed, ranking systems are typically designed to maximize utility and return the results
most likely relevant for each query [Rob/7]. This can have potentially harmful down-stream
effects. For example, in 2015 Google became the target of heavy criticism after news reports
that when searching for "CEO" in Google’s image search, images of women were massively
underrepresented [Lo15]. Similar problems still exist in other ranking applications, such as
product recommendations or online dating.

To remedy such problems, a number of fair ranking methods have been proposed. However,
these suffer from two main shortcomings. Firstly, most existing works introduce specific fairness
notions, for example by demanding that two groups of items receive an amount of exposure
proportional to the groups’ utilities (SJ18]. While well-suited for a certain range
of applications, these notions may not be applicable in other scenarios, be it due to specific
ethical considerations, legislation or other reasons that require a different notion of fairness to
be considered. For example, when selecting a set of k applicants for a job interview, one may

71

6.5. Experiments

 

6.5 Experiments

We conduct experiments to validate the practicality and performance of our method for
training fair LTR systems, including in a large-scale setting. Our emphasis lies on studying
the interaction between model quality and fairness, the effectiveness of our proposed method
for optimizing these notions on real data and on the comparison to previous fair ranking
algorithms.

6.5.1 Datasets and experimental setup

We report experiments on two datasets. As a measure of ranking quality we use NDCG@k
for k € {1,2,3,4,5}, with results for P@k in the supplemental material. To quantify fairness,
we evaluate the three different empirical measures of fairness violation.

TREC Fairness data We use the training data of the TREC 2019 Fairness track dataset [BDEK19].
It consists of 652 queries taken from the Semantic Scholar search engine, together with a set
of scientific papers for each query and binary labels for the relevance of every query-paper
pair. The average number of labeled papers per query is 7.1, out of which 3.4 are relevant on
average. Because of the rather small number of queries, we use five-fold cross-validation to
evaluate our method and report averages and standard errors across the folds. As an exemplary
protected attribute we use a proxy of the authors’ seniority. We split the set of documents
into two groups based on whether the mean of their authors’ 110-index proxies (as provided
in the TREC data) exceed a threshold t or not. For t € {3,4,5} we get different amounts
of group imbalance, with the minority group consisting of around 46%, 26% and 9% of all
papers, respectively.

Inspired by the learning to rank approach for the TREC track of [Boni9], we pre-compute
9-dimensional embeddings of every query-paper pair by using the following hardcrafted features:

= the BM25 score of the query with the title, abstract, authors, topics and publication
venue of the paper (5 values),

= the number of in- and out-citations (2 values),
= the publication year of the paper (1 value)

= the character length of the query (1 value).

Each feature is normalized by substracting the mean of the feature over the dataset and
dividing by its standard deviation.

MSMARCO We also perform experiments on the passage ranking dataset v2.1 of MS-
MARCO [NRS*16]. It consists of approximately one million natural language questions, which
serve as queries, associated sets of potentially relevant passages from Internet sources, and
binary relevance labels for all provided query-document pairs. On average, there are 8.8
passages per question, and the average number of relevant ones is 0.65. We use the default
train-development split and report average performance and standard deviation over 10 random
seeds. To create a protected attribute, we split the passages into two groups based on their
top-level domains, as a proxy of the answers’ geographic origin. Specifically, we split by ". com

81

5.

FAIRNESS- AWARE PAC LEARNING FROM CORRUPTED DATA

 

can possess. More generally, in order to avoid a cat-and-mouse game of designing defenses
and attacks for fair machine learning models, one would need to be able to certify fairness as
a property that holds when training under arbitrary, even adversarial, manipulations of the

training data [KL93}.

Outline In this chapter, we address the aforementioned issues by studying the effect of
arbitrary data corruptions on fair learning algorithms. Specifically, we explore the fundamental
limits of fairness-aware PAC learning within the malicious adversary model of [Val85]. We
focus on binary classification with a binary protected attribute and on the demographic parity

KP09} and equal opportunity [HPS16] fairness notions.

First we show that learning under this adversarial model is provably impossible in a PAC sense
- there is no learning algorithm that can ensure convergence with high probability to a point on
the accuracy-fairness Pareto front on the set of all finite hypothesis spaces, even in the limit of
infinite training data. Furthermore, the irreducible excess gap in the fairness measures we study
is inversely proportional to the frequency of the rarer of the two protected attributes groups.
This makes the robust learning problem especially hard when one of the protected subgroups
in the data is underrepresented. These hardness results hold for any learning algorithm based
on a corrupted dataset, including pre-, in- and post-processing methods in particular.

Perhaps an even more concerning result from a practical perspective is that the adversary
can also ensure that any learning algorithm will output a classifier that is optimal in terms
of accuracy, but exhibits a large amount of unfairness. The bias of such a classifier might
go unnoticed for a long time in production systems, especially in applications where sensitive
attributes are not revealed to the system at prediction time for privacy reasons.

We also show that our hardness results are tight up to constant factors, in terms of the
corruption ratio and the protected group frequencies, by proving matching upper bounds.
To this end we study the performance of two natural types of learning algorithms under the
malicious adversary model. We show that both algorithms achieve order-optimal performance
in the infinite data regime, thereby providing tight upper and lower bounds on the irreducible
error of fairness-aware statistical learning under adversarial data corruption.

We conclude with a discussion on the implications of our hardness results, emphasizing the
need for developing and studying further data corruption models for fairness-aware learning,
as well as on the importance of strict data collection practices in the context of fair machine
learning.

5.2 Related work

To the best of our knowledge, we are the first to investigate the information-theoretic limits of
fairness-aware learning against a malicious adversary. There is, however, related previous work
on PAC learning analysis of fair algorithms, robust fair learning, and learning with poisoned
training data, that we discuss in this section.

Fairness in classification Fairness-aware learning has been widely studied in the context of
classification: we refer to our discussion in Section [2.3) for a brief overview and to for
an exhaustive introduction to the field. On the methodological side, our upper bounds analysis
employs a technique for proving concentration of estimates of conditional probabilities that

has previously been used in the context of group fairness by and [ABD 18]. A

52

are all the same. Similarly, if the clean distribution is P,

(£(5") = ho} = fP"(e(S"),Py) - 1° Be) > apa I

Therefore, depending on whether we choose Pp or P; as a clean distribution, we have

n

Popa (ro"ce«s", Po) — TPP (hi, Po) > ) = Psonr, (£(S?) = hi)

and
Posen, (ro"ceis"),P _ PPP (hy, P,\) > tm) = Popup, (L(S?) = ho)

Finally, note that P, = P4, so that either Psp. (L(S?) = hy) > 1/2 or Psp. (L(S?) = ho) =
1/2. Furthermore, L(£(S?),P:) = 7/2 holds for both i € {0,1}, for any realization of the
randomness. Therefore, for at least one of 7 = 0,1, both

L(L(S”),P;) = L(hi,B,) = 2

and
n _ a
= 2P(1—Po)  2P(1— P)(1—a)

both hold with probability at least 1/2. This concludes the proof in the first case.

PPP(£(S?),B,) — PPP (hi, Pi) >

I-
— a= = 2Py(1 — Po). Note that since f F(a z= is monotonically increasing in (0,1), ay is
unique and ay, < a.

 

Case 2 Now suppose that 7 = ~*~ > 2Po(1— Po). Let a; € (0,0.5) be such that
r) =

Now repeat the same construction as in Case 1, but with 1 = Tos = 2P(1 — Po). For
every marked point, the adversary does the same as in Case 1 with probability a,/a and does
not change the point otherwise. Then the same argument as in Case 1 shows that for one
i € {0,1}, both

 

L(L(S?),P;) = L(hi, P;) = + = P(l— Py)

and
PP (e(s? T??(h;, Pi) > ——" — =
(€(9"),P) ~ P?P(hiP) > spe
both hold with probability at least 1/2. This concludes the proof of Theorem [10]

 

 

 

 

Theorem 11. Let 0 < a < 0.5, Pio < Py < 1 be such that Py + Py, < 1. For any input

set X with at least five distinct points, there exists a finite hypothesis space H, such that for

any learning algorithm L : Unen(¥ x A x Y)" > H, there exists a distribution P for which

P(A =a,Y =1) = Py, fora € {0,1}, a malicious adversary A of power a and a hypothesis
€ H, such that with probability at least 0.5

L(L(S?),P) = L(h*,P) = min L(h,P)
and

P, P
Tr(£(S?), P) — P#°P(h*, P > min | qn. (1- Fe) a Beh
( ( ) ) ( . )2 2(1—a)Pio Py . Py

Proof. Let n = 72z, so thata = 7h.
7

131

3.

ON THE SAMPLE COMPLEXITY OF ADVERSARIAL MULTI-SOURCE PAC LEARNING

 

a certain subset of them. The two notions are inspired by the two adversarial models presented
in Section |2.2.3|

Definition 10 (Fixed-Set Adversary). Let G Cc [N]. An adversary is called fixed-set (with
preserved set G), if it only influences the datasets outside of G. That is, S; = s for allie G.

Definition 11 (Flexible-Set Adversary). Let k € {0,1,...,N}. An adversary is called
flexible-set (with preserved size k:), if it can influence any N — k of the N given datasets.
That is, there exists a set G C [N], such that |G| = k and S; = S! for alli € G.

In both cases, we call the fraction a of corrupted datasets the power of the adversary, i.e.
a= Nel for the fixed-set and a = Xk for the flexible-set adversaries.

While similarly defined, the fixed-set adversary is strictly weaker than the flexible-set one, as
the latter one can first inspect all data and then choose which subset to modify, while the
former one is restricted to a fixed, data-independent subset of sources. In particular, the
flexible-set adversary can already bias the distribution of the data by throwing out a carefully
chosen set of sources, before replacing them with new data.

Both adversary models are inspired by real-world considerations and analogues have appeared
in a number of other research areas. The fixed-set adversary is essentially the multi-source
equivalent of the malicious adversary model [KL93]. It is an appropriate model in situations in
which WN parties collaborate on a single learning task, but an unknown and fixed set of them
are compromised, e.g. by hackers that can act maliciously and collude with each other or due
to systematic data issues, such as label bias. This is a similar reasoning as in Byzantine-robust
optimization [AAZL18], where an unknown subset of computing nodes are assumed to behave
arbitrarily, thereby disrupting the optimization progress.

The flexible-set adversary corresponds to the nasty adversary model of [BEKO2]. It models
a situation where a malicious party can observe all of the available datasets and choose
which ones to corrupt, up to a certain budget. This is similar not only to the classic model
of [BEK02], but also to models from robust mean estimation, e.g. [DKK*19a], where the
adversary can again influence which subset of the data to modify once the whole dataset is
observed.

Whether robust learning in the presence of an adversary is possible for a certain hypothesis set
or not is captured by the following definition, which is a multi-source analogue of Definition [6

Definition 12. A hypothesis set, H, is called multi-source PAC-learnable against the class
of fixed-set adversaries (or flexible-set adversaries) and with respect to ¢, if there exists a
multi-source learner L and a function m : (0,1)? +N, such that for any €,6 € (0,1), any
distribution D € P(X x Y) and any set G C [N] of size |G| > 4.N (or any a < 4), whenever
S' € (&xy)"*” is a collection of N datasets of m > m(e,6) i.i.d. labelled samples from D
each, then with probability at least 1 — 5 over the sampling of S':

R(L(A(S')) < min R(h) +, (3.4)

uniformly against all fixed-set adversaries with preserved set G (or all flexible-set adversaries
of power a). A learner, £, with this property is called robust multi-source learner for H..

28

6.

FAIRNESS THROUGH REGULARIZATION FOR LEARNING TO RANK

 

These expressions can be derived readily as approximations of the conditional probabilities
of the individual fairness measures by fractions of the corresponding examples in S. This is
done by assuming that the marginal probability of any data point in S is P, and inserting
the assumed relation s(p,q) = P(f(p,q) = 1). Note that Definition |20| applies also to
binary-valued functions, so it can also be used to evaluate the fairness of a learned item
selection function f on a dataset.

Learning with fairness regularization Let L(s,5') be any loss function ordinarily used
to train an LTR model. Instead of optimizing this fairness-agnostic loss, we propose a
fairness-regularized objective:

L®it(s: $) = L(s, $) + al(s, S) (6.8)

for w > 0, where I'(s; S) is any of the empirical measures of fairness violation. The larger the
value of a, the more the resulting rankers will take the fairness of their decisions into account.
However, as our experiments in Section 6.5) show, the relation between fairness and ranking
quality is not necessarily adversarial.

Optimization The fairness regularization terms, al'(s,S) and their gradients can be com-
puted efficiently using standard numerical frameworks. In large-scale settings, where memory
and computational concerns may arise, the regularized objective (6.8) can also be optimized by
stochastic gradient steps over mini-batches, as long as the unregularized loss function L(s, 3)
supports this as well. The resulting per-batch gradient updates are not unbiased estimators
of the full gradient, though, so the characteristics of the fairness notion change depending
on the batch size. For example, if each batch is formed of a single query with all associated
documents, fairness would be enforced individually for each query, instead of on average across
all queries. In our experiments, however, we did not observe any deleterious effect of stochastic
training when using a moderate batch size of 100.

6.4.3. Generalization

We now show that, given enough data, our train-time regularization procedure will also
ensure fairness at prediction time. Our results are similar to the ones in for the
classification setting. However, in the case of ranking there is additional dependence between
the samples, which complicates the analysis and influences the complexity term.

Data generation process To study the generalization properties of our fairness measures,
we first formally define the statistical properties of the training data. We assume the following
data generation process which is consistent with the structure of LTR datasets, with the only
simplifying assumption that the item sets for all queries are of equal size m. For a given
data distribution D € P(Q x Z x {0,1}), a dataset S = {(q;, di, 14) }ietyje[m}, is sampled

peg
as follows: 1) queries, q1,...,qy, are sampled i.i.d. from the marginal distribution D(q); 2)
for each query q; independently a set of items, Dy, = {di,...,di,}, is sampled in an arbitrary

way with the only restriction that the marginal distribution of each individual di should be
D(d\q:); 3) for each pair (q:,d;) independently, the relevance r is sampled from D(r|qi, d;).

Note that while each resulting data point has marginal distribution D, a lot of flexibility
remains about how the actual items per query are chosen. In particular, the item set can
have dependencies, e.g. avoiding repetitions or diversity constraints. While incorporating

78

Proof. For any h € H:

i=1

Now applying this bound twice and using Lemma/1\ we get that with probability at least 1 — 6:

Rr(ha) < Ra(he) + y aidy (Di, Pr)

 

 

 

 

 

 

 

 

 

 

i=1
8 N log (4) M? [%o? 2
< Ra(ha) +2 S ai; (H) 3 E S aidy (Di, Dr)
i=1 2 Pr re
. N log (4) M2 [Ng
< Ralhp) +235 aM; (H) + 3' a > mt d aidu (Di, Pr)
i=l i=l f=
N log (4) M2 [Nn a? N
< Ra(hp) +r 45° ah; (#1) +6 a S Wie +r ajdy (Di, Dr)
i=l imi j=
N log (3) M2 [X a2 N
< Rr(hr) +430 aR; (#1) + 64 — >). +23 aidy (D;, Dr)
i=l i= 0% i=l

 

 

 

 

122

2.3. Fairness in machine learning

 

demographic parity, but only through the value of the true label. Equivalently, for the case of
binary classification, equalized odds is satisfied if and only if:

 

 

P(A(X) = 1]A=0,Y =y) =P(h(X) = 1/A=1,Y =y), for both y € {0,1}. (2.20)

As in the case of demographic parity, one can consider the amount of unfairness that a classifier
possesses, in terms of the deviation from equalized odds, for example

 

 

 

pels(,P) = +P

ye{0,1}

P(A(X) = 11|4=0,Y = y) —P(h(X) =1]A=1,Y =y)|. (2.21)

 

Equality of opportunity For some binary classification tasks, one might only be interested
in the fairness of classification decisions regarding instances for which the correct label indicates
a positive outcome (e.g. receiving a job or being granted a loan). In that case, the constraint
of equalized odds can be relaxed for the true negatives. Assuming without loss of generality
that a positive outcome is indicated by Y = 1, we obtain the equality of opportunity fairness
constraint h(X) IL A|Y = 1, or equivalently:

 

 

P(h(X) = 1]A =0,Y =1) = P(A(X) = 1A=1,Y = 1). (2.22)
Again, we can also measure the amount of unfairness by

T(h, P) = |P(h(X) = 1|A =0,Y =1) — P(h(X) = 1|A=1,Y =1)]. (2.23)

 

 

 

21

Step 1 Note that By) + By, < Bo + Bi ~ Bin(n, a), and so

3 3 6
pt (Bw + By > =n) < pt (Bo + By => on) < enon/2 < 3"

Similarly, Cio ~ Bin(n, (1 — a@) Pia) and Cy, ~ Bin(n, (1 — a) Pi) and so

1- * Pion) < eo (l-a)Pion/8 6
3 S$

 

pa (Co <

and

l-a )
p4 (Cu € 5 “ Pun) < ea) Piin/8 < T

 

Now since n > max { Fees) Een and Pip < Pu

 

 

3a l-a l-—a ee)
pa ((B. + By = n) Vv (Cro < 2 Pron) Vv (Cu < 2 Pun)) < 3 + 4 + a
(C.12)

<6, (C.13)

Step 2 Now assume that all of Byy + By < 22, Cio > 52Pion, Cu > 452 Pun hold.

Consider an arbitrary, fixed h € H. Since h is fixed, we drop the dependence on h from the
notation for the rest of the proof and write 72, = 77,(h), CL, = Ch,(h) ete.

We now prove that for both a € {0,1}

Bra
Aia = |v? - ¥%| < =". C.14
=a Val S GB (C.14)
For each a € {0,1}, this can be shown as follows. First, if 0", l{a? = a,y? = 1} =
Bia + Cia = 0, then both yf,(h) and ¥f,(h) are equal to 0, because of the convention that
3 = 0. In addition, Bj, = Cig = 0. Therefore, inequality C.14) trivially holds.
Similarly, if Bi, = 0 and Cy, > 0, then yf,(h) = yf,(h) and so Aj, = 0 and ((C.14) holds.

Assume now that B,, > 0. Note that if Cia = 7, L{a? =a, y? = 1,1 g B} =0, then
Bi,
Bia

Bia Bia Bra
Bra Bia t+ Cia ~ Cia + Big

 

 

 

 

Ara = lalh) — v(h)| | Q

Finally, assume that both Ci, > 0 and By, > 0. Note that under any realization of the

randomness of the data sampling and the adversary, for any a € {0,1}

_ Ve WA?) = la? = a.y? = Lig BR+ UE UA?) = La = aye = Lie Bt
Shy Ma? =a, y? = 1,1 EP} + ON, a? = a,y? = Lie P}

 

Ya

 

 

 

 

 

 

 

on Tv Big
Ca T Bia
Next we bound how far this quantity is from the clean estimator Cha
5 Cl + Bi B B} B
Ata *= tha — Vial = |F2——"* — ha ~— |i. - =| < =.
Cha T Bra Cia + Bra Bra Cia + Bia

 

 

 

143

2.

PRELIMINARIES

 

identify the inputs with their feature representations directly, since we assume that the feature
extractors are given and fixed. Therefore, we will also identify 1 with the corresponding metric
space of the features, e.g. R%.

A label space Y The label (or output) space is the set of all possible labels that can
be assigned to an instance. A common example is the case of classification, where Y =
{0,1,...,k—1}. Problems for which & = 2, such as those mentioned above, are called binary
classification tasks and problems where & > 2 are known as multi-class classification tasks. In
other cases labels can take continuous values, for example when Y = R, a case that we refer
to as regression.

A training dataset The training set refers to the provided example input-label pairs. We
will often denote the training data by S = {(x;,y;)}%_, € (¥% x Y)". This is the set of
examples that should be used for constructing a labeling function. An important parameter is
the number of samples, denoted here by n. Intuitively, the larger the value of n, the more
information we have about the prediction task at hand. In other words, one would expect that
the learning problem gets easier as access to more data is given.

A hypothesis space A hypothesis space is a set of possible labeling rules to choose
from. Formally, we assume that a hypothesis space H C * is given. Each element h
of H is a function h : Y — Y from the input space to the label space and represents
a rule for labeling inputs. For example, if 4 = R? for some d € Nt and Y = {0,1},
then a commonly used hypothesis space is the one of all linear classifiers on R*, namely
H= {h h(a) = sign (wx) for some w € Ri}.

A learner A learner is a procedure that takes the set S of labeled examples and outputs a
hypothesis from H that, ideally, accurately predicts labels given inputs. Formally, a learner is
a function:

LU (4 x VJ" SH. (2.1)

Note that here we think of the learner as any procedure that takes a dataset of arbitrary size
and returns a hypothesis, thereby focusing on the statistical, rather than the computational,
questions regarding learning.

A loss function The loss function measures the penalty incurred when assigning a wrong
label to an instance. Intuitively, the goal of learning is to find a hypothesis that accurately
predicts labels given inputs. For any hypothesis h € H one can measure the quality of its
prediction on a input-label pair (a, y) via a loss function £ : Y x Y + R*, by computing
(h(x), y). A natural requirement is that ¢(y, y) = 0 and ¢(y1, y2) > 0 for any y, yi, yo € Y
with y1 A yo.

Commonly used loss functions are the 0—1 loss for classification, that is €(y1, yo) = 1 {y 4 yo};
as well as the square loss for regression, i.e. &(y1, yo) = (yi — Yo).

Although the loss function gives a way of measuring the performance of a hypothesis on one
input-label point, it is unclear how to formalize the objective of finding a hypothesis that works
well on multiple future inputs. Moreover, in order to have any hope of constructing such a
hypothesis based on the training data, one needs to ensure that the points in S are in some
sense representative of the future input-output pairs that the learner will be tested on.

6

b) First we argue that there exists a distribution D, on Y x Y and a fixed-set adversary 4},

such that:
F S'~D. R(L(A py R(h 320Nm Nir 4 A.42

This follows directly from the classic lower bound for binary classification in the unrealizable
case. Indeed, applying Theorem 3.23 in JMRT18] and setting the adversary to be the identity
mapping gives the result.

Now, since any hypothesis space with VC dimension d > 2 is non-trivial, we also know from

a) that there exists an adversary @3 and a distribution Dz on Y x Y, such that:

Sol : Qa 1
Pomp, (Ricans ))) = min R(h) > =) > (A.43)

Fix any set of values for N,m,d,k. Then at least one of the pairs (@j,D1) and (@, D2)
satisfies:

°(5"))) — mi d oO
Ps (ree ())) — aig > == + ica]

sy ar d a
> Ps (rica (S"))) > 2max{ Ta80Nm? o)]

= Py (rece > max{ ut 2)

> a
64

 

 

 

 

118

APPENDIX

Proofs from Chapter 4

Here we present a proof of Theorem 7, To this end, we first show a concentration result
about the a-weighted empirical risk Ro(h). Then we use this result, together with a standard
discrepancy argument, to prove the statement of the theorem.

First we bound |R, (h) — Ra (h)| with high probability and uniformly over H. We adapt
the classical proofs of generalization bounds in terms of the Rademacher complexity of a

hypothesis class, e.g. [BBLO4].

Lemma 1. Given the setup and assumptions described above, for any 6 > 0 with probability
at least 1 — 6 over the data, for any function h € H:

[Ra (h) — Ra (h)| < os aM; (H) +3 (B.1)

i=1

 

where for each i = 1,2,...,N:

 

 

 

 

WR; (H) = E, (3 (A vuttsteu)ons))) , (B.2)

SEH \ Mi 5

and where o;,; are independent Rademacher random variables.

Proof. Write:

Rea (h) < Ra (h) + sup (Ra (f) — Reo (f)) (B.3)

To link the second term to its expectation, we prove the following:

Lemma 2. Define the function 6: (X x Y)™ + R by:

, OG
sup [OZ as cewy Figs vs oe Zig) —o (eis: = eoaguce : sony) | < iv (B 4)
’ Li :

ZL Lys ZNymy 1%5,5

119

5.

FAIRNESS- AWARE PAC LEARNING FROM CORRUPTED DATA

 

Bound for equality of opportunity Similarly, we consider the following estimate for the
equality of opportunity deviation measure:

rar L{h(a?) = La? =0,yP = 1 i Ufh(a?) = La? = Ly? = 1b

a

PLO’ p) —
(”) mile =, =H} Ela =Lye =}

 

(5.18)

with the convention that f = 0 for the purposes of this definition. Suppose that a learner
LEO? Use (¥ x Ax Y)" > H is such that

n=1

LXOP(S?) € aremin(R?(h) + AT#°"(h)), for all S?,

heH
that is, always returns a minimizer of the \-weighted empirical objective. Then:

Theorem 13. Let H. be any hypothesis space with d = VC(H) < oo. Let PE P(X x Axy)
be a fixed distribution and let @ be any malicious adversary of power a < 0.5. Then for any

6 € (0,1) andn > max { $e), tales /) ay

P# (L£"(C£0"(S?)) < min LE0%(h) + AR”) > 1-6,

 

where
AROP — 3a + M(2APO?) 4.0 | 4/ & af
x n Pron
and
2a a
AEOp _ _— (+)
Pio +a Pr

Again, for a sufficiently large sample size, this result implies an upper bound on the excess
loss of the learner LEP in terms of the weighted objective

EOp; pEOp; ap\\ _ ++. 7 EOp a )
LEP (CE O"(S?)) — min LE (t) <O (a+r (5.19)

which is again order optimal, according to Theorem 9| and Inequality (5.14).

5.5.2 Component-wise upper bounds

We now introduce a second type of algorithms, which return a hypothesis that achieves both a
small loss and a small fairness deviation measure on the training data, or, if no such hypothesis
exists, a random hypothesis. We show that, in the case when there exists a classifier that is
optimal in both accuracy and fairness, with high probability such learners return a hypothesis
h € H that is order-optimal in both elements of the objective vector L(h), as long as 1 is of
finite VC dimension and n is sufficiently large. Finally, in the case of realizable PAC learning
with equality of opportunity fairness, we are able to provide an algorithm that achieves such
order-optimal guarantees with fast statistical rates, for any finite hypothesis space.

Throughout the section, we assume that there exists a classifier h* € H, such that UW(h*) x
Uh) for all h € H. That is, R(h*) < R(h) and P(h*) < P(A) for all h © H. We also
assume that d = VC(H) < oo.

64

5.

FAIRNESS- AWARE PAC LEARNING FROM CORRUPTED DATA

 

That is, 3 is the set of classifiers that are not far from optimal on the train data, in terms
of equality of opportunity fairness. Now define the component-wise learner for equality of
opportunity:

anyhEHiNHs, if HiNH3 40

LEOp SP _—
ew "(5") any hE H, otherwise,

that returns a classifier that is good in both metrics, if such exists, or an arbitrary classifier
otherwise. Then the following result holds.

Theorem 15. Let H. be any hypothesis space with d = VC(H) < oo. Let PE P(X x Axy)
be a fixed distribution and let A be any malicious adversary of power a < 0.5. Suppose that
there exists a hypothesis h* € H, such that U(h*) < Uh) for all h © H. Then for any

6 € (0,1) andn > max { Seels/0), Tleg3/5) | sh, with probability at least 1 — 6

LPO? (£2OP(SP)) (, +0 (\") AAPOP 4. 6 ( d ))
i Pion

Since AZOr = O (4), in the large data limit we obtain that

 

 

LFon(c£or) < (O(a),0(+-)). (5.21)

° Pro
Note that this bound is order-optimal for the class of finite hypothesis spaces, and hence
also for the class of hypothesis spaces with finite VC dimension, according to Theorem 9] and

Inequality (5.15).

Upper bound with fast rates Finally, we study learning with the equality of opportunity
fairness notion, in the realizable PAC learning framework, where a perfectly accurate classifier
exists. Given this additional assumption, we are able to certify convergence to an order-optimal
error in both fairness and accuracy at fast statistical rates. For simplicity we assume that H. is
finite here.

Specifically, note that while the results presented already achieve order-optimal guarantees in

the limit as n — oo, for a finite amount of samples they incur an additional loss of O (Fz)

Regarding P (for demographic parity) or Pio (for equality of opportunity) as fixed, all previous
1

algorithms need O (+) samples to achieve an excess risk and fairness deviation measure of

O(a). In contrast, the algorithm we present now only requires O (4) samples.

Formally, assume that the underlying clean distribution P is such that there exists a h* € H,
for which P(h*(X) = Y) = 1. This implies that L(h*) = 0 and [”°?(h*) = 0.

Key to the design of an algorithm that achieves fast statistical rates for the objective L are
the following empirical estimates:

fer L{h(a?) = 0, a? =a, y? = 1}
Yh, Ha? =a,y? = 1}

of 7,,(h) = P(A(X) = 0|A =a, Y = 1) =0 for a € {0,1}. Given a (corrupted) training set
S?, denote by

 

Va(h) = (5.22)

H*(S?) = {h EH max Yq (h) < APO A RP(h) <

 

| 2

\ (5.23)

66

2.2. Learning from corrupted data

 

2.2 Learning from corrupted data

As discussed in the previous section, one of the central assumptions within the framework of
PAC learning is that the training set S is sampled i.i.d. from the target data distribution D.
While being natural and convenient from a theory perspective, this assumption hardly ever
holds in practice. There are various real-world data problems that can lead to a deviation
from the i.i.d. model, for example dependence between the samples or underrepresentation of
the certain subpopulations. In this thesis we focus on another issue that is of high relevance
in practice, namely that of data corruption, that is, the presence of noisy, biased or even
purposely manipulated entries within a subset of the training data.

In the presence of data corruption, a cause of concern is that the machine learning guarantees
that are traditionally developed under the idealistic i.i.d. assumption may not apply anymore
because this assumption is not fulfilled. Unfortunately, empirical observations have shown that
such deviations from the classic theory can indeed lead to poor performance of learned models
[SNT+20]. In other words, models trained via classic machine learning
techniques are often not robust to the data corruption. This naturally hinders the applicability
of such systems to real-world tasks.

A major focus of this thesis is on overcoming the aforementioned issues by developing
machine learning algorithms that offer provable performance guarantees and strong empirical
performance, even when a fraction of the training data is corrupted. In this section, we first
motivate this problem and give specific examples of the types of data corruptions that can be
expected in machine learning datasets. Then we introduce several classic models that allow us
to reason about learning from a potentially corrupted dataset.

We note that learning from corrupted training data is a field with long history, where both
theoretical and experimental issues have been widely studied, e.g.
IKL93} [CBDF*99) [BEKO02] (CS04] [BNL12a] [CSVI7} SKET7] [CLL *17] [DKK*19b]. A complete
overview of this field is therefore unavoidably beyond the scope of this thesis. Instead, we
focus on concepts related to robustness in the context of PAC learning.

2.2.1 Motivation

There are various types of data corruption that are known to occur in real-world training data.
Here we present a few motivating examples.

Label noise One of the most commonly cited problems with real-world data is that of
label noise [EVI3]. This issue is particularly prevalent in recent years, since it is becoming
increasingly common to label datasets via crowdsourcing platforms, such as Amazon Turk,
and the quality of the obtained labels is often low.

Measurement errors Various types of random or systematic errors can also appear on
the level of the input variables [BEKO2]. For example, in many scientific fields data is often
obtained via taking measurements by using sophisticated devices. Since these devices are often
used as black boxes, a practitioner may not notice certain malfunctions that result in random
or systematic errors for some data entries.

Historical bias In many cases when machine learning models are trained as part of a real-
world decision making system, one may expect training data issues related to bias stemming

13

2.

PRELIMINARIES

 

situations protected attribute information may be unavailable at test time, for example for
privacy reasons.

Similarly to the classic PAC learning setup, the performance of the learner is measured in terms
of the performance of the returned hypothesis h € H on the distribution D. For measuring
accuracy, the expected risk R(h) can be used again. For fairness, we will again be interested
in the extent to which the fairness property is satisfied by h on a population level, that is,
with respect to the distribution D. To this end, one can define a fairness deviation measure
T(h, D), that quantifies the amount of unfairness that h possesses, with respect to the fairness
definition of interest. We give a few examples of such measures in the following section.

2.3.3. Notions of group fairness

Finally, we present the three arguably most popular group fairness notions from the literature,
which will also be considered throughout this thesis: demographic parity, equalized odds and
equality of opportunity. We also give examples of corresponding fairness deviation measures.
The fairness notions are well-defined for a general space A, but we will explicitly consider
the case where A = {0,1} is binary, for example corresponding to a setup where a single
disadvantaged group of candidates for a job should be protected from discriminatory decisions.
We study the binary case separately since corresponding fairness deviation measures can be
readily defined in this context.

Demographic parity Given a distribution D € P(X x Ax Y) and a classifier h: ¥ > Y,
a natural fairness property is to require that the decisions of the classifier are independent of
the protected attribute, that is h(X) IL A. This fairness notion is known in the context of
machine learning as demographic parity [HPS16}. In the case when A = {0, 1}, this

is equivalent to enforcing:
Prx,a,y)~v(h(X) = 1A = 0) = Pcx,a,y)~v(h(X) = 1|A = 1). (2.18)

In practice, few classifiers will achieve exact fairness in the sense of demographic parity. In
addition, even if a classifier is perfectly fair, this will be impossible to infer from training data,
due to the finite sample size effects. Therefore, it is natural to consider a corresponding fairness
deviation measure |WMI9], describing the extend to which a classifier
h is unfair. Here we adopt the mean difference score measure of and for
demographic parity

TP" (h, D) = |Pcx.ay)~p(h(X) = 1|A = 0) — Pa ayyeo(h(X) =1/A=1)). (2.19)

While being a natural notion of fairness, demographic parity can in many cases be impossible
to achieve alongside with high accuracy. In particular, in a realizable PAC learning setup,
a perfectly accurate classifier h* € H exists, but it may be the case that h*(X) strongly
correlates with the variable A. This reflects the observation that in many cases the true label,
e.g. the suitability for a job, may in fact be correlated with the protected attribute, for example
because of better access to education for people from non-disadvantaged backgrounds. This
observation naturally leads to the notion of equalized odds.

Equalized odds Equalized odds requires that h(X) IL A|Y. That is, the decisions of
the classifier can depend on the protected attribute A, in contrast to the implications of

20


2.2. Learning from corrupted data

 

The adversary Due to the aforementioned issues, in this thesis we will mostly adopt an
orthogonal, fully worst-case approach to modeling data corruptions. Following an established
framework from the fields of cybersecurity and the software verification, we will assume the
presence of a malicious opponent, called the adversary [BEY05], that may insert corrupted
data points that can be generated with infinite computational resources and can be chosen
with a broad knowledge of the underlying setup, the clean data points and even the learning
algorithm. While in some cases such a worst-case approach may appear overly pessimistic, our
treatment has the advantage of providing a “certificate” for the model performance: whenever
our theoretical analysis provides guarantees against the adversary, these guarantees hold under
a very broad range of possible data problems and therefore cover a rich set of applications and
secure the model against multiple, known or unknown, data issues.

Formally, the adversary is simply a procedure that takes in a clean dataset and output a new,
corrupted set of points of the same size as the original training set. That is, the adversary
is a potentially randomized function @ : US, (¥ x Y)" > U%, (& x Y)", with the only
constraint that |@(.S)| = |.S| for any S C (4 x Y). Typically, we will assume that the adversary
is subject to certain limitations, for example, only being able to affect a certain fraction of
the data. We will denote the set of all possible adversaries, that is, all functions that satisfy
the limitations, by 21. Depending on the type of restrictions that are imposed on @, various
adversarial models are obtained. We will present several examples of popular adversarial models
in Section |2.2.3|

 

One common limitation enforced on the adversary that will be recurring throughout the thesis
is that only a certain fraction of the data, say a € [0, 1], can be manipulated. That is, the
adversary can only manipulate up to an points, with this upper bound being either approximate
(e.g. Bin(n, a) points can be changed) or exact. In the fields of robust statistics and machine
learning, it is standard to think of a as a small constant and in particular to consider a < 0.5.
Indeed, in the case when a > 0.5, robust estimation has only been shown to be possible under
additional assumptions, for example the possibility of returning multiple candidate estimates
or the availability of a small subset of trusted, clean data [CSV17].

We will consider a as a crucial parameter that describes the power of the adversary and often
refer to an adversary that can corrupt (approximately) an points as an adversary of power a.

PAC learning against an adversary We now formalize the learner's objective in the
presence of an adversary. Intuitively, given a supervised learning problem with a hypothesis
space H and a loss function @, a learner L : US, (X x VY)" + H will be successful against
the set of adversaries 2 if it is able to guarantee PAC learnability based on the corrupted data
against any adversary in @ € 2l and for any clean data distribution D. Here we formalize this

as follows:

Definition 6. A hypothesis space H. is adversarially agnostic PAC learnable with respect
to a loss function ¢: Y x Y — R* and against the set of adversaries 2, if there exists a
learner L: US, (¥ x VY)" > H and a function my, : (0,1) x (0,1) + N, such that for
any €,6 € (0,1), any distribution D and any adversary @ € 2, whenever S° is an clean i.i.d.
dataset of at least my,(€,d) points sampled from D and S? = Q(S°), if the learner takes as
input the set S? , then with probability at least 1 — 6 with respect to the sampling of the
points in S° and the randomness of the adversary:

R(L(S)) = R(L(A(S*))) < inf R(h) +. (2.17)

15

2.

PRELIMINARIES

 

Intuitively, it is assumed that the adversary has access to an initial clean training dataset S°
and manipulates it into a poisoned (corrupted) dataset S?. The learner then works with the
dataset S? and hence only has access to the corrupted data.

Crucial in this definition is the ordering of the quantifiers. As discussed above, our approach to
the problem of learning from corrupted data is a worst-case one. Therefore, we are assuming
that the adversary acts with a full knowledge of the setup, including the hypothesis space
and loss function, but also the clean distribution, the clean training data and even the learner
itself. In contrast, the learner is assumed to have access only to the hypothesis space, the loss
function and the corrupted data.

This is indeed reflected in the formulation of Definition [6| since the adversary is chosen after
the learner. In particular, for a learner £ to achieve PAC learnability, £ needs to “work” in the
usual PAC sense under any distribution-adversary pair. Since the adversary is chosen after £
is fixed, this is a way to formalize the claim that the adversary “knows” the learner.

This reasoning is also reflected in the results in the following sections of the thesis. Whenever
we state positive results that certify the existence of a learner achieving a certain performance,
these results will be structured as follows:

There exists a learner £, such that for any distribution D, any adversary A € 2 and any
6 € (0,1), with probability at least 1-6...

Since the learner is fixed before the distribution and the adversary are, it has to work for any
such pair. In contrast, hardness results that show that the adversary can prevent the learner
from finding models with certain properties will have the form:

For any learner £ there exists a distribution ’D and an adversary A € 2,
such that with constant probability ...

Note in particular that the adversary can be chosen after the learner is constructed and together
with the distribution and it can therefore be tailored to their choice.

2.2.3. Adversarial models

We now discuss two established worst-case models of data corruption, namely the malicious
adversary model and the nasty adversary model [BEKO2]. Throughout the thesis we
will be considering adversaries that are inspired by these classic models and adapted to the
corresponding contexts that we study.

For both models, we assume that a clean dataset S° ~ D” is sampled i.i.d. from D and we
describe the (randomized) procedure for computing a corresponding corrupted dataset S?.

The malicious adversary model The malicious adversary model was first introduced by
Val85] and extensively studied by and [CBDF*99}. Informally, given a clean dataset,
the malicious adversary has the power to arbitrarily manipulate a randomly chosen subset of
the data of size Bin(n, a), for some corruption ratio a € [0,1]. Within this random subset,
the adversary can substitute each data point with an arbitrary input-label pair, chosen with
knowledge of the remaining data, the clean distribution and the learning algorithm. Outside
of this set, the points have to remain the same.

16

[Fen17]

[FGC20]

[FKT*18]

[FV13]

[FXM14]

[FYB18]

[GAK19]

[GBY*18]

[GDL20]

[HG17]

[HK20]

[HMPW16]

[HMWG18a]

[HMWG18}]

Jiashi Feng. On fundamental limits of robust learning. arXiv preprint
arXiv:1703.10444, 2017.

Riccardo Fogliato, Max G’Sell, and Alexandra Chouldechova. Fairness evaluation
in presence of biased noisy labels. In Conference on Uncertainty in Artificial
Intelligence (AISTATS), 2020.

Golnoosh Farnadi, Pigi Kouki, Spencer K Thompson, Sriram Srinivasan, and
Lise Getoor. A fairness-aware hybrid recommender system. arXiv preprint
arXiv:1809.09030, 2018.

Benoit Frénay and Michel Verleysen. Classification in the presence of label
noise: a survey. /EEE transactions on neural networks and learning systems,
2013.

Jiashi Feng, Huan Xu, and Shie Mannor. Distributed robust learning. arXiv
preprint arXiv:1409.5937, 2014.

Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. Mitigating sybils in
federated learning poisoning. arXiv preprint arXiv:1808.04866, 2018.

Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. Fairness-aware
ranking in search & recommendation systems with application to linkedin talent
search. In Conference on Knowledge Discovery and Data Mining (KDD), 2019.

Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter,
and Lalana Kagal. Explaining explanations: An overview of interpretability of
machine learning. In International Conference on Data Science and Advanced
Analytics (DSAA), 2018.

Sruthi Gorantla, Amit Deshpande, and Anand Louis. Ranking for individual
and group fairness simultaneously. arXiv preprint arXiv:2010.06986, 2020.

Dan Hendrycks and Kevin Gimpel. Improving the generalization of adversarial
training with domain adaptation. In International Conference on Learning
Representations (ICLR), 2017.

Steve Hanneke and Samory Kpotufe. A no-free-lunch theorem for multi-task
learning. arXiv preprint arXiv:2006.15785, 2020.

Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Woot-
ters. Strategic classification. In Innovations in Theoretical Computer Science
Conference (ITCS), 2016.

Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using
trusted data to train deep networks on labels corrupted by severe noise. In
Conference on Neural Information Processing Systems (NeurIPS), 2018.

Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Us-

ing trusted data to train deep networks on labels corrupted by severe noise.
Conference on Neural Information Processing Systems (NeurlPS), 2018.

96

3.2. Related work

 

adversarially corrupted. There are a number of conceptually related works, though, which we
will discuss for the rest of this section.

The limits of learning from unreliable data sources Most related is the work of [QV18],
as well as the follow-up works of (JOT9], that aim at estimating discrete distributions
from multiple batches of data, some of which have been adversarially corrupted. The main
difference to our results is the focus on finite data domains and on estimating the underlying
probability distribution rather than learning a hypothesis.

Qial8] studies collaborative binary classification: a learning system has access to multiple

training datasets and a subset of them can be adversarially corrupted. In this setup, the
uncorrupted sources are allowed to have different input distributions, but share a common
labelling function. The author proves that it is possible to robustly learn individual hypotheses
for each source, but a single shared hypothesis cannot be learned robustly. For the specific
case that all data distributions are identical, the setup matches ours, though only for binary
classification in the realizable case, and with a different adversarial model.

In a similar setting, show, in particular, that an adversary can increase the probability
of any "bad property" of the learned hypothesis by a term at least proportional to the fraction
of manipulated sources. These results differ from ours, by their assumption that different
sources have different distributions, which renders the learning problem much harder.

Byzantine-resilience in distributed and federated learning Another related general
direction is the research on Byzantine-resilient distributed learning, which has seen significant
interest recently, e.g. [AAZL18]. There the focus is on
learning by exchanging gradient updates between nodes in a distributed system, an unknown
fraction of which might be corrupted by an omniscient adversary and may behave arbitrarily.
These works tend to design defences for specific gradient-based optimization algorithms, such
as SGD, and their theoretical analysis usually assumes strict conditions on the objective
function, such as convexity or smoothness. Nevertheless, the (nearly) tight sample complexity
upper and lower bounds developed for Byzantine-resilient gradient descent and its
stochastic variant are relevant to our results and are therefore discussed in detail
in Sections [3.4.2] and [3.5.2| The worst-case performance of distributed SGD has also been
studied in the context of asynchronous training [ADSK18}.

The non-i.i.d. split of the data on local devices is one of the main characteristics of federated
learning and the pioneering work of addresses this by occasionally averaging local
models to ensure global consistency. There is also a large body of literature on attacks and
defences in this context, e.g. [FYB18}. Apart from focusing on iterative
gradient-based optimization procedures, these works also allow for natural variability in the
distributions of the uncorrupted data sources.

 

Other approaches to robust learning from unreliable sources Learning from multiple
sources is a topic relevant for many applications of machine learning and data corruption is a
problem acknowledged in some of these areas. In particular,
and references therein consider the problem of label noise in crowdsourced data. However,
they only focus on label corruptions. [Fen17]| considers the fundamental limits of learning
from adversarial distributed data, but in the case when each of the nodes can iteratively
send corrupted updates with certain probability. provide a method for distributing
the computation of any robust learning algorithm that operates on a single large dataset.

25

4. ADVERSARIAL MULTI-SOURCE LEARNING IN PRACTICE

 

We therefore again adopt the discrepancy distance [MM12], whose empirical version we used
in Chapter |3|as a specific notion of distance that depends on the hypothesis class and allows us
to reason about the change in performance of a predictor from H learned on one distribution,
but applied to the other. Formally, define the discrepancy between the distributions D; and
Dr with respect to the hypothesis class H. as:

dy (Di, Dr) = teu (Ri(h) — Rr(h)I)- (4.3)

Recall that the discrepancy between the two distributions is large, if there exists a predictor
that performs well on one of them and badly on the other. On the other hand, if all functions
in the hypothesis class perform similarly on both, then D; and Dr have low discrepancy.

The following theorem provides a bound on the expected loss on the target distribution of the
predictor ha, i.e. the minimizer of the a-weighted sum of the empirical losses over the source
data.

Theorem 7. Given the setup above, let ha = argminjeyRa(h) and h% = argminneyRr(h).
For any 6 > 0, with probability at least 1 — 6 over the data:

. N N log (4) ) Me
Rr(ha) < Rr(hr) +4 > anh; (H) +2 > ajdy (Di, Dr) +6 a Hie
» a

i=l i=l

 

 

 

 

RK; (H) = E, | sup i (Wig), Yag
(#) (a (FE one 4 hn)

and 0;,; are independent Rademacher random variables.

A proof is provided in the supplementary material.

We note that a similar result appears as Theorem 5.2 in the arXiv version [ZZY13] of the NIPS
paper [ZZY12]. The authors bound the gap between the weighted empirical loss on the source
data of any classifier and its expected loss on the target task, with the additional assumption
of a deterministic labeling function for each source. Based on this, they study the asymptotic
convergence of domain adaptation algorithms as the sample sizes at all sources go to infinity.
In contrast, our theorem compares the performance of the minimizer ho of the a-weighted
empirical loss on the target task to the performance of the optimal (but unknown) hi, and
does not require deterministic labeling functions. Our target application is also different, since
we use the bound to design learning algorithms that are robust to corrupted or irrelevant data,
given finite amount of samples from each source.

4.3.2 From bound to algorithm

Algorithm description. To obtain a good predictor for the target task, we would like to
choose a, such that Rr(hq) is as close as possible to Rr(h7) (the expected loss of the best
hypothesis in H). This suggests selecting the weights by minimizing the right-hand side of

(4.4).

40

[cSv17]

[CSV18]

[CSX17]

[CV10]

[CZ13]

[DADC18]

[DCLT19]

[DCM*12]

[DHP+12]

[DKK*16]

[DKK* 19a]

[DKK*19b]

[DSZO+*15]

Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted
data. In ACM Symposium on Theory of Computing (STOC), 2017.

L Elisa Celis, Damian Straszak, and Nisheeth K Vishnoi. Ranking with fair-
ness constraints. In International Colloquium on Automata, Languages, and
Programming (ICALP). Schloss Dagstuhl — Leibniz Center for Informatics,
2018.

Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning
in adversarial settings: Byzantine gradient descent. Proceedings of the ACM
on Measurement and Analysis of Computing Systems (POMACS), 1(2):1-25,
2017.

Toon Calders and Sicco Verwer. Three naive Bayes approaches for discrimination-
free classification. Data Mining and Knowledge Discovery (DMKD), 2010.

Toon Calders and Indré Zliobaité. Why unbiased computational processes can
lead to discriminative decision procedures. In Discrimination and privacy in the
information society, pages 43-57. Springer, 2013.

Maria De-Arteaga, Artur Dubrawski, and Alexandra Chouldechova. Learn-
ing under selective labels in the presence of expert consistency. In Fairness,
Accountability, and Transparency in Machine Learning (FAT/ML), 2018.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
Pre-training of deep bidirectional transformers for language understanding. In
Annual Meeting of the Association for Computational Linguistics (ACL), 2019.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark
Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale
distributed deep networks. In Conference on Neural Information Processing
Systems (NIPS), 2012.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. Fairness through awareness. In Innovations in Theoretical Computer
Science Conference (ITCS), 2012.

llias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra,
and Alistair Stewart. Robust estimators in high dimensions without the com-
putational intractability. In /EEE Symposium on Foundations of Computer
Science (FOCS), pages 655-664. IEEE, 2016.

llias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart. Robust estimators in high-dimensions without the computa-
tional intractability. SIAM Journal on Computing, 48(2):742-864, 2019.

Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt,
and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization.
In International Conference on Machine Learing (ICML), 2019.

Christopher M De Sa, Ce Zhang, Kunle Olukotun, Christopher Ré, and Christo-
pher Ré. Taming the wild: A unified analysis of hogwild-style algorithms. In
Conference on Neural Information Processing Systems (NIPS). 2015.

95

List of Collaborators and Publications

This thesis is written based on the following first-author publications and manuscripts of Nikola
Konstantinov. For each, a summary of the contributions of every author is provided below.

« Nikola Konstantinov, Elias Frantar, Dan Alistarh, and Christoph H. Lampert. On the
sample complexity of adversarial multi-source pac learning. In International Conference
on Machine Learing (ICML), 2020

This paper constitutes the core of Chapter [3]

Nikola Konstantinov formulated the problem and research objectives. He proved
the upper bounds results, as well as Theorem [6 He also created an initial draft of
the paper and was involved in the editing later on.

Elias Frantar was working on the experimental validation of the algorithm achieving
the upper bounds and later on on the lower bounds. In particular, he formulated
and proved an early version of Theorem |5| During this work, Elias Frantar was

advised by Christoph H. Lampert and worked closely with Nikola Konstantinov.

Dan Alistarh was involved in the lower bounds work, in particular contributing with
important discussions and references related to the relevant proof techniques. He
also contributed to the comparison to prior work, in particular to the Byzantine
learning literature.

Christoph H. Lampert advised Nikola Konstantinov and Elias Frantar throughout
the project, providing important suggestions about related work, proof techniques,
experimental design and writing. Christoph H. Lampert also participated extensively
in the editing of the paper.

= Nikola Konstantinov and Christoph H. Lampert. Robust learning from untrusted sources.
In International Conference on Machine Learing (ICML), 2019

This paper constitutes the core of Chapter |4|

Nikola Konstantinov formulated the problem and research objectives, proved the
theoretical results and conducted the experiments. He also created the initial draft
of the paper and was involved in the editing later on.

Christoph H. Lampert advised Nikola Konstantinov throughout the project, with
important suggestions about related work, proof techniques, experimental design
and writing. Christoph H. Lampert also participated extensively in the editing of
the paper.

= Nikola Konstantinov and Christoph H. Lampert. Fairness-aware learning from corrupted
data. arXiv preprint arXiv:2102.06004, 2021

xi

3.

ON THE SAMPLE COMPLEXITY OF ADVERSARIAL MULTI-SOURCE PAC LEARNING

 

platforms, through collecting data from different websites and social networks profiles, to
collaborating with other parties working in similar domains. Once access to such datasets is
granted, they might either be available for training centrally, or be stored distributedly and
used for training via a distributed learning procedure, e.g. via using the recently developed
techniques for federated learning [MR17].

Naturally, datasets obtained from such sources vary greatly in quality, reliability and relevance
for the learning task. For instance, genetic data from multiple laboratories may have been
obtained via different measurement devices or data preprocessing techniques [WMP*03]. In
the case of crowdsourcing, a typical problem is label bias and label noise, due to incompetent
or malicious workers [WLC*10]. More generally, such datasets might also contain gross errors,
contaminations and adversarial modifications of the data [BGS*17]. The variety of possible
deviations from the target data distribution, as well as the large volume and dimensionality of
the data in real-world applications, make the assessment of the quality of the provided data a
difficult task.

In this and the next chapter we will study the problem of how to learn from multiple untrusted
sources, while being robust to any corruptions of the data provided by some of them, be it
such coming from negligence, bias or malicious behavior. The analogous question to the classic
problem of robust learning from one dataset against an adversary is as follows. Given a number
of i.i.d. datasets, a constant fraction of which might have been adversarially manipulated, is
there a learning algorithm that overcomes the effect of the corruptions and approaches an
optimal model?

For the rest of this chapter we study this problem, in the centralized data case, from a formal
PAC learning perspective and provide a positive answer. Specifically, our main result is an
upper bound on the sample complexity of adversarial multi-source learning, that holds as long
as less than half of sources are manipulated (Theorem /4).

A number of interesting results follow as immediate corollaries. First, we show that any
hypothesis class that is uniformly convergent and hence PAC-learnable in the classic i.i.d. sense
is also PAC-learnable in the adversarial multi-source scenario. This is in stark contrast to the
single-source situation where, as mentioned above, no non-trivial hypothesis class is robustly
PAC-learnable. As a second consequence, we obtain the insight that in a cooperative learning
scenario, every honest party can benefit from sharing their data with others, as compared to
using their own data only, even if some of the participants are malicious.

Besides our main result we prove two additional theorems that shed light on the difficulty
of adversarial multi-source learning. First, we prove that the naive but common strategy of
simply merging all data sources and training with some robust procedure on the joint dataset
cannot result in a robust learning algorithm (Theorem 5). Second, we prove a lower bound
on the sample complexity under very weak conditions (Theorem (6). This result shows that
under adversarial conditions a slowdown of convergence is unavoidable, and that in order to
approach optimal performance, the number of samples per source must necessarily grow, while

increasing the number of sources need not help.

3.2 Related work

To our knowledge, our results are the first that formally characterize the statistical hardness of
supervised learning from multiple i.i.d. sources, when a constant fraction of them might be

24

Case 1 Assume that 7 = ;S. < 2min{Pio,1 — Pio — Pu}. Take five distinct points
{x1, ©2, U3, €4, U5} € A. We consider two distributions Py and P,, where each P; is defined as

Pu
Pio — 9/2
n/2

P(x, a,y) = n/2

1— Pro — Pu- 1/2

0

ife=a,a=ly=1
if =x,a=0,y=1
if x =23,a=1,y=-71
if =a,a=71,y=1
ifx=25,a=0,y=0

otherwise

Note that these are valid distributions, since 7 < 2Pio,7 < 2(1 — Pio — Pi1) by assumption,
and that P;, = P;(A = a,Y = 1) for both a € {0,1},i € {0,1}. Consider the hypothesis

space H = {ho, hi}, with

ho(ai1) =1  ho(x2) =1  ho(w3) =1  ho(ts) =O ho(a5) =0

and

hi(x1) =1 hy (a2) =1 hi (a3

)=0 ha(as)=1 hi(as) =0

Note that L(h;,P;) = 0 and [”°?(h,,P;) = 0 for both i = 0,1. Note also that L(hy,Po) =

L(ho, Pi) = 7. Moreover,

10P(hy, Po) = |Pcx.a,y)~po (a (X) = 114 = 0,¥ = 1) — Poxayyney(Mu(X) = 1A = 1, = 1)|

Pro — 7/2
Bo —a/2 + nfl

= 3p

and similarly [¥°?(ho, P,) = oA

Consider a (randomized) malicious adversary @; of power a, that given a clean distribu-
tion P;, changes every marked point to (x3,—i,7) with probability 0.5 and to (24,7, 77)
otherwise. Under a distribution P; and an adversary @;, the probability of seeing a point
(#3,4, 72) is 31 —a) = aor = a/2, which is equal to the probability of seeing a point
(x3, i, 7). Therefore, denoting the probability distribution of the corrupted dataset, under a
clean distribution P; and an adversary @;, by P’, we have

(l-a)Pu

(1— a)(Pio — 9/2)
a/2

a/2

a/2

a/2

P(x,a,y) =

ife=a,,a=l1y=1
ifx=%,a=0,y=1

ifx=23,a=1,y=-7%
if xv =2%3,a=-71,y =1
ife=x%,a=71,y=1

ife=x%,a=1,y=-7%

(l-—a)(1- Po —Pu-7/2) ife=25,a=0,y=0

0

In particular, P65
under the adversarial manipulation.

127

otherwise

= P%, so the two initial distributions Pp and P, become indistinguishable

APPENDIX

Proofs from Chapter 6

Here we present the complete proof of Theorem /17| To this end, we first show in Section |D.1)
how the technique of for studying the large deviations of sums of dependent random
variables can be used to derive large deviation bounds for the three fairness notions, given a
fixed classifier. The proof is similar to the corresponding i.i.d. result of [WGOS17], however
an application of the results from is needed because of the dependence between
the samples. Then in Section [D.2) we show how these bounds can be made uniform over
the hypothesis space by adapting the classic symmetrization argument (e.g. [Vap13}) toa
dependent data scenario.

D.1 Non-uniform bounds

First we use the tools from the previous section and a technique of ABD*18] to
show a non-uniform Hoeffding-type bound for equality of opportunity and equalized odds:

Lemma 8. Fix 5 € (0,1) and a binary predictor f : Q x I — {0,1}. Suppose that
N> Bie) where T = ming, P(A(d) = a,r(q,d) =r), then:

 

P (iu S) PP f)| > 2 eal ") oe (D.1)
and
P (im S)- Ped fy} >2 ea) <6. (D.2)

Proof. Denote by Ia, = {(#, 3) : A(di) = a,r(qi,d) = r} the set of indexes of the training
data for which the document belongs to the group a and the relevance of the query-document
pair is r. Notice that J,, is a random variable and that |Jo,| = |Sa,-|. We first bound the
probability of a large deviation of

 

rel = Ty land)


the empirical risk of any hypothesis h € H on the dataset S; and by:

dy(S;, 5;) = sup |Ri(h) — R;(h)| (A.4)
heH

the empirical discrepancy between the datasets 5; and Sj.

We show that a learner that first runs a certain filtering algorithm (Algorithm A.1) based on
the discrepancy metric and then performs empirical risk minimization on the remaining data
to compute a hypothesis satisfies the properties stated in the theorem. The full algorithm for
the learner is therefore given in Algorithm A.2|

(a) The key idea of the proof is that the clean sources are close to each other with high
probability, so they get selected when running Algorithm |A.1| On the other hand, if a bad
source has been selected, it must be close to at least one of the good sources, so it can not
have too bad an effect on the empirical risk.

For all i € G, let €; be the event that:
= 6
sup |R(h) — Ri (n)| <s («. —, s) : (A.5)
hen 2N

Further, let Eg be the event that:

 

= }
sup |R(h) — Ra(h)| < s (i =, Sc) , (A.6)

heH 2

where
Reh) = EY hla), Yyig)
m i€G j=1
Denote by i E£ and EG the complements of these events. Then we know that P(€%) < s and
P(E) < & for all i € G. Therefore, if € = Eg A (AiccE;i), we have:
}6

P(€°) = P(ESV (ViegEf)) < P(EG) + SO PES) < = St ks <6. (A.7)

ieG 2N
Hence, the probability of the event € that all of (A.5) and (A.6) 6) hold, is at least 1— 6. We now
show that under the event €, Algorithm [A.2) returns a hypothesis that satisfies the condition

in (a).

Algorithm A.1: Dataset filtering for robust multi-source learning

Inputs: Datasets S),..., Sy
Initialize T = {} // trusted sources
fori=1,...,N do

if dy (Si: S;) <s (m, ae Si) +s (m, ow S;) ;

for at least |£| values of j 4 i, then
T=Tuf{i}

end if
end for
Return: U,er 5; // indices of the trusted sources

 

 

108

Funding sources This project has received funding from the European Union's Horizon
2020 research and innovation programme under the Marie Sktodowska-Curie Grant Agreement
No. 665385.

The candidate has also received funding from the ELISE Mobility Program for PhD students
and postdocs funded by the project European Learning and Intelligent Systems Excellence
(ELISE, Grant Agreement No. 951847).

[AL89]

[ALM17]

[BBL03]

[BBL04]

[BCD+19]

[BCMC19]

[BDBC*+10]

[BDEK19]

[BDJ+19]

[BEK02]

[BEY05]

[BEYS21]

[BGS*17]

[BGW18]

Dana Angluin and Philip Laird. Learning from noisy examples. Machine
Learning, 2(4):343-370, 1988.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat
baseline for sentence embeddings. In International Conference on Learning
Representations (ICLR), 2017.

Olivier Bousquet, Stéphane Boucheron, and Gabor Lugosi. Introduction to
statistical learning theory. In Summer School on Machine Learning, 2003.

Olivier Bousquet, Stéphane Boucheron, and Gabor Lugosi. Introduction to
statistical learning theory. In Advanced lectures on machine learning, pages
169-207. Springer, 2004.

Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt,
Zhe Zhao, Lichan Hong, Ed H Chi, et al. Fairness in recommendation ranking
through pairwise comparisons. In Conference on Knowledge Discovery and
Data Mining (KDD), 2019.

Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
Analyzing federated learning through an adversarial lens. In /nternational
Conference on Machine Learing (ICML), 2019.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira,
and Jennifer Wortman Vaughan. A theory of learning from different domains.
Machine Learning, 79(1-2):151-175, 2010.

Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, and Sebastian Kohlmeier.
Overview of the TREC 2019 fair ranking track. In The Twenty-Eighth Text
REtrieval Conference (TREC 2019) Proceedings, 2019.

Samy Bengio, Krzysztof Dembczynski, Thorsten Joachims, Marius Kloft, and
Manik Varma. Extreme classification. In Dagstuh! Reports 18291. Schloss
Dagstuhl — Leibniz Center for Informatics, 2019.

Nader H Bshouty, Nadav Eiron, and Eyal Kushilevitz. Pac learning with nasty
noise. Theoretical Computer Science (TCC), 2002.

Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis.
Cambridge University Press, 2005.

Amanda Bower, Hamid Eftekhari, Mikhail Yurochkin, and Yuekai Sun. Individ-
ually fair rankings. In International Conference on Learning Representations
(ICLR), 2021.

Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning
with adversaries: Byzantine tolerant gradient descent. In Conference on Neural
Information Processing Systems (NIPS), 2017.

Asia J Biega, Krishna P Gummadi, and Gerhard Weikum. Equity of attention:
Amortizing individual fairness in rankings. In /nternational Conference on
Research and Development in Information Retrieval (SIGIR), 2018.

92

CHAPTER

Preliminaries

In this chapter we introduce several machine learning concepts and notions that will be central
to the work presented in this thesis. First, we give a short introduction to (supervised) machine
learning. We place particular emphasis on the classic PAC learning framework, which provides
a tool for studying the hardness of learning problems and analyzing the properties of learning
algorithms. We also discuss several concepts from trustworthy machine learning research, with
a focus on those notions from the robustness and fairness literature that will be central to our
analysis later on.

2.1 Supervised machine learning

2.1.1 Elements of the supervised learning problem

In a nutshell, the goal of supervised machine learning is to use a dataset of input-label pairs
to construct a general rule for assigning (predicting) labels to future inputs. The underlying
hypothesis is that if such a labeling rule is constructed in a way that explains the train data,
then this rule will also generalize, in the sense that it will be accurate at predicting the true
labels for the future inputs as well.

To formalize this, we adopt a classic statistical learning framework, following e.g. BD14
MRT18}. In this context, the supervised learning problem has the following ingredients.

An input space The input space is the set of instances that we will be aiming to label.
This can for example be the set of all possible emails or the set of all possible applicants for
a job position. In the case of emails, one may be interested in constructing a classifier that
decides if an email is spam or ham. In the case of job applicants, a useful classifier is one that
is able to predict if an applicant will do well at the job they are applying for or not.

Commonly, the inputs (i.e. the elements of 1) are represented via elements of some metric
space, for example R¢. That is, instead of working directly with the text of the emails, or the
CV of the applicants, feature representations for each input are given instead. There could
be handcrafted features based on domain-specific considerations (for example, bag-of-words
representations for the case of text), or could themselves be extracted via a machine
learning model pretrained on massive amounts of data for a similar machine learning tasks (for
example, using a state-of-the-art NLP model, such as BERT [DCLT19]). In such cases, we will

5

5.

FAIRNESS- AWARE PAC LEARNING FROM CORRUPTED DATA

 

= The corrupted dataset S” is then passed on to the learner, who computes £(S?).

For a fixed a € [0,0.5), we say that @ is a malicious adversary of power a. Note that the
number of marked points is |$8| ~ Bin(n, a).

Since no assumptions are made on the corrupted data points, they can, in particular, depend
on the learner £, the data distribution P, the clean data S° and all other parameters of the
learning problem. That is, the adversary acts with full knowledge of the learning setup and
without any computational constraints, which is in lines with our worst-case approach. Note
that this is in contrast to the learner £ that can only access the data points in S?. We refer
to Section [5.3.4] for a more formal treatment.

5.3.3. Multi-objective learning

Our goal is to study the performance of the classifier £(.S?) learned on the corrupted data,
both in terms of its expected loss R(L(S”), P) and its fairness deviation [(£(S”),P) on the
clean (test) distribution P. We will be interested in the probabilities of these quantities being
large or small, under the randomness of the sampling of S? - that is the randomness of the
clean data, the marked points and the adversary.

Note that it is not a priori clear how to trade-off the two metrics and that this is likely to be
application-dependent. Therefore it is also unclear how to evaluate the quality of a hypothesis.
Here we study two possible ways to do so.

Weighted objective One approach is to assume that a (application dependent) trade-off
parameter is predetermined, so that the learner has to approximately minimize

Ly(h) = R(h) + AP (h). (5.6)

In particular, the quality of the hypothesis £(h°) can be directly measured via Ly(L(h*)) —
minney L(h). We will use LP? and L¥°? to denote the weighted objectives with I?” and
[0 respectively.

Element-wise comparisons Alternatively, one may want to consider the two objectives
independently. Given a classifier h € H, denote by U(h) = (R(h), P(h)) the vector consisting
of the values of the two objectives. Note that UJ does not, in general, induce a total order on
H. Instead we can only compare two classifiers hi, ha € H if, say, hy dominates hz in the sense
that both R(h1) < R(h2) and ['(hy) < P'(h2). We denote this relation by W(h) X VW(ha).
As we will see, these component-wise comparisons are still useful for understanding the limits
of learning against an adversary.

To be able to measure how far a classifier is from optimal under the ~ relation, it is natural
to consider situations where there exists a classifier that is optimal both in terms of fairness
and accuracy. Then the quality of any other hypothesis can be measured with respect to
this optimal classifier. That is, one may assume that there exists a h* © H, such that
h* € argminyey R(h) and h* € argminye, P(h), so that W(h*) < Wh) for all h € H. Then
the quality of £(9”) can be measured as the R? vector

L(£(S”)) = B(L(S?)) — Bh’). (5.7)

As with the weighted objective, we use L??(£(S?)) and L”°?(L(S?)) to denote the loss
vector when demographic parity and equality of opportunity are used respectively.

56

[VBC20]

[WBU11]

[WGN*20]

[wG0S17]

[WLC+10]

[WLL20]

[WM19]

[WMP*03]

[WPT19}

[WZW18]

[Xie17]

[XLSA18]

[YCKB18]

Robin Vogel, Aurélien Bellet, and Stéphan Clémencon. Learning fair scoring
functions: Bipartite ranking under roc-based fairness constraints. Conference
on Uncertainty in Artificial Intelligence (AISTATS), 2020.

Jason Weston, Samy Bengio, and Nicolas Usunier. WSABIE: Scaling up to large
vocabulary image annotation. In International Joint Conference on Artificial
Intelligence (IJCAI), 2011.

Serena Wang, Wenshuo Guo, Harikrishna Narasimhan, Andrew Cotter, Maya
Gupta, and Michael | Jordan. Robust optimization for fairness with noisy
protected groups. In Conference on Neural Information Processing Systems
(NeurIPS), 2020.

Blake Woodworth, Suriya Gunasekar, Mesrob | Ohannessian, and Nathan
Srebro. Learning non-discriminatory predictors. In Conference on Computational
Learning Theory (COLT), 2017.

Paul Wais, Shivaram Lingamneni, Duncan Cook, Jason Fennell, Benjamin
Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. Towards build-
ing a high-quality workforce with mechanical turk. In N/IPS Workshop on
Computational Social Science and the Wisdom of Crowds, 2010.

Jialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent
label noise. arXiv preprint arXiv:2011.00379, 2020.

Robert Williamson and Aditya Menon. Fairness risk measures. In /nternational
Conference on Machine Learing (ICML), 2019.

Douglas Wahlsten, Pamela Metten, Tamara J Phillips, Stephen L Boehm,
Sue Burkhart-Kasch, Janet Dorow, Sharon Doerksen, Chris Downing, Jennifer
Fogarty, Kristina Rodd-Henricks, et al. Different data from different labs:
Lessons from studies of gene-environment interaction. Journal of neurobiology,
54(1):283-311, 2003.

Michael Wick, Swetasudha Panda, and Jean-Baptiste Tristan. Unlocking
fairness: a trade-off revisited. In Conference on Neural Information Processing
Systems (NeurIPS), 2019.

Yongkai Wu, Lu Zhang, and Xintao Wu. On discrimination discovery and
removal in ranked data using causal graph. In Conference on Knowledge
Discovery and Data Mining (KDD), 2018.

Tianpei Xie. Robust Learning from Multiple Information Sources. PhD thesis,
University of Michigan, 2017.

Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and Zeynep Akata. Zero-
shot learning - a comprehensive evaluation of the good, the bad and the ugly.
IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI),
2018.

Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-
robust distributed learning: Towards optimal statistical rates. In International
Conference on Machine Learing (ICML), 2018.

104

P((Iyor — Yor] > #) V (vir — Yael > €))
P(IYor — Yorl > #) + P(lvte — Yael > 4)
ze j

— + 4exp(—t?N7).

IA IA

iN

Setting t = ty) = \/ testi6/) gives:

; 5 log(16/6 oy 6 _9
P (Ini 981 Inw => 2 0)
:

Setting r = 1 gives the first result.

For the second result, note that taking the union bound over r € {0,1} shows that with
probability at least 1—6 both ||+¥99—Yi0|—|%00— iol] S 2to and || 91 — 771 |—|701 — N11] S 2to
hold.

Under this event we have:

(yoo — Yio] + Yor — mu)|

. g “ ; ; il
[PPO8(f, S) — PPO8(f 1=|5( yoo — Hol + lyon — val) 735

 

 

7 1 ; ;
5 (indo — 81 = Ineo — anol) + 5 (In8, = 781 = Iron — )|

 

‘ il 9 ‘
s 5 [rns Viol — boo — oll 4 5 [hin vil — lon vil
< 2to

 

 

 

and hence the result follows.

 

An identical argument, by conditioning on the values of the set J, = {(i,j) : A(d;) = a} gives
a similar result for demographic parity:

Lemma 9. Fix 6 € (0,1) and a binary predictor f : Q x I + {0,1}. Suppose that
N> Sloss) where v = min, P(A(d) = a), then:

P (iru S)—TPP(f)| > 2 ee) <6. (D.6)

D.2 Uniform bounds on proof of Theorem (17)

In this section we show how to formally extend the non-uniform bounds from the previous
section to hold uniformly over the hypothesis space H.

Let S’ = {(q),d?, r(q, d))} ie{xj.je{m] be a ghost sample independent of S and also sampled
via the same procedure as S, as described in the main body of the paper. In the proof of
Lemma 8|we showed that for any classifier f and any ¢ € (0, 1):

 

27 27.
P (IPP°?(f) — PPP(f,.8)| > 2t) < 2exp (-) + 4exp (-! a")

2-2
< 6exp ( 2) (D.7)

 

2

157

[MSRO8]

[MTSVDH15]

[MTV+20]

[MW18a]

[MW18b]

[NC19]

[INCGW20]

[NRS*16]

[PK19]

[PL17]

[Pre82]

[PSBR18]

[PSM14]

[Qia18]

Christopher D Manning, Hinrich Schiitze, and Prabhakar Raghavan. /Introduc-
tion to Information Retrieval. Cambridge University Press, 2008.

Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.
Image-based recommendations on styles and substitutes. In ACM SIGIR
Conference on Research and Development in Information Retrieval, 2015.

Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma,
Abhishek Singh, Ramesh Raskar, and Hadi Esmaeilzadeh. Privacy in deep
learning: A survey. arXiv preprint arXiv:2004.12254, 2020.

Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary
classification. In Conference on Fairness, Accountability and Transparency
(FAccT), 2018.

Aditya Krishna Menon and Robert C Williamson. The cost of fairness in
binary classification. In Fairness, Accountability, and Transparency in Machine
Learning (FAT/ML), 2018.

Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv
preprint arXiv:1901.04085, 2019.

Harikrishna Narasimhan, Andrew Cotter, Maya R Gupta, and Serena Wang.
Pairwise fairness for ranking and regression. In AAA/ Conference on Artificial
Intelligence, 2020.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, and Li Deng. MS MARCO: A human-generated machine read-
ing comprehension dataset. In Conference on Neural Information Processing
Systems (NeurIPS), 2016.

Alexander Peysakhovich and Christian Kroer. Fair division without disparate
impact. arXiv preprint arXiv:1906.02775, 2019.

Anastasia Pentina and Christoph H. Lampert. Multi-task learning with labeled
and unlabeled tasks. In International Conference on Machine Learing (ICML),
2017.

Daryl Pregibon. Resistant fits for some commonly used logistic models with
medical applications. Biometrics, pages 485-498, 1982.

Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep
Ravikumar. Robust estimation via robust gradient estimation. arXiv preprint
arXiv:1802.06485, 2018.

Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global
vectors for word representation. In Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2014.

Mingda Qiao. Do outliers ruin collaboration? In International Conference on
Machine Learing (ICML), 2018.

101

Finally, let h* = argminye,, R(h) and nd = £(A(S’)) = argmin, cy Rr(h). Then:

R(nA) — R(h*) = (RHF) — Rex(h¥)) + (Rex(hn¥) — R(h*))
(A9) — Rx(h9)) +

 

< (R(h WA) + (Re(h*) — R(h*))
< 2sup [Rr(h) — R(h)|. (A.11)

Since we showed this result for an arbitrary fixed-set adversary with preserved set G, the result
follows.

(b) The crucial difference in the case of the flexible-set adversary is that the set G is chosen
after the clean data is observed. We thus need concentration results for a/l of the subsets of
[N] of size k, as well as all individual sources.

For all i € [.N], let €; be the event that:

sup |R(h) - Ri(h)| <s («. 2 s' j (A.12)
heH 2N
where im
R= +S e(nla,).uy) (A.13)
L

Further, for any A C [N] of size |A| = k, let €4 be the event that:

sup |R(H) ~ R(h)| <s (in xy si (A.14)

where 5’, = UjeaS! and
=> 1
Riy(h) = -— TD eHe4,), 4) (A.15)
TM ie Al=1

for all i € [N] and P(E%) < aC) for all A C [N] with
k

 

Then we know that P(E?) < sf

|A| =k. Therefore, if € = (AnEa) A (Aietmé) we have:

P(E°) =P ((VaE4) V (Viemé)) < Le (€4) + x P(E) < (7) x) + Now =6.
(A.16)

Hence, the probability of the event € that all of (A.12) and (A.14) hold, is at least 1 — 6. In
particular, under €:

R, R! om 2. '|=stkm Ts
sup [R(h) — Re(h)| = sup|R(h) — RG(h)| < s (: ay) (i ay)

heH heH k k

and

Dp 7D! 6 1 _{, 6
a |R(h) 7 Ri(h)| = sup [R(h) - Ri(h)| <s (». on? s) =5s (». an s) , (A.18)

110

Case 2.2 In the case when 1 — Pip — Py < Pio we have that ;*~ > 2(1 — Pio — Pi).
Then, denote by a the unique number between (0, 0.5), such that °2- = 2(1— Pig — Py) =

l-a2

 

 

2min { Pio, 1 — Pio — Pir}, and note that a2 < a. Then repeat the same construction as in
Case 1, but with m2 = io and an adversary that with probability a2/a does the same as in

Case 1 and leaves a marked point untouched otherwise.

Then the same argument as in Case 1 gives that for some i € {0,1}, with probability at least
0.5, both of the following hold

 

L(L(S?), P;) — L(hi, Pi) > = 2(1— Pio — Pi)

l-ag

and
1—Po—P
P¥Or(c(S), P;) — P£0P(h;, P;)) > WB = Ae
(£(S"),By) -P°°(hi,Py) > ge =

 

This concludes the proof of Theorem 9

 

 

 

C.1.2 Hurting fairness without affecting accuracy - proofs

Theorem 10. Let 0 < a < 0.5,0 < Py < 0.5. For any input set X with at least four
distinct points, there exists a finite hypothesis space H, such that for any learning algorithm
Lo: Unen(¥® x Ax Y)" > H, there exists a distribution P for which P(A = 0) = Py, a
malicious adversary A of power a and a hypothesis h* € H, such that with probability at least
0.5

L(L(S?), P) = L(h*,P) = min L(h,P)

and

DP p _ pPP(p* P) > mi a ; > min {1h
Tr” (£(8?),P) —T GP) > min | Se — Bt > mt oP’!

Proof. Let 7 = 7“, so that a = a

Case 1 First assume that 7 = ToS 2Po(1—Pp). Take four distinct points {21, 22,73, v4} €
X. We consider two distributions Py and P,, where each P; is defined as

1-P-n/2 ifa=a,a=1y=1

Py — 7/2 if x =%,a=0,y=0

Pi(x,a,y) = 47/2 ifa=a3,a=i1,y=1
n/2 ifa=a,a=7i,y=1
0 otherwise

Note that these are valid distributions, since 7 < 2P9(1— Py) < 2P) < 2(1 — Py) by
assumption and also that Pp = P;(A = 0) for both i € {0,1}. Consider the hypothesis space
H = {ho, hi}, with

ho(#1) =] ho(x2) = ho(x3) = | ho(aa) =

and
hi(21) = 1 hi (a2) = hi (a3) =0 hi(x4) = 1.

129

 

 

    

2H (a)N log(2) 2log(#)
~ km (l-—a)Nm (l-—a)Nm

21o (4x)
+a soy“ 6\ 2lo8 (5)
| d Va | d /log(.N)
< 7 a SS 5
<0 (z km > Vm ee m +e m ,

where for the last inequality we used H(a) < 2\/a(1— a) and 1— a € (5, 1].

 

 

 

 

 

 

A.2 Proof of Theorem |5|

Theorem 5. Let H be a non-trivial hypothesis space. Let m and N be any positive integers
and let G be a fixed subset of [N] of sizek € {1,...,.N—1}. Let£L:(¥ x V)N*™ SH
be a multi-source learner that acts by merging the data from all sources and then calling a
single-source learner. Let S’ € (¥ x pyran be drawn i.i.d. from D. Then there exists a
distribution D with minjey R(h) = 0 and a fixed-set adversary A with index set G', such
that:

Ponn(R(CA(S')) > ina) > 5 (A.27)

N—k ;,

where a = “>

 

                        

We use a similar proof technique as in the lower bound results in [BEKO2] and in the classic

sample complexity lower bound for binary classification, e.g. Theorem 3.20 in |MRT18]. An

overview is as follows. Consider a distribution on VY that has support only at two points -
a

the common point x; and the rare point x. Take P(a2) = O(;%_). Then the expected

l-a
number of occurrences of the point x2 in G is O (3 5(1-a)N m) = O(aNm). Thus, one
can show that with constant probability the number of x's in G is at most aNm and hence
the adversary (that has access to exactly aNm points in total) can insert the same number
of 9's, but wrongly labelled, into the final dataset. Therefore, based on the union of the
corrupted datasets, no algorithm can guess with probability greater than 1/2 what the true
label of x» was.

Proof. We prove that there exists a distribution D on ¥ and a labelling function f € H, such
that the resulting joint distribution on ¥ x Y, defined by x ~ D and y = f(x), satisfies the
desired property.

Without loss of generality, let G = [1,2,...,k]. Since 1 is non-trivial, there exist hy, ho © H
and 21,2 € XY, such that hy(x1) = ho(x1), while hy (v2) = 1, but ho(x2) = —1. Consider

113

[QV18]

[RBCJO9]

[RDS*15]

[RFMZ20]

[RLWS20]

[Rob77]

[SBC20]

[SCST17]

[SHWH19]

(SJ17]

[SJ18]

[SJ19]

[SKL17]

[SL18]

Mingda Qiao and Gregory Valiant. Learning discrete distributions from untrusted
batches. In L/Plcs-Leibniz International Proceedings in Informatics, volume 94.
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.

Filip Radlinski, Paul N Bennett, Ben Carterette, and Thorsten Joachims.
Redundancy, diversity and interdependent document relevance. In /nternational
Conference on Research and Development in Information Retrieval (SIGIR),
2009.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
et al. Imagenet large scale visual recognition challenge. International Journal
of Computer Vision (IJCV), 115(3):211-252, 2015.

Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian Ziebart. Fairness
for robust log loss classification. In AAA/ Conference on Artificial Intelligence,
2020.

Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. FR-
Train: A mutual information-based approach to fair and robust training. In
International Conference on Machine Learing (ICML), 2020.

Stephen E Robertson. The probability ranking principle in ir. Journal of
Documentation, 33(4):294-304, 1977.

David Solans, Battista Biggio, and Carlos Castillo. Poisoning attacks on
algorithmic fairness. In European Conference on Machine Learning and Data
Mining (ECML PKDD), 2020.

Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar.
Federated multi-task learning. In Conference on Neural Information Processing
Systems (NIPS), 2017.

Chuanbiao Song, Kun He, Liwei Wang, and John E. Hopcroft. Improving the
generalization of adversarial training with domain adaptation. In International
Conference on Learning Representations (ICLR), 2019.

Ashudeep Singh and Thorsten Joachims. Equality of opportunity in rankings.
In Workshop on Prioritizing Online Content (WPOC) at NeurIPS, 2017.

Ashudeep Singh and Thorsten Joachims. Fairness of exposure in rankings. In
Conference on Knowledge Discovery and Data Mining (KDD), 2018.

Ashudeep Singh and Thorsten Joachims. Policy learning for fairness in ranking.
In Conference on Neural Information Processing Systems (NeurlPS), 2019.

Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses
for data poisoning attacks. In Conference on Neural Information Processing
Systems (NIPS), pages 3517-3529, 2017.

Remy Sun and Christoph H. Lampert. KS(conf): A light-weight test if a
convnet operates outside of its specifications. In German Conference on
Pattern Recognition (GCPR), 2018.

102

for all i € G.

Now, for any flexible-set adversary with preserved size k, the same argument as in (a) shows
that:

i€[N]

6 N-k 6
Qa) *)<26(k i
R(h*) — R(h*) < 2s (i 20) ’ ss + 6—— Ny Max s (». 3N? s) (A.19)

 

 

 

holds under the event €.

 

We now show how to obtain data-dependent guarantees, via the notion of Rademacher
complexity. Let

Ms (CoH) = B, (sup) = > olla), Yi =u) (A.20)

heH

be the (empirical) Rademacher complexity of #1 with respect to the loss function @ on a
sample S = {(21,41),---;(®n.Yn)}. Here {o;}"_, are iid. Rademacher random variables.
Let Sg = Uieg Si, Ri = Rg, (C0 H) and Re = Rs,(C0H). Assume also that the loss
function ¢ is bounded, so that for some constant M > 0, (y1, y2) < M for all yi, yo € Y.
Then we have:

Corollary 1. In the setup of Theorem |4, against a fixed-set adversary, it holds that

 

 

R(L(A(S’))) — min R(h) < 4R%#e + 6M log(3) + a(18 tos (‘5) +12 max)
heH yee Qkm Im ie(N] 7°)”
(A.21)
Proof. We use the standard generalization bound based on Rademacher complexity. Assume
that S = {(21,y1),---,(n,Yn)} ~ D, then with probability at least 1 — 6 over the data
MRT18]:
sup |E (€(h(2), y)) ~~ 32 e(h(w:),4:)| < 2s (CoH) +3M oe (3) anny

 

 

 

Substituting into the result of Theorem 4| gives the result.

 

Corollary 2. In the setup of Theorem |4, against a flexible-set adversary, it holds that

¥ . ~ [va
_ < + eae i
R(L(A(S ))) min R(h) < 4Rg + 12a mae, +0 (“4 . (A.23)

Proof. Using the concentration result from Corollaryiland (2 G ) = (ayy) = (\) < gHe@)N
where H(p) = —plog,(p) — (1 — p) log,(1 — p) is the binary entropy function, we obtain:
as) pr site +0n] 2) a (ssn
a Wat z ye a{_—__\ 7 Ma|—"4
R(L(A(S’))) min R( 1) <4Re + 6 dkm + 18 amt ca

 

 

log((%)) 4 log(#)

= Big + BAL Qkm 2km

 

111

so:

SUEPP(s") — FPPC", 8) Bt) < Por TPP(s", $1) — F*P(F*, 8) > 1/2).

Taking expectation with respect to S:

Pg (LP0P(f*) — PPOP(f*, 5) > t) < Wes.o(TPP(f*, S’) — TPOP(f*, S) > t/2)
< 2Ps 9/(sup(l™°P(f, 9’) — PE°P(f, S)) > t/2).
foF

 

 

 

 

Given a set of n input datapoints z1,..., 2, with z; = (qi, di, (qi, di)), consider:
Frywnin ={(F(Gh), +--+; f (dni dn)) 2 f © FS (D.13)
Then the growth function of F is defined as:

Se(n) = sup Farce

(215.-52n)

 

(D.14)

We can now present a proof of Theorem [I7;
Theorem 17. Suppose that v = VC(F) > 1 and that 2Nm > v. Then for any 6 € (0,1):

2eNm) 1 5 ( 24
Pg | sup("?( f) —T2"(f, S)) > 8 ptlos(Tr™) + los(s) <6 (D.15)
ter \ N72

| 2eNm 1] »( 24
dlog( 7") 4 aD) os (D.16)

DP; ¢ DP; ¢
Ps (sc (f) - T°" (Ff, S)) 2 82 “a

 

Ps (spoon — TPF, S)) >8 tema) <6 (D.17)

fF Nr2
Proof. Again we present the proof for equality of opportunity, with the other inequalities
following in an identical manner.

Note that given sets S and S’, the values of [®°P(f,S) and TP°P(f, S’) are completely
determined by the values of f on S' and S’ respectively. Therefore, for any t € (4 ae 1)
using Lemma 10) and the union bound:

Ps (surrey — PPOr(y $)) > ‘ < Pas (suirru. s") — P®P(f, 8) > i”)

SEF
< 25¢(2Nm)Ps,s (FPOP(f,$’) — PPP(f, S) > t/2)
< 2Sz(2Nm)Ps,sy ((\PPOP(F,S") — PPP(f)| > t/4)
V (IDPOP(f) — TOP(f,S)| > t/4))
< 45¢(2Nm)Ps (|FPOP(f) — PPOP(f, S)| > t/4)

PN7?
< 245'-(2Nm) exp (- ae )

159

consider two distributions Po and P,, where each P; is defined as

Pu — 7/2 ife=%,a=1y=1
Py — 1/2 ife=2%,a=0,y=1

P,(a,a,y) = n/2 if tga = HI
n/2 ifr=x%,a=i,y=1
1-—Po—-Pui ife=25,a=0,y=0
0 otherwise

Note that these are valid distributions, since 7 < 2Pig < 2Pi, by assumption, and that
Pig = P(A = a,Y = 1) for both a € {0,1},i € {0,1}. Consider the hypothesis space
H = {ho, hi}, with
ho(t1)=1  ho(w2) = 1 ho(ws) = 1 ho(t4) = 9 ho(xs) = 0

and

hi(x i= 1 hi(a2) = 1 hi(a3) = 0 hi(a4) = 1 hi(as) =0
Note that L(h;, P;) = L(h-i, Pi) = 7/2. Moreover,
peo (ho, Po) = - (X,A,Y)~Po (Ai (X )=1/A=0, Y=1)- P(x,ayy~ro(ha(X) = 114 = 1,Y = 1)|

7 Py _— n/2

Pu —n/2+n/2

        

and similarly [¥?(h,, Po) = . Since Pig < Py, P??(ho, Po) < F?°?(hy, Po) and
EOp __ PEOp a ( ).
T (hi, Po) Tr (ho, Po) = IP 1- Pr
Similarly 1”? (ho, P,) = (hi, P1) = spy, So that T#OP(h,, Pi) < P¥°?(ho, Pi)

and

P#0P(hg, Ps) —T#0"(Iu, Pa) = = (1 - 7)
10 11

Consider a (randomized) malicious adversary 4; of power a, that given a clean distribution
P;, changes every marked point to (23, 77, 1) with probability 0.5 and to (4,7, 1) otherwise.
Under a distribution P; and an adversary @;, the probability of seeing a point (23,7, 1) is
a(1—a) = fc = = a/2, which is equal to the probability of seeing a point (x3, 77, 1).
Therefore, denoting the probability distribution of the corrupted dataset, under a clean

distribution P; and an adversary @;, by P’, we have

(1 —a)(Pu — 7/2) ife=a1,a=ly=1
(1 — a) (Pio — 7/2) ifx=a%,a=0,y=1
a/2 ife=23,a=i,y=1

Pi(x,a,y) = a/2 frog = her 1
a/2 ife=aya=-i,y=1
a/2 ife=aa=i,y=1
(l-a)(L—-Po-Pu) ife=a5,a=0,y=0
0 otherwise

132

Lemma 4. Under the setup of Lemma 3, assume additionally that H. has a finite VC-dimension

d. Then for any n > max { P20), Z ute/8) $} and 6 € (0,1)

 

 

A 2d log(#2") + 2 log(48/65)
P4, | sup |P??(h) — P??(h)| < APP 41 d
8 sup| (h) (A)| < + 16 (1—a)Pon

 

>1-6. (C6)

Proof. From Proposition [2 we have that whenever n > max { 20e)., Bres(6/0) } and é€

p4 (sup (Ao(h) + Ai(h)) > arr) acy (C.7)
heH 2
Additionally, in the proof of Lemma 3] we showed that for a fixed classifier h € H for any
6 € (0,1),t € (0,1) and both a € {0,1}, we have

Cota) + 2exp (-?( - o) Pan)

P(1—a)Pan
5 .

P4 ({4¢(h) — ra(h)| > #) < exp (-

< 3exp (- (C.8)

The proof consists of two steps. In Steps 1 and 2 we show how to extend inequality (C.8) to
hold uniformly over 1. Then, we combine the two uniform bounds with a similar argument as
in the proof of Lemma 3)

The first step uses the classic symmetrization technique |Vap13] for proving bounds uniformly
over hypothesis spaces of finite VC dimension. However, since the objective is different from
the 0-1 loss, care is needed to ensure that the proof goes through, so we present it here in full
detail.

Step 1 To make the dependence of the left-hand side of ((C.8) on both h and the data S?
_ Ohh)

explicit, we set y¢(h, S?) = “3.

Introduce a ghost sample S' = {(x!,a!,y!)}”_, also sampled in an iid. manner from P#,
that is, S1 is another, independent poisoned dataset ?| Let °(h, S') be the empirical estimate
of y4(h) based on St.

First we show a symmetrization inequality for the y, measures

 

PS, (sup lya(h) — 76(h, 5”)| > ) < 2PS,,51 (sup 7e(h, $1) — ¥6(h, 8”) > 7 . (C9)
heH heH

 

for any constant 1 >t > 2,/ eee.

Indeed, let h* be the hypothesis achieving the supremum on the left-hand side Rl Note that
1 (lya(h*) — 96(h*, S?)] > #) (|ya(h*) — 76 (h*, S?)] < t/2)
Formally, we associate S! also with a set 8, of marked indexes.

3If the supremum is not attained, this argument can be repeated for each element of a sequence of
classifiers approaching the supremum

 

139

6.

FAIRNESS THROUGH REGULARIZATION FOR LEARNING TO RANK

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

   
   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

+ Quality [0.45 sue +E auality |o 450 m.050) Ab Quality |o.450
8 0.40 Boos 0.425 on gp 9.025 0.425 ony
= = 0.375 = 0.015 ;
z oa0Q — B0.0075 2 8 23508
© 3 0.0050 0.350 £ 0.010 0.3252
2 0.25 0.0025 0.325 0.005, 0.300
—E Unf 1 Unfairness —E Unfairness 3
nfairness 0.20 0.0000 1 0.000 eee
0.0 0.060.120.2505 10 20 4.0 0.0 0.06 0.12 0.25 05 1.0 2.0 4.0 0.0 0.06 0.12025 05 10 2.0 40
a a a
(a) Ours, equal opportunity (b) Ours, demographic parity c) Ours, equal odds
0.175 o14 06
ty 0-150 0.46 aio aa’ wl?
8 oaas 045% Goao gee
E o.100 au® £00 £ 0.10
0. Mag we 0.08
ee QD Eos E 006
0.050 ose
= S 0.04 > 0.04
a — Unfairess 0.42 0.02} ++ Unfairness , 0.02 | + Unfaimess
0.02 0.21 0.4 0.6 0.79 0.98 0.02 0.21 0.4 0.6 0.79 0.98 0.02 0.21 0.4 0.6 0.79 0.98
Pp Pp Pp
(d) FA*IR, equal opportunity — (e) FA*IR, demographic parity (f) FA*IR, equal odds
a 0.4650 0.0175 —E Quality }o.45 0.030 Quality |o.46eq
0.4625 0.0150
G 040 0.460% — Bo.or2s oon 0 eae
@ wv ® we ©
g g 8 o.026 0.460
€ 0.4575 E 0.0100 oO E oO
-& 0.035 = os. = 0.024 0.458
2 0.45505 (0.0075 2 & ee
E 0.030 04525 © 0.0050 0308 ooze oaseD
0.4500 0.0025 aa 0.020 0.454
1025 .
—t Unfairness: ears aooee —t Unfairness: 0.018 —t Unfairness 0.452
0.0 0.06 0.12 0.25 05 10 20 4.0 0.0 0.06 0.12 0.25 0.5 1.0 2.0 4.0 0.0 0.06 0.12 0.25 0.5 1.0 2.0 4.0
a a a
(g) Per-query, equal opportunity (h) Per-query, demographic parity (i) Per-query, equal odds

 

 

Figure 6.2: MSMARCO: Test-time performance of fair rankers with equal opportunity, de-
mographic parity and equalized odds fairness, achieved by our algorithm and the baselines:
unfairness (left y-axes) and NDCG@3 ranking quality (right y-axes); after training with different
regularization strengths (x-axis).

of regularization and across the studied fairness measures. In contrast, the fairness curves of
the baselines behave erratically with respect to the trade-off parameters.

The possibility of increasing the fairness of learning models without damaging their accuracy
has been previously observed in the context of supervised learning [WPT19], but not, to our
knowledge, in a ranking context. This effect is more expressed in the experiment on the TREC
data than for MSMARCO, possibly due to the higher number of relevant items per query
in TREC, which results in more flexibility to rearrange items without decreasing the ranking
quality.

We obtained similar results for the other setups, e.g. different values of k, fairness measures
and protected attributes and for PQk. Table|6. 1| summarizes some of the results in a compact
form. For different fairness notions and splits into protected groups (rows), it reports the
maximal and mean reduction of the fairness violation measure over the range of values of the
trade-off parameter for which the corresponding model’s prediction quality is not significantly
worse than for a model trained without a fairness regularizer (i.e. « = 0). Here we call a model
significantly worse than another if the difference of the mean quality values of the two models
is larger than the sum of the standard errors/deviations, for TREC/MSMARCO respectively,
around those averages (i.e. if the error bars, as in Figures |6.1| and 6.2) would not intersect).
The results are averaged over k € {1, 2,3, 4, 5}.

84

[HMZ18] Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory
for multiple-source adaptation. In Conference on Neural Information Processing
Systems (NeurIPS), 2018.

[Hoe63] Wassily Hoeffding. Probability inequalities for sums of bounded random variables.
Journal of the American Statistical Association, 58, 1963.

[HPS16] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised
learning. In Conference on Neural Information Processing Systems (NeurlPS),
2016.

[HSNL18] Tatsunori B Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy
Liang. Fairness without demographics in repeated loss minimization. In
International Conference on Machine Learing (ICML), 2018.

[Hub64] Peter J Huber. Robust estimation of a location parameter. The Annals of
Mathematical Statistics, 1964.

[Hub11] Peter J Huber. Robust statistics. Springer, 2011.

[Hun16] Elle Hunt. Tay, microsoft’s ai chatbot, gets a crash course in racism from twitter.
https://www.theguardian.com/technology/2016/mar/24
tay-microsofts-—ai-chatbot-—gets-—a-crash-course-in-racism-from-t

 

2016. Accessed: 2021-11-06.

[HWBN20] = Shuguang Han, Xuanhui Wang, Mike Bendersky, and Marc Najork. Learning-
to-rank with BERT in TF-ranking. arXiv preprint arXiv:2004.08476, 2020.

[HZRS16] Kaiming He, Xiangyu Zhang, Shaoging Ren, and Jian Sun. Deep residual
learning for image recognition. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.

[ICS*20] Alexey Ignatiev, Martin C Cooper, Mohamed Siala, Emmanuel Hebrard, and
Joao Marques-Silva. Towards formal fairness in machine learning. In /nter-
national Conference on Principles and Practice of Constraint Programming,
2020.

[IKL21] Eugenia lofinova, Nikola Konstantinov, and Christoph H. Lampert. Flea:
Provably fair multisource learning from unreliable training data. arXiv preprint
arXiv:2106.11732, 2021.

[IS15] Sergey loffe and Christian Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In /nternational Conference
on Machine Learing (ICML), 2015.

[Jan04] Svante Janson. Large deviations for sums of partly dependent random variables.
Random Structures & Algorithms, 24(3):234-248, 2004.

[JEP* 21] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek,
Anna Potapenko, et al. Highly accurate protein structure prediction with
alphafold. Nature, 2021.

97

Algorithm A.2: ERM on the trusted sources
Inputs: Datasets S),..., Sy
Run Algentim A.1| to compas Sr = Vier Si

Compute h® = = argminney El Vieyesr (A(x), y)
Return: 1

 

Indeed, fix an arbitrary fixed-set adversary @ with preserved set G. Whenever € holds, for all
i,7 € G we have:

dy (Si, 53) = sup(|Ri(h) — Rj (h)|) < sup (|Ri(h) — R(h)|) + sup ([R(h) — Rj (h)])

hen heH heH
6 6
<s G aN’ 5) +8 («. ON’ 5) .
(A.8)

Now since k > [4 | +1, we get that G C T. Moreover, for any i € T \ G, there exists at
least one j € G, such that dy(5;,S;) < s (m, Si) +s (m, 7, 5})- For anyi€ T\G,
denote by f(i) the smallest such j. Therefore, for any i € (T \ G):

|Ri(h) — R(h)| < [Ri — Rycy(h)| + [Ry~(h) — R(A)|

}
< dy (Si, Spi) +8 (». oN’ S,)

6 )
<s («. aN’ 5) + 2s (m, BN’ S,0) (A.9)
Denote by
p(h) 1 Ril (h) SS &A(z),y) (A.10)
~ {T] i€T = Fa, (a,yJESp

the loss over all the trusted data. Then for any h € H we have:
|Rv(h) — R(h)| < arm (5 en U(2i.1), Yaa) ) — Rim)

SS (C(A(@in), Yin) — 0)

i€(T\G) Il

 

+

 

 

 

 

 

 

koe a
= Rel) — RO) + oy De (Ril) — RH)
i€(T\G)
k 6 i =
< pape (dam 5S) + pay So [Rt — RCH)
|| ITI scfKey
k 6 1 6 6
< ins s(t, 3? Sc) + it] ox (: («. BN’ s) + 2s («. aN’ S10)
k 6 T|-k 6
< = + ax ¢ Dap hi
< IT) s(t, 9? Se) 2 IT] ne S («. ON’ s)
<:

km, ° Sc) + 3 B ai 8 (». a s)

109

[BHN19]

[BHPQ17]

[BIK*+17]

[BMo2|

[BNBR19]

[BNL12a]

[BNL12b]

[Bon19]
[BR18]

[BS20]

[BSOG18]

[Bur17]

[BWKT14]

[Cas19]

[CBDF*99]

Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine

Learning. fairmlbook.org, 2019.

Avrim Blum, Nika Haghtalab, Ariel D Procaccia, and Mingda Qiao. Collabo-
rative pac learning. In Conference on Neural Information Processing Systems
(NeurIPS), 2017.

Keith Bonawitz, Vladimir lvanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical
secure aggregation for privacy-preserving machine learning. In Proceedings of
the 2017 ACM SIGSAC Conference on Computer and Communications Security,
2017.

Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities:
Risk bounds and structural results. Journal of Machine Learning Research
(JMLR), 2002.

Sina Baharlouei, Maher Nouiehed, Ahmad Beirami, and Meisam Razaviyayn.
Rényi fair inference. In International Conference on Learning Representations
(ICLR), 2019.

Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against
support vector machines. In /nternational Conference on Machine Learing
(ICML), 2012.

Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against
support vector machines. In /nternational Conference on Machine Learing
(ICML), 2012.

Malte Bonart. Fair ranking in academic search, 2019.

Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of
adversarial machine learning. Pattern Recognition, 2018.

Avrim Blum and Kevin Stangl. Recovering from biased data: Can fairness
constraints improve accuracy? In Foundations of Responsible Computing,
volume 156. Schloss Dagstuhl — Leibniz Center for Informatics, 2020.

Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. Balanced neighbor-
hoods for multi-sided fairness in recommendation. In Conference on Fairness,
Accountability and Transparency (FAccT), 2018.

Robin Burke. Multisided fairness for recommendation. arXiv preprint
arXiv:1707.00093, 2017.

Wei Bi, Liwei Wang, James T Kwok, and Zhuowen Tu. Learning to predict
from crowdsourced data. In UAI, pages 82-91, 2014.

Carlos Castillo. Fairness and transparency in ranking. In International Conference
on Research and Development in Information Retrieval (SIGIR), 2019.

Nicolo Cesa-Bianchi, Eli Dichterman, Paul Fischer, Eli Shamir, and Hans Ulrich
Simon. Sample-efficient strategies for learning in the presence of noise. Journal
of the ACM (JACM), 1999.

93

C8(h) = TR UAC?) = 1a? = 0,4 ¢ P} and BUA) = TL, Lh(a”) = 1a? = ai EB}.
Denote
D U{A(a?) =1a =a
qa (h) = eee
imi 11a; =
and
na(h) = P(A(X) = 1]A =a),

so that P??(h) = |42(h) — 7?(h)| and P??(h) = |yo(h) — 71(h)|. Note that y?(h) is an

estimate of a conditional probability based on the corrupted data. We now introduce the
corresponding estimate that only uses the clean (but unknown) subset of the training set S?

 

wip) — Cult) _ Chea Mila!) = La? = 0,5 ¢ B}
ral) = Gh) SE Mae oaigpy

First we bound how far the corrupted estimates 7?(h) of y,(h) are from the clean estimates
7(h), uniformly over the hypothesis space H:

 

 

Proposition 2. /f n > max {520 ee) we have
P ( sup (/98(h) — 26(h)| + [aP(s) — 8(0))) = <2 —) <6. (c.1)
hen ~ Po/3+a

Proof. First we show that certain bounds on the random variables B, and C, hold with high
probability. Then we show that the supremum in equation (C.1) is bounded when these bounds
hold.

Step 1 Specifically, since By + By ~ Bin(n, a), by the Chernoff bounds and the assumption
onn

pa (2 +B, > en) < gers '

Similarly, Co ~ Bin(n, (1 — a)P 9) and C) ~ Bin(n, (1 — @)P;) and since Py < P, we get

 

 

p24 (co < 1 Pon) < en (l-a)Pon/8 < 46
2 4
and
pA (c eis Pn) < en(i-arin/s < 9
2 4
Therefore, by a union bound
1 sy

 

 

q 3a a l-a )) 6.6 6
> < < n <s=4+-4+-
P ((Bo+ Bi = =n) v (Cos 3 Pon) v (Crs 3 Pun Sgtqtqz<é

135

2.1. Supervised machine learning

 

Theorem 1 (The Fundamental Theorem of Statistical Learning, [SSBD14]). Let Hc {0, 1}
and let the loss under consideration be the 0 — 1 loss. Then the following statements are
equivalent:

« His agnostic PAC learnable.

= His PAC learnable.

« H is uniformly convergent.

« H has a finite VC dimension.

# ERM is a successful (agnostic) PAC learner for H.

Furthermore, one can quantitatively describe the speed at which the uniform convergence
property holds for H, via the following result:

Theorem 2 (p. 342 in [SSBD14]). Let D € P(X x J) be a fixed distribution and suppose
that H is of finite VC dimension d = VC(H). Then for any 6 € (0,1)

<6. (2.8)

a 8dlog(@) + 2log(4/é5
P (wp |R(h) — R(h)] > 2 sieve hese)
heH

This result, together with the link between uniform convergence and agnostic PAC learning
described in Section |2.1.2| can be used to show that a hypothesis space with VC dimension d
is agnostic PAC learnable with sample complexity:

tfiesle,8) =O (eee) (2.9)

 

€

As an alternative formulation, Theorem [2|implies that given a training set of size n, one can

guarantee that the distance between the empirical and the true risk of a hypothesis is at most
A ( [a

of O(,/2).

In the realizable PAC learning case one can show that the sample complexity of a hypothesis

space with a finite VC dimension is actually smaller that in the agnostic case. Specifically, one

can show that BD14

mile, 8) <0 (Ctoxdo x teat/9)) , (2.10)

In particular, only O (4) samples are needed to find an ¢-optimal solution, as compared to

oO (3 in the general case. It is therefore standard to refer to the O () rates achievable in
the realizable PAC learning scenario as “fast statistical rates”.

Rademacher complexity In the case of a non-binary label space and a general loss function
a more sophisticated complexity measure, namely the Rademacher complexity, can be used to
describe the sample complexity and uniform convergence rates of a hypothesis space.

11

C.2 Upper bounds proofs

We now present the complete proofs of our upper bounds. The main challenge lies in
understanding the concentration properties of the empirical estimates of the fairness measures,
as introduced in the main body of the paper. To this end, we first bound the effect that
the data corruption may have on these estimates. We then leverage classic concentration
techniques to relate the “ideal” clean data estimates to the corresponding population fairness
measures.

C.2.1 Concentration tools and notation

We will use the following versions of the classic Chernoff bounds for large deviations of Binomial
random variables, as they can be found, for example, in [KL93]. Let X ~ Bin(n,p). Then

P(X < (l—a)pn) < ee np/2
and
P(X > (1+a)pn) < eer,
for any a € (0,1). We will also use the Hoeffding's inequality [Hoe63]. Let X, ,Xo,.-.;Xn be

independent random variables, such that each X; is bounded in [a;, b;] and let X = 2 Py Xi:
Then

 

 

 

 

Throughout the section we denote the clean data distribution by P € P(X x Ax). As
in the main body of the paper, we denote P, = P(A =a) and Py = P(Y = 1,A =a) for
both a € {0,1}. We assume without loss of generality that 0 < Py < 4 < P, (when studying
demographic parity) and 0 < Pig < Py (when studying equality of opportunity).

We will be interested in the concentration properties of certain empirical estimates based
on the corrupted data S?. Therefore, we denote the distribution that corresponds to all the
randomness of the sampling of S?, that is the randomness of the clean data, the marked
points and the adversary, by P4_ Here we consider both P and @ arbitrary, but fixed.

C.2.2 Concentration results

We study the concentration of the demographic parity and the equality of opportunity fairness
estimates in Sections |C.2.2| and |C.2.2) respectively.

 

 

Concentration for demographic parity

We use the notation C, = 7, L{a? = a,i ¢ $} for the number of points in S? that were not
marked (that is, are clean) and contain a point from protected group a and B, = 7, [{a? =
a,i € 3B} for the number of points in S? that were marked (that is, are potentially baa!)
and contain a point from protected group a. Note that By + B, = |B] is the total number
of poisoned points, which is Bin(n, a), and By + By = n — Co — C). Similarly, denote by

1We use B, with B for bad here, instead of P for poisoned, to avoid confusion with the protected group
frequencies P;.

134

[AAB*15]

[AAZL18]

[ABD*18]

[ABHM17]

[ADSK18]

[AEIK18]

[AHJ+18]

[AJSD19]

[AKM20]

Bibliography

Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, lan Goodfellow, Andrew Harp, Geoffrey Irving, Michael
Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous
systems, 2015. Software available from tensorflow.org.

Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient
descent. In Conference on Neural Information Processing Systems (NeurIPS),
2018.

Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna
Wallach. A reductions approach to fair classification. In International Conference
on Machine Learing (ICML), 2018.

Pranjal Awasthi, Avrim Blum, Nika Haghtalab, and Yishay Mansour. Efficient
PAC learning from the crowd. Conference on Computational Learning Theory
(COLT), 2017.

Dan Alistarh, Christopher De Sa, and Nikola Konstantinov. The convergence of
stochastic gradient descent in asynchronous shared memory. In ACM Symposium
on Principles of Distributed Computing (PODC), 2018.

Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing
robust adversarial examples. In International Conference on Machine Learing
(ICML), 2018.

Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola Kon-
stantinov, and Cédric Renggli. The convergence of sparsified gradient methods.
Conference on Neural Information Processing Systems (NeurlPS), 2018.

Abolfazl Asudeh, HV Jagadish, Julia Stoyanovich, and Gautam Das. Designing
fair ranking schemes. In International Conference on Management of Data
(COMAD), 2019.

Pranjal Awasthi, Matthaus Kleindessner, and Jamie Morgenstern. Equalized
odds postprocessing under imperfect group information. In Conference on
Uncertainty in Artificial Intelligence (AISTATS), 2020.

91

3.5. On the hardness of adversarial multi-source learning

 

information that the data comes in multiple datasets, some of which remain uncorrupted even
after the adversary was active, that gives the multi-source learner the power to learn robustly.

An immediate consequence of Theorem | is also that the common practice of merging the
data from all sources and performing a form of empirical risk minimization on the resulting
dataset is not a robust learner and therefore suboptimal in the studied context.

3.5.2 How hard is robust learning?

As a tool for understanding the limiting factors of learning in the adversarial multi-source
setting, we now establish a lower bound on the achievable excess risk in terms of the number
of samples per source and the power of the adversary.

Theorem 6. Let Hc {h: X — Y} be a hypothesis space, let m and N be any integers and
let G be a fixed subset of [N] of sizek € {1,...,N—1}. Let S’ € (X x y)"*” be drawn
i.i.d. from D. Then the following statements hold for any multi-source learner L:

(a) Suppose that H is non-trivial. Then there exists a distribution D on X with minnexy R(h) =
0, and a fixed-set adversary A with index set G’, such that:

Po (R(c(A(S')) >) > ms (3.20)

(b) Suppose that H has VC dimension d > 2. Then there exists a distribution D on X x Y
and a fixed-set adversary A with index set G’, such that:

ji . d a 1
Ps (leas )) — min R(h) > V pega + | > 1)

In both cases, a = Ak is the power of the adversary.

 

The proof is provided in the supplemental material. As for Theorem pI it is clear that the
same result holds also for flexible-set adversaries with preserved size k.

Analysis. Inequality (3.20) shows that even in the realizable scenario, the risk might not
shrink faster than with rate Q(a/m), regardless of how many data sources, and therefore data
samples, are available. This is contrast to the i.i.d. situation, where the corresponding rate is
Q(1/Nm). The difference shows that robust learning with a constant fraction of corrupted
sources is only possible if the number of samples per dataset grows.

In inequality (3.21), the term Q(,/d/Nm) is due to the classic lower bound on the sample
complexity of binary classification (e.g. Theorem 3.23 in [MRT18]) and corresponds to the
fundamental limits of learning, now in the non-realizable case. The Q(a/m)-term appears as
the price of robustness, and as before, it implies that for constant a, m— oo is necessary in
order to achieve arbitrarily small excess risk, while just NV —> 00 does not suffice.

Relation to prior work. Lower bounds of similar structure as in Theorem 6) have also been
derived for Byzantine optimization and collaborative learning. In particular, RB18] prove
that in the case of distributed mean estimation of a d-dimensional Gaussian on NV machines,
an a fraction of which can be Byzantine, any algorithm would incur loss of Gs + xh).
AAZL18] construct specific examples of a Lipschitz continuous and a strongly convex function,

35

Table of Contents

 

vii
vii
[About the Author! x
List of Collaborators and Publications xi
[Table of Contents) xiii
xiv
\List_of Tables| xv
xv
1
5

2.1 Supervised machine learning)... . 2... .......222000000.0. 5

2.2 Learning from corrupted data... 2... ee 13
2.3. Fairness in machine learning)... 2.2... 20... .2220000000. 18

On the Sample Complexity of Adversarial Multi-Source PAC Learning 23

 

 

 

 

 

3.1 Motivation and outlind. 2. 23
3.2 Related work). 2. 24
3.3 Preliminaries) 2. 26
3.4 On the sample complexity of adversarial multi-source learning)... . . . . 29
3.5 On the hardness of adversarial multi-source learning ............- 34
3.6 Summary and subsequent work) .............22000.200.-. 36
[4 Adversarial Multi-Source Learning in Practice| 37
[4.1 Motivation and outline]... 2. 37
4.2 Related work)... 2. 2. ee 38

4.3 Robust learning from untrusted sources)... 2... 2.00000. .00.-. 38
4.4 Experiments .......... 000.000.000.000 00000000. 43

 

4.5 Summary)... 2... 49
5 Fairness-Aware PAC Learning from Corrupted Data 51
5.1 Motivation and outline... 2... ee 51
5.2 Related work. 2... ee 52

and

DP DP Ul
Pore, (Cea — L(h,P1) 21) A (r (£(S"), Pa) ~T?"(u, Pa) 2 sm)
= Psprv, (£(S?) = ho)
Finally, note that P, = P{, so that either Psp. (L(S?) = hy) > 1/2 or Psp. (L(S?) = ho) =
1/2. Therefore, for at least one of i = 0,1, both

Qa

 

L(L(S?),P:) — L(hi, P;) > 9 = i=s
and

P??(c($?),P,) — P2?(h,,P;) > ——/- —- = —___*%
both hold with probability at least 1/2 when the choice of distribution and adversary is P; and
G; respectively. This concludes the proof in the first case.

Case 2 Now suppose that 7 = —* > 2P)(1— Po). Let a, € (0,0.5) be such that

1l-a

 

 

roy = 2Fo(1 — Po). Note that since f(x) = 75, is monotonically increasing in (0,1), a1 is
unique and a, < a.
Now repeat the same construction as in Case 1, but with 7, = me = 2Py(1 — P).

For every marked point, the adversary does the same as in Case 1 with probability a,/a and
does not change the point otherwise. Then the same argument as in Case 1 shows that for
one 7 € {0,1}, both

 

L(L(S"), Pi) — L(hi, Pi) > m =

and

DP P) PP.) — PPP(h.. P.) > Th
Pr’ (£(8"),P;) —T°?* (hi, Pi) > SPL — Po) 1

both hold with probability at least 1/2. This concludes the proof of Theorem 8]

 

 

 

 

Theorem 9. Let 0 < a < 0.5, Pio < Pi < 1 be such that Py + Py, < 1. For any input set
X with at least five distinct points, there exists a finite hypothesis space H, such that for
any learning algorithm L : Unen(¥ x A x Y)" + H., there exists a distribution P for which
P(A =a,Y =1) = Py fora € {0,1}, a malicious adversary A of power a and a hypothesis
h* € H, such that with probability at least 0.5

L(£(S?),P) — L(h",P) > min {os 2Py9, 2(1 — Pro — Puy}
—-a
and

1— Po — P,
P20r(2($?), P) —T2r(h* ,P > min |p ta Fgn Fal
(£(9"),P) ~ 1POW(h*,P) > min § 5 1

—i a —
Proof. Let n = 755, so that a = re

126

In particular, if d = VC(F), by Sauer’s lemma S7(2Nm) < (am) whenever 2Nm > d,
So:

 

Th d 2 2
Pg ( sup(T®°P(f) — PPP(f, 8)) >t < 24 (0) exp £2
foF d 128

It follows that:

 

Ps (ayer — TPF, S)) > 8 ee re) <6 (D.18)

fF Nr?

whenever:

 

 

 

2eNm 24
tt ay/atoal 7) + log() > uy 29002)
Nr? Nr?
It is easy to see that the right inequality holds whenever d > 1, 2Nm > d and 0 <1. In

addition, inequality (D.18) trivially holds if the left inequality is not fulfilled. Hence the result
follows.

 

 

 

 

160

5.2. Related work

 

number of hardness results for fair learning are also known. In particular, |KMRI1/| prove the
incompatability of three fairness notions for a broad class of learning problems and
quantify fundamental trade-offs between fairness and accuracy. Both of these works, however,
focus on learning with i.i.d. clean data.

Fairness and data corruption Most relevant for our setup are a number of recent works
that empirically study attacks and defenses on fair learners under adversarial data poisoning. In
particular, [SBC20}, and consider practical, gradient-based poisoning
attacks against machine learning algorithms. All of these works demonstrate empirically that
poisoned data can severely damage the performance of fair learners that are based on empirical
loss minimization. In our work we go beyond this by proving a set of hardness results that hold
for arbitrary learning algorithms. On the defense side, design and empirically study
an adversarial training approach for dealing with data corruption when training fair models.
Their defense is shown to be effective against specific poisoning attacks that aim to reduce the
model accuracy. In contrast, for our upper bounds we are interested in learners that provably
work against any poisoning attack, including those that can target the fairness properties of
the model as well.

Among works focusing on weaker adversarial models, a particularly popular topic is the one
of fair learning with noisy or adversarially perturbed protected attributes
{CMV21]. Under the explicit assumption that the corruption does
not affect the inputs and the labels, these works propose algorithms that can recover a fair
model despite the data corruption. A related, but conceptually different topic is the one of
fair learning without demographic information [LBC+20]. Another
commonly assumed type of corruption is label noise, which is shown to be overcomable under
various assumptions by [DADC18], [JN20], and [EGC20]. A distributionally robust
approach for certifying fairness is taken by [TNKB20], under the assumption that the real
data distribution falls within a Wasserstein ball centered at the empirical data distribution. In
a formal methods framework for certifying fairness through unawareness, even in the
presence of a specific type of data bias that targets their desired fairness measure, is provided.
The vulnerability of fair learning algorithms to specific types of data corruption has also been

demonstrated on real-world data by [CZ13] and [KZi8].

An orthogonal line of work shows that imposing fairness constraints can neutralize the effects
of corrupted data, under specific assumptions on the type of bias present [BS20]. Also related
are the works of [[RO*19] and [LBSS21] who propose procedures for data cleaning /outlier

detection, without a specific adversarial model, that in particular improve fairness performance.

Learning against an adversary Learning from corrupted training data is a field with long
history, where both the theoretical and the practical aspects of attacking and defending ML
models have been widely studied
IDKK*19b]. In this work we study fair learning within
the malicious adversary model [CBDF+99]. This chapter adds an additional
dimension to this line of work, where fairness is considered alongside with accuracy as an
objective for the learner.

53

Since suppey [R?(h)—R(h)| < SUPpeH |R?(h) —Re(h)

= dlog(£) + 2log(16/6
pa (were R(h)| > 3% 4.9, / Stowe) + 2lo8(26/ 3 ae (c.23)
heH 2 n 2

In addition, Lemma 3] implies that

~ 2d log( 22") + 2log(96/6) }
P# | sup |P2?(h) — P??(h)| > APP 4 16,) ——P a Oc C2
[sn (h) (h)| > APP + 16, i a)Fin <5. (C24)

Now let hy = argminyes,(R?(h) + ATP? (h)) and let

+suppex |R¢(h) —R(h)

 

, we obtain

 

 

8dlog() + 2log(16/6) 2d log (2) + 2 log(96/6)

DAP? 4-30
w re (1—a)Pon

APP = 3a+4

n
«(fda d
-0( fff).

Then, using (C.23) and ((C.26), we have that with probability at least 1 — 6

LEP (LNP(S?)) = R(LPP(S?)) + ADPP(LY?(8?))
a) os J
< RP(LRP(SP)) + PPP (LPP SP) + SARP
= min (R por 1. pp
= min (RP(h) +r (h)) + sh

< min LP? (h) + APP.
~~ hEH x (h) + s

 

 

 

 

Bound for equality of opportunity We now show a similar result for the weighted-objective
with the equality of opportunity deviation measure

1X0%(h) = R(h) + PPP (Hy.

Let £90? » US, (¥ x Ax VY)" +H be such that

n=1

££0"( S$") & argmin( RP(h) + AP®°"(h)), for all S?.
heH

That is, [Bee always returns a minimizer of the \-weighted empirical objective. Then:

Theorem 13. Let H. be any hypothesis space with d = VC(H) < oo. LetP€ P(X x Ax y)
be a fixed distribution and @ be any malicious adversary of power a < 0.5. Denote by pa
the probability distribution of the poisoned data S?, under the random sampling of the clean

data, the marked points and the randomness of the adversary. Then for any 6 € (0,1) and

8log(16/5) 12log(12/5)
Sp, we have

n= max { A—a)Pio ? a

 

PH (2E7(LkP(S")) < min LE%(h) + AFP) <6,

148

6.1 TREC: Test-time performance of fair rankers with equal opportunity, demographic
parity and equalized odds fairness, achieved by our algorithm and the baselines:

unfairness (left y-axes) and NDCG@3 ranking quality (right y-axes); after training

with different regularization strengths (a-axis).

6.2 MSMARCO: Test-time performance of fair rankers with equal opportunity, de-

mographic parity and equalized odds fairness, achieved by our algorithm and the

        

 

 

 

after training with different regularization strengths (x-axis).

List of Tables

oO

4.1 Results from the experiment on all 957 products... ...........00.. 4

.2 Summary of the results from the Animals with Attributes 2 experiments, over all

85 prediction tasks and all 6 types of corruption. Given a number of corrupted
sources n (columns) and a baseline (rows), we report values in the form A/B/C,

NS

 

 

ee a7

4.3 Summary of the results for p = 0.5, over all 85 prediction tasks and all corruptions.| 49
[4.4 Summary of the results for p = 0.2, over all 85 prediction tasks and all corruptions.| 49

6.1 Maximal and mean relative fairness increase, achievable without a significant

decrease of ranking quality, for our algorithm and the baselines. See main text for

 

 

 

 

Le 85
List of Algorithms
3.1 Dataset filtering for robust multi-source learning... .........0.. 32

4. Robust learning from untrusted sources}... ..............00. 41

A.1_ Dataset filtering for robust multi-source learning... .........2.. 108

A.2 ERM on the trusted sources) .. 2... 2.2... . 2 ee ee 109

XV

 

= In Chapter B we study the interplay between robustness and fairness and tightly charac-
terize the limits of fairness-aware learning from corrupted data.

. Chapter [6] deals with fairness in ranking. We show how a number of popular fairness
notions from binary classification can be integrated into modern learning-based ranking
methods with little computational overhead and with theoretical guarantees. Furthermore,
we show that improving fairness in the context of ranking often comes with little to no

sacrifice of model utility.

5. FAIRNESS-AWARE PAC LEARNING FROM CORRUPTED DATA

 

Theorem 10. Let 0 < a < 0.5,0 < Py < 0.5. For any input set X with at least four
distinct points, there exists a finite hypothesis space H, such that for any learning algorithm
Lo: Unen(¥® x Ax Y)" > H, there exists a distribution P for which P(A = 0) = Py, a
malicious adversary A of power a and a hypothesis h* € H, such that with probability at least
0.5

R(L(S?), P) = R(h*,P) = min R(h, P)

and
T??(c(S?),P) —T??(h*,P) > min {= i}
2Po

We also present a corresponding result for equality of opportunity.

Theorem 11. Let 0 < a < 0.5, Pio < Py < 1 be such that Pio + Py, <1. For any input
set X with at least five distinct points, there exists a finite hypothesis space H, such that for
any learning algorithm L : Unen(¥ x A x Y)" + H., there exists a distribution P for which
P(A =a,Y =1) = Py, fora € {0,1}, a malicious adversary A of power a and a hypothesis
h* €H, such that with probability at least 0.5

R(L(S?), P) = R(h*,P) = min R(h, P)
and

y P, P,
P20»(c(S?), P) — P2OP(n*, P > min {gp (1- Fe) a Beh
(E(S"),P) WP)2 2(1 — a) Pro Pus’ Pu

Once again the error terms on the fairness notions are inversely proportional to Pp and Pio
respectively, indicating that datasets in which one of the subgroups is underrepresented are
particularly vulnerable to data manipulations.

5.5 Upper bounds

We now prove that the (sample-size-independent) lower bounds from the previous section are
tight up to constant factors, by providing matching upper bounds for the same problem. We
do so by studying the performance of two natural types of fairness-aware learning algorithms
under the malicious adversary model. We find that these algorithms achieve order-optimal
performance in the large data regime.

Complete proofs of all results in this section can be found in Appendix |C.2, A sketch
of the proofs is also presented in Section 5.5.3]

5.5.1 Upper bounds on the \-weighted objectives

The first type of algorithms we study simply minimize an empirical estimate of the \-weighted
objective L,. We show that with high probability such learners achieve an order-optimal
deviation from minjcx Ly(h) in the large data regime, as long as H. has a finite VC dimension.

62

1.

INTRODUCTION

 

With these considerations in mind, the first part of this thesis deals with the problem of
designing machine learning algorithms that are robust to corruptions in data coming from
multiple sources. In the setup we study, some of the data sources may provide data that is
noisy or systematically or even maliciously perturbed. We show that, in contrast to the single
dataset case, successful learning within this model is possible both theoretically and practically.
To this end, we describe the fundamental limits of learning against a malicious agent that
controls some of the sources. We also design a practical algorithm for such grouped data and
show that it exhibits strong performance in an extensive experimental evaluation.

Next, we move on to explore another property of machine learning models that is crucial for
their real-world reliability, namely their fairness. Indeed, some of the areas where machine
learning has shown promising results require careful considerations regarding the social impact
that automated decisions can have [MMS*19]. For example, whenever a learned
system is used for screening the quality of job or loan applications, it is crucial to ensure that
the model does not discriminate applicants based on their gender, race or other attributes
protected by law or by ethical considerations. Similarly, if a machine learning model is used for
suggesting personalized recommendations on online music platforms, it is important to ensure
fair exposure for the performers.

There are multiple challenges related to achieving fairness in the context of machine learning.
Firstly, an obstacle can again be the presence of data corruption, because real-world data is
usually based on past human decisions, which are often prone to historical bias [MMS*19}.
At the same time little is known about the effect of general data corruptions, such as label
noise or adversarial manipulations, on fairness-aware learning. A second challenge is that
the majority of fairness-aware learning algorithms and the corresponding theoretical results
only consider the case of binary decision making. In contrast, machine learning is extensively
used in much more complicated domains, such as recommended systems, in which, as already
argued, fairness concerns can also arise.

Therefore, a second topic addressed in this thesis is that of machine learning fairness. In
particular, we study the limits of fairness-aware learning under worst-case data corruption. We
find that, alarmingly, guaranteeing fairness under such data issues is impossible and moreover
the amount of unfairness that can not be avoided increases for problems where one of the
protected subpopulations in the data is underrepresented (e.g. there are not many applicants
of color for a job position). We also demonstrate a way of transfering classic fair learning
techniques from binary classification to ranking, thereby providing theoretical guarantees for
fair ranking and designing a scalable fairness-aware algorithm for this task.

The rest of the thesis is structured as follows:

= In Chapter |2| we introduce several notions that are central to machine learning theory
and on which we heavily rely throughout the thesis. We also discuss various concepts
from the theory of robust and fair machine learning, including such that will be recurrent
throughout the text.

» Chapters 3|and 4) are dedicated to the topic of robust learning from multiple unreliable
data sources. In Chapter BI we study the theoretical limits of learning against an
adversarial opponent that controls a subset of the data sources. In Chapter 4) we provide
a practical algorithm for the studied problem, under the additional assumption that a
small trusted reference dataset is given.

2.1. Supervised machine learning

 

However, this does not follow directly from the law of large numbers, since Leri(S) depends
on the data points in S and therefore standard concentration arguments are insufficient to
show that the empirical and the true risk of this hypothesis are close to each other. Instead, in
order to show this one needs to require that the empirical and the true risk are close to each
other for all hypothesis in H, that is, that concentration holds uniformly over the hypothesis
space. This naturally needs to the concept of uniform convergence, which is presented in the
following definition.

Definition 3 (Uniform Convergence). A hypothesis space H. is uniformly convergent with
respect to a loss function ¢: Vx Y + Rt, if there exists a function mY : (0,1) x (0,1) +N,
such that for any €,6 € (0,1) and any distribution D, if the learner takes as input a set S'
of at least mY (e,6) points sampled i.i.d. from D, then with probability at least 1 — 6 with
respect to the sampling of the points in S:

sup |R(h) — R(h)| <e. (2.7)
heH

Whenever H is uniformly convergent, we refer to the component-wise smallest possible function

.
mi" as the rate of uniform convergence of H.

It is easy to see that any uniformly convergent hypothesis space is also agnostic PAC learnable.

Proposition 1. Whenever H. is uniformly convergent with rate mY", H. is also agnostic PAC

learnable with sample complexity at most mye (s, 8).

Proof. Fix any €,5 € (0,1) and let D be an arbitrary distribution on © x Y. For any dataset
S of size at least mi° (5 8), sampled i.i.d. from D, with probability at least 1 — 5 we have
that
sup |R(h) — R(h)| < é
hEeH. 2
and so
R(Leru(S)) < R(Cena(S)) + 5 = min Rh) +5 < inf R(h) +¢

Therefore the ERM learner is an agnostic PAC learner for H, with sample complexity my(e, 5) <
me (s. 8).

In particular, this result suggests that the ERM rule is a reasonable approach to learning and
that, if the hypothesis space is uniformly convergent, ERM is in fact an agnostic PAC learner.

 

 

 

 

2.1.3 Measures of complexity

The bias-variance trade-off From a more practical perspective, one may want to decide
what type of a hypothesis space should be chosen given a particular supervised learning
problem, that is, for a fixed but unknown distribution and given some amount of data.

In the context of learning with the ERM rule, an intuitive trade-off emerges. On the one hand,
selecting a hypothesis space of large size will increase the likelihood of finding a hypothesis
that performs well on the training data set and so the ERM rule will yield a hypothesis with
small empirical loss. On the other hand, if the hypothesis space is too large, the uniform

9

 

_ n/2 1—Po-— 1/2
Py-—n/2+n/2 1-—Po-—n/24+n/2
= 0

—1,—-—___/ _
and

P?? (ho, P1) = |P(x,ayysrs (ho(X) = 1A = 0) — Poxayyne, (ho(X) = 14 = 1)
=|0-]|
=1,
so that TPP (ho, P) _ TPP (hy, Pi) = IPP):
Consider a (randomized) malicious adversary @; of power a, that given a clean distribution
P;, changes every marked point to (23, 77,7) with probability 0.5 and to (24,7, 77) otherwise.
Under a distribution P; and an adversary @;, the probability of seeing a point (23, i, 77) is
#(l-—a) = Ve = a/2, which is equal to the probability of seeing a point (x3, 7,7).
Therefore, denoting the probability distribution of the corrupted dataset, under a clean
distribution P; and an adversary @;, by P’ (as a shorthand for p%), we have

(l-a)(l1—-Py-7/2) ife=m,a=1ly=1

(1 — a)(Po — 7/2) ife=a%,a=0,y=0
a/2 ifr=2%3,a=i,y=-7%

Pi(z,a,y) = 4 a/2 ifv =23,a=-i,y =1
a/2 ifr=24,a=7i,y=i
a/2 ifr=24,a=i,y=-7%
0 otherwise

In particular, Pj = P{, so the two initial distributions Po and P; become indistinguishable
under the adversarial manipulation.

Fix an arbitrary learner £L : Unen(¥ x Ax VY)" > {ho, hi}. Note that, if the clean distribution
is Po, the events (in the probability space defined by the sampling of the poisoned train data)

{L(L(S”), Po) — L(ho, Po) > ny = {L(S?) = hi}

DP p DP it
={r (£(S?), Po) —T (1o.Pe) > spas}

are all the same. Similarly, if the clean distribution is P,
{L(L(S?), Pi) — L(hi, Pi) 2 n} = {£(S?) = ho}

_ Jpop p — TPP 4
= {r (£(S?), Pr) — PP" (hi, Pi) = SPF

Therefore, depending on whether we choose Pp or P; as a clean distribution, we have

Pspwpy, (recs). a) — L(ho, Po) >) A (ro"ceis"), Po —T?? (ho, Po) > SRB)

= Popa, (£(S?) = hy)

125

4.4. Experiments

 

Table 4.1: Results from the experiment on all 957 products.

 

 

 

 

 

 

Algorithm Mean classification error
Ours 0.289 + 0.0016
Reference only 0.301 + 0.0019
All data 0.312 + 0.0017
Median of probs. 0.325 + 0.0021
Geom.median 0.329 + 0.0021
Comp.median 0.329 + 0.0021
Robust loss 0.353 + 0.0021
Batch norm 0.298 + 0.0016

 

 

As a first, illustrative, experiment, we chose 20 books and 20 other, purposely different,
products (e.g. USB drives, mobile apps, meal replacement products). For simplicity, we refer
to these additional products as "non-books". Intuitively, when learning to classify book reviews
and given access to reviews from both some books and some non-books, a good learning
algorithm will be able to leverage all this information, while being robust to the potentially
misleading data coming from the less relevant products.

We randomly sample one of the books and 300 positive and 300 negative reviews for it. Out
of those, 100 randomly selected reviews are made available to the learner as a reference
dataset. The 500 remaining reviews from the product are used for testing. For a given value of
n € {0,1,..., 10} the learner also has access to 100 labeled reviews from each of 10 — n other
randomly selected books and from each of n randomly selected non-books. Our algorithm, as
well as all baselines, are trained on this available data and the learned predictors are evaluated
on the test set for the target product. For each n, we repeat this experiment 1000 times.

The results are plotted in Figure 4.1 The «-axis corresponds to the number n of non-books
and the y-axis gives the average classification error. The error bars correspond to the standard
errors of the mean estimates. We see that our method (green) performs uniformly better
than the naive approaches of training on the reference dataset from the target product only
(red) and training by merging all data together (blue). When reviews from many books
are available, our algorithm is able to use this additional information even better than the
model learned on all data. As the proportion of non-books increases, the performance of the
second approach degrades, confirming the intuition that the reviews for the non-books provide
less useful information for the target task. On the other hand, our algorithm successfully
incorporates the information from the useful sources only, converging to the performance of
the model learned on the reference data as all additional sources become non-books.

Our algorithm also outperforms all baselines. The batch normalization approach appears
to reduce the effect of irrelevant sources, but its performance degrades as n — 10. The
median-based approaches perform reasonably when at most half of the sources are non-books,
but eventually become worse than the other methods. The component-wise median and the
robust loss baselines were excluded from the plot for clarity, as they performed uniformly worse
than the other baselines, ranging in average classification error from 0.338 to 0.375 and from
0.348 to 0.372 respectively. Note that the robust loss function of is non-convex, so
the poor performance of this baseline is presumably due to failure of the gradient descent
optimization procedure to converge to a good local minimum.

Additionally, we performed an experiment on the set of all 957 products. With every product as
a prediction task, we randomly selected 100 reviews from it as a reference dataset, leaving 500

45

4.3. Robust learning from untrusted sources

Algorithm 4.1: Robust learning from untrusted sources
Inputs: 1. Loss @, hypothesis set H, parameter
2. Reference dataset S7
3. Datasets S,,...,S'y from the N sources
for i = 1 to N do {Potentially in parallel}
Compute dy, (S;, Sr)
end for
Select a by solving (4.6).
Minimize a-weighted loss: he = argminyeRa(h)
Return: he

 

While the Rademacher complexities are functions of both the underlying distribution and
the hypothesis class, in practice one usually works with a computable upper bound that is
distribution-independent (e.g. using VC dimension). For some common examples of such
bounds we refer to [SSBD14]. In our setting the hypothesis space H is fixed and
therefore these bounds would be identical for all 7. Therefore, we expect the 9; (H.) to be
of similar order to each other and the impact of a on the second term in the bound to be
negligible. We thus concentrate on optimizing the remaining terms.

Because the true discrepancies are unknown, we estimate them from the data by their empirical
counterparts:

nn (S;, Sr) = sup (\Ri(h) - Rr(h)l)
heH

1
- we (|- Yel (h (aig) Yag abe (try). ¥ry)
hEF

Mi faa

 

 

) (4.5)

In summary, the bound suggests to choose a weighting for the sources by minimizing:

a
ai a dy (Si, Sr) +2 ye
= a (4.6)

N

subject to: So a; = 1 and a; > 0 for all 7,
i=l

where \ > 0 is a hyperparameter that can be selected by cross-validation on the reference
dataset. The algorithm then proceeds to minimize the a-weighted empirical risk over the
sources (4.2), possibly with a regularization term. Pseudocode of the algorithm is given in

Algorithm 4.1)

We note that in general computing the empirical discrepancies can be a hard problem. However,

41

5.3. Preliminaries

 

corresponding fairness deviation measures I'(h) MW18al |WM19}. Here we adopt
the mean difference score measure of and [MW18a] for demographic parity

 

 

 

TP? (h, P) lPemx) 1A = 0) — P(A(X) = 1A | (5.4)

and its analog for equality of opportunity

 

 

 

P£0r(h, P) Pou) 1|A=0,Y =1) —P(A(X) =1|A=1,Y 1) (5.5)

To avoid degenerate cases for these measures, we assume throughout the chapter that
P, =P(A=a) >0 and Py = P(Y =1,A =a) > 0 for both a € {0,1}. For the rest of the
chapter, whenever we are interested in demographic parity fairness, we assume without loss
of generality that A = 0 is the minority class, so that Py < 3 < P,. Similarly, whenever the
fairness notion is equality of opportunity, we will assume that Pig < Pi.

Whenever the underlying distribution is clear from the context, we will drop the dependence
of R(h,P) and [(h, P) on P and simply write R(h) and P(h).

5.3.2. Learning against an adversary

As argued previously, machine learning models are often trained on unreliable datasets, where
some of the points might be corrupted by noise, human biases and/or malicious agents. To
model arbitrary manipulations of the data, we assume the presence of an adversary that can
modify a certain fraction of the dataset and study fair learning in this context. In addition to not
being partial to a specific type of data corruption, this worst-case approach has the advantage
of providing a certificate for fairness: if a system can work against a strong adversarial model,
it will be effective under any circumstances that are covered by the model.

Similarly to the classic robust machine learning setup, a fairness-aware adversary is any
procedure for manipulating a dataset, that is a possibly randomized function

A: Unen(¥ x Ax V)" > Unen(¥ x Ax Y)”

that takes in a clean dataset S° = {(«¢, af, y£)}?_, sampled i.i.d. from P and outputs a
new, corrupted, dataset S? = {(x?,a?,y?)}"_, of the same size. Depending on the type of
restrictions that are imposed on the adversary, various adversarial models can be obtained.

In this chapter we focus on the malicious adversary model, as introduced in Section |2.2.3)
and adapt it to the fairness-aware learning case. The formal data generating procedure is as
follows:

 

» An iid. clean dataset S° = {(xf, af, y£)}", is sampled from P.

« Each index/point 7 € {1,2,...,n} is marked independently with probability a, for a
fixed constant a € [0,0.5). Denote all marked indexes by 8 C [n].

= The malicious adversary computes, in a possibly randomized manner, a corrupted dataset
SP = {(aP, ab yP) hi, © (x x Ax Y)", with the only restriction that (x?,a?, y?) =
(xf, af, y£) for all i ¢ $8. That is, the adversary can replace all marked data points in
an arbitrary manner, with no assumptions whatsoever about the points (x?, a?, y?) for

cep.

55

4. ADVERSARIAL MULTI-SOURCE LEARNING IN PRACTICE

 

for the 0/1-loss, a symmetric hypothesis class H and for Y = {—1,+1}, we have:

 

 

mr
dy (Sts Sr) = sup mt {h(2.3)45<0} wan Biber Jun j<0}
= sup u As Ling (ai,j)yig<O} — s Linc (wr, )yr.j <0}
heH \ Mi jay
a ne (4.7)
=sup[1— | — Yo 1ae,,) Bsc + Mn.) Jur j <0}

heH Mj j=l

mi

(1 1
=1— inf | rm 2 midst Fag D YHoer erg) <0}

where y;; = 1 — yj; is the flipped label of the j-th data point from the i-th source. Now
notice that computing the infimum in equation (4.7) is equivalent to solving a (weighted)
empirical risk minimization problem with the input data from the source and the target merged
and the labels being the flipped labels from the source and the actual labels from the target.

Therefore, computing the empirical discrepancies in this case is equivalent to solving an
empirical risk minimization problem and standard convex upper bounds on the 0 — 1 loss can
be applied to design computationally efficient approximate algorithms. In our experiments, we
solve the ERM problem by using the square loss.

Discussion. While derived from our theoretical results, the minimization procedure for
selecting the weights also has an intuitive interpretation. Note that the first term in (4.6)
is small whenever large weights are paired with small discrepancies and hence encourages
trusting sources that provide data similar to the reference target sample. The second term is
small whenever the weights are distributed proportionally to the number of samples per source.
Thus, it acts as a form of regularization, by encouraging the usage of information from as
many sources as possible.

The hyperparamater controls a trade-off between exploiting similar tasks and leveraging
information from all sources. As A — oo, all tasks are assigned weights proportional to the
amount of training samples they provide and the model minimizes the empirical risk over all
the data, regardless of the quality of the samples. In contrast, as A — 0, the model becomes
more sensitive to differences between the source data and the clean reference set, until all
weight is assigned to the source closest to the target domain. Assuming that the reference
set is included as one of the data sources, these extremes correspond to the naive approach
of trusting all sources and training on a merged dataset and not trusting any of them and
training on the initial clean data only. In our experiments in Section 4.4} we will see that there
is a better operating point between those two extremes. It naturally depends on the actual
quality of the available data and our algorithm identifies it successfully.

4.3.3. Learning from private or decentralized data

The described algorithm is straightforward to implement on top of any standard learning
procedure, when the data from all NV sources is directly available to the learner. We now
discuss how we can learn robustly in cases where the sources cannot fully reveal their data.
There are many applications where such a situation can arise. For example, this can be due
to privacy reasons in the case of medical and biological data or to communication costs and
storage limitations in the case of distributed learning [MMR*17].

42

3.

ON THE SAMPLE COMPLEXITY OF ADVERSARIAL MULTI-SOURCE PAC LEARNING

 

Robustness has also been explored in the context of multi-view learning, where data arrives

from various feature extractors [ZXXS17 ZIK17].

Robust learning from a single data source There is a vast body of literature focusing
on robustness of learning algorithms to corruptions within a dataset, e.g.
[PSBRI8], and on identifying data corruptions at prediction time,
eg. (SLI8]. These lines of work are orthogonal to ours, since we consider multiple
training datasets, some of which are corrupted, and hence a literature review in this direction
is beyond the scope of our discussion. Most relevant are the works of |[BEK02], who,
as already discussed in Chapter 2 study the fundamental limits of PAC learning from a single,
adversarially corrupted data source. We compare our results to this classic scenario in Section

3.4.1)

3.3. Preliminaries

In this section we introduce the technical definitions that are necessary to formulate and prove
our main results. We start by introducing the relevant notation and reminding the reader of
the setup of supervised learning, as laid out in Section 2.1.2] We then introduce the setting of
learning from multiple sources and notions of adversaries of different strengths in this context.

 

3.3.1 Notation and Background

Let XY and Y be given input and output sets, respectively, and D € P(X x) be a fixed
but unknown probability distribution. By ¢: Yx Y — R we denote a loss function, and by
HC {h:X — Y} a set of hypotheses. All of these quantities are assumed arbitrary but fixed
for the purpose of the work within this chapter.

A (statistical) learner is a function £L : U%_,(4 x Y)™ — H. In the classic super-
vised learning scenario, the learner has access to a training set of m labelled examples,
{(x1,y1),--+;(@m;Ym)}, sampled i.i.d. from D, and aims at learning a hypothesis h € H with

small risk, i.e. expected loss, under the unknown data distribution,

R(h) = Evey0(E((2),9))- (3.1)

Recall that PAC-learnability is a key property of the hypothesis set, which ensures the existence
of an algorithm that performs successful learning:

Definition 7 (PAC-Learnability). We call H (agnostic) probably approximately correct (PAC)
learnable with respect to 0, if there exists a learner £ and a function my, : (0,1)x(0,1) +N,
such that for any €,6 € (0,1), whenever S is a set of m > my,¢(e, 4) i.i.d. labelled samples
from D, then with probability at least 1 — } over the sampling of S:

R(L(S)) < min Rh) +e. (3.2)

Another important concept related to PAC-learnability is that of uniform convergence. Here
we use a version of this property that is slightly different than the one presented in Section

2.1.2)

Definition 8 (Uniform convergence). We say that H. has the uniform convergence property
with respect to ¢ with rate sy 9, if there exists a function 83, : Nx(0,1)xU%_, (&# x)” > R,
such that for any distribution D € P (¥ x) and any 6 € (0,1):

26

2.2. Learning from corrupted data

 

Formally, the process of computing the corrupted dataset S? = {(x?, y?)}™_, from the clean
data S° = {(«¢, yf) }"_, ~ D” is as follows:

= The malicious adversary attempts to mark every index/point i € {1,2,...,n} and
succeeds independently with probability a, for a fixed constant a € [0,0.5). Denote the
set of all marked indexes by $8 C [n] and note that || ~ Bin(n, a).

= The adversary computes, in a possibly randomized manner, a corrupted dataset S? =
{(xP, yP)}, © (X x Y)", with the only restriction that (2?, y?) = (xf, yf) for all ¢ P.
That is, the adversary can replace all marked data points in an arbitrary manner, with
no assumptions whatsoever about the points (x?, y?) for i € $B.

A classic result by states that in the context of binary classification any hypothesis
space, excluding degenerate cases, is not adversarially agnostic PAC learnable against the set
of all malicious adversaries. That is, there is no learner that can achieve close to optimal error
with high probability against the malicious adversary, even in the infinite data limit. Moreover,
Theorem 1 in [|KL93] shows that a malicious adversary of power a can always ensure that the
error incurred by the learning algorithm is at least =°..

Crucially in this adversarial model, the indexes of the points that the adversary can change are
independent of the observed data. This assumption is relaxed in the next adversarial model
we study.

The nasty adversary model The nasty adversary model was introduced by [BEKQ2}.
Just like the malicious adversary, the nasty adversary can arbitrarily manipulate a subset of
the data of size Bin(n,a). However, the adversary can choose any randomized procedure,
possibly dependent on the observed data, for selecting the subset that can be corrupted. The
only constraint is that the adversary needs to ensure that the size of the corrupted subset
is distributed as Bin(n,a) with respect to the randomness of the clean data and of the
procedure.

Formally, the process of computing the corrupted dataset S? = {(2?, y?)}"_, from the clean
data S° = {(xf, yf) }_, ~ D” is the following:

= The nasty adversary observes the clean data and based on S® computes, in a possibly
randomized manner, a subset 98 C [n] of marked indexes. The only constraint on this
procedure is that it must be the case that |§B] ~ Bin(n, a) with the randomness taken
with respect to both the clean data and the adversary’s computations.

= The adversary computes, in a possibly randomized manner, a corrupted dataset S? =
{(xP, yP)}, € (X x Y)", with the only restriction that (2?, y?) = (x, yf) for all i Z P.
That is, the adversary can replace all marked data points in an arbitrary manner, with
no assumptions whatsoever about the points (x?, y?) for i € P.

The nasty adversary model is strictly stronger than the malicious one. This is because the
nasty adversary can always choose to mark every index independently with probability a, hence
simulating the malicious adversary. In addition, the nasty adversary can already introduce a
harmful bias into the data through the way that it selects the marked data points already. For
example, if the clean data contains a small subpopulation of rare, but important, data points,
the adversary can choose to always mark the points from this subpopulation. Therefore, the

17


[MC18]

[Mc21]

[MM12]

[MMM19]

[MMRO09]

[MMR*17]

[MMS*19]

[MNMG20]

[MOS20]

[MPL15]

[MR17]

[MRT18]

[MS14]

[MSHJ20]

Bhaskar Mitra and Nick Craswell. An introduction to neural information
retrieval. Foundations and Trends in Information Retrieval, 2018.

Anay Mehrotra and L Elisa Celis. Mitigating bias in set selection with noisy pro-
tected attributes. In Conference on Fairness, Accountability and Transparency
(FAccT), 2021.

Mehryar Mohri and Andres Munoz Medina. New analysis and algorithm for
learning with drifting distributions. In International Conference on Algorithmic
Learning Theory (ALT), 2012.

Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed. Universal
multi-party poisoning attacks. In International Conference on Machine Learing
(ICML), 2019.

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation
with multiple sources. In Conference on Neural Information Processing Systems
(NIPS), pages 1041-1048, 2009.

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. Communication-efficient learning of deep networks
from decentralized data. In Conference on Uncertainty in Artificial Intelligence
(AISTATS), 2017.

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and
Aram Galstyan. A survey on bias and fairness in machine learning. ACM
Computing Surveys, 2019.

Ninareh Mehrabi, Muhammad Naveed, Fred Morstatter, and Aram Gal-
styan. Exacerbating algorithmic bias through fairness attacks. arXiv preprint
arXiv:2012.08723, 2020.

Hussein Mozannar, Mesrob | Ohannessian, and Nathan Srebro. Fair learning
with private demographic data. In International Conference on Machine Learing
(ICML), 2020.

Julian McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of
substitutable and complementary products. In ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 2015.

H. Brendan McMahan and Daniel Ramage. Federated _ learn-
ing: Collaborative machine learning without centralized training
data. https: / /research.googleblog.com/2017 /04/federated-learning-

collaborative.html, 2017.

Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of
Machine Learning. MIT press, 2018.

Yishay Mansour and Mariano Schain. Robust domain adaptation. Annals of
Mathematics and Artificial Intelligence, 71(4):365-380, 2014.

Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. Control-
ling fairness and bias in dynamic learning-to-rank. In International Conference
on Research and Development in Information Retrieval (SIGIR), 2020.

100

4. ADVERSARIAL MULTI-SOURCE LEARNING IN PRACTICE

 

Intuitively, the weights are assigned to the sources according to the quality and reliability of
the data they provide, quantified by an appropriate measure of trust we introduce. This is
achieved by comparing the data from each source to the small reference dataset. The measure
can also be computed locally at every source or by a gradient-based optimization procedure,
which allows for the implementation of the algorithm under privacy constraints, as well as its
integration into any standard distributed learning framework.

We perform an extensive experimental evaluation] of our algorithm and demonstrate its
ability to learn from all available data, while successfully suppressing the effect of corrupted
or irrelevant sources. It consistently outperforms both naive approaches of learning on all
available data directly or learning on the reference dataset only, for any amount and any type
of data contamination considered. We also observe its performance to be superior to multiple
baseline methods from robust statistics and robust distributed learning.

4.2 Related work

In addition to the related work discussed in Chapter |3) a number of works are relevant to the
approach to the adversarial multi-source problem taken in this chapter.

In particular, HMWG18a] also study learning with a reference dataset as a protection
against data corruption, but focus on a single untrusted dataset only and on convex objectives
and label noise respectively.

On the methodological level, we borrow techniques from the field of domain adaptation. To
measure the difference between data distributions, we use the same integral probability metric
as (ZL17]. The problem we study is related to multi-source domain adaptation, e.g.
|BDBC*10}, and to multi-task learning with unlabeled data [PL17]. In particular,
our Theorem [7is similar to a result in [ZZY13]. We refer to the paragraph after Theorem [?|for
a more detailed comparison. However, all these works focus on sharing information between
similar domains, in order to obtain better predictors for a target task, while we are interested
in applying such techniques for detecting untrustworthy sources of data and improving the
robustness of the learning procedure.

A relation between robustness and domain adaptation has been explored in the work of
[MS14], who use a property called algorithmic robustness to derive generalization bounds
for domain adaptation. Another related line of work is the one of [HMZ18], who
provide guarantees for a classifier learned on data from N domains on any target distribution
that is a mixture of the distributions of the sources. Domain adaptation techniques were also
used by [SHWH19], for improving the test-time robustness of predictive models to adversarial
examples.

4.3. Robust learning from untrusted sources

Given a small reference dataset, we want to leverage additional training data from multiple
untrusted sources in an optimal way, so that the obtained predictor performs well on a target
distribution. A naive approach will be to trust all data, merge it into one dataset and train
end-to-end to obtain a predictive model. Such an approach will intuitively be vulnerable to

1Code is available at https: //github.com/NikolaKon1994/Robust-Learning-from-Untrusted-Sources

38



Note that L(h;,P;) = L(h-i, P:) = n/2 for both 7 = 0,1. Moreover,

DP( (ho, Po) = Pox.ay) )~Po (ho(X )= = 1|A = = 0) - P(x,A,Y)~P, (ho(X) — 1|A = 1)|

 

 

 

n[2 1-Py-n/2
~|P,— ae 1—Po-n/24+n/2
_| 7 —2F)-7
~ |2Pp 21 — Py)
——. - l
-lad 1- Po)
-j- 0
2P)(1— Po)’
since 1 < 2P (1 — Po) by assumption. Furthermore, P??(h,, Po) = 1, so that 1?" (hy, Po) —
T?? (ho, Po) = IOP) Similarly,
rPP( (hy, Pi) = |P (X,A,Y)~P1 (hi (X ) = 1A = 0) — Poxayyers(t(X) = 1A = 1)
n[2 1-Py— 0/2

 

_PoeRe 1—Pyo-n/24+n/2

ee — Po)
and TPP (Ro, P1) =

Consider a (randomized) malicious adversary 4; of power a, that given a clean distribution
P;, changes every marked point to (23, 77, 1) with probability 0.5 and to (4,7, 1) otherwise.
Under a distribution P; and an adversary @;, the probability of seeing a point (23,7, 1) is
a(1-a)= im = a/2, which is equal to the probability of seeing a point (x3, 7i, 1). There-
fore, denoting the probability distribution of the corrupted dataset, under a clean distribution

P; and an adversary @;, by P’, we have

(l-a)(1—Py—n/2) ife=a,a=ly=1

(1 — a)(Py — 7/2) if t= a%,a=0,y=0
a/2 ife=273,a=i,y=1

Pi(x,a,y) = 4 a/2 ife=273,a=-7i,y=1
a/2 ife=a,a=—i,y=1
a/2 ife=aja=iy=1
0 otherwise

In particular, P) = P{, so the two initial distributions Py and P, become indistinguishable
under the siiveesarial manipulation.

Fix an arbitrary learner L : Unen(¥ x Ax VY)" > {ho, hi}. Note that, if the clean distribution
is Po, the events (in the probability space defined by the sampling of the poisoned train data)

{L(S?) =h} = {1°" C15"), Po —T?? (ho, Po) > oRG By

130

<1(

 

ye(h*, S*) — 9£(h*, S?)| > t/2).
Taking expectation with respect to S*

1 (ya(h*) — 76(h*, S?)| > PL (
<PS (

 

yah") — 75(h", S")] < t/2)
7a(h*, S*) — 96(h*, S?)| > t/2).

 

Now using Lemma 3|

Ph (

 

ya(h*) — 76(h*, S*)| < t/2) > PB

 

na(h*) — 92h", $4] < 8 log(6)

(l—a)P,n

Nl RAT

IV

1

il
a
so

1s eg. (3 a8
5H lra(h") — 746(h", S?)] 2 t) < PE

 

7E(h*, S*) — 46(h*, 8?)| > t/2).
Taking expectation with respect to S?

PS,(lya(h*) — 76(h*, S?)| > t) < 2PS, or(

 

7e(h*, S*) — 76(h*, 5?)| > t/2)

< 2P§, 5. (sup 7 (h, 5") — 76(h, S”)| > t/2).
heH

 

Step 2 Next we use the growth function of H and the symmetrization inequality (C.9) to
bound the large deviations of y¢(h) uniformly over 11.

Specifically, gives n points 71,...,%, € ¥, denote

Hory,...,an{(h(21),-..,h(an)) -h © H}.

Then define the growth function of H. as

 

Sy(n) = sup |Hay,..cn|-

Lyn

We will use that well-known Sauer’s lemma (see, for example, [BBLO3]), which states that
whenever n > d, Sx(n) < (ey?

Notice that given the two datasets S?, 51 and the corresponding sets of marked indexes, the
values of 7°(h,.9?) and 7(h, S) depend only on the values of h on S? and S! respectively.

8 log(6)
(l-a)Pon'

 

Therefore for any 1 > t > 2

pa (sup bra(h) — 96(h, 5°) > ‘
heH

 

7é(h, $1) — 76 (h, 8”)| > up)

7a(h, S*) — 6(h, S?)| > t/2)

< 2p4, gi | Sup
; heH

< 2Sy(2n)P%, 1 (

 

140

161

5.4. Lower bounds

 

L??(L£(5?)) > (2 (a), (+)) (5.13)

0

In the second case, when ;*~ > 2/)P, the adversary can force a constant increase in the
loss and make the classifier completely unfair, so that [??(£(S?)) = 1. These observations,
combined with the rates from the first case, indicate that unless a = o(P), the adversary
can ensure that the resulting model’s demographic parity deviation measure is constant. In
particular, if one of the protected groups is rare, even very small levels of data corruption can
lead to a biased model.

Next we show a similar result for equality of opportunity.

Theorem 9. Let 0 < a < 0.5 and Pyy < Py, < 1 be such that Pyy + Py, <1. For any input
set X with at least five distinct points, there exists a finite hypothesis space H, such that for
any learning algorithm L : Unen(¥ x A x Y)" + H., there exists a distribution P for which
P(A =a,Y =1) = Py, fora € {0,1}, a malicious adversary A of power a and a hypothesis
h* €H, such that with probability at least 0.5

R(L(S?),P) —R(h*,P) > min {os 2Pi9, (1 — Py — Pu}

and
g 1-—Pyo- P
1°P(£(S?), P) —I BOP (p* P) 3 min { o a uh ;

——___},
2(1 = a) Pig’ Pr

Discussion A similar analysis to the one after Theorem |8| applies here as well. In particular,
whenever 7 < 2min {Pio, 1 — Pio — Pui}, we obtain

EOp P\\ ws 7 EOp ~~)
LY"? (£(S?)) min Ly (W) > O(a+ ag (5.14)

LPOr(£(s?)) = (O(a),0 (5). (5.15)
Pro
The case when 7°. > 2min {Pio, 1 — Pio — Pir} leads to a constant equality of opportunity
deviation measure. If in addition we have that 1 — Pig — Pi, > Pio, a completely unfair
classifier will be returned. Consequently, if positive examples associated with one of the
protected groups are rare (that is, if Pio = P(Y = 1, A = 0) is small), then even very small
corruption ratios can lead to a biased model.

5.4.2 Hurting fairness without affecting accuracy

While the results above shed light on the fundamental limits of robust fairness-aware learning
against an adversary, models that are inaccurate are often easy to detect in practice. On
the other hand, a model that has good accuracy, but exhibits a bias with respect to the
protected attribute, can be much more problematic. This is especially true in applications
where demographic data is not collected at prediction time for privacy reasons. In this case
the model's bias might go unnoticed for a long time, thus adversely affecting one of the
population subgroups and potentially extrapolating existing biases from the training data to
future decisions.

We now show that such an unfortunate situation is indeed also possible under the malicious
adversary model. The following results show that any learner will, in some situations, be
forced by the adversary to return a model that is optimal in terms of accuracy, but exhibits
unnecessarily high unfairness in terms of demographic parity.

61

6.2. Related work

 

methods, e.g. [BNBR19] [CHS20] [KCT 20] [RFMZ20] [TYFT 20] [ZLM18], may be applicable to

our framework.

Fairness in ranking Fairness in ranking has so far received less attention than fairness in
classification. For an overview of recent techniques, see [Cas19]. Most existing works introduce
a particular fairness notion, often inspired by a specific application. One popular concept
is fairness of exposure [ZC20).
It states that the exposure received by a group of items or an individual item should be
proportional to its utility. Other works aim at ensuring sufficient representation of items from
different groups in the top-k positions of a ranking [ZBC*17].
Besides group fairness also fair treatment of individuals has been studied in the context of
ranking [YGS19]. In contrast to these works, we do not introduce a specific new
fairness criterion, but instead provide a formalism that allows for transferring existing fairness
notions from classification to ranking.

Perhaps closest to our work is the one of [SJI7], who introduce a number of fair ranking
definitions and draw parallels to equalized odds and demographic parity from fair classification.
However, they do not provide a formal framework for studying the correspondence between the
two setups, and do not study how to optimize these measures in a learning to rank context.
Moreover, their fairness measures concern fair rankings for a fixed query, which also holds for
the causal fairness notion of [WZWI8]. In contrast, our notion of ranking fairness is amortized
across queries, similarly to |BGW18].

Another line of work that adapts fair classification techniques to ranking is the one of pairwise
fairness IKVR19] INCGW20]. However, their classification task is the proxy commonly
employed by pairwise ranking methods, namely predicting which one of two items is more
relevant than the other for a given query. In contrast, we define fairness in relation to the
end decision of whether to return an item for a query or not, which is a more direct measure
of the real-world impact of the ranking system. |KZ19] and also study pairwise
fairness notions, but with the aim of learning fair continuous risks scores. Among other papers
considering broader notions of fairness in ranking, design learning algorithms that
can work with any fairness oracle. The framework however is limited to linear classifiers and
the authors do not propose any new fairness notions.

Two further distinctions between our work and previous methods are as follows. Firstly, few
prior works consider fairness in the context of learning, and those who do usually propose new
training techniques. Instead, the fairness regularizers we introduce can be combined with any
existing LTR procedure that can be formulated as learning a score function by minimizing a
cost function. Secondly, no prior work provides generalization guarantees for fair ranking as
we do.

Fairness in recommender systems For recommender systems, fairness can be studied with
respect to the consumers/users (known as C-fairness) or with respect to the providers/items
(known as P-fairness) [Burl17]. consider calibration and bias disparity within
recommender systems with respect to recommended items. In [BSOG18} (ZHC18}
various hybrid approaching for achieving both C-fairness and P-fairness are
presented. In contrast to our paper, these works are specific to collaborative filtering or
tensor-based algorithms and do not carry over to approaches based on supervised learning.

A concept from recommender systems related to demographic parity fairness is that of neutrality
KAAS12al], in which one aims to provide recommendations that are independent of a certain

73

5.

FAIRNESS- AWARE PAC LEARNING FROM CORRUPTED DATA

 

5.3. Preliminaries

In this section we formalize the problem of fairness-aware learning against a malicious adversary,
by giving precise definitions of the learning objectives and the studied data corruption model.

5.3.1 Fairness-aware learning

Throughout the chapter we adopt the standard group fairness classification framework, as
introduced in Section |2.3| We consider a product space V x Ax J, where 4 is an input space,
Y = {0,1} is a binary label space and A = {0,1} is a set corresponding to a binary protected
attribute (for example, being part of a majority/minority group). We assume that there is an
unknown true data distribution P € P(X x A x Y) from which the clean data is sampled.
Denote by H C {h: © — Y} the hypothesis space of all classifiers to be considered. We
denote the random variables corresponding to randomly sampled inputs, labels and protected
attributes as X,Y and A respectively.

PAC learning Adopting a statistical PAC learning setup, we are interested in designing
learning procedures that find a classifier based on training examples. Formally, a (statistical)
fairness-aware learner L : Unen(¥ x A x Y)" > H is a function that takes a labeled dataset
of an arbitrary size and outputs a hypothesis. Note that we again consider learning in the
purely statistical sense here, focusing on any procedure that outputs a hypothesis, regardless
of its computational complexity, and seeking learners that are sample-efficient instead.

In a clean data setup, the learner is trained on a dataset S° = {(a%, af, y)}"_, sampled i.i.d.
from P and outputs a hypothesis h := £(S°). The performance of a learner can be measured
via the expected 0/1 loss (a.k.a. the risk) with respect to the distribution P

R(h,P) = P(h(X) #Y). (5.1)

Group fairness in classification In (group) fairness-aware learning, an additional desirable
property of the classifier h = L(S°) is that its decisions are fair in the sense that it does
not exhibit discrimination with respect to one of the protected subgroups in the population.
Many different formal notions of group fairness have previously been proposed in the literature.
Here we consider two of the group fairness measures we discussed in Section 2.3.3] namely
demographic parity and equality of opportunity. Recall that demographic parity [CKP09],
requires that the decisions of the classifier are independent of the protected attribute, that is

P(h(X) = 1|A = 0) = P(h(X) = 1|A = 1). (5.2)

The second notion, equality of opportunity |HPS16], states that the true positive rates of the
classifier should be equal across the protected groups, that is

 

 

P(h(X) = 114 = 0,Y = 1) = P(A(X) = 1]A=1,Y = 1). (5.3)

In this definition, an implicit assumption is that Y = 1 corresponds to a beneficial outcome
(for example, an applicant receiving a job), so that this fairness notion only considers instances
where the correct outcome should be advantageous.

In practice, it is rarely the case that a classifier achieves perfect fairness. Therefore, we will
instead be interested in controlling the amount of unfairness that h possesses, measured via

54

APPENDIX

Proofs from Chapter 3

In this chapter we present the proofs of all results from Chapter 3} In particular, in Section
|A.1|we prove our upper bound result and its corollaries. We then move on to the lower bounds
and prove Theorems [5| and (6| {6| in Sections |A.2| 2) and |A |A.3] respectively.

A.1 Proof of Theorem /4 and its corollaries

Theorem 4. Let N,m,k € N be integers, such that k € (N/2,N]. Leta = * < } be
the proportion of corrupted sources. Assume that H has the uniform convergence property
with rate function s. Then there exists a learner £L : (X xy)‘*™ + H with the following
two properties.

(a) Let G be a fixed subset of [N] of size |G] =k. For S' = {S',..., Sy} °" D, with
probability at least 1 — 5 over the sampling of S’:

fl . 6 )
R(L(A(S ))) — min R(h) < 2s(km, 3? Sa) + 6a ne s(m, aN? Si), (A.1)

uniformly against all fixed-set adversaries with preserved set G, where S = {S),..., Sw} =
as’) is the dataset modified the adversary and Sg = UjeqS; is the set of all uncorrupted
data.

(b) For S’ ={S\,..., Sy} °" D, with probability at least 1 — 6 over the sampling of S':

fi . . el, 9 o.
R(L(A(S ))) — min R(h) < 2s(km, ay) + 6a se s(m, aN? Si), (A.2)

N
a(t
uniformly against all flexible-set adversaries with preserved size k, where S = {S),..., Sw} =
A(S') is the dataset returned by the adversary, G is the set of sources not modified by
the adversary and Sg = UieS; is the set of all uncorrupted data.

Proof. Denote by Si = {(2j1,)1),---; (Chim: Yim) } for i= 1,...,N the initial datasets and
by S; = {(@i, Yi), +++, (Simms Yim)} fori =1,...,.N the datasets after the modifications of
the adversary. As explained in the main body of the paper, we denote by:

1 m
moh (2:5), Yi) (A.3)

107

Since the above holds for any S’ € S, it also holds in expectation, conditioned on S’ € S:
Egres (R(hs, hi) + R(hs, h2)) > 4e. (A.32)

Therefore, Egres (R(hs, hi)) > 2€ for at least one of i = 1,2. Take f to be fy, if hy satisfies
the inequality, and hy otherwise. Conditioning on {R(hs, f) > e} and using R(hs, f) <
Pp(a 4 x) = 4e:

 

 

 

 

 

 

 

 

2 < Egres (R(hs, f)) = Esres (R(hs, f)|R(hs, f) = ©) Psves (R(hs, f) = ©)

+ Esres (R(hs, f)|R(hs, f) < €) Psves (R(hs, f) < €)

< 4eP seg (R(hs, f) 2 €) + Psres (R(hs, f) < €)

=€4+ 3ePses (R (hs f= €).
Hence,

. 1 il
Pres (R(hs, f) = €) = 3c (2e—€) = 3 (A.33)

Finally,

Ps (R(L(A°(S'))) 2 6) = Ps (Rihs, f) 2 €) = Pores (R(hs. f) 2 €) Ps (S'€ S)
13
~ 320
_ il
==.

 

 

 

 

A.3 Proof of Theorem (6!

Theorem 6. Let Hc {h: X — Y} be a hypothesis space, let m and N be any integers and
let G be a fixed subset of [N] of sizek € {1,...,N—1}. Let S’ € (X x y)"*” be drawn
i.i.d. from D. Then the following statements hold for any multi-source learner L:

(a) Suppose that H is non-trivial. Then there exists a distribution D on X with minney R(h) =
0, and a fixed-set adversary A with index set G’, such that:

Psy (R(cas) > =) > a (A.34)

(b) Suppose that H has VC dimension d > 2. Then there exists a distribution D on X x Y
and a fixed-set adversary A with index set G’, such that:

‘4 ’ d Qa 1
Ps (leas )) -— min R(h) > \ T80Nm + “| > ay (A.35)

In both cases, a = a is the power of the adversary.

115

APPENDIX

Here we present the proofs of all results from Chapter |5| The chapter is structured as follows.

=» Appendix |C.1) contains the proofs of all lower bounds results. Section |C.1.1 focuses on
the Pareto lower bounds. Section |C.1.2| contains the proofs for the lower bounds on
fairness, given that accuracy is kept optimal.

 

Appendix [C-2] contains the complete proofs of our upper bound results. In particular,
Section |C.2.1) explains the notation and introduces the classic concentration tools that
we will use. In Section |C.2.2)a number of concentration results under corrupted data for
the demographic parity and equal opportunity fairness notions are shown. Finally, Section
|C.2.3| gives the formal proofs of all upper bound results, building on the concentration
inequalities from the previous section.

 

C.1 Lower bounds proofs

In the proofs of our hardness results we use a technique from called the method of
induced distributions. The idea is to construct two distributions that are sufficiently different,
so that different classifiers perform well on each, yet can be made indistinguishable after the
modifications of the adversary. Then no fixed learner with access only to the corrupted data
can be “correct” with high probability on both distributions and so any learner will incur
excessively high loss and/or exhibit excessively high unfairness on at least one of the two
distributions, regardless of the amount of available data.

The proofs of the four results are structured in a similar way, but use different constructions
of the underlying learning problem, tailored to the fairness measure and the type of bound we
want to show.

C.1.1 Pareto lower bounds proofs

Theorem 8. Let 0 < a < 0.5,0 < Py < 0.5. For any input set X with at least four
distinct points, there exists a finite hypothesis space H, such that for any learning algorithm
L: Unen(¥® x A x Y)" + H, there exists a distribution P for which P(A = 0) = Py, a
malicious adversary A of power a and a hypothesis h* € H, such that with probability at least

123

Studying the function f(x) = ie + rr ae we see that

 

cbf. 1 1
Plz) =2 (a —a)Pon+3an- 2x)? ((lL—a)Pon+ a) ,

Note that By < By + By < eo so we may assume 0 < x < 3o Therefore, both (1 —
a) Pon + 3an — 2a > 0 and (1—a)Pon + 2x > 0. Therefore, f’(x) = 0 if and only if
(1 — a) Pon + 8an -— 24 = (1 - oi Ean + 2a, that is, r = *2n. Moreover f!(x) < 0 if
x € (0, 22n) and f’(x) > Oif x € (32n, 22n). Therefore, f(x) is minimized at x = 2@n and
so

 

1
Ag+ Ai <2-(1 Pon
ph. & ta) (< —a)Pon+2Bo  (1—a)Pon + 3an — om)

 

 

 

 

1 1
<2—(1—a)Pon (« a)Pon + *2n — (1— a) Pon + 3an =]
_ 6a
~ 21 —a)Po + 3a

6a a
=Pjt3a P/3sta

 

and hence (C.2) holds in this case as well. Since the derivations hold for any classifier h € H,
the result follows.

 

 

 

 

For the rest of the section, we keep the notation A,(h) = |7?(h) — 7(h)| for a € {0,1} and
DP _ _2a

A ~~ Po/3+a°

Next we use the previous result and the technique of for proving concentration

results about conditional probability estimates to bound the probability of a large deviation of

TP (h) from TP? (h), for a fixed hypothesis h € H.

Lemma 3. Let h € H be a fixed hypothesis and P € P(X x Ax Y) be a fixed distribution.
Denote P, = P(A = a) fora € {0,1}. Let @ be any malicious adversary and denote
by P4 the probability distribution of the poisoned data S?, under the random sampling

of the clean data, the marked points and the randomness of the adversary. Then for any
n> max { S!2a6/2) 5 12tea(6/) | and 6 € (0,1)

Po?

 

 

pa (rere —DPP(h)] < APP +2 an >1-6. (C.3)

Proof. Again we write 7? = 7?(h),C} = C}(h), etc. since h is fixed. First we study the
concentration of the clean estimate ch around 7. To this end, denote by S¢ = {i : a? =
a,i ¢ 3B} the set of indexes of the poisoned data for which the protected group is a and the
corresponding point was not marked for the adversary. Notice that S¢ is a random variable

and that |.S¢| = Cy. Since n > eee for both a € {0,1}, we have

P8 (/4¢ — yal > t) = = TPA (lag — al > #192) P(SS)

5. FAIRNESS-AWARE PAC LEARNING FROM CORRUPTED DATA

 

and
ya(h) = P(h(X) = 1A =a),

so that P??(h) = |42(h) — 7?(h)| and T??(h) = |yo(h) — 1(h)|. Note that y?(h) is an
estimate of a conditional probability based on the corrupted data. We now introduce the
corresponding estimate that only uses the unknown clean subset of the training set S?

Yah) = EE Hal = 0,7 ¢ B}

Bounding the effect of the adversary First, we bound how far the corrupted estimates
7?(h) of ya(h) are from the clean estimates 7“(h), uniformly over the hypothesis space H:

8log(4/d) 12log(3/6)
(1-a)Po? a

 

 

Proposition 2. /fn > max { }, we have

 

2a
pa (sup cos — wh) + T(r) — W(2))) 2 ) <6.
heH. 3 +a

Informally, this proposition allows us to connect the corrupted estimate PP (h) with the
corresponding ideal clean estimate '°(h) = |y6(h) — 7f(h)]-

Bounding the deviation of the clean data estimate Secondly, a technique used by
and for proving concentration of fairness measures is used to derive a
concentration result for the clean estimates 76(h), around the true population values 7,(h).
This, together with Proposition 2 allows us to bound the gap between the corrupted estimate
P??(h) and the true population value [??(h), for a single hypothesis.

Making the bound uniform over H Finally, the bound obtained is made uniform over H.
For this, we use the classic symmetrization technique for proving bounds uniformly
over hypothesis spaces of finite VC dimension. However, since the objective is different from
the 0-1 loss, care is needed to ensure that the argument goes through, so the proof is given in
full detail in the supplementary material.

Once a uniform bound on the deviations of the corrupted fairness estimates from the true
underlying population values is obtained, the results of Theorems |{12| {13} [14] [15] follow similarly
to most classic ERM results.

Proof of Theorem {16 Similarly to the other results, the proof of Theorem {16] first links the
corrupted estimates 7}, to their clean counterparts and then uses the clean data concentration
to study the behavior of the corrupted estimates. However, an important tool that allows us
to obtain the fast statistical rates, is a set of multiplicative concentration bounds on the 7},
estimates. Full details and a complete proof can be found in the supplementary material.

5.6 Summary and discussion

In this chapter we explored the statistical limits of fairness-aware learning algorithms on
corrupted data, under the malicious adversary model. Our results show that data manipulations

68

 

 

P (IPPO"(f) — TP £,,8)| > 2t) < 4exp (- aa + 8exp (-}

2: 2.
< 12exp (- Nr ) (D.8)

2
PNv “66 PNv?
2X _
7 J noe 2
(D.

We will use these in particular to prove the following symmetrization lemma:

 

Similarly, from the proof of Lemma 9}

 

 

 

P(|E?(f) —LPP(f,S)| > 2t) < 2exp (-) +4exp (-

Lemma 10. For any 1 >t > 4/22)

Ps (sure — pP(,$)) > ‘ < Ps (sure. s') —F#"(f, 8)) > v2)
SEF fEeF
(D.10)

For anyl >t > 4,/ 2180s) 24).

Ps (suvcre™) — PFW F,§)) > ) < 2s (sures 5") —P0"(F, §)) > v2)
SEF SEF
(D.11)

For any 1 >t> 4/7):

Ps (sup(t?"(4) —P"(F,8)) > ¢) < 2539 (sup(rP"(Fs") — FPF.) > #12).
[6F feF
(D.12)
Proof. We show the result for the equality of opportunity fairness measure, the rest follow in
an identical manner.
Let f* be the function achieving the supremum on the left-hand side || Note that:
1(DPOP( f°) — PPOP(f*, 8) > t)(TPOP(f*) — PPC", 8") < #/2)
1(DPOP(F*) — PEOP(#*, 8) > EA TPOP(F*, 5") — PPOP(F*) > 4/2)
< 1(TEP(f*, $4) — PEP +, 8) > 1/2).
Taking expectation with respect to S’:
L(TPOP(f*) — PPOP(F*, S) > t)Psr(TPOP(f*) — PPOP(f*, 8") < t/2)
< Ps (PPOP(f*, 5’) — PPOP(F*, 8) > t/2).

Now using ((D.7):

EOp/ ¢* EOp/ ¢* a ?N7? 1
Psy (EPOP(f*) — POP(f*, 8’) > t/2) < Gexp(-—5—] <5.

1If the supremum is not attained, this argument can be repeated for each element of a sequence of
classifiers approaching the supremum

 

158

C.2.3. Upper bound theorems - proofs
We are now ready to present the proofs of the upper bound results from the main body of the
paper.
Upper bounds on the \-weighted objective
First we prove the bounds for the A-weighted objective.
Bound for demographic parity Let \ > 0 be fixed. Recall our notation for the -weighted
objective:
LR? (h) = R(h) + AVP? (h).
Suppose that a learner LP? : U%,(¥X x Ax Y)" > His such that
£PP(S?) € argmin(R?(h) + TP? (h)) for all S”.
heH
That is, £2? always returns a minimizer of the \-weighted empirical objective. Then we have
the following:

Theorem 12. Let H. be any hypothesis space with d = VC(H) < oo. Let PE P(X x Ax y)
be a fixed distribution and @ be any malicious adversary of power a < 0.5. Denote by pa
the probability distribution of the poisoned data S?, under the random sampling of the clean
data, the marked points and the randomness of the adversary. Then for any 6 € (0,1) and

 

n> max { S8Us/?), 1leg(2/) 4}, we have
PH (LEP(£PP(S")) < min ERP(h) + ARP) > 1-6,
where")
DP DP, @ d d
APP = 304 2\APP 4.6 (4/ = + yf
n Pon
and

2a a
apr“ _-0().
Po/3 +a Py

Proof. By the standard concentrations results for the 0/1 loss (see, for example, Chapter 28.1

in [SSBD14])

oe llog( 4 2log(16/6
P (x IR°(h) — R(h)| > 9, | Sloat) + 21og(16/6) é e
heH n 4

where R°(h) = +2, 1(h(x3) & yf) is the loss of h on the clean data. Since the total

12 log(4/6)

number of poisoned points |98| ~ Bin(n, a) and since n > —S

p@ sup |R°(h) — R?(h)| > 3a < Pa (is > sn) <emPc o
hEeH 2 2 4

4the O-notation hides constant and logarithmic factors

147

Robustness and Fairness in Machine
Learning
by
Nikola Konstantinov

February, 2022

A thesis submitted to the
Graduate School
of the
Institute of Science and Technology Austria
in partial fulfillment of the requirements
for the degree of
Doctor of Philosophy

Committee in charge:

Prof. Edouard Hannezo, Chair
Prof. Christoph H. Lampert
Prof. Dan Alistarh
Prof. Ingo Steinwart

Institute of
Science and
Technology
Austria

Lemma 6. Under the setup of Lemma 5) assume additionally that H. has a finite VC-dimension

d. Then for any n > max { ee, : a(6/5) $} and 6 € (0,1)

 

 

a 2d log (7) + 2log(48/5
pa, sup |[P20?(h) — T#?(h)| < AY? + 16 2dlog("Z) + 2log(48/4)

>1—6.
nen (1 —a)Pion =

(C.17)

Finally, we prove multiplicative bounds and claims in the case when P(h(X) = 1|A=0,Y =
1) = P(A(X) = 1][A = 1, Y = 1) = 1 (which holds for example when h(X) = Y almost
surely). These will come in useful for proving the component-wise upper bound with fast rates.

We will be interested in the estimate

m  U{h(2?) =0,a? =a,y) = 1}
” {a =a,y? = 1}

 

 

Valh)

of 7,,(h) = P(h(X) = 0|A = a,Y = 1). Again, we also introduce the corresponding clean
data estimate C?(h) = 0%, Wi: h(a?) =0,a? =a, y? = 1,1 € P} and

C9,(h) Dy Ui: h(x?) = 0,0? = a,y? = 1,1 ¢ B}
Cia ier L{t sa? =a,yf, = 1,1 ¢ P} :

 

 

 

Denote also
Aia(h) = |¥halh) — Yfalh)I

We only show non-uniform bounds for a fixed h € H here, so we omit the dependence of
these quantities on h. We have:

Lemma 7. Let P€ P(X x A x Y) bea fixed distribution and let h € H be a fixed classifier.
Denote P,, =P(A=a,Y = 1) forae {0,1}. Let @ be any malicious adversary and denote
by P4 the probability distribution of the poisoned data S?, under the random sampling of the
clean data, the marked points and the randomness of the adversary. Then:

(a) For any n > 0 and any n,6 € (0,1)

pa (Wa > (1+)F1q + Aa) < exp (- her) + exp (-jra = a) Ping) .
(C.18)
and
_ 1—a)Pyan 1
P* (4%. < (1-)71a — Ata) < exp (-—ore) +exp Gust = a) Pica)

(C.19)

 

 

(b) Assume further that P(h(X) = 0|A = 0,Y = 1) = P(A(X) = O|A=1,Y = 1) = 0.
Then for any 6 € (0,1) andn > max { Fee) Bles/s) \

PA (5% + 7h > APP) <5 (C.20)

145


5.

FAIRNESS- AWARE PAC LEARNING FROM CORRUPTED DATA

 

Moreover,

PP (hi, Pi) — PP? (hi, P;) = for both i = 0,1. 5.9

(haP) ~LP" (hi P) = spp. for both i =0, (5.9)
Now what the adversary does is to use all of the marked data to insert points with inputs x3
and 24, but with flipped labels and protected attributes. Then, since the points with inputs x3
and «4 in the original data are sufficiently rare, the adversary manages to hide which one of
the two distributions was the original one.

Specifically, consider a (randomized) malicious adversary @; of power a, that given a clean
distribution P;, changes every marked point to (3, 77,7) with probability 0.5 and to (x4, 7, 72)
otherwise. Under a distribution P; and an adversary @;, the probability of seeing a point
(v3,4,72) is }1—a) = ny = a/2, which is equal to the probability of seeing a point
(a3, i, 7). Therefore, denoting the probability distribution of the corrupted dataset, under
a clean distribution P; and an adversary @;, by P’, one can verify that Pj = P{, so the two

initial distributions Py and P; become indistinguishable under the adversarial manipulation.

The proof concludes by formalizing the observation that any fixed learner L : Unen(¥ x
Ax Y)" > {ho, hi} will perform poorly on at least one of the distribution-adversary pairs
(P;,4;), since the resulting corrupted data distributions are the same, but the optimal classifiers
differ.

 

 

 

 

Discussion Our hardness result implies that no learner can guarantee reaching a point on
the Pareto front in a PAC learning sense, even for a simple family of hypothesis spaces, namely
the finite ones. To prove the theorem we explicitly construct a hypothesis space that is not
learnable against the malicious adversary. As discussed in Section 5.3.4] a constructive proof
is necessary here, because fairness can be trivially satisfied on some hypothesis spaces, for
example those that contain a constant classifier, which is fair under any distribution and
against any adversary.

We now analyze the bounds and their behavior for small values of a and P. First assume that
Toa < 2PoP1, which in particular is the case whenever 2a < P. Then under the conditions

of the theorem, with probability at least 0.52]

R(L(S?)) — R(h*) > 2(a) (5.10)
and
PP (£($?)) — PPP(h*) > Q (=) (5.11)

The lower bound on the excess loss (6.10) is known to hold for any hypothesis space as shown
by [KL93] (see also the discussion in Section (2.2.3). What Theorem 3) adds to this classic
result is that for certain hypothesis spaces: 1) the learner can at the same time be forced to
produce an excessively unfair classifier; 2) the fairness deviation measure I?” can be increased
by Q(a/Pp). Note that these results hold regardless of the sample size n.

Equations (5.10) and (5.11 immediately imply the following lower bounds on Ly and L??:

DP P)) — min £PP(h) > >)
LRP(L(S")) ~ min £RP(h) > 0 (+r (5.12)

1We use the Q-notation for lower bounds on the growth rates of functions.

60

5.3. Preliminaries

 

One particular situation that we will study in which a component-wise optimal classifier h*
exists, is within the realizable PAC learning model with equality of opportunity fairness. Indeed,
whenever a classifier h* € H satisfies P(h*(X) = Y) = 1, we have that both R(h*) = 0 and
PPOP(h*) = 0 and so L#°?(£(S?)) = GPOP(L(S?)).

5.3.4 The limits of fairness-aware learning against an adversary

Lower and upper bounds analysis Over the next sections we will be showing lower and
upper bounds on L)(L(S?)) and L(L(S?)), that is, the risk and the fairness deviation measure
achieved by the learner when trained on the corrupted data. Our lower bounds can be thought
of as hardness results that describe a limit on how well the learner can perform against the
adversary. These are based on explicit constructions of hard learning problems and adversaries
that demonstrate these limitations. Our upper bounds complement the hardness results by
constructing learners that recover a classifier with guarantees on fairness and accuracy that
match the lower bounds, for a wide range of learning problems and adversaries.

As discussed in Section |2.2.2| crucial in these results is the ordering of the quantifiers. These
matter not only for the comparison between the upper and the lower bounds, but also for
the sake of formalizing the powers of the adversary and the learner. Recall that the learner
only operates with knowledge of the corrupted dataset. At the same time, the adversary is
assumed to know not only the clean data, but also the target distribution and the learner.
Therefore, our lower bounds are structured as follows:

 

For any learner £ there exists a distribution P and an adversary A,
such that with constant probability ...

Note in particular that the adversary can be chosen after the learner is constructed and together
with the distribution and it can therefore be tailored to their choice. At the same time, our
upper bounds read as:

There exists a learner £, such that for any distribution P, any adversary @ and any 6 € (0,1),
with probability at least 1—6 ...

Since the learner is fixed before the distribution and the adversary are, it has to work for any
such pair.

We note that all probability statements refer to the randomness in the full generation process of

the dataset S?, that is the randomness of the clean data, the marked points and the adversary.

For a fixed clean data distribution P and a fixed adversary @, we denote the distribution of S?
a

as P*.

Role of the hypothesis space _Learnability in our setup can be studied either as a property
of any fixed hypothesis space, or as a property of a class of hypothesis spaces, for example
the hypothesis spaces of finite size or finite VC dimension. However, one can easily see that
for certain hypothesis spaces fairness can be satisfied trivially. For example, whenever H.
contains a classifier that is constant on the whole input space (that is, always predicts 1 or
always predicts 0), a learner that returns this constant classifier, regardless of the observed
data, will always be perfectly fair with respect to both fairness notions, under any distribution

57

3.4. On the sample complexity of adversarial multi-source learning

 

that we define and discuss below. It guarantees that if two sources are close to each other
then the difference of training on one of them compared to the other is small.

To identify the trusted sources, the algorithm checks for each source how close it is to all
other sources with respect to the discrepancy distance. If it finds the source to be closer than
a threshold to at least half of the other sources, the source is marked as trusted, otherwise it
is not. To show that this procedure does what it is intended to do it suffices to show that two
properties hold with high probability: 1) all trusted sources are safe to be used for training, 2)
at least all uncorrupted sources will be trusted.

Property 1) follows from the fact that if a source has small distance to at least half of the other
datasets, it must be close to at least one of the uncorrupted sources. By the property of the
discrepancy distance, including it in the training set will therefore not affect the learning very
negatively. Property 2) follows from a concentration of mass argument, which guarantees that
for any uncorrupted source its distance to all other uncorrupted sources will approach zero at
a well-understood rate. Therefore, with a suitably selected threshold, at least all uncorrupted
sources will be close to each other and end up in the trusted set with high probability.

Discrepancy Distance. For any dataset S; € (4 x)”, let

= 1 en(a), 7) (3.16)

Me weSi

be the empirical risk of a hypothesis h with respect to the loss ¢. The (empirical) discrepancy
distance between two datasets, 5; and Sj, is defined as

dyu( Si, $3) = sup (|Ri(h) — Rj(h))). (3.17)
This is the empirical counterpart of the so-called discrepancy distance, which, together with
its unsupervised form, is widely adopted within the field of domain adaptation
[MM12]. Intuitively, the discrepancy between the two (empirical) distributions is
large, if there exists a predictor that performs well on one of them and badly on the other.
On the other hand, if all functions in the hypothesis class perform similarly on both, then the
distributions have low discrepancy. Therefore, the discrepancy is used to bound the maximum
possible effect of distribution drift on a learning system.

Unlike other notions of distance between distributions, such as the total variation distance
or the Kullback-Leibler divergence, the discrepancy is easy to estimate from samples and is
not overly strict, since it depends on the learning setup. As shown in |BDBC*10},
for randomly sampled datasets, the empirical discrepancy concentrates with well-understood
rates to its distributional value, in particular to zero, if two sources have the same underlying
data distributions. The empirical discrepancy is well-defined even for data not sampled from
a distribution, though, and together with the uniform convergence property it allows us to
bound the effect of training on one dataset rather than another.

Step 2. Let Sr = User S; be the output of the filtering algorithm, i.e. the union of all trusted
datasets. Then, for any h € H, the sone risk over Sp can be written as

7(h) S-Ri( (h) (3.18)
a eT

We need to show that training on Sy, e.g. by minimizing Ry (h), with high probability leads
to a hypothesis with small risk under the true data distribution D.

33

Now since Byy + By < 22n, Cio > 45% Pron, Gu > “3° Pin hold, we get

 

 

 

 

Bio By
Aw+ Aus Cio + Bio * Cut Bu
2—— Bio of Bu
=" Pion + By =" Pun + By
<i Bio tr Bu
=" Pion + Bio =" Pion + By
_ Bio ii 452Pion
52 Pion + Bio ES Pion — By + (Bi + Bu)
<i Bio 41-7 45°Pion ;
"Pion + Bio "Pion — By + Hn

 

 

 

 

 

1 1
2-(1 Pion
Q—a)Fisn C — a)Pion + 2Bio + (1 —a)Pion + 38an — ome)
The same argument as in Proposition 12] shows that this is maximized at Bip = 32n, and so

Bro By

AitA ——_ + ——_——_ G15
” ns = Ga + Bio * is + By ( )

 

 

ll 1
<2-(l-a)P, 4
2) wa a) Pion 4 Bain (1—a)Pion + 3an =)
2a
< .—_—..
~ Pyo/3+a

 

 

 

Since this holds for any arbitrary hypothesis h € H, the result follows.

 

Denote the irreducible error term for equality of opportunity by AZO? — We then

have the following bound for a fixed h € H:

 

Pi ae

Lemma 5. Let h € H be a fixed hypothesis and D€ P(X x Ax Y) be a fixed distribution.
Denote Pi, = P(A = a,Y = 1) fora € {0,1}. Let @ be any malicious adversary and
denote by P4 the probability distribution of the poisoned data S”, under the random sampling

of the clean data, the marked points and the randomness of the adversary. Then for any

n> max { Fee) cet and 6 € (0,1)

 

< log(16/6)
a EOp(},\ __ PEOp < A£Op g a
P (i (h) — P2P(h)| < APP 4.2 ala) -) BIJ (C.16)

Proof. The root! is pacily the same as the one of Lemma 3 BI, but with conditioning on
Se, = {i: a? =a,y? = 1,1 ¢ } (the set of indexes of the poisoned data for which the
protected group is a, the label is 1 and the corresponding point was not marked for the
adversary) instead.

 

 

 

 

The same argument as in Lemma |4] gives a uniform bound over the whole hypothesis space,
provided that H. has a finite VC-dimension d := VC(H):

144

In particular, Pj = P{, so the two initial distributions Po and P; become indistinguishable
under the adversarial manipulation.

Fix an arbitrary learner £L : Unen(¥ x Ax VY)" > {ho, hi}. Note that, if the clean distribution
is Po, the events (in the probability space defined by the sampling of the poisoned train data)

{L(S”) = hy} = {r#°"(c(S"),Po) — PF0P( hy, Py) > ate (1 _ Fi)

are all the same. Similarly, if the clean distribution is P,

{£(S®) = ho} = {£#°"((S"),P,) ~T*P(hy, Py) > A (1-52) b.

Therefore, depending on whether we choose Pp or P; as a clean distribution, we have

Pose, (T#2%(C(S"),Po) — P®°°(o,Po) = 5 (1- PY) = Porazg (£(5") = in)
2 2Pio Pu °
and
EOp p EOp ) Po)) _ p\
Pspvp r (L(S ),P:) -T (hi, Pi) > —— (J -— —— = Psevp (L(S ) = ho)
‘ 2Pio Pu :

Finally, note that P, = P4, so that either Psp. (L(S?) = hy) > 1/2 or Psp (L(S?) = ho) =
1/2. Moreover, L(£(S?), P;) = L(h;, P;) = 7/2 holds for both i € {0, 1}, for any realization
of the randomness. Therefore, for at least one of 7 = 0,1, both

L(L(S?), Pi) = L(hi, Pi) = ;

and

 

P; Qa P,
T#r(£($?), P,) — P2?(h;,P;) > n (2 2) _ (1 2)
(£(5"),Pi) ( )2 2Pio Pu 2Pi9(1 — a) Pu

both hold with probability at least 1/2. This concludes the proof in the first case.

Case 2 Now assume that 7°, > 2Pio. Then denote by a; the unique number between
(0,0.5), such that ia = 2P\o, and note that a; < a. Then repeat the same construction as
in Case 1, but with 7, = =a and an adversary that with probability a, /a does the same as

 

 

l-a
in Case 1 and leaves a marked point untouched otherwise.

Then the same argument as in Case 1 gives that for some i € {0,1}, with probability at least
0.5, both of the following hold
L(L(S?),P;) = L(hi, Pi) = 4 = Pro

and

 

P, P,
p2op P) P,) —[£OP(h, P,) > ™m _ | (1 ) _ 10
(£(S?),P,) — P8°%(h,,B,) > SB = sh

 

 

 

 

This concludes the proof of Theorem

133

5.5. Upper bounds

 

Bound for demographic parity Let h € H be a fixed hypothesis. We consider the

following natural estimate of ['??(h), as given in equation (5.4), based on the corrupted

dataset S? = {(x?,a?,yP)}™):

MU A(a?) =1,a =0} SE, h(a?) = 1,0? = 1}
Da L{a? = Of ier L{a? = 1}

 

P??(h) = (5.16)

with the convention that Q = 0 for the purposes of this definition. We also denote the empirical
risk of hon S? by RP(h) = 40, U{h(w?) A yf}.

Now let \ > 0 be a fixed trade-off parameter. Suppose that the learner LEP : U(X x Ax
Y)" > H is such that

LPP(S?) € argmin(R?(h) + TP? (h)), — for all S?.
hEeH

That is, £2? always returns a minimizer of the A-weighted empirical objective. Then the
following result holds.

Theorem 12. Let 1. be any hypothesis space with d = VO(H) < oo. LetPE P(X x Ax)
be a fixed distribution and let A be any malicious adversary of power a < 0.5. Denote by pa
the probability distribution of the corrupted data S?, under the random sampling of the clean
data, the marked points and the randomness of the adversary. Then for any 6 € (0,1) and

- 8log(16/5) 12log(12/5) d *
n> max { a)Po a sh, we have:

 

Pa (LEP(£R"(S")) < min ERP(h) + ARP) > 1-6,

x d d
DP _« DP e |——
Ay” =3a+ (2A 0(four in)

2a a
PP a me (+).
bra 2 Po

where2|
and

This result shows that for any H of finite VC dimension, any distribution P and against any
malicious adversary @ of power a, the learner £2" is able, for sufficiently large values of the
sample size n > Q((Py/a)), to return with high probability a hypothesis such that

DP( pDP(ap\\ _ win FDP ao
LRP(CRP(S")) — min Lf (n) <O(a+rz). (5.17)

Note that these rates on the irreducible error term match our lower bound from Theorem
[8] and Inequality (5.12). Indeed, the hardness result shows that no algorithm can guarantee
better error rates than those in (6.17) on the family of finite hypothesis sets and hence also
on the hypothesis sets with finite VC dimension.

2The O-notation hides constant and logarithmic factors.

63

Crucially, the resulting (unordered) collection is the same no matter if the original labelling
function was hy or hg. In particular, £(S) will be the same in both cases.

In the case when C, > aN, the adversary leaves the data unchanged, i.e. S = S".

Finally, we argue that under the event C;, < aN and the chosen adversary, the learner would
incur high loss and show that this implies the result in (A.34). Let S be the set of all datasets
in (& x Y)**™, such that C, < aN holds. We just showed that Ps(S’ € S) > & and that
whenever S’ € S, £(@*(S’)) is independent of whether the original labelling fiinction was hy
or he.

 

Now the proof proceeds just as in Theorem 5] Consider a fixed set S’ € S and let S = @*(S")
and hs = L(S). Denote by R(hs, f) = Po(hs(x) 4 f(x) Nx F x1) and note that
Rihs, f) < Po(hs(x) 4 f(x)) = R(L(A*(S’))). Notice that:

R(hg, hi) + R(hs, he) = Yo Vases ¢ha(ei) Leper P(@i) + SS Uns (ei Aho (ai) Lai ¢er P(ai)

i=1.2 i=1,2
= Vns(w2)#hr (2) 4€ + Uns (2) Aho(w2) 4€
= 4e,

where we used that hi(w2) = 1 = —h2(x2) and that hg is independent of the underlying
labelling function.

Since the above holds for any S’ € S, it also holds in expectation, conditioned on S’ € S:
Esres (R(hs, hy) + Rihs, hg)) > 4e. (A.40)

Therefore, Egres (R(hs, hi)) > 2e for at least one of i = 1,2. Take f to be hy, if hy satisfies
the inequality, and hy otherwise. Conditioning on {R(hs, f) > e} and using R(hs, f) <
Pp(a 4 2) = 4e

2€ < Egres (R(hs, f)) = Esres (R(hs, f)|R(hs, f) 2 ©) Peres (R(hs, f
Egres (R(hs, f)|R(hs, f) < €) Psres (R(hs, f)
4€P ses (R(hs, f) 2 ( + Poses (R(hs, f) < €)
=€ + 3ePsres (R(hs, f) = €).

 

 

 

 

 

 

 

> €)
<

5)

+

 

lA

Hence,

Pores (R(hs, f) > 6) > =< (2-6) = ; (A.41)

Finally,

Ps (R(L(A°(S’))) > €) > Ps (R(hs, f) = €)
2 Psres (R(hs, f) 2 €) Ps (S' € S)
13
* 320
_ il
= 5°

117

CHAPTER

Introduction

Machine learning algorithms have shown great potential in recent years by improving substan-
tially upon the state of the art in multiple real-world applications of computer science, for
example in image recognition [HZRS16], language understanding and protein struc-
ture prediction [JEP*21]. Due to this outstanding performance of learning-based algorithms,
there is an increasing amount of interest by practitioners in producing such models tailored to
their purposes.

Because of the popularity of machine learning methods, it is becoming increasingly important
to understand the impact of such learned components on automated decision-making systems
and to guarantee that their consequences are beneficial to society. In other words, it is
necessary to ensure that machine learning is sufficiently trustworthy to be used in real-world
applications.

One of the key challenges towards building reliable machine learning models is that of
ensuring robustness against training data corruption. Indeed, previous work has shown that
modern machine learning models are not robust to problems in the train data. In particular,
issues varying from label noise through model misspecifications to worst-case train data
corruptions (a.k.a. data poisoning) can easily affect the performance of learned classifiers
leading to poor model performance at test time. In addition,
several classic hardness results state that there is no learning algorithm that
can provably recover an optimal decision rule in the presence of data manipulations, unless
restrictive assumptions about the type of corruptions are made.

 

These issues are especially relevant from a present perspective, since modern machine learning
methods are particularly data hungry. Therefore, when training models practitioners often rely
on data collected from various external sources, e.g. from the Internet, from app users or via
crowdsourcing. Naturally, such sources vary greatly in the quality and reliability of the data
they provide. Moreover, the structure of the resulting training dataset is different than what is
assumed in classic robust machine learning theory. In contrast to a single block of i.i.d. data,
with some potential outliers inside, a dataset obtained from multiple external sources consists
of groups/batches, one from each source, and we expect some of these groups to contain
clean data, while others - potentially corrupted. Due to these differences, classic techniques for
defending machine learning against data corruptions may be suboptimal in such a multi-source
scenario.

3.

ON THE SAMPLE COMPLEXITY OF ADVERSARIAL MULTI-SOURCE PAC LEARNING

 

such that no distributed stochastic optimization algorithm, working with an a-fraction of
Byzantine machines, can optimize the function to error less than OF + x), where d is
the number of parameters. For realizable binary classification in the context of collaborative
learning, prove that there exists a hypothesis space of VC dimension d, such that no

learner can achieve excess risk less than Q(ad/m).

Besides the different application scenario, the main difference between these results and
Theorem (6) is that our bounds hold for any hypothesis space 1 that is non-trivial (Ineq. (3.20)),
or has VC-dimension d > 2 (Ineq. (3.21)), while the mentioned references construct explicit
examples of hypothesis spaces or stochastic optimization problems where the bounds hold. In
particular, our results show that the limitations on the learner due the finite total number of
samples, the finite number of samples per source and the fraction of unreliable sources a are
inherent and not specific to a subset of hard-to-learn hypotheses.

3.6 Summary and subsequent work

We studied the problem of robust learning from multiple unreliable datasets. Rephrasing this
task as learning from datasets that might be adversarially corrupted, we introduced the formal
problem of adversarial learning from multiple sources, which we studied in the classic PAC
setting.

Our main results provide a characterization of the hardness of this learning task from above
and below. First, we showed that adversarial multi-source PAC learning is possible for any
hypothesis class with the uniform convergence property, and we provided explicit rates for
the excess risk (Theorem |4| and Corollaries). The proof is constructive and shows also that
integrating robustness comes at a minor statistical cost, as our robust learner achieves optimal
rates when run on data without manipulations. Second, we proved that adversarial PAC
learning from multiple sources is far from trivial. In particular, it is impossible to achieve for
learners that ignore the multi-source structure of the data (Theorem |5). Third, we proved
lower bounds on the excess risk under very general conditions (Theorem (6), which highlight
an unavoidable slowdown of the convergence rate proportional to the adversary’s strength
compared to the i.i.d. (adversarial-free) case. Furthermore, in order to facilitate successful
learning with a constant fraction of corrupted sources, the number of samples per source has
to grow.

Two relevant subsequent works are those of and [HK20]. In particular, extend
the framework of to learning of distributions over infinite domains, from untrusted
batches of data. They also develop a robust algorithm for binary classification, achieving similar
statistical rates to ours. Regarding hardness results, they show how a result from

can be adapted to prove a lower bound of the form O ( x + se), essentially closing the
gap between the lower and the upper bounds known for the adversarial multi-source learning
problem. Also regarding hardness results, recently gave a number of sample complexity
lower bounds for multi-task learning with NV tasks that share an optimal hypothesis. Their
work shows in particular that no learning procedure can achieve optimal risk as N — oo, even
in this non-adversarial setting, as long as the number of data points per task is constant and
the learner has no additional information about the relationship between the tasks, apart from

the given data.

36

Step 2 Now assume that all of By + By < 22n, Co > 452Pon, Ci > 45° Pin hold. This
happens with probability at least 1 — 6 according to Step 1. Let h be an arbitrary classifier.
Since we consider h fixed, we will drop the dependence on h from the notation for the rest of
this proof and write y? = y?(h),C} = Cl(h), etc.

We now prove that for both a € {0, 1}
B
Ag = |7? — ¥5| < a,
le — Val S GB

For each a € {0,1}, this can be shown as follows. First, if 17, 1{a? =a} =Bi,+C, =0,
then both 7?(h) and 7¢(h) are equal to 0, because of the convention that $ = 0. In addition,
By, = C, = 0. Therefore, inequality (C.2) trivially holds.

Similarly, if B, =0, but C, > 0, then 7?(h) = yS(h) and so A, = 0 and (C.2) holds.
Assume now that B, > 0. Note that if C, = 07, l{a? =a,i ¢ B} =0, then

 

(C.2)

Bi
Ba

B B Ba
Be BatO,~ Cat+Ba

 

 

 

 

Ba = hb(h) — r2(0) | 0

Finally, assume that both C, > 0 and B, > 0. Note that under any realization of the
randomness of the data sampling and the adversary, for any a € {0,1}

 

 

 

 

 

 

~P(h) =
ve) = SE Hal = ag}

Sh Uf h(w?) = 1a? = ai ¢ BY + ON Ala”) = La? = ai € B}

Dik La? = 4,1 ¢ P}+ DL Ha? = 4,1 € P}
cl 4 Bi
~ Gt Be
Therefore,
p c Ci+Bi Cc Ba ci Bi Ba
Aa = |%2 — Yal i = S
Co+Ba Ca Cat BalCa Ba C+ Ba

 

 

 

 

and so (C.2) holds in all cases. Therefore, we can bound the sum Ag + A, as follows:

 

 

Bo By,
A Ay <
OTN S GTR TOATR

 

 

 

 

 

 

 

eq te
S2Pyn+ Bo 8Pin+ By
B B
B 154 Pyn
Fin F Bo” ' 52 Pyn — By +B + Bi)
B 1-2 Pp
< Pat Bo +t 5a a ot 32n,

 

 

1 1
2-—(1 Po +
(1—a)Fon (a —a)Pon+2Byo  (1—a)Pyn + 3an — =|

136

6.

FAIRNESS THROUGH REGULARIZATION FOR LEARNING TO RANK

 

viewpoint. In particular, JKAAS12a) |KAAS14] apply a neutrality enhancing regularizer to a
recommender system model. The focus of these works, however, lies on dealing with filter
bubble problems and no formal links to classification or fairness are made.

Diversity in ranking Another related topic is the one of diversifying the output set of
ranking system, see,e.g., [RBCJO9]. However, diversifying rankings generally has the goal of
improving the user experience, not a fair treatment of items. A discussion on the relationship
between fairness and ranking diversity can be found in [SJ18].

6.3. Preliminaries

In this section we introduce some background information on the learning to rank (LTR) task.

For a thorough introduction see IMC18].

Learning to rank Let Q be a set of possible queries to a ranking system, and let Z be
a set of items (or documents) that are meant to be ranked according to their relevance for
any query. A dataset in the LTR setting typically has the form S = {(4i; di, 7!) }ietv).gelmi)s
i.e. for each of N queries, qi,...,qv € Q, a subset of the items Z,, = {d},d5,...,d),,} CL
are annotated with binary labels r} = r(qi, di) € {0, 1} that indicate if item di, is relevant to
query q; or not. In practice m, is often much smaller than |Z], since it is typically impractical
to determine the relevance of every item for a query.

 

The goal of LTR is to use a given training set to learn a ranking procedure that, for any future
query, can return a set of items as well as their order. That is, the learner has to construct a
subset selection function,

R:Q>2(Z), (6.1)

where ‘3 denotes the powerset operation, as well as an ordering of the predicted item set. The
size of the predicted set will depend on the application and may in general be smaller than the
total number of relevant items. For example, for ads selection only a limited number of slots
may be available on a website.

In practice, R is typically constructed by learning a score function, s: Q x ZI — R. For
any fixed g, s(q,-) induces a total ordering of Z, and the set of predicted items is obtained
by thresholding or top-k prediction. The function s is usually learned by minimizing a loss
function on the quality of the resulting ranking on the train data. Classic examples of this
construction are SVMRank or WSABIE [WBUTI]. Most other pointwise, pairwise and
listwise ranking methods can also be phrased in this way, with differences mainly in how the
loss is defined and how the score function is learned numerically [Liu11].

Evaluation measures Many measures exist for evaluating the quality of a ranking system,
arguably the simplest being precision at k.

Definition 14. Let S be a test set in the format introduced above. For any query qj, let
d,d5,... be a ranking of the items in Z,, with associated ground-truth values r(q;,di,). For
any k € N\ {0}, the precision at k is defined as PQ@k = + DN, P@k(q;) with

P@K(q) = +

M-

r(qi, di). (6.2)

j=1

74

5.6. Summary and discussion

 

can have an inevitable negative effect on model fairness and that this effect is even more
expressed for problems where a subgroup in the population is underrepresented. We also
provided upper bounds that match our hardness results up to constant factors, in the large
data regime.

While the strong adversarial model and the statistical PAC learning analysis we have considered
are mostly of theoretical interest, we believe that the hardness results have several important
implications. Indeed, crucial to increasing the trust in learned decision making systems is
the ability to guarantee that they exhibit a high amount of fairness, regardless of any known
or unforeseen biases in the training data. In contrast, we have shown that this is provably
impossible under a strong adversarial model for the data corruption.

We believe that these results stress on the importance of developing and studying further data
corruption models in the context of fairness-aware learning. As discussed in the related work
section, previous research has shown that it can be possible to recover a fair model under
corruptions of the labels or the protected attributes only. While real-world data is likely to
contain more subtle manipulations, one may hope that for certain applications there will be
models of data corruption that are, on the one hand, sufficiently broad to cover the data issues
and, on the other hand, specific enough so that fair learning becomes possible.

Our results can also be seen as an indication that strict data collection practices may in fact
be necessary for designing provably fair machine learning models. Indeed, our bounds hold
under the assumption that the learner can only access one dataset of unknown quality. In
contrast, it has been shown that the use of even a small trusted dataset (that is, a certified
clean subset of the data) can greatly improve the performance of machine learning models
under corruption, both in the context of classic PAC learning and in the
context of fairness-aware learning [RLWS20]. Such data can also be helpful for the sake of
validating the fairness of a model as a precautionary step before its real-world adoption.

In summary, understanding and accounting for the types of biases present in machine learning
datasets is crucial for addressing the issues brought up in this chapter and for the development
of certifiably fair learning models.

69
