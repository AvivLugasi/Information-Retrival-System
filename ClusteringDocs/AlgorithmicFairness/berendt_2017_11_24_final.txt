A computer scientist’s view of people

  

Gan Feb mar apr May Sun aul ‘aug Sep oct Nov Dec ‘Annual Average
i958 -30.98 98.99 31573745, 3175 3172631586 318.93 3132 sz 313.33 99.35
i959 «31s.62 «6.287172 —~—*B29—SCNBIGSCe Se 3148 anes 13.26 314.0 315.38
19eo eas, hes? ive 3190232003, 3isss deus issue, 313.88 315 sues
i961 316.93 31773831948 520583197787 316.78 3188 315.38, 316. 317.68
9e2—=«31798~—=~CNe SG aDge—SCOSS SOLUS 3174 31625——«S42_——«16 69 Beas
iees 318.74, 319.08 disse suse 2224 Szl.e7, 39.743) Se ak S99, S172 3ies
i964 319.57 96.99 93.99 99.99 322.24 «521.89 —«a20.4e 3187 3167 316.739.317.793 99.35
6s 310.45 320.08 a20e9 = «22132218? 321.39, 3ies 317.81 73 3887 320.04
ases 3062 aatsa 392300387 aecol32375, dasa an) 3ia1 31979 338
i967 322.05, 322.5 323.04 328.42 32532409 seas 2B. ——«320.77, 322.16
396832257 32sd5 sss) asda 2557325. Sate aes 320.2521 SL 323.05
1968 Soa 2422 Psa S286 aaa 876 ase aa) 02382 78a as Saas
1970 325.03, «32589 —a2e.67 Sha sen? «32765 «326.35 528.58 3231 323.16 32388 325.58
3971 326.17, «366822718 27.7e—~S—«BIZ—~—DOS?—aa73e aS SLI S«23.87 3268 326.32
4972 3677s 3977839972 80.07, 32809 ass Seas 395.08 es 27s
3973-326. —_-329.56 330.3 3315 30240 —332.07 330.87 323.31 —a27.51 27.18 328.16 329.66
i974 32025 390.72—SgaLae as 3309S BL 3204 74s 2737 328.46 330.25
4975 ‘S04 3anet 32043331 3.98 Hie 33191, 339.06 aass aaa eae sais
3976 331.75 «332.86 30S 30458 39487 —-330,38 99305 390.94 3293 328.94 33031 2s
397732933332 3.733607 336,74 «3362733493 ——i332.78—SGL Sa a aha 380)
i978 dasa? dasa9s3ase S376 aR.ol 37 kode Stage 376288 3a 3581
39738 -336.23,—-36.76 337.96 30ng9 a3947——«390.29 397.73, 398,09 303.81 39.85 995.29 336.85
1980 336.01 «336.36 24008 340.77 aaa? 339186 337.6 aus88 336.02 3e7-t 338.59
agai 330.23 haoa? aetna 342.25 anaa—tkas 3366238685 3S 339.83
3982 340.75 as4.62 342.7 343.57 340.3 343534206 393.81 _—au7.98 337.85 339.26 313
ies aa? 34282 Ba Mage 345754532, 3.093230 ads 3999 aL I B78
1988 33.7 Mes aun7e Gaze 3a7aa a8 79, Base eae Sater Saas aa Baa
399s 342.97 34647474. ——MB.93——iaHBZS MoS 48.68 343.09 2834424 345.5
1985 346.3, «Mee —a7i8e seas 80.21 S87 Se Biss geass aed? 34s 66 pers
198734802, Sasa? 4902350985188 S125 48.52 34.1 30685 346.35 3478) 348.53
90835043, «351,73 352.22 352.59 ——sase22 «380.79 «952.28 350.42 ——aam72 aa. 350.07 351.48
asso 352.75 —asz07—=sassee —ssaz2_——asS.67 355.13, 3sn9 sls? 3439 ags 351.29 35291
i990 355.65, 354735538 3562 35718 356.23 354.82 352.51 380.95 S118 35.83 354.19
i991 38472, —«935.75. «35716 3s06 359.33 —=—«358.24 «386.17 —=—CSA.O~S«SRIS—CBSAZL——«SD.7S 355.59
iosz3s5.98 386.92 asvisl —as.ls—s.98, 38825 357.02 35s anol sak Sad6 33637
1993 3567 35716 35838 35946 360.28 359.6 35757 355.52 «9526835389 55.34 357.08
i994 350.37, —=—«3S051—=—«aSD.a7—=CGL.2G SCG] ~=«CG0.8S «SSS —CaS7.MG~SCaSSLO—CSS.99.~—~—CS 5 350.00
ioss 38.97 Bei denies Seaas 363.79, 382.26 Bela sas 3se05 357.76 380.86 380.88
i996 362.05 363.25 36407 364.72 Se 35897365 SLA 59.45 359.5 350.76 362.58
1997 360.18 36h «GSE «66.35 —~—OG.7D=SCRS G2 S««964A?~—=~«CSGAS—~—~C*EO.29—=~SC«O OWT 88 363.76
jose 36523, 30.15, 367.31 Send 3ea.3. 3887 eee 365.77 363930023 es08 386.63
jo9s 366.15 3588736958 STL Ad S7L 37035-36827 365.93 368.63 365.13 366.67, 366.31,
2000 360.18, «360.46 —«a70S2 «a7 «7.82 2,7 37012, ——«6B.12—~—«6.62 «96673 ——«D 369.48
200137028 wis 721g 9287 ya 02 3733 ave Sess 3795 dono seo 3ru.02
200237243, «3730837352 SraaG S35 aS 37.02 371.48 3707 37025 372.08 373.4
200337468 «375.63 «3761 «778 ~~~ —~SCPBAR «BOD 37a§ 3299730137435 25.64
00a 39679, 377.37 ayaa! bans? aves 47087, 377.79 376 3407 de 2d 7B 377.38
2005 376.37 «379.69 300.41 3021 3622838213 «300.66. ——«S7R.71 «a6? 976.08 ——_—376.32 379.67
2006381383823, Ciao SSB GZ MeSH. 3.2) BRON —CaR «79.08, 8014 sais
2007 aes 3asen 3823 seas be, 38S. 87, Saas SaL7a 380.7318 33, 383.55
2008 365.07, —+395.72 305.05 306.71 s68.45 307.68 366.3 303.95 32.91 362.73 383.86 385.34

 

 

‘Values above represent monthly concentrations abjusted to represent 2400 hours on the 15th day of each month. Units
are parts per millon by volume (pomv] expressed Inthe 2003A SIO marometrie mole fraction scale. The "snnual average"
Ie the artnmetic mean of the fwaive monthly values where ne menthiy values are MisiNg

Outlook:
Discrimination and accountability

Decisions are generally made/helped
by algorithms in a decision context

Human
Organisational
Wider systems

Decisions > data > decisions > ...

Legal notion of “safeguards”: human
involvement

Challenge 2:
Discrimination =?= not
distinguishing by a given attribute

(with some notes on possibly
fundamental limits on formalization)

Legal view: Differentiation

Has no checking
account?

Savings < 10 000?

 

Solution approach

¢ An approach based on reinforcement learning

¢ Ensign et al. 2017: Runaway feedback loops in
predictive policing (currently on arxiv, to appear
in FAT* 2018)

¢ “In order to act near optimally, the agent must
reason about the long term consequences of its
actions (i.e., maximize future income), although
the immediate reward associated with this might

be negative.”
(https://en.wikipedia.org/wiki/Reinforcement_learning)

Not applicable?!
Example: the SCHUFA Judgements

Basisscore: Was die SCHUFA-Score-Werte Uber
das Kreditrisiko aussagen

oe

cy Boh) iiber 97,5% , sehr geringes Risiko”
cot 95-97,5% , geringes bis iiberschaubares Risiko“
0 50-95°5 , zufriedenstellendes bis erhohtes Risiko”
FLO 80-90% ,,deutlich erhdhtes bis hohes Risiko”
10h 50-80% ,,sehr hohes Risiko”

Salt unter 50% ,,sehr kritisches Risiko”

 

Solution approach

(our proposal)
Consider the whole system
Draw on value-based design

Use tried-and-tested structures and
procedures

— E.g. legal principles —

 

Principle > strategy > pattern > technology:
Examples data minimisation and control

DY eK hed ged Leda Design pattern Privacy-enhancing
technology (ex.)

MINIMISE

the amount of personal
data with respect to the
purpose > proportionality

HIDE
SEPARATE
AGGREGATE
INFORM

CONTROL

Data subjects should be
provided agency over
processing

“Select before you collect”
Anonymisation
Pseudonymisation

(specific approaches, e.g.
anonymisation schemes:
k-anonymity and beyond)

 

-

  

rf ,

ED

Privacy and Date Protection by Design
~ from policy to engineering

User-centric identity
management

End-to-end encryption
Intervenability-enhancing
techniques

Data Track m™@ 0

based on (Hoepman 2013; Danezis et al., 2014)

New categories; which categories?

¢ Examples:
— Black women (Crenshaw, 1989)
— Mothers (Fine, 2010)
* Questions:
— Statistics
— Algorithm design + user interface: constraints vs. exploration
— sociological
¢ When is a disadvantaged group a group? “Experience” and “stat.s of decisions”?
¢ Which grounds do we accept? (e.g. “black women” within “seniority-based
layoff” in DeGraffenreid vs. General Motors)
— Legal
« “the prospect of opening the hackneyed Pandora’s box” (DeGraffenreid vs.
General Motors - 1977)

¢ “multiple/pluridimensional disadvantaging is rarely explicitly mentioned in legal
rules against discrimination” (Baer, Bittner & Boettsche, 2011)

Principle > strategy > pattern > technology:
accountability principle; non-discrimination

DY eK hed ged Leda Design pattern Privacy-enhancing
technology (ex.)

ENFORCE Access control
A privacy policy compatible Sticky policies
with legal requirements Privacy rights management

should be in place and

should be enforced. ;
Privacy management systems

DEMONSTRATE
Requires the controllerto Logging
demonstrate (“prove”) Auditing

compliance with the
privacy policy and legal
requirements

 

DEMONSTRATE-ND Detect discrimination Data exploration for D.

cont.; Berendt & Preibusch (2017)

People Algorithme fothucre Dulos

Summary Hi
of
findings

provides or generates

informs computes
and presents

becomes

can make

impacts

structures

27/11/2017

 

Challenge 1:
Intersectionality

Equal treatment = non-discrimination?

“| treat all my employees the same —
They enter the office by the stairs.”

 

Coding the tree-form comments

ans arers
- I: The participant in a first-person narrative (e.g., “I”, “me")
- YOU: The reader or a fictitious dialogue partner who is given advice (e.g., “you”)
- NAME: The name of the person described (the loan applicant)
mae a\e 1
- DISCRIMINATION: discrimination as such (e.g., “discrimination”,
“discriminatory”)

- DISC_FEATURES: discriminatory features of the applicant; specifically: age,
gender, nationality or family status (including obvious synonyms or labels)

- OTHER_FEATURES: other features of the applicant or the application

- Why?

- WEIGHTS: Numerical weights as given by the data-mining results, with an
explicit comparison between positive and negative weights (e.g., “outweigh”,
“more than”, “stronger”, “higher”)

- WORLD_KNOWLEDGE: Real-world knowledge beyond the data mining tool's
rules was referred to (e.g., “unreliable job’).

- CAUSAL: Reasoning, establishing a causal relationship (e.g., “because”, “since”,
SO")

- META: A meta-comment on the task was given.
27/11/2017 Bl

Argumentation and demographics

Experience of discrimination

 

Other Loan denied

Total

NAME

|

YOU

DISC FEATURES
OTHER_FEATURES
DISCRIMINATION
CAUSAL
WEIGHTS
WORLD_KNOWLEDGE
M

27/11/2017

 

A recent example:

Taddeucci and McCall vs. Italy
European Court of Human Rights judgment, 30/6/2016

The case concerned a refusal by the Italian authorities to grant a residence
permit to a gay couple on family grounds.

The Court found in particular that the situation of Mr Taddeucci and Mr
McCall, a gay couple, could not be understood as comparable to that of an
unmarried heterosexual couple. As they could not marry or, at the relevant
time, obtain any other form of legal recognition of their situation in Italy, they
could not be classified as “spouses” under national law. ....

Thus the Court concluded ... that there had been a violation of Article 14
(prohibition of discrimination) taken together with Article 8 (right to respect
for private and family life) of the European Convention on Human Rights.

Conclusion: “Fixing the algorithm”?

 

“You may no longer ...”

European Court of Justice (2011) Case C-236/09, Association Belge des
Consommateurs Test-Achats ASBL and Others v Conseil des ministres:
— ... the use of sex as an actuarial factor should not result in differences in
individuals’ premiums and benefits. ...
General Data Protection Regulation (EU law as of 2018)
— Various recitals, among others:

¢ 71: the data controller should take measures to “[prevent] discriminatory
effects on natural persons on the basis of racial or ethnic origin, political
opinion, religion or beliefs, trade union membership, genetic or health
status or sexual orientation ...”

Historical examples: only { rich | white | male } people get to vote

Pedreschi, Ruggieri, & Turini (2008)

¢ PD and PND items: potentially (not) discriminatory
goal: want to detect & block mined rules such as
purpose=new_car & gender = female > credit=no
measures of discriminatory power of a rule include
elift (B&A > C) = conf (B&A > C) / conf (B > C),
where A is a PD item and B a PND item

Note: 2 uses/tasks of data mining here:
* Descriptive

— “Inthe past, women who got a loan for a new car often defaulted on it.”
* Prescriptive

— (Therefore) “Women who want a new car should not get a loan.“

Unlawful indirect discrimination

Has no checking
account?

Savings < 10 000?

ViVfol a ecmey a)

building site?

 

Identity politics revisited — do the
categories themselves induce biases?

"Have you ever noticed that [...] we talk incessantly about <Anerkennung>
[recognition, appreciation, acknowledgement, acceptance, respect] and
diversity, but hardly ever any more about social inequality?

The obsession with which, in rich societies, for example the
<Anerkennung> of even the most peculiar sexual orientation is struggled
for, is symptomatic of a setting in which one should not any more talk
about social, i.e. changeable inequality.

This is not any more about the abolition of inequality, but only about the
<Anerkennung> of diversity, and all of this in a morally hypercharged
discourse.” (Welzer, 2016)

What do you believe are the two most self-reported grounds of
discrimination in Germany in 2016?
(as reported in Berghahn et al., 2016)

An experimental user study

 

(Berendt & Preibusch, 2014, 2017)

How we all take part

 

Ad related to latanya sweeney @

Latanya Sweeney Truth
www.instantcheckmate.com/

Looking for Latanya Sweeney? Check Latanya Sweeney's Arrests.
Ads by Google

1) Enter Name and State. 2) Access Full Background

Checks Instantly.
www. instantcheckmate.cony

Public Records Found For: Latanya Sweeney. View Now.
www.publicrecords.com/

Search for La Tanya Look Up Fast Results now!
www.ask.com/La+Tanya

(Sweeney, 2013)

Image credits

p. 3: http://eyeingchicago.com/storage/northpo
http://i.dailymail.co.uk/i/pix/2015/07/01/13/2A23A22200000578-0-

Google has issued an apology after computer programmer Jacky Alc-a-

28 1435752503903.jpgint.png? SQUARESPACE CACHEVERSION=1465952 722226

p. 26: http://www.visionlearning.com/img/library/large_images/image 2555.png

pp. 34f.: http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2016.00960.x/full
http://www.sciencemag.org/news/2016/09/can-predictive-policing-prevent-crime-it-happens

p. 36: http://latanyasweeney.org/index.html

p. 37: https://en.wikipedia.org/wiki/File:AnAmericanDilemma.jpg

http://www. blackwestchester.com/wp-content/uploads/2017/03/13th-e1489199239580.png
http://www.radio3.rai.it/dl/img/2017/05/1494496317916LETTERA-A-UNA-PROFESSORESSA640. jpg
p. 43: https://www.aktuelle-bauzinsen.info/images/desktop/schufa-score-bewertung.png

p. 62: http://www.rp-online.de/polopoly_fs/this-photo-provided-on-thursday-feb-
1.5424969.1505916915!image/984164559.jpg gen/derivatives/d940x528/984164559.jpg

 

 

 

 

 

 

 

 

 

 

 

 

 

 

algorithms discriminate { #2 diri noir avec banan tk
Y

 

aaa stings ~~ Google Photos, y'all ggup. My friend's
Staak 210 Deira (149 isaccas not a gorilla.

 
 
  
  
  
 
 
 
 
  
  
 
   

 

www.aljazeera.com/ yalonttiassscieinctis robotics- ai-ethics- 471117133216779 v

2 days ago - Stephen Roberts discusses the threats and promises of artificial intelligence and machi
learning. ... We live in an age of rapid technological advances where artificial intelligence (Al) is a real}
not a science fiction fantasy Al Jazeera talks to Stephen Roberts, professor of

te - The New

ew Yo
https JW. -nytimes com/201 5/07/10/upshot/when- aigoritinnes aiscrwiinile html ¥
Jul § 15 - But can computer programs be discriminatory? There is a widespread belief that softw4

   

and algorithms that rely on data are objective

 

https /Isinews.siam org/About- the-Author/when- big-data- algorithms-discriminate
When Big Data Algorithms Discriminate. By James Case. Weapons of Math Destruction: How Big

  

Prediction Fails Differently for Black Defendants
Nae Ue Wise)

Labeled Higher Risk, But Didn't Re-Offend

Labeled Lower Risk, Yet Did Re-Offend

OMe ie eee ee eee eee Uh eee ee eel ea eee ima eee Lh
as whites to be labeled a higher risk but not actually re-offend. It makes the opposite mistake among whites: They are much
epee cede ase eat aie eee eee

 

»YOu May not...” vs. “you may ...”

e In many areas, including
— Labour
— Loans
— Insurance

¢ The protected-by-law grounds differ by area, but
usually include gender, disability, age and sexual
orientation, cultural, religious and linguistic
beliefs/affiliation

Approach:

Discrimination-aware data mining
(Pedreschi, Ruggieri, & Turini, 2008; & many others since)

Problem solved?

As computer scientists ...

e ... we (believe we can) define anything
¢ We then believe this is the truth

¢ We then believe we can “solve” it
> Solutionism (Morozov)

Argumentation patterns

NAME | YOU DISC_FEATURES OTHER_FEATURES DISCRIMINATION CAUSAL WEIGHTS WORLD KNOWLEDGE

Total 19 48 4 35 11
NAME 10 1 8
| 2 20
YOU 2
DISC_FEATURES

OTHER_FEATURES

DISCRIMINATION

CAUSAL

WEIGHTS

WORLD_KNOWLEDGE

M

 

Discrimination — discrimination-related features
and
Other features — world knowledge - causal

27/11/2017 wy

Argumentation, task, and user interface

Setting Mining form

Bank ADA cDADM- eDADM

Total

NAME

|

YOU
DISC_FEATURES
OTHER_FEATURES
DISCRIMINATION
CAUSAL

WEIGHTS
WORLD_ KNOWLEDGE
M

 

27/11/2017

Soke]

From indirect to explainable to ...

¢ What if all people in the red districts
(“indirect D“) have no collateral

¢ And “having no collateral“ is a an
accepted reason?

> “explainable discrimination“
(Kamiran, Zliobaite, Calders, 2013)
° l.e. itis ok to not give them loans?

¢ What if not getting loans leads to people
not having collateral?

 

= One reason for exploratory DADM
based in interactive systems!

Unlawful direct discrimination

Has no checking
account?

Savings < 10 000?

 

Why formalizing fairness
won't fix
(algorithmic) discrimination

Bettina Berendt
KU Leuven

Publications and materials on privacy, privacy education,
discrimination, and ethics at
https://people.cs.kuleuven.be/~bettina.berendt/

This presentation’s core: (Berendt & Preibusch, 2014, 2017)

See end of this slide set for references and URLs

¢ What is your society’s worst discrimination
problem at the moment?

¢ What has been your own worst discrimination
experience?

Dynamics: Feedback loops

 

Actionability and decision quality

 

(One way of)
Fixing data mining algorithms

Has no checking
account?

Savings +

      
 
 

genaer (~ nalt/n alf) i
(Of course, in general, this does not
need to lead to the selection of the
same attribute!)

(Kamiran et al., 2010)

Framing

id €NYA-10 1410 r= 4
= Detection: agency

= $6.00 show-up fee

Tasks

= 3 Exercise tasks
= 6 Assessed tasks

« $0.25 performance
eleya Ue ol

Questionnaire

«= Demographics

" Quant/bank job

« Experience with
discrimination

 

Decision-making scenario

Assessed task 3 of 6

 

Task structure

» Vignette, describing applicant and application
" Rules: positive/negative risks, flagged
" Decision and motivation, optional comment

Required competencies

" Discard discrimination-indexed rules
» Aggregate rule certainties
= Justify decision by categorising risk factors

Background:
Data mining
Discrimination

Rule visualisation by treatment

Constrained Exploratory (not DA)
DADM DADM DM

= Hide bad features = Flag bad features = Neither flagged
am celal olamse cl ar-lale " Detection scenario nor hidden

       

residence savings residence foreigner residence foreigner

Challenge 3:
Representation (1), or:
Categories can serve to detect,
but also to perpetuate discrimination

Solution approach:

segregation discovery
(Baroni & Ruggieri, 2017 — picture from a different study)

 

Solution approaches

¢ Similarity measures

— Dwork et al. (2012)

— Situation testing (Luong et al., 2011)
¢ Counterfactual reasoning

Data mining is discrimination!

Has no checking
account?

Savings < 10 000?

 

References

Berendt, B. & Preibusch, S. (2014). Better decision support through exploratory discrimination-aware data mining: foundations and empirical evidence. Artificial Intelligence
and Law, 22 (2), 175-209. https://people.cs.kuleuven.be/~bettina.berendt/Papers/berendt_preibusch 2014.pdf

Berendt, B. & Preibusch, S. (2017). Toward accountable discrimination-aware data mining: The importance of keeping the human in the loop — and under the looking-glass.
Big Data, 5(2). DOI: 10.1089/big.2016.0055. https://people.cs.kuleuven.be/~bettina.berendt/Papers/berendt_preibusch 2017 last_author version.pdf

Baer, S., Bittner, M., & Gottsche, A.L. (2011). Mehrdimensionale Diskriminierung - Begriffe, Theorien und juristische Analyse. Antidiskriminierungsstelle des Bundes.
http://www.antidiskriminierungsstelle.de/SharedDocs/Downloads/DE/publikationen/Expertisen/Expertise Mehrdimensionale Diskriminierung jur Analyse.pdf? blob=pu
blicationFile

Berghahn, S., Egenberger, V., Klapp, M., Klose, A., Liebscher, D., Supik, L.,, & Tischbirek, A. (2016). Evaluation des Allgemeinen Gleichbehandlungsgesetzes [Evaluation of the
German General Anti-discrimination Law], erstellt im Auftrag der Antidiskriminierungsstelle des Bundes.
http://www.antidiskriminierungsstelle.de/SharedDocs/Downloads/DE/publikationen/AGG/AGG Evaluation.pdf?__blob=publicationFile&v=14

Baroni, A. (2017). Segregation Aware Data Mining. PhD Thesis, Dipartimento di Informatica, Universita di Pisa. (thesis supervisor: Salvatore Ruggieri)

Crenshaw, K. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. In: The University of Chicago Legal Forum, S.
139-167.

http://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1052&context=uclf

Danezis, G., Domingo-Ferrer, J., Hansen, M., Hoepman, J.-H., Le Métayer, D., Tirtea, R., & Schiffner, S. (2014). Privacy and Data Protection by Design — from Policy to
Engineering. ENISA Report.

https://www.enisa.europa.eu/publications/privacy-and-data-protection-by-design

Dwork C, Hardt M, Pitassi T, et al. Fairness through awareness. ProcITCS 2012;2012:214—-226.

Ensign, D., Friedler, S.A., Neville, S., Scheidegger, C., & Venkatasubramanian, S. (2017). Runaway feedback loops in predictive policing. To appear in Proc. FAT* 2018.
https://arxiv.org/abs/1706.09847

European Court of Justice (2011) Case C-236/09, Association Belge des Consommateurs Test-Achats ASBL and Others v Conseil des ministres.
http://curia.europa.eu/juris/liste.jsf?language=en&num=C-236/M09

Fine, C. (2010). Delusions of gender. The real science behind sex differences. Icon Books, London.

Hoepman, J.-H. (2013/14). Privacy design strategies — (extended abstract). In ICT Systems Security and Privacy Protection - 29th IFIP TC 11 International Conference, SEC
2014, Marrakech, Morocco, June 2-4, 2014. Proceedings, pages 446-459, 2014. Cited from the 2013 preprint at https://arxiv.org/abs/1210.6621v2

Kamiran, F., Calders, T., & Pechenizkiy, M. (2010). Discrimination Aware Decision Tree Learning. ICDM 2010: 869-874. http://wwwis.win.tue.nl/~tcalders/pubs/TR10-13.pdf
Kamiran F, Zliobaite |, Calders T (2013) Quantifying explainable discrimination and removing illegal discrimination in automated decision making. Knowl Inf Syst 35(3):613-
644. http://repository.tue.nl/737123

Luong BT, Ruggieri S, Turini F (2011). k-nn as an implementation of situation testing for discrimination discovery and prevention. In: KDD, pp 502-510. ACM.

Naudts, L. (2015). Algorithms — Legal Framework. Presentation in the Privacy and Big Data course, KU Leuven.

Pedreschi D, Ruggieri S, Turini F (2008) Discrimination-aware data mining. In: Proceedings of KDD’08, pp 560-568. ACM.
http://www.di.unipi.it/~ruggieri/Papers/kdd2008.pdf

Sweeney, L. (2013). Discrimination in Online Ad Delivery. Communications of the ACM, 56(5), 44-54. cited from earlier version available at SSRN:
http://ssrn.com/abstract=2208240 or http://dx.doi.org/10.2139/ssrn.2208240

Welzer, H. (2016). Die smarte Diktatur. Der Angriff auf unsere Freiheit. [The Smart Dictatorship. The Attack on our Freedom]. Frankfurt am Main: S. Fischer Verlag.

Society is discrimination!

The Importance of Aristotelian Equality in Law

« Equality and discrimination are two sides of the same coin:

“The principle of equality requires that equal situations are treated
equally and unequal situations differently. Failure to do so will amount
to discrimination unless an objective and reasonable justification
exists” - Explanatory memorandum protocol 12 to the ECHR

Westen: “Equality is an empty vessel, with no substantive moral content of its own”
1. When should individuals/situations be considered equals?

2. What constitutes equal treatment?

3. What should we be fundamentally concerned to equalize?

iapltmenatonase
to these questions

(Slide: thanks to Laurens Naudts, 2015)

Challenge 3:
About vicious cycles
(and virtuous ones)

Solution approach

¢ An approach based on reinforcement learning

¢ Ensign et al. 2017: Runaway feedback loops in

predictive policing (currently on arxiv, to appear
in FAT* 2018)

Article 22 GDPR

(quite similar to Article 15 Data Protection Directive)

Automated individual decision-making, including profiling

1. The data subject shall have the right not to be subject to a decision based solely on
automated processing, including profiling, which produces legal effects concerning him or her or
similarly significantly affects him or her.

2. Paragraph 1 shall not apply if the decision:

(a) is necessary for entering into, or performance of, a contract between the data subject and a
data controller;

(b) is authorised by Union or Member State law to which the controller is subject and which also
lays down suitable measures to safeguard the data subject's rights and freedoms and legitimate
interests; or

(c) is based on the data subject's explicit consent.

3. Inthe cases referred to in points (a) and (c) of paragraph 2, the data controller shall
implement suitable measures to safeguard the data subject's rights and freedoms and legitimate
interests, at least the right to obtain human intervention on the part of the controller, to express
his or her point of view and to contest the decision.

4. Decisions referred to in paragraph 2 shall not be based on special categories of personal data

referred to in Article 9(1), unless point (a) or (g) of Article 9(2) applies and suitable measures to
safeguard the data subject's rights and freedoms and legitimate interests are in place.

Why not just delete the “problematic”
attributes?

¢ If focus is detection:
— Prevents detection

= If focus is
prevention:

= May reproduce
indirect
discrimination

= ... and this
indirect
discrimination
will also not be
detected!

 

A well- known phenomenon?!

LETTERA
A UNA PROFESSORESSA

HU

   
 
  
    
   
  

PVC \ I
LIBRERIA
EDITRICE
DILEMMA tpiraict
Hada cbesidt Lane A NETFLIX ORIGINAL DOCUMENTARY
locas FROMAVA DuVERNAY
. DIRECTOR OF SELMA

GUNNAR MYRDAL TH

 

Drug users in Oakland heatmap & PredPol predictions

      
 
 
   
  

 

  
 

aa Latayette aipany, Lafayette
Unive: Siesta Valley rinda é z
hlin Berkeley > Calif Becrerey ; Meeahin Berkeley >
a at " § ; Eastshore
a State Park
Die % SS
Volcanic
Regional
ore
Eastport
Moraga Moraga
imeryvill j Canyon ‘anyo
—v 1 £ Piedmont F Valle Vista Valle Vista
Bate tes arrests negoees days
1edwood
egional Park 209} 5 Regional Park -
d i a 3
- 150 : .
100 = he Bh “ 200
: 50 al 100
2 rot , a
Alameda , 0 Alameda :
Island Island
Alameda : 4 Alameda ) . eZ)
Soret Ant “40. coColisedy ' ¢ on
a chi Ookla ;
Bay Farm
Bay Farm
Island Island
(a) San Leandro (a) San Leandro
Google 6 Map data ©2016 Goog Google ae Map data. 2016 Google

"predictive policing" program. Police car laptops will display maps showing locations
where crime is likely to occur, based on data-crunching algorithms [...] [The algorithms]
replace more basic trendspotting and gut feelings about where crimes will happen and

who will commit them with [...] objective analysis.
(Lum & Isaacs, 2016)

Conceptual challenges

The notion of discrimination

)

Outlook: representation (2) — “collectivisation’

 

 

Pelatclae

Data 1 Pi

   

Decision

A fundamental question regarding
categories

Are categories of humans necessary to identify discrimination?

— See also Challenge 3 below, on Why not just remove category
information in data

Or does their use (also in positive senses, cf. identity politics) serve
to perpetuate structures of inequality, segregation, discrimination
we?

— Categories of humans can become dis-used and “disappear”, ex.
Estates of the realm, religion (in some countries)

— Or does their use create discrimination problems (and worse)? — see

http://genocidewatch.org/genocide/tenstagesofgenocide.html and its

criticisms
This has been debated for a long time.
There is no easy answer.

The human in the loops
challenge(s)

Algorithms don’t discriminate,
people do.

any

Demographics in policed areas & a simulation model

 

a . zs Ve a) i Left:

e oe A ages Top: Number of days with
targeted policing for drug
_ crimes in areas flagged by
» PredPol analysis of Oakland
* police data.

Middle: Targeted policing
for drug crimes, by “race”.
Bottom: Estimated drug use
by “race”.

    

Right:

Top: Number of drug arrests
made by Oakland police

= department, 2010. (1) West
> Oakland, (2) International
Boulevard. Bottom:
Estimated number of drug
users, based on 2011
National Survey on Drug
Use and Health

=

Percent of population (%)

white black other

(Lum & Isaacs, 2016)

A People Algorithms, Software, Rules

Processing
develops

systems
VISA

provides or generates

informs computes
and presents

becomes

can make

impacts

structures

 

Argumentation and decision outcomes

WEIGHTS : ~ DISC_FEATURES

DISCRIMINATION

1 or 2 decisions correct (n=15)
= @3 or 4 decisions correct (n=41)

eee 5 or 6 decisions correct (n=55)

 

27/11/2017 55
