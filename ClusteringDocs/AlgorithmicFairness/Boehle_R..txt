CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

False Positive Rate difference as fairness metric

False Positive Rate of a classified model measures the proportion of positives that are mistakenly classified of all
actual non-fraudulent transactions.

The formula for FPR is FPR = FP/((FP+TN).

If there is a difference between the FPR's of the protected groups, this indicates that the model more often
classifies one protected group 2s fraudulent. The model might have a misplaced biased against a protected group.

&
§ Particulier Ga) @ Particulier
B Zakelijxill 0.03 Zakelijk
pa Al @ al
° 0.2 04 0.6 0.8 1
FPR

Figure 5.9: Use case 1: False Positive Rate bar chart

Precision difference as fairness metric

Precision of a classified model measures the proportion of positives that are correctly classified of all predicted
fraudulent transactions.

The formula for Precision is PPV = TP/(TP+FP).

Alarge difference between the Precision scores of the protected groups, indicates that the model is better in
distinguishing fraud from non-fraud in one protected group. It might be the case that certain rules only work well on
a certain protected group, and not in the other(s).

x
5 Particuicr iar IE particulier
§ Zakeiijk (os) © Zakelijk
& es mal

° 0.2 a4 06 0.8 1

Precision

Figure 5.10: Use case 1: Precision bar chart

 

In Figures 5.8, 5.9, and 5.10, the fairness metrics for each protected group are visualized in a

bar chart for easy comparison.

TPR and Precision, and TPR and FPR, are trade offs. To get a better understanding of this
trade off, a ROC curve (Figure 5.11) and a Precision Recall curve (Figure 5.12) are included in
this tool.

Confusion matrices can give the rule builder more concrete understanding of how the fairness
metrics are calculated. Two types of visualizations are given for the confusion matrices: a band
diagram which present the percentages of all four categories of the confusion matrix (Figure 5.13),
and the confusion matrix in a table with the absolute numbers (Figure 5.14).

 

 

 

21

Assessing fairness in anomaly detection

CHAPTER 6. QUALITATIVE EVALUATION OF THE DESIGNED TOOL

 

overview of all the disparities between FP, TP, FN, TN, Positives and Negatives. In
his prototype, the visualization was sub-optimal.

e Understanding fairness metrics: how well do rule builders understand the information
presented? In Section 6.2.5, we see that understanding the visualizations took some
ime, and not all visualizations were correctly understood. Spending more time with
his tool could solve this problem.

2. Usefulness

e Analyzing visualizations: what information is analyzed by the rule builders? Most of
he visualizations should be part of such a tool. The harm on individual rules should
not be presented in the way as it was now. This visualization was confusing to the user
and not well implemented. It is important to realize that simple calculations are easier
o interpret this study.

e Drawing conclusions fairness metrics: how can the rule builders draw conclusions on
fairness in this context? The more complex the calculation of a fairness metric, the less
he participants would use this metric. The curves might still be good visualizations
© understand trade-offs of the rule-based model, if the participants would have spend
more time with the tool. In this study, the difference in predicted and true fraudulent
ransactions, the absolute confusion matrices, TPR and Precision were mostly used for
drawing the conclusions on fair rule-based models. We interpreted this result as an
indicator of how useful these parts are in the designed tool.

 

e Perceived usefulness: how useful do the rule builders perceive the tool? The participants
all mentioned that they now had concrete points to address fairness in rule-based mod-
els, and such a tool might be useful in these use cases.

6.3. Discussion and conclusion

6.3.1 Discussion

A considerable portion of the tasks executed by the participants was spent on understanding the
fairness metrics. Eventually, the metrics were understood, but the experiment was limited by
spending time on this part. For future work, it saves time during the experiment if a more elab-
orate explanation of the fairness metrics is provided beforehand, such that during the experiment
it is possible to go more in-depth. However, it is interesting to understand how domain experts
relate these metrics to their model. From this train of thought, the researcher could extract what
elements of the metrics were useful towards determining fairness in the model or rules. Addition-
ally, requesting more time of the participants was not always feasible in this time span.

The qualitative study could not be conducted physically. This led to a lack of interpreting the
body language and natural interaction. The researcher could not control the setting of the par-
ticipant. This could lead to viewing the results on varying screen sizes, which could have made
some numbers more unclear. The participants were sometimes distracted by events happening at
home. If this experiment could have been conducted physically in a quiet space, then there could
have been a more in-depth analysis of the participants.

The participants knew that the researcher had also designed the tool of the experiment. A po-
tential risk might be that some feedback was withhold to not hurt the feelings of the researcher.
However, the participants did not seem to withhold on feedback. They mentioned during the
experiment when parts were unclear to them or seemed irrelevant.

One of the participants was involved in the design process, so this participant did know what parts
were included in the tool. This participant did not know the tasks of the experiment. The parti-
cipant had already given feedback on how useful the tool was and this was taken into consideration
by the researcher, so this could interfere in the experiment. However, during the interpretation of
the results of all participants, this participant differed only in moving and understanding quicker

 

 

36 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

only developed with or without one protected group in mind. This might be unfair, all protected
groups should have been considered while detecting fraud.

Quality-of-Service harm

If a rule performs well on a protected group, but not on another group, it might be the case that
not all protected groups are taken into consideration while developing a rule.

5.3 Visualization and mock ups

In the human-computer interaction literature, it is clear that it is important to work closely
with the specific target audience to iteratively refine a user-centered design [31]. The following
visualizations were included in the design process:

e Bar chart: is a chart that presents categorical data with rectangular bars with heights pro-
portional to the values that they represent. The bars can be plotted vertically or horizontally.
This visualization shows comparisons among discrete categories.

Figure 5.1: A bar chart

e Donut/pie chart: is a circular statistical graph that aim to illustrate proportions. Each slice
of the chart is proportional to the quantity it represents.

0 G

Figure 5.2: A donut chart and a pie chart

e Scatter plot: is a type of plot using Cartesian coordinates to display values for two variables
for a data set. The position on the horizontal and vertical axis indicates values for an
individual data point, and aim to show relationships between the two variables of the axes.

Figure 5.3: A scatter plot

 

Assessing fairness in anomaly detection 15

Chapter 1

Introduction

1.1 Research problem

Anomalies are unusual patterns which deviate from the majority of the data. Anomaly detection
can be useful for various tasks in the financial domain, including tax evasion, insurance fraud, or
financial fraud. The difficulty of detecting anomalies is that there is a large number of normal
data (negatives) as opposed to very few anomalies (positives) [13]. Anomalies are usually of many
different types and future anomalies may have very different characteristics than the ones that
came across [10]. As a result, anomaly detection tasks are typically very complex and the solution
imperfect. There are many anomaly detection methods developed [21], mostly these methods
result in prediction models that can be divided in glass box models or black box models. Glass
90x models are transparent models, for example rule-based models. The rules are fully understood
y the domain experts who develop these models. In black box models, including several classes of
artificial intelligence models, it is not clear to the model developers what and how attributes are
used for detecting anomalies [32]. Nowadays, artificial intelligence systems are starting to influence
daily life more often, also in anomaly detection. As these systems interact with society they can
seen as socio-technical systems. The scale of the action space of these socio-technical systems
is large; such systems can influence billions of users every day [24]. Because these systems cannot
rovide a perfect solution, errors are made. These errors can propagate very quickly because of
he large action space. If these errors result in having a bias for specific demographic groups,
hen these systems might be causing discrimination on a large scale. In recent years, algorithmic

 

airness has become a popular topic. Nowadays, there is a lot of academic interest in measuring and
ensuring fairness in socio-technical systems, for example in domains such as criminal justice [15],
welfare screening [17] and hiring staff [7]. However, there is no single solution that can be applied
‘o ensure fairness in every context because implications of bias and discrimination in the context of
operationalized complex models are difficult to judge. There is a need for well-established fairness
definitions that can be applied to every context, presented to the relevant stakeholder in a way
hat makes sense.

In this study, we research fairness in fraud detection models of a major Dutch bank in order to
research the fairness in a context-aware manner of the socio-technical system: anomaly detection
in financial fraud. Banks have to process many transactions in a short time and are mandated by
aw to investigate fraudulent cases. Finding fraudulent cases in transactions is a complex problem.
Due to the highly imbalanced data set and the evolving trends of frauds, there is no easy way to
find the fraudulent needle in the non-fraudulent haystack.

 

1.2. Contributions

The main contribution of this thesis is an approach to assist model developers of complex
rule-based models with novel visualization techniques in order to assess on fairness.

 

Assessing fairness in anomaly detection 1

CHAPTER 6. QUALITATIVE EVALUATION OF THE DESIGNED TOOL

 

e Precision difference as a fairness metric (Figure 5.10): this metric was also used to
determine fairness between the groups. The metric represented how good the predictions of
fraud alerts were.

e Ranking on largest True Positive, False Positive, False Negative, and Positive
disparity (Figures 5.15, 5.16, 5.17, 5.19): these visualizations were analyzed when looking if
rules were unfair. The participants took the first ranked rule in the false positives disparity
and calculated the performance of the rule with the other disparities.

e Switching the rule off was analyzed by every participant and was relevant to them.
The following parts of the tool were analyzed less and were experienced as relevant.

e False Positives, False Negatives and True Positives in confusion matrices relative
(Figure 5.13): to analyze the differences in performance.

e FPR difference as a fairness metric (Figure 5.9): this metric was not used in determining
fairness in the fraud detection model, but was used to see how much positives were allocated
in both groups. Additionally, this metric was used to understand the ROC curve.

e The table Summary fairness metrics (Figure 5.7) was used in combination with the tabs
on the right-hand side. The summary was mostly used during the rule-tuning task where
they compared the numbers between tables.

e The ROC curve (Figure 5.11) was analyzed by the participants. Every participant needed
time to understand this graph. The trade-off of FPR and TPR was interesting to the
participants once they understood this trade-off. However, drawing conclusions on the ROC
curve seemed to be too complex or not intuitive for the participants.

 

e The Precision-Recall curve (Figure 5.12) was seen as often as the ROC curve by the
participants, but the participants were less familiar with this curve. The participants looked
at the curve, but barely used the curve for drawing conclusions.

The following parts of the tool were barely used in drawing conclusions, and were mostly scanned
over by the participants.

e Harm on individual rules (Figure 5.21): because it was not easy to comprehend the
meaning of this visualization, the participants did not use this at all.

e TPR, FPR and Precision difference bar charts of the adjusted model (Figure ??).
The differences in the confusion matrices were clear enough for the participants.

6.2.5 Understanding fairness metrics

The bar chart visualizing the predicted and true fraud alerts was well-understood as being part
of Allocational harm. Furthermore, the participants recalculated fairness metrics for Quality-of-
Service harm often in order to be able to use the absolute confusion matrices. There was sometimes
confusion what TPR, FPR and Precision actually did mean. Eventually, they were able to figure
it out.

FPR as seen in Figure 5.9 was not immediately recognized as a measure for fairness even when
they correctly identified the possible harm that this metric measures. However, when a difference
in TPR or Precision occurred, they considered how many False Positives were made to equalize
the results. Additionally, the ROC curve and the Precision-Recall curve were seen as familiar by
two out of three participants. The one participant that was not familiar did not use this part
of the tool for the tasks. The curves were harder to understand. This is mainly because they
are not used to seeing individual points on the curve. The distance that represents the difference
in fairness was a hard concept to grasp. The impact of individual rules on the complete model
was well-understood. The component of the experiment where a rule was switched off was also
well-understood. As mentioned before, the bar chart including both harms for each rule was not
understood well.

 

34 Assessing fairness in anomaly detection

Chapter 7

Conclusions

7.1 Conclusion

This study answers the research question: To what extent can model developers be assisted
in assessing fairness of a fraud detection model?

In conclusion, assisting model developers in developing a fair model in context consisted in this
thesis of three steps:

 

1. Investigate what types of harm can impact the stakeholders in the specific context.
2. Design a visualization tool in an iterative process with model developers.
3. Evaluate the tool to conclude if the model developers are actually assisted.

In this study, we investigated assisting model developers of a fraud detection model at a large
in bank in the Netherlands.

The definition for a fair rule-based model in fraud detection is based on the impact that the
model has on its stakeholders. The impact the model has on its clients should be analyzed on
protected groups. The impact is determined by measuring a Quality-of-Service harm and Alloca-
tional harm. These two types of harm are relevant for rule-based models in the context of fraud
detection. If there is more harm in a protected group than another group, then this should be
justified.

In order to visualize the differences in harms between protected groups, a tool has been de-
signed for model developers in an iterative process with model developers. The goal of the tool
was to provide all relevant information to model developers in order to draw conclusions about
violating QoS-harm and Allocational harm. This tool was designed for two use cases: reporting
fairness on a global level and tuning rules.

To evaluate if the tool was actually useful to the model developers, an exploratory qualitative
study was conducted. The results of this qualitative study show that QoS-harm and Allocational
harm were both useful to analyze in both use cases. The two use cases were translated into two
tasks to make the rule builder familiar with the tool. In the first task, the researcher investigated
what visualizations for reporting fairness on a global level were important to the model developers.
A visualization of TPR and Precision differences between protected groups were to the model de-
velopers how to measure QoS-harm most useful. The visualization of predicted and true fraud
alerts (positives) between protected groups were useful to measure Allocational harm. For under-
standing metrics in the tool, confusion matrices of protected groups were highly relevant to the
rule builders. In the second task, the researcher investigated what visualizations for turning rules
were useful. Having the absolute number of the classifications for each rule is useful to analyze the

 

38 Assessing fairness in anomaly detection

APPENDIX B. MOCK UPS

 

These distances are directly linked to a specific rule, so the rule builder has a concrete cause for
this difference, and could change the rule after seeing these results.

Visualization 2 of Screen 3

The largest distance represents the
largest difference between male
and female

Distance

 

 

 

 

 

 

Figure B.10: Version 0.0 - Screen 3 - Visualization 2

What?
In Figure B.10, the rules are ranked on the largest distance between protected groups. The rule
builder could select the rules to get them into the visualization of Figure B.9 to avoid information
overload.

Why?
The most disturbing rules are listed, and give an easy way of interacting with the tool.

Visualization 3 of Screen 3

vier eto ts
|

 

Figure B.11: Version 0.0 - Screen 3 - Visualization 3

What?
In Figure B.11, the selected rules of Figure B.10 are listed. The content of the rule is displayed.

Why?
A better understanding of why the rules would create such a distance. It is possible that the
sensitive attribute, or a proxy of the sensitive attribute, was included in the rule.

 

Assessing fairness in anomaly detection 53

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

5.2 Quantifying harms

In fraud, it’s important to incorporate timely manual reviews that help reveal inconsistencies in
decision-making in general and in fairness. During a manual review, it is expected that the rule
builder will first inspect the model at a global level, to assess the complete model on fairness. In
case there are any issues, the rule builder can dive deeper into specific rules. This brings us to the
following two use case:

e Reporting fairness on a global level: to analyze the impact of the model on protected groups
and to present concrete metrics on the Quality-of-Service harm and Allocational harm. This
use case makes it possible for rule builders to communicate about fairness.

e Tuning rules: for understanding what parts of the model cause any type of Quality-of-Service
harm or Allocational harm and finding concrete rules to reconsider benefits or harm of the
rule.

5.2.1 Determining fairness metrics for Use case 1: Reporting fairness
on a global level

Specific fairness metrics that quantify the potential harms on a global level are elaborated in this
section.

Allocational harm

This type of harm refers to an allocating or withholding opportunities for certain protected groups.
We consider Predicted Positives for every protected group, this is also called Demographic Parity.
This group fairness metric is selected because it visualizes the distributions of Predicted Positives
of the model. It is important to consider the distribution of the actual frauds among the protected
groups as well, because it might be the case that protected groups correlate with fraudulent
transactions.

Quality-of-Service harm

With QoS-harm we measure if a protected group’s outcomes disproportionately have more false
predictions than the other protected group. In other words, more errors are made between pro-
tected groups. We consider True Positives, False Positives, True Negatives, and False Negatives
of a fraud detection data set to measure the errors. From these measures you can create other
numbers to reach the goal: understanding how rules impact the fairness of the global fraud detec-
tion model. In Table A.1 in Appendix A, all calculations of the confusion matrix can be found.
For Quality-of-Service harm, the following metrics were selected that can be calculated for each
protected group, and compared to analyze possible harm.

Taking the context into consideration, we choose not to include any measurements that focus
on the negatives. A highly imbalanced data set is dominated by the negatives and makes it more
difficult to deduce relevant information from this majority. Additionally, if two measurements are
the complement of each other, one of those is chosen to be used instead of both. For example,
True Positive Rate is 1— F. NR and False Negative Rate is is 1—-7T7PR. Using both measurements
will not result into an additional valuable insight. In Table 5.1, the last column presents the
information that is presented of the corresponding measurement.

5.2.2 Determining fairness metrics for use case 2: Rule tuning

The fraud detection model uses rules to detect fraudulent transactions. An individual rule only
covers a certain aspect of the model. Quality-ofService harm and Allocational harm can be
calculated over individual rules as well. There are two options to analyze individual rules:

 

Assessing fairness in anomaly detection 13

APPENDIX B. MOCK UPS

 

concrete conclusions out of this.

Visualization 2 of Screen 2

Analysis false positives

 

Rules that caused the Rules that caused the

most false positives in {50 most false positives in SS

protected group: fame protected group: ven

Female 7 Male ry
| :
a -

i | iy
_ g :

al:

 

Figure B.7: Version 0.0 - Screen 2 - Visualization 2

What?
In Figure B.7, the false positives of a protected group are presented. In the light blue box, a
total of false positives of the protected group is shown, which comes from the rules of the fraud
detection model. The rules are listed in order of the most common rules that causes a fraud alert.

Why?
The rule builder can investigate rules which cause more false positive in one protected group.
These could be causing discriminating effects in the fraud detection model.

 

Assessing fairness in anomaly detection 51

Chapter 5

Implementing assessing fairness in
fraud detection

In Chapter 4, fairness of rule-based models in fraud detection was determined; a model where
Quality-of-Service harm and Allocational harm are equal or justified in protected groups. In this
chapter we answer the question: How can we assist model developers in analyzing fairness of their
rule-based model in fraud detection using visualization techniques? To answer this question, a
design-oriented research was conducted.

5.1 Method

The fraud detection model is developed by rule builders. Presenting Quality-of-Service harm and
Allocational harm to these rule builders can be done in countless ways, using different visualization
techniques or data aggregation steps. Visualizations techniques should be investigated to find a
suitable way of presenting relevant data such that the rule builders are sted in developing fair
fraud detection models. At it’s core, design-oriented research is a process of developing solutions
to problems and is meant for the knowledge that comes from the process of bringing a product
into being. The process of designing the artifact should be seen as the main contribution, while
the artifact that has been developed becomes more of a means than an end [18]. This research
method is suitable in this study, because we develop a framework for context-aware fairness tools

 

to assess rule-based models, and not to design the best tool possible in this context. There is not
enough existing theory on what would work well in assessing fairness in fraud detection models,
which makes it difficult to set up a research-oriented study. An iterative process with feedback
cycles is a well-suited contribution.

 

The process of determining the implementation of these notions of fairness in fraud detec-
tion is as follows. First, all relevant information should be listed that could be presented to the
model developers (Section 5.2: Quantifying harms). This information and conversations with rule

 

builders will lead to specific use cases of fairness in fraud detection. These use cases describe the
ways in which rule builders can interact with a tool that visualizes relevant information to analyze
the impact of the fraud detection model. Second, visualization techniques were researched in the
literature and mock ups are can be designed (section 5.3: Visualizations and mock ups). These
mock ups will be evaluated with the rule builders to gather feedback. The fraud detection data
set should be analyzed and transformed to a data set that can be used for a tool (section 5.4: The
data set). Eventually, a tool is developed with the data set and a prototype that will be created
based on the research, mock ups, and iterations with the rule builders (section 5.5: From design
to prototype). This prototype will then be evaluated with rule builders until a final version of the
prototype and its documentation are finalized.

 

12 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

1. The rule builder must be able to inspect the sizes of the protected groups.
This information is important to interpret the severity of the potential harm. Other visual-
izations include absolute or relative numbers because the rule builder should be aware of the
proportions.

2. The rule builder must be able to report the ratios of (true) fraudulent transactions in all
protected groups.
A typical fraud detection data set is highly imbalanced, the number of fraudulent transactions
(positives) is less than %1. If there is a difference between protected groups in the number
of actual (true) fraudulent transactions, then the rule builder should take these numbers into
consideration when analyzing the other visualizations and metrics.

3. The rule builder must be able to report the ratio of predicted fraudulent transactions by the
model.
The difference between the number of predicted fraudulent transactions of the protected
groups, might give an indication of Allocational harm (i.e. the model is more focused
on one group than another, so more service is dedicated to that group). It is up to the
rule builder to determine if this difference is preferred (this can be determined by combining
visualizations and metrics in this tool).

4. The rule builder must be able to report the True Positive Rate (TPR) of the protected
groups.
TPR (also known as Recall) of a classified model measures the proportion of positives that
are correctly classified of all actual fraudulent transactions:

TPR= TP+FN

This metric is relevant because TPR is a measure of how well a model performs, which relates
to the Quality-of-Service harm. If there is a difference in TPR between the protected
groups, then this indicates that the model is worse in classifying true fraudulent transactions
in, for example, the minority group. The model might be developed with a majority group
in mind, and therefore misses fraudulent transactions in the minority group. Analyzing only
the TPR difference between groups would lack the information of how many false positives
are generated in these groups (i.e. is the model ignoring this group, or does the model make
more false classifications in this group). Combining TPR difference with FPR or Precision
will allow the rule builder to draw better informed conclusions on fairness in fraud detection.

a

The rule builder must be able to report the False Positive Rate (FPR) of the protected
groups.

FPR (also known as Type I error) of a classified model measures the proportion of positives
that are mistakenly classified of all actual non-fraudulent transactions:

FP
FPR = gpyTN

This metric is relevant because FPR is a measure of how well a model performs, which relates
to the Quality-of-Service harm. If there is a difference in FPR between the protected
groups, then this indicates that the model more often mistakenly classifies one protected
group as fraudulent, for example, the minority group. The model might have a misplaced
bias against this minority group, and therefore is predicting worse than for the majority
group. Analyzing only the FPR difference between groups would lack the information of how
many true positives are generated by the model in these groups (i.e. is generating more false
positives a byproduct of catching many fraudulent transactions). Combining FPR difference
with TPR or Precision will allow the rule builder to draw better informed conclusions on
fairness in fraud detection.

 

Assessing fairness in anomaly detection 17

APPENDIX A.

QUALITY-OF-SERVICE HARM CONSIDERED FAIRNESS METRICS

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

Name(s) Formula Fraud detection context
True Positive TPR = TP = TP — The fraction of all actual frauds
Rate (recall, ap OE P+FN | that resulted in a fraud alert by
eae 1-FNR
sensitivity) the rule model.
Tite Neg: TN TN The fraction of classified ac-
ative Rate | TNR= = = 7-S5 = ~ .
es Ni ENTER tual non-fraudulent transactions
(specificity, 1—FPR .
ne among all negatives.
selectivity)
False Neg- FN FN ie cyl 7 ‘
‘ FNR = => = =o = | The fraction of all false negatives
ative Rate P ENTTP :
. 1—TPR among all fraud alerts.
(miss rate)
False Posit- | FPR = EP = ho = | The fraction of all false positives
ive Rate 1-—TNR among all negatives.
Positive P Of all fraud alerts, the frac-
Teisarwsl a PPV = wyparp Dor p = 1-— | tion of true fraudulent cases.
as FDR Includes only fraud alerts, no
(precision) . :
missed cases.
False Discov- | FDR = pore = 1 -— | Ofall fraud alerts, the fraction of
ery Rate PPV false fraud alerts.
False Omis. | FOR = —£N = 1- Of all classified non-fraudulent
. FN+TN transactions, the fraction of ac-
sion Rate NPV :
tual fraudulent cases.
Negative Npvy = -2NX_ =1- Of all classified non-fraudulent
Predictive ~ TN+FN transactions, the fraction of ac-
FOR :
Value tual non-fraudulent cases.
The LR+ represents the trade-
Positive like- off between TPR and FPR. The
lihood ratio | LR+= fat more positives, the higher the
(LR+) TPR, but also the higher the
FPR.
The LR- represents the trade-
Negative off between FNR and TNR. The
likelihood LR- = exe more negatives, the higher the
ratio (LR-) TNR, but also the higher the
FNR.
Diagnostic LR+ The LR+ and LR- combined in
: DOR = 3 .
odds ratio LR a metric.
“ees The F-score is the harmonic
ci _ Precision*Recall
FI score fr =2* pre ston+ Recall mean of precision and recall.
. TPLTN The fraction of well classified
ACC PEN = :
Accuracy TPLTN : transactions among all transac-
TP+IN+FP+FN tions.

 

 

 

 

Table A.1: All QoS-harm measurements of confusion matrix

 

46

Assessing fairness in anomaly detection

 

APPENDIX B. MOCK UPS

 

What?

In Figure B.4, a similar bar plot as Figure B.3 is presented, but adjusted on how often the fraud
detection model predicts a fraud alert for each protected group. The difference with the previous
bar plot is presented with a lighter piece of bar when the fraud detection model missed fraudulent
transactions, and a darker piece of bar when more fraudulent transactions were investigated than
necessary. It is important to realize that this does not include false positives and false negatives.
These can weigh each other out and present a bar that might look fine, but has many falsely
predicted transactions.

Why?
If this visualization might not present how well the fraud detection model predict if a transaction
is fraud or not, why visualize this part? This bar plot might give an indication which protected
group has more investigations. Does one protected group gain more attention than another? This
part should leave the rule builder with questions like: how often is the model correct? Is there a
reason that one protected group is investigated more often?

 

Assessing fairness in anomaly detection 49

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

6. The rule builder must be able to report the Precision of the protected groups.

Precision (also known as PPV: Positive Predictive Value) of a classified model measures the
proportion of positives that are correctly classified of all transactions that are predicted to be
fraudulent:

TP.
PPV = ppirP

This metric is relevant because it is a measure of how well the model performs, which relates
to the Quality-of-Service harm. A difference between the Precision scores of the protec-
ted groups indicates that the model is better in distinguishing fraud from non-fraud in one
protected group, for example, in the majority group. It might be the case that certain rules
only work well in a majority group than in minority groups. Analyzing only the Precision
difference between these groups would lack the information of how many false negatives are a
result of this model. It is not clear how many fraudulent transactions are missed. Therefore,
combining Precision (PPV) with TPR will allow the rule builder to draw better informed
conclusions on fairness in fraud detection.

7. The rule builder must be able to analyze the confusion matrices of the protected groups.

Confusion matrices of protected groups will help the rule builder by giving perspective in the
absolute numbers of classifications. The fairness metrics that are presented in the tool are
derived from these numbers.

8. The rule builder must be able to report the distance between the protected groups on the

ROC curve.

The ROC curve works well for analyzing the trade-off between TPR and FPR, making this
an important part of the Quality-of-Service harm. The distance between the points of
protected groups on the curve, represents the fairness metric Equality of Odds. By plotting
the ROC curve, the rule builder can interpret the trade-off (the trade-off: the higher the
amount of positives, the higher the chance of increasing the TPR, but also in increasing the

FPR).

9. The rule builder must be able to report the distance between the protected groups on the

Precision Recall curve.

The fraud detection data set is dominated by the negatives. The ROC curve might limit the
interpretation because of the large number of negatives. A large change in the number of false
positives can lead to a small change on the ROC curve (because of the FPR), while Precision
captures the effect of the large number of false positives. The distance on the Precision Recall
curve between the protected groups is also a way of measuring the Quality-of-Service.

From these requirements, it is expected that the rule builder could report, on a global level,

how fair the results are that are generated by the model.

 

18

Assessing fairness in anomaly detection

APPENDIX B. MOCK UPS

 

B.1.3 Screen 3

Analysis individual rules

Step 3:

On selected
measurements of the
confusion matrix, the
distance between the
two protected groups
are calculated and
presented.

ROC curve

The largest distance represents the
largest difference between male and
female

Distance

a mv
Ea a]
is oF

 

 

 

 

 

 

 

 

 

 

 

Information selected rules

BochelorEducstion = 05 AND sored» 05

[2] bacheloréducation > 0.5 AND joss <= 1845.0 and isMarried > 0.5
Figure B.8: Version 0.0 - Screen 3

The goal of the last screen of this version (Figure B.8) is to find unfair rules, determined by
the quality-of-service harm. Unfair rules should be presented, and the rule builder should get a
clear idea why a certain rule is unfair.

Visualization 1 of Screen 3

ROC curve

Rule, Cat
© ri, Female
#1, Mole
12, Female
£2, Male

eee

TPR

 

° oor 0.02 0.03 0.04 0.05 0.06
Figure B.9: Version 0.0 - Screen 3 - Visualization 1

What?
In Figure B.9, a ROC curve is displayed with the True Positive Rate on the y-axis and False
Positive Rate on the x-axis. The distance represents the difference on this curve between the two
protected groups.

Why?
The closer the rule is to left above, the better rule in a protected group works. If the rule has a
difference on the protected groups, then this visualization shows what type of trade off the rule
makes. An increase in false positives could result in a higher recall, and the other way around.

 

52 Assessing fairness in anomaly detection

CHAPTER 2. BACKGROUND AND RELATED WORK

 

and justice are properties of social and legal systems, not properties of technical tools that can
be optimized [36]. Existing fairness tools appear to be created with the availability of algorithmic
methods in mind, instead of the actual needs of stakeholders. Holstein et al. suggest there is
a demand for a design to achieve algorithmic fairness which is informed by an understanding of
practitioners’ actual challenges [24].

Barocas et al. argue that impact assessments can be a solution for companies to approach fairness
[7]. There are practical approaches to enhance fairness as discussed in the paper of Bakalar et al.,
where the authors discuss how to disentangle normative questions of product and policy design
from empirical questions of system implementation [5]. In the paper of Raji et al., an algorithmic
auditing framework has been designed to support artificial intelligence system developments end-
to-end [33]. This study is new in the sense that it is focused on designing a tool in order to assist
model developers in understanding the concepts of fairness in their context. This study can be
used as an example on how to address fairness problems in real life contexts.

 

Assessing fairness in anomaly detection 5

CHAPTER 3. PROBLEM FORMULATION AND RESEARCH FRAMEWORK

 

 

 

 

  
  

  

Environment Design Science Research Knowledge Base
Application Domain Foundations
* People Build Design * Scientific Theories
Artfacts & & Methods
* Organizational Processes
Systems
* Experience
* Technical & Expertise
Systems Design
Cycle
* Problems
0p ties Evaluate * Meta-Artfacts
(Design Products &

 

 

 

Design Processes)

 

 

 

 

 

 

 

 

Figure 3.1: Design Research Cyc

®

Answering the research questions with this research framework is applied in this study is
divided in the following steps:

I Investigate what types of harm can impact the stakeholders in the specific context.
II Design a visualization tool in an iterative process with model developers.
III Evaluate the tool to conclude if the model developers are actually assisted.

Capturing the requirements for the research is described in Chapter 4, where a stakeholder
analysis is conducted, and the relevant types of harm are determined. In Chapter 5, a design-
oriented research is conducted where the knowledge that comes from process of designing the
artifact is the main contribution, while the artifact is more a means than an end. The artifact
is designed while evaluating its relevance with model developers. To ensure that this research
is state of the art, in Chapter 2, a related work section elaborates existing solutions to similar
challenges. The evaluation of the designed artifact can be found in Chapter 6, where a qualitative
exploratory study has been conducted to test the artifact’s intuitiveness and usefulness within its
environment.

 

8 Assessing fairness in anomaly detection

Chapter 2

Background and related work

In this chapter, background knowledge and related work that are relevant to this thesis can be
found. In Section 2.1, several fairness definitions and notations are introduced that will be used in
this report. In Section 2.2, related work to this thesis is summarized and this study is positioned.

2.1 Background

2.1.1 Algorithmic fairness research in social and legal systems

The definition of fairness according to the Cambridge Dictionary is: the quality of treating people
equally or in a way that is right or reasonable [12]. Fairness is addressed in laws, such as the
relatively new GDPR law. The General Data Protection Regulation (GDPR) law is created in
2018 to regulate how companies must manage personal data by restricting automatic decision-
making processes affecting users [22, 1]. However, a fraud detection model is not an automatic
decision-making model, because it generates alerts for suspicious transactions, which have to be
investigated by domain experts. If these models are made completely automatic in the future, then
the GDPR law becomes much more important in this process. In this research, this law has been
kept in mind. The law implies a need for algorithms and their accompanying evaluation frameworks
to avoid discrimination. The GDPR considers special categories of personal data, namely sensitive
data. Sensitive data includes: personal data revealing racial or ethnic origin, political opinions,
religious or philosophical beliefs, or trade-union membership, and the processing of genetic data,
biometric data for the purpose of uniquely identifying a natural person, data concerning health
or data concerning a natural person’s sex life or sexual orientation [43]. In this thesis, we define
discrimination as an unequal treatment of individuals of certain groups. These groups will be
called protected groups and are based on the gathered sensitive data. An unequal treatment would
mean that the fraud detection model generates less alerts in a protected group which cannot be
justified.

 

2.1.2 Algorithmic fairness research in computer science
Formalization of the fraud detection problem

Fraud detection in this research is a binary classification; the model classifies each transaction
as potential fraud or not. To formalize the binary classification problem of fraud detection we
will use the following notation. Suppose we have feature space X"™ consisting of m non-sensitive
features (the feature space can correspond to any data type included in transaction data, such
as real numbers in which case X = R”) and a single binary sensitive attribute A € {0,1} (e.g.
majority/minority).

sifier C' is a binary predictor that uses X"™ and A to predict the target space Y where

 

 

Assessing fairness in anomaly detection 3

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

a

the amount of transactions that is affected by the rule (otherwise rules with just one false
positive and nothing else, would be ranked very high). The outcome of this calculation is
normalized for visualizing both types of harm in the same graph. Allocational harm is for
every rule determined as the fraction of positives of all affected transactions that is affected
by the rule, this is also multiplied by the amount of transactions that is affected by this rule,
and also normalized.

The rule builder must be able to analyze a ranking of the largest difference in number of
False Positives between protected groups in rules.

Given the confusion matrices of the protected groups as in Figure 5.13 and 5.14, the rule
builder might want to find rules that cause a large amount of False Positives to analyze the
Quality-of-Service harm.

The rule builder must be able to analyze a ranking of the largest difference in number of
True Positives between protected groups in rules.

Given the confusion matrices of the protected groups as in Figure 5.13 and 5.14, the rule
builder might want to find rules that cause a large amount of True Positives to analyze the
Quality-of-Service harm.

The rule builder must be able to analyze a ranking of the largest difference in number of
False Negatives between protected groups in rules.

Given the confusion matrices of the protected groups as in Figure 5.13 and 5.14, the rule
builder might want to find rules that cause a large amount of False Negatives to analyze the
Quality-of-Service harm.

The rule builder must be able to analyze a ranking of the largest difference in number of
True Negatives between protected groups in rules.

Given the confusion matrices of the protected groups as in Figure 5.13 and 5.14, the rule
builder might want to find rules that cause a large amount of True Negatives to analyze the
Quality-of-Service harm.

The rule builder must be able to analyze a ranking of the largest difference in number of
Positives between protected groups in rules.

When analyzing Allocational harm, the rule builder might want to see the largest disparity
of positives between protected groups.

The rule builder must be able to analyze a ranking of the largest difference in number of
Negatives between protected groups in rules.

When analyzing Allocational harm, the rule builder might want to see the largest disparity
of negatives between protected groups.

The rule builder must be able to select a rule, in order to analyze the impact of the single
rule on the given data set. This rule must be able to switch off in the complete model, and
see relevant metrics of the perturbed model.

The analysis of the rule can be emphasized by switching the rule off. Rule builders might get
a better understanding of the effect of that rule in the complete model.

Design choices

All the figures in this section contain random numbers and are not based on a real data set.
This use case requires that the rule builder is familiar with the model performance on the protected
groups. This means that the rule builder should have done the first use case before starting with
rule tuning. The rule builder can select in the drop down a type of disparity, in Figure 5.15, False
Positive disparity is selected. The rules on the top have the highest disparity. The rule builder
can scroll down for rules with descending disparity values.

 

24

Assessing fairness in anomaly detection

CHAPTER 7. CONCLUSIONS

 

impact of this part of the model. By switching the rule off, the impact is emphasized. However,
we can conclude that switching off high impact rules was not a solution to model developers,
but adding rules for other protected groups were. For example, if a protected group had much
less fraud alerts, while missing many fraudulent transactions, then an additional rule might be
able to detect those missed frauds. Of course, a good understanding of why this protected group
requires a different rule is of high importance. Adding rules in this way must avoid throwing in
rules that do not make sense, just to attempt to make the model more fair. Adding rules while
analyzing fairness requires a good understanding in the generalizations that are caused by the
rules. Furthermore, the connection between individual rules and the complete model resulted in
an improved understanding of the impact of rules and the model on the clients in protected groups.

 

7.2. Contributions

7.2.1 Academic contribution

e The identification of notions of fairness that are applicable to rule-based models in fraud
detection.

e An approach to design a tool that shows domain experts fairness in their rule-based model.

e An evaluation of the extent to which rule builders may be assisted in identifying violations
of fairness of a rule-based model.

7.2.2 Business value

This thesis has been executed in cooperation with a bank. The business value of this thesis lies
in the suggested framework of tool development for assessing fairness in fraud detection. The
qualitative study evaluated whether the tool was useful and intuitive for rule builders at the bank
with an experiment. As discussed in the results of the experiment, the designed tool was intuitive
and could lead to an in-depth conversation of fairness in fraud detection, supported by fairness
metrics. The most useful visualizations of fairness metrics were the predicted and true fraud alerts
of each protected group, and the differences in TPR and Precision, supported by the confusion
matrices of the protected groups. We recommend the bank to implement an impact analysis with
such a tool on protected groups in order to ensure an equal treatment to every protected group.
Especially when black box models were to be implemented on a large scale, a tool to assess fairness
becomes more important.

7.3 Limitations and Future Research

7.3.1 Defining fairness in fraud detection

This report focuses only on the Allocational and Quality-of-Service harm. Each type of harm or
definition of fairness covers another aspect. For example, individual fairness, where similar clients
should get similar predictions by the model, would also be a very interesting topic to investigate.
The difficult part of applying individual fairness in such a tool is communicating this fairness
metrics to model developers. The individual fairness metric is a distance metric, which should
be chosen very carefully in high dimensional data sets such as fraud detection [16]. Additionally,
counterfactual fairness, where changing the sensitive attribute should not change the prediction
in order to be fair, is also a very interesting topic to measure fairness in fraud detection [25].
However, this fairness definition must take proxies of sensitive attributes into consideration [14],
and different contexts [34].

As there is no one true definition of a fair model, it is important that the impact on stakeholders

  

 

Assessing fairness in anomaly detection 39

CHAPTER 6. QUALITATIVE EVALUATION OF THE DESIGNED TOOL

 

experiment.

6.1.1 Participants and design

The participants in this study were rule builders at a compliance department of a major Dutch
bank. These rule builders are domain experts in fraud detection. Ideally, the sample size would
be determined by reaching a saturation in the results such that it addresses the core explanatory
questions [28]. However, testing with small sample sizes will lead to the most important results,
and is not likely to change after running more tests [20].

This experiment was a qualitative exploratory research. Qualitative research focuses on un-
derstanding the context and on using a holistic approach [29]. The researcher does not test a
hypothesis in this study. Qualitative research can provide complex textual descriptions of how
people experience a given research issue. Quantitative research tries to test specific hypotheses.
Quantitative research would lack the in-depth information that can be obtained by qualitative
research [6].

In this experiment, the rule builders were asked to perform a small number of tasks to get
familiar with the tool, and some questions to evaluate the value of the tool. The questions were
asked in a semi-structured way. In this way, the analysis of the questions can be conducted in a
structured fashion, while still being flexible enough to allow for more in-depth exploration [27].
During the tasks, each participant was asked to explicitly voice his/her thought processes to get
an understanding of how the participant perceives the tool. All tasks and questions were kept the
same for each experiment. It is expected that after the experiment the researcher can understand
which parts of the tool were well understood and which parts were useful in analyzing the fairness
of the fraud detection model.

6.1.2 Procedure

The participants were contacted through colleagues and received a short explanation of what the
experiment would entail. For every participant, an individual meeting was planned. Due to the
Covid-19 pandemic, this experiment was forced to be conducted remotely. This experiment was
conducted through Microsoft Teams. The meeting was recorded. During the experiment, the tool
was shared on the screen of the researcher, and the participant had control over the tool. The
researcher could support the participant when the participant got stuck. When the meeting of
the experiment started, the participant was welcomed and received a quick explanation of the
experiment. The quick explanation entailed the research subject and how the tool was developed.
The participant was asked to sign an informed consent form. The questions and tasks during the
experiment were conducted in a semi-structured manner. Semi-structured interviews are inter-
views where the main questions are prepared beforehand, leaving some freedom to the researcher
on how to formulate the questions. The researcher has a listening function and encouraged the
participants to elaborate on their answers. To have a natural conversation, the researcher gave
appropriate verbal cues as a listener on a regularly basis for continuing the conversation. The
experiment was conducted in Dutch as it is the native language of the participants.

6.1.3. Experiment

First, a few questions were asked before the participant started with the tasks. These questions
were about the function of the participant in the company and how often the participant was
working on fairness in fraud detection. Additionally, the participants were asked how they would
describe a fair fraud detection model. These questions were asked in order to get an indication of
how well-acquainted the participants are with the topic of fairness in models.

After these questions the participant was asked to perform two tasks, while thinking aloud. The
tasks included analyzing a binary attribute on the fairness notions; whether the transactions
was a private or a business transaction. During these tasks the researcher would give minimal
support. The researcher would notice whether the participant understood the notions of fairness,

 

Assessing fairness in anomaly detection 31

Abstract

Anomaly detection systems are used for various tasks in the financial domain, such as financial
raud detection, tax evasion and insurance fraud detection. As these systems interact with society,
hey can be seen as socio-technical systems with a large action space. Socio-technical anomaly
detection systems cannot provide a perfect solution, because the anomaly detection problem is
oo complex. This can result in various errors, which can lead to structural biases for one or
more demographic groups. These errors can propagate quickly in the large action space, which
might cause discrimination on a large scale. In recent years, academic interest in developing fair
algorithms has grown. A large part of the existing literature describes methods to implement
airness in production systems with the availability of algorithmic methods in mind. The actual
needs of the stakeholders involved have not received such academic interest. There is a need for
frameworks to achieve algorithmic fairness that are designed practitioners’ actual challenges in
mind.

The main contribution of this work is a framework to assist model developers of complex rule-
yased models by providing them with novel visualization techniques, in order to assess fairness
in fraud detection. Fairness in fraud detection can be assessed by Quality-of-Service harm and
Allocational harm. These types of harm are presented in a tool with novel visualization techniques.
This fairness assessment tool for model developers in fraud detection is developed in an iterative
rocess in cooperation with model developers at a large bank in the Netherlands. This tool is
evaluated in an exploratory qualitative study in order to assess its intuitiveness and usefulness.
This study suggests that such a tool is useful and intuitive for model developers in assessing
‘airness in fraud detection models. This framework can be used to develop a tool to assess fairness

 

 

in anomaly detection.

 

ii Assessing fairness in anomaly detection

CHAPTER 2. BACKGROUND AND RELATED WORK

 

Cex e(X™, A) € {0, 1} (2.1)

Formalizations of fairness in literature

This section describes what formalizations of fairness exists in the literature that is relevant to
this study.
Fairness can be achieved in many ways. The most distinct fairness definitions are listed here:

1. Unawareness. A classifier is fair if protected attributes are not explicitly used [19]:

C =c(a, A) = c(X)

2. Demographic parity. A classifier is fair if all protected groups have the same rate of
predicted positives [42]:

Pro[C = ¢] = Pri[C =c]

3. Equalized odds. A classifier is fair if all protected groups have an equal TPR (True Positive
Rate) and an equal FPR (False Positive Rate) [38]:
ProfY =y|C =e] = PrilY =y|C =e]

4. Individual fairness. A classifier is fair if all individuals that are similar, are treated similar
[16]:
D(m(X), M(X’)) < d(X, X’), where X, X’ € R® are input feature vectors and D and d are
two metric functions on the output space.

Counterfactual fairness. A classifier is fair if the sensitive attribute can be changed,
without changing the prediction [35]:
Pr(C_{A ¢ 0} =¢|X,A =a] = PIC {A+ 1} =c|X,A =a]

a

 

 

 

Unawareness (1) as fairness definition can be easily checked in fraud detection models. It is possible
to analyze whether the sensitive attributes are used as input for classifying the transactions.
Demographic parity (2) and equalized odds (3) as fairness definitions are more difficult to achieve,
and are relevant to fraud detection. Demographic parity would show if the fraud detection model
focuses more on a protected group (i.e. detects more fraudulent transactions in one group than
the other). There might be more rules developed with a certain group in mind. This could cause
clients in the other protected group more financial harm when fraud is less detected. Equalized
odds would show whether the fraud detection model is more often correct in one group as opposed
to another. Individual fairness (4) as fairness definition would show for an individual transaction
whether these are classified fairly by the fraud detection model. In this research, we focus on
investigating whether a model is fair. This conclusion cannot be easily drawn from this fairness

  

metric, because distance metrics for this fairness are difficult to determine, and to interpret.
Counterfactual fairness (5) as fairness definition would show the affect of the sensitive attribute in
the fraud detection model. In this research, counterfactual fairness would take causal relationships
with the sensitive attribute into account. As this would be interesting to investigate, it does not
demonstrate the impact of the fraud detection model on the protected groups.

2.2 Related work

In recent years, an increased academic interest for developing fair algorithms has grown. An im-
portant problem in the literature on algorithmic fairness is formalizing mathematical definitions of
what constitutes a fair model. These definitions all have their own advantages and disadvantages,
which in turn depend on the context the model is used in [9, 19, 25, 30, 38]. Toolkits have also
been developed by companies to measure multiple fairness-oriented metrics [8, 11].

However, not many literature describes methods to implement fairness in production systems with
complex models, and see discrimination in socio-technical systems as a technical problem. Dis-

  

crimination in socio-technical systems is a social issue first, and a technical issue second. Fairness

 

4 Assessing fairness in anomaly detection

CHAPTER 3. PROBLEM FORMULATION AND RESEARCH FRAMEWORK

 

3.1.2 Requirements of fraud detection

Within the problem context of the bank, there are several requirements which solving this problem
must adhere to.
Fraud detection is a classification problem with the following characteristics.

e Fraud detection in this use case is a binary classification; the model classifies each transaction
as potential fraud or not.

e Less than 1% of the transactions are fraudulent. Data sets for fraud detection are highly
imbalanced.

e The fraud detection model is a rule-based model which contains a set of complex decision
rules.

e Rule-builders develop the rule-based models in order to detect as many instances of frauds
as possible while at the same time keeping the amount of false positives at a manageable
level.

3.2 Research Framework and Questions

The field of algorithmic fairness in practice is still in its infancy. Most existing research focuses
on developing new principles and measurements methods rather than the few works that focus
on tackling challenges of operationalizing fairness in practice. A proper review framework might
be an approach for monitoring fairness in complex rule-based systems which has a focus on the
context.

3.2.1 Research Questions

The main research question of this study is as follows:

To what extent can model developers be assisted in assessing fairness of a fraud
detection model?

The research question is answered by answering the following sub questions.

I What is an appropriate fairness definition in detecting fraud with rule-based models?

II How can we assist model developers in analyzing fairness of their rule-based model in fraud
detection?

II To what extent does the developed tool assist model developers in understanding and identi-
fying violations of the notions of fairness in fraud detection?

3.2.2 Research Framework

In this study, we solved the presented problem with a paradigm which aims at creating and
evaluating innovative artifacts that address important and relevant organizational problems [23].
Adikari et al., proposed a simplified design version of research framework of Hevner et al. as can
be found in Figure 3.1 [3, 23].

This research cycle connects the contextual environment with the design science activities.
Its main goal is to capture the requirements for the research in order to provide design solution
artifacts to the environment for study and evaluation in that specific domain. The rigor circle
ensures innovation by providing existing knowledge to the research. The internal design cycle
iterates between core activities of building and evaluating the design artifacts and processes of the
research [3].

 

Assessing fairness in anomaly detection t

CHAPTER 4. DEFINING FAIRNESS IN FRAUD DETECTION

 

complicit in societal bias or prejudice. When systematic, unfair decisions are made, it hurts the
relationship between clients and company. On the other hand, perceiving information about com-
panies protecting their clients from unfairness of discrimination greatly enhances user trust and
strengthen their relationship with their company. Clients want to know that companies take their
feelings into consideration. In fraud detection, clients would want the fraud detection model clas-
sifies their transactions as correct as possible. If the model misses an alert, then the client might
suffer from financial damage. If the model classifies non-fraudulent alerts as potential fraudulent,
this may cause discomfort when the transaction has to be investigated. A structural mistake of
the model which is based on a sensitive attribute, might decrease the trust of clients in their bank.

4.2.2 Compliance department

The bank is responsible for abiding to laws of detecting fraud and laws of protecting the personal
data of the clients. The compliance department manages these laws in the processes of the bank.
It is important to the bank that clients are satisfied with the services that are provided. The bank
and compliance department must provide policies for the fraud detection model that is developed
by the rule builders.

4.2.3 Rule builders

Rule builders aim to create models with a high recall and low false positive rate. They can see
the performance of the complete model, and might be able to detect systematic problems given
the appropriate tools.

4.3 Fairness-Related Harms

The risk of harming clients can be measured by analyzing the impact of the fraud detection models
on protected groups as opposed to unprotected groups.

Lipton et al. describe two notions of discrimination; disparate treatment and disparate impact
[26]. We can also call these two notions respectively: direct and indirect discrimination. Disparate
treatment (or direct discrimination) is intentional discrimination where the model explicitly makes
decisions based on sensitive attributes. In the rule-based model of fraud detection, this would mean
that a rule uses a sensitive attribute in order to detect fraud. This type of discrimination is not
difficult to address, a search through the rules on sensitive attributes would be enough. Disparate
impact (or indirect discrimination) is an unjustified impact on members of a protected group. If
this impact is not a result of disparate treatment, then it can be a result of correlations between
sensitive and non-sensitive attributes. This type of discrimination is more difficult to detect and
to regulate.

In this study, we have chosen for a moral approach in determining fairness in fraud detection;
fairness in fraud detection depends on the impact of the model on clients.

Fairness is achieved when the fraud detection model does not harm a protected group. There
are different types of harm that can be considered.

e Harms of representation: when developing a system, a certain protected group is not con-
sidered. Representational harm could be: stereotyping, recognition, denigration, and under-
representation [7]. This type of harm is a result of a long term process, is difficult to formalize
and is dependent on culture.

e Harms of allocation: when a system withholds certain groups an opportunity or a resource [2].
This type of harm can be immediately detected and is easily quantifiable. An Allocational
harm can range from a small but significant and systematic difference in treatment between
protected groups, all the way to complete denial of a service.

 

10 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

Confusion matrices

Confusion matrices of protected groups will give insight in how well the model performs for both groups. If there
are large differences in the confusion matrices, then there might be Quality-of-Service harm. Curious which rules
are causing differences? Check out the tab Individual rules: Classification

Zakelijk

  

Figure 5.13: Use case 1: Confusion matrices (relative)

Particulier confusion matrix

 

Zakelijk confusion matrix

Figure 5.14: Use case 1: Confusion matrices (absolute)

5.5.2 Use case 2: Tuning rules

When the rule builder has an inkling that the performance or amount of service of the model
differs between protected groups, it should be further investigated. What a rule builder needs is
a concrete rule (or subset of rules) that are at the root of the observed disparity. In case a rule is
found to be potentially problematic, the rule builder might consider leaving the rule out because
of fairness considerations.

Requirements

In this use case, a rule builder can find the unfair rules which may be unfair (largest disparity),
and analyze the impact of that rule on the data set.

1. The rule builder must see a ranking of rules that have a large disparity on Quality-of-
Service harm and Allocational harm to have an idea what rules are interesting to ana-
lyze.

It is expected that rule builders prefer a set of concrete rules that should be analyzed because
they have relatively high scores on Quality-of-Service harm and Allocational harm.
Quality-of-Service harm is for every rule determined as the fraction of falsely predicted
transactions of all transactions that are affected by the rule. This fraction is multiplied by

 

Assessing fairness in anomaly detection 23

Appendix B

Mock ups

B.1_ Final version

B.1.1 Screen 1

Senisitiveiattribute: There are 2 categories in gender

C=] ® in the complete data set:

© Femaie:
In order to determine if the ° Male:
modal treats the protected

groups based on gender

evenly fair, this tool presents

several steps to inform you:

 

Step 1: becoming aware of the
occurrence of all categories of

 

gender
Detected actual fraud alerts: Predicted fraud alerts:
distribution between groups distribution between groups

 

 

  

 

 

 

: 8
ge Average of Be werage of
ess tana Classification 2s nes
Sue by rule mode!
Hf
=Bb
Fraud Fraud Fraud Fraud
Slots sate ots aes
forcies mate feneles aes

Figure B.1: Version 0.0 - Screen 1

The goal of the first screen (Figure B.1) is to introduce the rule builder in the chosen sensitive
attribute. No fairness is defined here yet.

Visualization 1 of Screen 1

What?
In Figure B.2, the categories of the sensitive attribute are listed. The number of transactions that
belong to a protected group (male or female in this case), are visualized in a circle diagram.

Why?
The rule builder might consider the sizes of the protected groups to get a better understanding of
the impact of the model on the data.

 

Assessing fairness in anomaly detection AT

APPENDIX B. MOCK UPS

Number of instances in protected group

There are 2 categories in gender
in the complete data set:

e Female:
e Male:

Female

 

Figure B.2: Version 0.0 - Screen 1 - Visualization 1

Visualization 2 of Screen 1

Detected actual fraud alerts:
distribution between groups

— Average of

both groups

Freud Fraud
alerts alerts
females males

protected group / size of

# fraud alerts of
protected group

Figure B.3: Version 0.0 - Screen 1 - Visualization 2

What?
In Figure B.3, the actual distribution of between the protected groups should become clear. In
fraud detection, not all frauds have been included in the data set. For this reason the known
fraudulent transactions (either falsely or truly classified by the model) are plotted. The bar plots
are scaled towards the size of the protected group. For instance, if there are 100 transactions
belonging to a female client, and 10 fraudulent transactions of female clients, will lead to a value
of oo on the bar plot. The average line will present the weighted mean of both protected groups.
Why?
The rule builder has seen Figure B.2 and might wonder if these protected groups differ in the
number of fraudulent transactions. An absolute value would be hard to interpret, so a scaled bar
plot (based on the size of the protected group) is proposed.

Visualization 3 of Screen 1

Predicted fraud alerts:
distribution between groups

 

Claeeeaon
bye model

 

Figure B.4: Version 0.0 - Screen 1 - Visualization 3

 

48 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

Select in the dropdown menu what you want to analyze.

 

Negatives difference x
Rule 1 r 2580 M Zakeliyk
Iss Mi Particulier
mule2 ane
Rule ae
360,

Rule 4

Figure 5.20: Use case 2: Ranking on largest Negative classification disparity

Because the disparity bar charts might be too detailed for drawing conclusion fast, the rule
builder might want a ranking of the highest Quality-of-Service harm and Allocational harm.
In Figure 5.21, the rules are ranked on the combined harm of both types (QoS and Allocational).
The visualization of these values are represented in a bar chart, although both types of harm are
not the same, and therefore harder to compare. On the other hand, the rule builder gets somehow
an idea of the severity of the harm is relative to the other rules. It is important to keep in mind
that this measure of harms is dependent on the selected data set.

Rule disparity on Allocational harm & QoS
harm

All these numbers can be combined to find the rule which is most interesting to investigate in terms
of Quality-of-Service harm or Allocational harm. Quality-of-Service harm is the difference in
protected groups in the error (complement of accuracy) of the rule multiplied by the number of
affected transactions of the rule. Allocational harm is the difference in affecting the transactions. All
these outcomes are normalized for ranking.

Rule1 HE  Allocational
Allocational HM Qos
Rule 2

Rule 3
Allocational

Figure 5.21: Use case 2: Ranking on QoS and Allocational harm disparity

 

Assessing fairness in anomaly detection 27

CHAPTER 1. INTRODUCTION

 

Different from previous work in this area, the novel visualization techniques were designed in an
iterative process, and the visualization techniques were evaluated with model developers
in a qualitative study. Additionally, this study approaches an existing large-scale and
complex production system at a large bank in the Netherlands. The study is executed with
real transaction data which is highly dimensional, and a real complex fraud detection model
which is used by model developers. This report might be useful to other practitioners facing
similar challenges, and to other researchers to address the needs of practitioners.

1.3. Outline

This thesis report is organized as follows. Chapter 2 covers the background knowledge and related
work of algorithmic fairness. In Chapter 3, the research problem context is described at the
fraud detection department of the bank. Moreover, the research questions and framework are
established. Chapter 4 consists of a stakeholder analysis and a literature study of harms to
formulate the relevant fairness harms in the fraud detection context. Chapter 5 uses this notion
of harm in order to design a tool to assist model developers in detecting and understanding the
determined fairness harms. Chapter 5 presents an approach to design a prototype of a tool for
model developers. This results of the tool evaluation are presented in Chapter 6. This chapter
covers an exploratory and qualitative study to investigate the intuitiveness and usefulness of the
developed prototype. The qualitative study was a conducted experiment with rule builders, which
had to perform several tasks to get familiar with the tool. Finally, the conclusions and limitations
of the complete study are discussed in Chapter 7.

 

2 Assessing fairness in anomaly detection

EINDHOVEN
eo UNIVERSITY OF
TECHNOLOGY

Eindhoven University of Technology

MASTER

Assessing fairness in anomaly detection
A framework for developing a context-aware fairness tool to assess rule-based models

Boehlé, Roxanne V.

Award date:
2021

Link to publication

Disclaimer

This document contains a student thesis (bachelor's or master's), as authored by a student at Eindhoven University of Technology. Student
theses are made available in the TU/e repository upon obtaining the required degree. The grade received is not published on the document
as presented in the repository. The required complexity or quality of research of student theses may vary by program, and the required
minimum study period may vary in duration.

General rights
Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners
and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.

+ Users may download and print one copy of any publication from the public portal for the purpose of private study or research.
+ You may not further distribute the material or use it for any profit-making activity or commercial gain

EINDHOVEN
e UNIVERSITY OF
TECHNOLOGY
Department of Mathematics and Computer Science
Data Mining Research Group

Assessing fairness in
anomaly detection

A framework for developing a context-aware
fairness tool to assess rule-based models

Master thesis by R.V. (Roxanne) Boehlé

Supervisors:
Prof. dr. M. (Mykola) Pechenizkiy (TU/e)
H.J.P. (Hilde) Weerts, MSc (TU/e)
W. (Werner) van Ipenburg, MSc (Rabobank)
J.W. Veldsink, MSc (Rabobank)

Assessment committee:
Prof. dr. M. (Mykola) Pechenizkiy (TU/e

)

H.J.P. (Hilde) Weerts, MSc (TU/e)

W. (Werner) van Ipenburg, MSc (Rabobank)
J.W. Veldsink, MSc (Rabobank)

Dr. R. (Renata) Medeiros de Carvalho (TU/e)

Final version

Eindhoven, March 26, 2021

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

After gathering all these different indications for both types of harm, a rule builder might
want to investigate more on a specific rule. In Figure 5.22, the rule builder can either search for
a specific rule, or select one of the suggested rules.

Tuning rules

Enter rule or select one of the listed recommended rules
input rule SUBMIT

> Rule 1

O Rule2
Rule 3

Figure 5.22: Use case 2: Selecting rule for tuning

When a rule is selected, the same figures are then presented based on the fact that the selected
tule is switched off in the complete model. This interaction shall make it possible to analyze
the impact of the individual rule on the complete performance of the model. The same table of
fairness metrics are visualized as in Figure 5.7, but now the model does not use the selected rule
to classify the data set. The rule builder can compare if the model has improved on those fairness
metrics by switching off this rule. The confusion matrices of the new model as in Figure 5.13 and
5.14 to analyze the performance and fairness of the model.

 

28 Assessing fairness in anomaly detection

Appendix A

Quality-of-Service harm
considered fairness metrics

In Table A.1, we present all calculations of the confusion matrix that are considered as fairness
metrics.

 

Assessing fairness in anomaly detection 4E

Chapter 4

Defining fairness in fraud
detection

In Chapter 3 the problem formulation and the research goal are discussed. In this chapter we
answer the question: What is an appropriate fairness definition in detecting fraud with rule-based
models? To answer this question, the fraud detection process is analyzed, and a stakeholder
analysis is conducted. Finally, types of harm relevant to fraud detection are elaborated on and
notion of fairness for this study is chosen.

4.1 Fraud detection process

Multiple stakeholders are involved in the process of detecting financial fraud in transactions. First,
the data originates from transactions that are executed by clients. A client can have different
types of transactions. One might actively transfer money to another client, a direct debit to
a company, or a payment request. This information is also included in the attributes of the
transaction.

When an individual, or series of, payment(s) have been made that are not in line with the
clients’ usual pattern of spending, the fraud detection model generates a fraud alert.

Further investigation is performed by a fraud analyst. The fraud analysts are in contact with
the model developers of the fraud detection model: the rule builders. The fraud analysts and
the rule builders discuss if the model should be adjusted. This adjustments has an effect on the
trade-off between catching enough fraudulent transactions and alerting too many non-fraudulent
transactions.

These particular fraud detection guidelines are determined by the compliance department.
This department determines a policy on how to abide to the relevant laws and regulations.

4.2 Main stakeholder analysis

The research problem of finding (un)fairness in fraud detection models involves several stakehold-
ers.

4.2.1 Client

Clients have chosen to trust their money to their bank. Woodruff et al. [41] highlight the import-
ance of company handling of algorithmic fairness, which interacts significantly with user trust.
Their trust can be broken by feelings of betrayal, disappointment, or anger when a company is

 

Assessing fairness in anomaly detection 9

Bibliography

an

 

 

2018 reform of eu data protection rules. 3

Mohsen Abbasi, Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian.
Fairness in representation: quantifying stereotyping as a representational harm. In Proceedings
of the 2019 SIAM International Conference on Data Mining, pages 801-809. SIAM, 2019. 10

Sisira Adikari, Craig McDonald, and John Campbell. Little design up-front: a design sci-
ence approach to integrating usability into agile requirements engineering. In International
Conference on Human-Computer Interaction, pages 549-558. Springer, 2009. 7

Alejandro Barredo Arrieta, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham
Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lépez, Daniel Molina, Richard Ben-
jamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities
and challenges toward responsible ai. Information Fusion, 58:82-115, 2020. 40

Chloé Bakalar, Renata Barreto, Miranda Bogen, Sam Corbett-Davies, Melissa Hall, Isabel
Kloumann, Michelle Lam, Joaquin Quifonero Candela, Manish Raghavan, Joshua Simons,
Jonathan Tannen, Edmund Tong, Kate Vredenburgh, and Jiejing Zhao. Fairness on the
ground: Applying algorithmic fairness approaches to production systems, 2021. 5, 6

Chris Barnham. Quantitative and qualitative research: Perceptual foundations. International
Journal of Market Research, 57(6):837-854, 2015. 31

Solon Barocas and Andrew D Selbst. Big data’s disparate impact. Calif. L. Rev., 104:671,
2016. 1, 5, 10

Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde,
Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic,
et al. Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating un-
wanted algorithmic bias. arXiv preprint arXiv:1810.01948, 2018. 4

Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in
criminal justice risk assessments: The state of the art. Sociological Methods and Research,
page 0049124118782533, 2018. 4

Monowar H Bhuyan, Dhruba Kumar Bhattacharyya, and Jugal K Kalita. Network anomaly
detection: methods, systems and tools. Ieee communications surveys & tutorials, 16(1):303-
336, 2013. 1

Sarah Bird, Miroslav Dudik, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa
Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit
for assessing and improving fairness in ai. URL https://www. microsoft. com/en-
us/research/uploads/prod/2020/05/Fairlearn_whitepaper. pdf, 2020. 4

Cambridge. | Cambridge dictionary, (accessed: 01.12.2020). https: //dictionary.
cambridge. org/dictionary/english/fairness. 3

 

42

Assessing fairness in anomaly detection

BIBLIOGRAPHY

 

13

14

16

17

18

19

20
21

22

23

24)

25

26

27

28

29

30

3l

 

32

 

Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM
computing surveys (CSUR), 41(3):1-58, 2009. 1

Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical
review of fair machine learning. arXiv preprint arXiv:1808.00023, 2018. 39

William Dieterich, Christina Mendoza, and Tim Brennan. Compas risk scales: Demonstrating
accuracy equity and predictive parity. Northpoint Inc, 7(7.4):1, 2016. 1

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science
conference, pages 214-226. ACM, 2012. 4, 39

Virginia Eubanks. Automating inequality: How high-tech tools profile, police, and punish the
poor. St. Martin’s Press, 2018. 1

Daniel Fallman. Why research-oriented design isn’t design-oriented research. Nordes, (1),
2005. 12

Pratik Gajane and Mykola Pechenizkiy. On formalizing fairness in prediction with machine
learning. arXiv preprint arXiv:1710.03184, 2017. 4

Kevin Godby. Why you only need to test with five users, 2012. 31

Markus Goldstein and Seiichi Uchida. A comparative evaluation of unsupervised anomaly
detection algorithms for multivariate data. PloS one, 11(4):e0152173, 2016. 1

Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-
making and a “right to explanation”. AI Magazine, 38(3):50-57, 2017. 3

Alan Hevner and Samir Chatterjee. Design science research in information systems. In Design
research in information systems, pages 9-22. Springer, 2010. 7

Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and Hanna
Wallach. Improving fairness in machine learning systems: What do industry practitioners
need? In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,
pages 1-16, 2019. 1, 5, 6

Atoosa Kasirzadeh and Andrew Smart. The use and misuse of counterfactuals in ethical
machine learning. arXiv preprint arXiv:2102.05085, 2021. 4, 39

Zachary C Lipton, Alexandra Chouldechova, and Julian McAuley. Does mitigating ml’s
impact disparity require treatment disparity? arXiv preprint arXiv:1711.07076, 2017. 10
Robyn Longhurst. Semi-structured interviews and focus groups. Key methods in geography,
3(2):143-156, 2003. 31

Jacqueline Low. A pragmatic definition of the concept of theoretical saturation. Sociological

Focus, 52(2):131-139, 2019. 31

Matthew B Miles, A Michael Huberman, and Johnny Saldana. Qualitative data analysis: A
methods sourcebook. Sage publications, 2018. 31

Shira Mitchell, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum.
Prediction-based decisions and fairness: A catalogue of choices, assumptions, and definitions.
arXiv preprint arXiv:1811.07867, 2018. 4

Tamara Munzner. Visualization analysis and design. CRC press, 2014. 15, 19

Arun Rai. Explainable ai: From black box to glass box. Journal of the Academy of Marketing
Science, 48(1):137-141, 2020. 1

 

Assessing fairness in anomaly detection 43

Contents

 

 

 
 

   

 

 

    
   

Contents iii

1 Introduction 1

1.1 Research problem... 2... 2 ee 1

1.2 Contributions . 2c cau sa ewe e Bae Hm RS eo Sa Oe eS EO 1

1.3 Outline 2

2 Background and related work 3

2.1. Backeroutid =<: sas ews yea bee HOE ERE ES EET eH E ERB ews Ee 3

2.1.1 Algorithmic fairness research in social and legal systems ........... 3

2.1.2 Algorithmic fairness research in computer science... . 0... 0.020008 3

2.2 Related work 4

3 Problem formulation and research framework 6

3. Problericontext: as aan ssn vores Re He Ee Vo So Rae Em Re He 6

SL PYatid detettiOis » sus para eee RRS ews EHR EERE REA ewe ES 6

3.1.2 Requirements of fraud detection .... 2.2... 20.0.0... 0.000000. 7

3.2 Research Framework and Questions. ............ 7

3.2.1 Research Questions... 6. bee ee ee ee .

32:2 Research Framework ~@ ¢ « 8.5 48 ERE EWE EES SHE ERE BHR Bw 7

4 Defining fairness in fraud detection 9

4.1 Fraud detection process 9

4.2 Main stakeholder analysis 9

4.2.1 Client 2... ee 9

4.2:2 Compliatice department «ous suk cw be CR eS RH Hw Bw 10

42.3 Riilé builders 2w2 gag e4k.d $e TRE EWS ES EAE EME BHR Bw 10

4.3 Fairness-Related Harms .. 2... 2... eee 10

4.4 Conclusion fairness notions in fraud detection... 2.2... 0.0. eee 11

5 Implementing assessing fairness in fraud detection 12

Del, (Method . cas wae aos sme Somme e GN MLE aE HOM © BONE HOR Moe HK 12

5.2 Quantifyiipharmis . aa. sain cae e we Swe ewe Bo RS 13
5.2.1 Determining fairness metrics for Use case 1: Reporting fairness on a global

level 2 13

5.2.2 Determining fairness metrics for use case 2: Rule tuning. .......... 13

5.3 Visualization. atid mick Ups! «.. cain 6 aus ee cas bes RE Re Ewe BI 15

4 Thedatacsét » sae ema EGS EHR HSE EME ROS ES SAE EME RHE BS 16

5.5 From design to prototype ..............200- 16

5.5.1 Use case 1: Reporting on fairness (global level) . . 16

5.5.2 Use 92: Tuning rules . 2.2 ees eee ewe es 23

5.6 Summary 29

 

 

Assessing fairness in anomaly detection iii

BIBLIOGRAPHY

 

[33] Inioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru,

34)

35

36

37

38

39

40

41

42

 

43

 

Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the ai
accountability gap: defining an end-to-end framework for internal algorithmic auditing. In
Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages
33-44, 2020. 5

 

Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions.
Journal of the American Statistical Association, 100(469):322-331, 2005. 39

Christopher Russell, Matt J Kusner, Joshua R, Loftus, and Ricardo Silva. When worlds collide:
integrating different counterfactual assumptions in fairness. Advances in Neural Information
Processing Systems 30. Pre-proceedings, 30, 2017. 4

Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet
Vertesi. Fairness and abstraction in sociotechnical systems. In Proceedings of the conference
on fairness, accountability, and transparency, pages 59-68, 2019. 5

Erik Strumbelj and Igor Kononenko. Explaining prediction models and individual predictions
with feature contributions. Knowledge and information systems, 41(3):647-665, 2014. 40

Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 ieee/acm international
workshop on software fairness (fairware), pages 1-7. IEEE, 2018. 4

Sandra Wachter, Brent Mittelstadt, and Chris Russell. Why fairness cannot be automated:
Bridging the gap between eu non-discrimination law and ai. arXiv preprint arXiv:2005.05906,
2020. 35

Hilde JP Weerts, Werner van Ipenburg, and Mykola Pechenizkiy. A human-grounded evalu-
ation of shap for alert processing. arXiv preprint arXiv:1907.03324, 2019. 40

Allison Woodruff, Sarah E Fox, Steven Rousso-Schindler, and Jeffrey Warshaw. A qualitative
exploration of perceptions of algorithmic fairness. In Proceedings of the 2018 chi conference
on human factors in computing systems, pages 1-14, 2018. 9

Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, and Adrian
Weller. From parity to preference-based notions of fairness in classification. In Advances in
Neural Information Processing Systems, pages 229-239, 2017. 4

Frederik Zuiderveen Borgesius et al. Discrimination, artificial intelligence, and algorithmic
decision-making. 2018. 3

 

44

Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

Predicted
@ True

% of fraudulent transactions

Particulier Zakelijk

  

Category

Figure 5.6: Use case 1: Bar chart visualizing predicted and true positives

The fairness metrics of the complete model are presented (see Figure 5.7). The fairness metrics

Summary: Fairness of model

Fairness of the model is measured with these fairness metrics. All metrics are the difference on these metrics
between the protected groups. More information on these fairness metrics can be found in the tab contents.

TPR difference FPR difference Precision difference Distance ROC Distance Prec-Rec

2.3 2.05 2.25 2.3 .4

Figure 5.7: Use case 1: Fairness metrics

in Figure 5.7 can be reported, but need some additional information. For example, it is not clear
which protected group has a better performance, and it is also not clear if the performances are

high or low.

True Positive Rate (Recall) difference as fairness metric
True Positive Rate of a classified model measures the proportion of positives that are correctly classified of all
actual fraudulent transactions.

The formula for TPR is TPR = TP/(TP+FN).

If there is a difference between the TPR's of the protected groups, this indicates that the model is worse in
classifying true fraudulent transactions. The model might be more ignorant for a protected group when the
difference is large.

5 Particle ee ee ee eee @@ Particulier
2 Zac es © Zakelijk
8 ail mal
° 0.2 0.4 06 oe a
TPR

Figure 5.8: Use case 1: True Positive Rate bar chart

 

20 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

 

 

 

 

 

 

 

Name(s) Formula Fraud detection context
True Positive TPR=TP2 = TP _ The fraction of all actual frauds
Rate (recall, 1_ FNR- TEEN that resulted in a fraud alert by
sensitivity) . the rule model.
The fraction of false positives
False Posit- | FPR = Ee = ePorE = | among all negatives. All false
ive Rate 1-TNR fraud alerts among all actual
non-fraudulent transactions.
Positive Pre- oP Of all fraud alerts, the frac-
dictive Value PPV = zpyrp = 1-— | tion of true fraudulent cases.
(precision) FDR Includes only fraud alerts, no
missed cases.
Actual frauds correctly classi-
fied by the model divided by
the misclassified fraud alerts over
Positive like- all non-fraudulent transactions.
lihood ratio | LR+ = eR The higher the TPR, the more
(LR+) of the correct fraud alerts are re-
called. The higher the FPR, the
more errors are made in generat-
ing a fraud alert.
F1 score The F-score is the harmonic
- mean of precision and recall.

 

 

 

 

 

Table 5.1: QoS-harm measurements of confusion matrix

1. Analyze the individual rule as a model. If you would classify the data set with only one
specific rule, then you would see which protected groups might be affected by this rule,
and on which protected groups the rule works well on. This would include classifying the
data set for each rule. However, this can give an unrealistic idea of the actual effect of the
rule. Hierarchical or overlapping rules can influence the specific rule, and can outbalance a
possible harmful impact.

2. Analyze the individual rule as part of a model. In this case, the complete model classifies the
data set using all rules. There is one rule that causes the final classification: TP/FP/FN/TN.
This analysis shows what rules are actually affecting the transactions. However, in case of
overlapping of hierarchical rules, some rules do not seem to impact the classifications while
this is not the case.

In this study, we have chosen to analyze the second case: analyzing the individual rules as part
of the model. This analysis captures the actual impact of the individual rules. In the case of
overlapping or hierarchical rules the following solution is proposed. Rules can be switched off
in the model and the tool presents what implications switching off that rule has on the complete
model. Such a perturbation includes crisp rules. If the fraud detection model also uses fuzzy rules,
another solution could be to adjust thresholds of fuzzy rules. If the fraud detection model uses
machine learning models, then the parameters of these models could be tuned. This thesis focuses
only on perturbing crisp rules to see the impact of the rules on the complete model, because the
rule model is primarily crisp rule based and also used more extensively than the machine learning
models.

Allocational harm

If there is a difference between protected groups in the number of transactions that are affected
by an individual rule, then that rule might be only focused on one group. This rule might be

 

14 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

Design choices

All the figures in this section contain random numbers and are not based on a real data set.
It is important avoid overloading the rule builder with information on the first screen. The rule
builder should interact with the tool to see more information.

Welcome

This tool will assist you in reporting the fairness of your model, and in finding possible unfair rules.
Fairness is measured with two definitions: Quality-of-Service harm and Allocational harm.

Quality-of-Service harm: the model should not make more mistakes in one protected group as opposed
to the other protected group(s).

Allocational harm: the model should provide as much effort and services in every group to be fair.

Choose your sensitive attribute you want to analyze and click on Start!

Zakelijk x ~

Start

Figure 5.4: Use case 1: Start screen

The first screen (Figure 5.4) presents information of Quality-of-Service harm and Allocational
harm. The rule builder can select a protected attribute. Then, the rule builder can click on Start
to begin with analyzing the fairness of the model on the protected groups.

Model analysis

The data set contains 23000 transactions. The chosen sensitive attribute is categorical and the protected groups
are Particulier (18000) and Zakelijk (5000) transactions.

Figure 5.5: Use case 1: Basic information

First, we should see some basic information as in Figure 5.5. The number of analyzed transac-
tions, and the selected protected attribute with corresponding categories (protected groups) with
their sizes, are displayed here. It is important to understand the number of transactions involved
in the analysis. Additionally, the number of transactions the model classifies as fraud, and how
many were truly fraudulent, are visualized in a bar chart (Figure 5.6). It shows relative, and
absolute, numbers to the protected group sizes. A bar chart was chosen because the rectangular
bars are easy to compare between the categories [31]. Allocational harm can be detected with this
visualization.

 

Assessing fairness in anomaly detection 19

CONTENTS

 

      

 
  
 

 
  
 

 

 

6 Qualitative evaluation of the designed tool 30
6.1 Methods......... 30
6.1.1 Participants and d 31

6.1.2 Procedure... . . 2 ee 31

G.L.8 Experinient . cas sues songs none Some aE Ro © BONE HO Roe EO 31

6.1.4 Analysis.ca. seu ean cee Be wR Hw Be RS RHA Ew EH 32

6:2 Result@. : ype sag 2GiE RES DER PE EWE EWS EEE ERE ReE ewe ES 32
G21, Participants 2 swe sag e 4k. eg BSE TRE EWE ES EAE EME BwE Bw 32

6.2.2 Completeness of tool. 2... ee 32

6.2.3 Workflow .............000..200.4 33

6.2.4 Analyzing visualizations . . 33

6.2.5 Understanding fairness metr 34

6.2.6 Drawing conclusions fairness metrics 35

6.2:7 Pétéeived usefulness 2s. ae ews Ewe ews EHR BH E RBS ews PES 35

62.8 Interpretation of results « 2.5 see Rs we EES SoHE ERE BwE Bw 35

6.3 Discussion and conclusion... 2... 36
6.3.1 Discussion. . 2... 36

6.3.2 Conclusion 2.2... ee 37

7 Conclusions 38
7.1 Conclusion 2... 38
7.2 Contributions... ee 39
7.2.1. Academic contribution . ...00 2405 cei be toe se ee eS 39

7.2.2 Business value 39

7.3 Limitations and Future Researc 39
7.3.1 Defining fairness in fraud detection 39

7.3.2. Implementation Allocational harm and QoS-harm in fraud detection .... 40

7.3.3 Evaluation tool... 2... eee 40
Bibliography 42
A Quality-of-Service harm considered fairness metrics 45
B Mock ups AT
B.l Final version . 2... 47
B.1.1 Screen 1 47

B.1.2 Screen 2 50

B.1.3 Screen 3 52

iv Assessing fairness in anomaly detection

Chapter 3

Problem formulation and research
framework

In this chapter, we describe the problem context in Section 3.1 where requirements for this research
are established. In Section 3.2, the research questions and the research framework are described.

3.1 Problem context

Algorithmic fairness is hard to determine in general. For each specific context, fairness must be
well defined in order to achieve a model that satisfies all ethical needs. Anomaly detection models
in financial industries are used for various tasks, including tax evasion, insurance fraud and fraud
detection. These models are used in many organizations and may impact a large part of the
society.

Socio-technical systems need to be assessed on the impact on stakeholders to be aware of
possible discrimination [24]. If anomaly detection systems are not assessed on fairness, then the
system might harm a protected group on structural level. There is a lack of research in designing
context aware fairness tools.

If complex modeling causes some sort of bias or discrimination, how can this be detected? Who
is going to detect it? How should it be measured how fair a model is? There is no simple way to
tell what the model exactly does and what implications arise from its use. Model developers are
trained to strive for good predictive performance in their models [5]. Creating a model that is fair
is difficult to achieve, because it requires a normative framework. Fairness is different in various
contexts, and every context needs a well-defined fairness concept.

That is why this study has created a framework for assessing fairness in anomaly detection in
a real-life application. For this research, we cooperated with one of the major Dutch banks in the
Netherlands.

  

3.1.1 Fraud detection

Banks process many transactions every day. The transaction data is highly imbalanced; only a
small part of the cases is fraudulent. All banks are mandated by law to investigate fraudulent cas
It is also important to know if fraud detection happens in a fair way as not to disproportionately
scrutinize a protected group, miss many fraudulent transactions in a protected group or burden the
company with many false positives. At the moment, there is no framework for assessing fairness
in fraud detection that can directly be applied in this context. The bank uses rule-based models
in order to detect fraud which are developed by domain experts. These domain experts are called
rule builders. These rules try to capture different types of fraud. A set of complex decision rules
will eventually lead to the complete fraud detection model. The system outputs a fraud alert when
a transaction is classified as a potential fraudulent case by the model.

   

Ss.

 

6 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

The use cases were the guidelines in order to design the visualizations. These visualizations were
included in mock ups. Mock ups are models of design and are feasible for translating abstract
concepts, such as Allocational and QoS-harm, into concrete visualizations. The mock ups were
created while obtaining iterative feedback of the rule builders. The mock ups, and their descrip-
tions can be found in Appendix B. These mock ups made it possible to communicate with the
rule builders what information is relevant in realizing the use cases.

5.4 The data set

The mock ups gave a good indication of the properties and visualizations of the tool. In the design
process, the data set should also be analyzed in order to transform the raw data into useful data.
The data set that was used for this research, was a sampled subset of transaction data. The raw
data was cleaned (remove duplicates, handle missing data), and then aggregated for analysis. For
this research we have chosen to make the prototype on the sensitive attribute: business or private
transactions. This attribute is not sensitive according to the GDPR law. However, this attribute
can show how such a tool would work on a binary variable. Additionally, even if the selected
sensitive attribute does not have to be considered from a moral perspective, it might be useful
to consider this from a business perspective. An example of the transformed transaction data is
given in the following table.

 

 

 

Transaction data transformed example
ID True label Predicted label Rule Protected attribute
1 0 0 Rule 004 Business
2 0 1 Rule 248 Private
3 1 0 Rule 008 Private

 

 

 

 

 

Each transaction has an attribute which contains the true label and an attribute which contains
the predicted label. The predicted label is established by a rule of the model. It could be the case
that this transaction is affected by many other rules, but in this data set only the rule that classifies
the transaction in the end is included. A fraud detection data set consists out of transactions.
Harm can be analyzed on several levels; the level of transaction or the level of a client. In this
study, only transactions are analyzed for simplicity. However, sequences of fraudulent patterns

 

 

 

 

 

 

would be interesting to investigate in future research.

5.5 From design to prototype

The use cases and the mock ups were the basis of the tool. In order to develop the prototype, the
use cases and visualizations were elaborated in requirements. In the section below the requirements
can be found, followed by the final design choices for the tool. The developed prototype was
finalized in an iterative process with the rule builders.

 

5.5.1 Use case 1: Reporting on fairness (global level)

The model can be investigated on how fair the model is to protected groups. In terms of Alloc-
ational harm: the model should allocate their services equally to be fair. In terms of Quality-of-
Service harm: the model should not make more mistakes in one protected group as opposed to the
other protected group(s). To analyze these two types of harm, the visualization tool must display
relevant metrics.

Requirements

In this use case, a rule builder reports how fair the model detects fraud on a selected protected
attribute. The requirements for the use case are determined, and followed by design choices in the
developed tool.

 

16 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

Analysis for individual rules

All classifications are eventually caused by a particular rule. In this overview you can explore the
impact of individual rules. The rules are ranked on the largest absolute difference between the
protected groups in the selected classification. The rules with the largest differences might be causing
an unfair treatment between the protected groups.

Select in the dropdown menu what you want to analyze.

FP difference

Ruled

Rule?
Rule 3

Rule 4

Figure 5.15:

@ Particulier
EG Zakelijkc

Use case 2: Ranking on largest False Positive disparity

This type of bar chart is made for False Positives (Figure 5.15), True Positives (Figure 5.16),
False Negatives (Figure 5.17), and True Negatives (Figure 5.18). These bar charts should assist
the rule builder to analyze the Quality-of-Service harm for individual rules.

Select in the dropdown menu what you want to analyze.

TP difference

Rule 1
Rule 2
Rule 3

Ruled

Figure 5.16:

M@ Particulier
76 Ml Zakelijk

Use case 2: Ranking on largest True Positive disparity

 

Assessing fairness in anomaly detection 25

CHAPTER 6. QUALITATIVE EVALUATION OF THE DESIGNED TOOL

 

e The confusion matrices relative (Figure 5.13) had some parts in the donut chart that
were small, these could have been made more visible.

 

e The confusion matrices absolute (Figure 5.14) would be easier to understand when the
captions TP, FP, FN, and TN were presented in the confusion matrices. Participants now
had to think for a considerable amount of time, or write it down for themselves.

e While investigating individual rules in the Ranking on largest True Positive, False
Positive, False Negative, and Positive disparity (Figures 5.15, 5.16, 5.17, 5.19), the
articipants would like to see the results for a specific rule. Now they had to search through
all the figures when they wanted to see a specific rule.

 

e In the tab Individual rules harm, the measurements in Figure 5.21 were not clear. If
he participant could not calculate these measurement themselves in order to verify their
correctness, then it was perceived as less useful.

e The participants mentioned that in order to get a better understanding of the impact of the
raudulent transactions, it might also be helpful to see the transferred amount of money of
he transactions.

e Another addition to the tool could have included the types of fraud.

 

e In the fairness metrics, the False Negatives were not directly included. A participant men-
ioned that this would also be intuitive.

 

6.2.3. Workflow

The tool seems to have a logical workflow according to the participants. During the experiment,
it also became clear that they started at the top left of the screen and going first to the bottom of
the left page, and then to the right (as was intended when designing the tool). Some parts of the
tool were not immediately clear. For example, the summary of fairness (Figure 5.7) did not make
sense at the beginning. However, when the participants had used the tool for some time, this
was not a problem anymore. This was clear when the participants had to compare the perturbed
model with the original model. The participants knew at that stage how to interpret the summary
of fairness table. The participants mentioned that the information presented began as simple at
the left top and became progressively more complex as they moved to the right bottom. The fact
that the tool had a left part and a right part was experienced positively. They thought it was easy
to compare the numbers on the right and left at the same time. Mostly the participants combined
all parts to each other: confusion matrix to TPR, FPR and Precision, TPR, FPR and Precision
to ROC and Precision-Recall curve.

6.2.4 Analyzing visualizations

The following parts of the tool were analyzed the most:

e Bar chart visualizing predicted and true positives (Figure 5.6): this gave a clear
indication on how many transactions were predicted as being fraudulent for each group, as
opposed to true fraudulent transactions.

e Confusion matrices absolute (Figure 5.14): these numbers were mostly used in order to
recalculate fairness metrics (TPR, FPR and Precision), and to get an idea how many cases
were involved.

e TPR difference as a fairness metric (Figure 5.8): this metric was used to determine
fairness between the two groups. It represented how well the model caught fraudulent
transactions.

 

Assessing fairness in anomaly detection 33

CHAPTER 6. QUALITATIVE EVALUATION OF THE DESIGNED TOOL

 

than other participants.
The data set used in this experiment is only a sample of a real data set. When using the real data,
it is expected that the tool can directly by applied.

6.3.2 Conclusion

The question: To what extent does the developed tool assist model developers in understanding and
identifying violations of the notions of fairness in fraud detection? can be answered as follows.
The model developers are assisted in understanding and identifying violations of Quality-of-Service
harm of the complete model mostly by analyzing the visualizations of the TPR difference and the
Precision difference. Understanding of fairness metrics was mostly build on recalculating the
metrics with the use of the confusion matrix. The Allocational harm aspect was mostly analyzed
by the bar chart visualizing the predicted and true fraud alerts of the model, which was also deemed
interesting according to the participants. The rule tuning did provide more concrete information
about the rules that cause the largest disparity in the complete model. Perturbing these rules did
provide additional insight to the rule builders, but did not give incentive to switch off a rule. This
use case gave incentive to research the effect of existing rules, and research possibilities of new
rules for protected groups that might require more involvement in the fraud detection model. We
can conclude in this qualitative study that the Quality-ofService and Allocational harm presented
in this tool is intuitive and can be useful for rule builders. Model developers in fraud detection
can be assisted in assessing fairness rule-based models by analyzing Quality-of-Service harm and
Allocational harm in such a tool as presented in this thesis.

 

 

Assessing fairness in anomaly detection 37

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

Select in the dropdown menu what you want to analyze.

FN difference xy

@ Particulier

Rule 1
@ Zakelijk

Rule2
Rule 3
5

Rule

ules
Figure 5.17: Use case 2: Ranking on largest False Negatives disparity

Select in the dropdown menu what you want to analyze.

TN difference % ow

3 Hl Particulier
Mi Zekelijk

Rule 1

Rule2
04

Rule3

Ruled ;
860

"|

Figure 5.18: Use case 2: Ranking on largest True Negatives disparity
For Allocational harm, the rule builder can also select Positives (Figure 5.19) and Negatives
(Figure 5.20) to analyze these numbers for each protected group for every individual rule.

Select in the dropdown menu what you want to analyze.

Positives difference xy

@ Particulier
279 Mi Zakelijk

Ruled

Rule?
er

I

Rules Te

Rule¢

Figure 5.19: Use case 2: Ranking on largest Positive classification disparity

 

26 Assessing fairness in anomaly detection

CHAPTER 6. QUALITATIVE EVALUATION OF THE DESIGNED TOOL

 

and whether the tool was easy to use. The two tasks that were performed were related to the two
use cases mentioned in Chapter 5; reporting fairness on a global level and tuning rules.

After these two tasks, the researcher would show how the tool would work with a numeric attribute
in order to give the participant an idea how to work with continuous variables in such a tool.
Finally, the last part of questions were asked in order to analyze what components of the tool were
useful for analyzing fairness in this rule-based model.

6.1.4 Analysis

The result of a qualitative study is unstructured data. It is the task to the researcher to analyze,
interpret and structure the recordings in order to gain insightful results, from which to draw
conclusions.

The recordings were analyzed in a selective way: all parts were identified and categorized in the
following categories.

e Completeness of tool: what information is missing in the tool?

e Workflow: is the information presented in a logical manner?

Analyzing visualizations: what information is analyzed by the rule builders?

e Understanding fairness metrics: how well do rule builders understand the information presen-
ted?

e Drawing conclusion fairness metrics: how can the rule builders draw conclusions on fairness
in this context?

e Perceived usefulness: how useful is the tool for rule builders?

 

When the text of the participant was transcribed and categorized, the meaning of the text was
interpreted by the researcher. These interpretation were summarized in Section 6.2.

6.2 Results

6.2.1 Participants

This experiment was conducted on three participants, all participants were rule builders. One of
the participants was included in the iterative design process of this tool. This participant had more
knowledge of fairness in fraud detection and the developed tool before the experiment started. The
other two participants were not familiar with the subject or the tool. During the experiment, the
first participant was able to draw conclusions faster and find the parts of the tool easier than the
other participants. The researcher guided the last two participants more by explaining the tool.
For example, if the participant was not able to find a certain part, then the researcher helped
the participant finding that part. The researcher was always silent at first, so the results would
contain the fact that some guiding was needed. In general, the researcher helped every participant
when it seemed as if the participant would get stuck in searching or faulty reasoning.

6.2.2 Completeness of tool

In general, the participants felt the tool contained all elements needed to fulfill the tasks used in
the experiment. However, some aspects of the tool could have been clearer or were missing:

e Participants expressed a need for a percentage of the number of true fraudulent transac-
tions divided by predicted fraudulent transactions at the bar chart visualizing predicted
and true positives (see Figure 5.6). During the experiment, they calculated this number
themselves.

 

32 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

 

Expectations use case 2

It is expected that analyzing individual rules provide something concrete to the rule builder. The
rule builder can reconsider having certain rules in the model. By giving the opportunity to see
the implications when the rule is not included, the rule builder will get a better understanding of
the impact of the rule on the complete model.

5.6 Summary

In order to answer the question How can we assist model developers in analyzing fairness of their
rule-based model in fraud detection using visualization techniques? a design-oriented research was
conducted. Two use cases were designed which are relevant in assessing fairness in the fraud
detection process. The harms of Chapter 4 were elaborated in these use cases and were quantified
into concrete fairness metrics. In order to visualize these fairness metrics, some literature study
was conducted and visualization techniques were applied. An iterative process with the model
developers resulted in a prototype of a tool which can assist model developers in assessing fairness
in fraud detection. A qualitative study in Chapter 6 will demonstrate whether the tool is useful
and intuitive.

 

 

Assessing fairness in anomaly detection 29

CHAPTER 4. DEFINING FAIRNESS IN FRAUD DETECTION

 

e Quality-of-Service harm: when a system is worse in classifying certain protected groups.
This type of harm can be immediately detected and is quantifiable. A QoS-harm can range,
just like Allocational harm, from a small to large systematic difference in treatment between
protected groups.

What can we define as fair in such models? Is it fair when all protected groups have the
same amount of fraud alerts? On the other hand, it is important to consider the distribution of
raudulent transactions among all groups. Or is it fair when the model has the same accuracy for
oth protected groups? There is no simple answer to this question, because this is both a social,
and a technical issue. In this research, a human should be in charge of analyzing the impacts of
a model in protected groups; analyzing relevant harms. As there may be representational harm
in fraud detection models, this is hard to quantify, and is less relevant to this specific context.
n fraud detection models, it might be the case that a certain demographic group, for instance
males, were always the victim of fraud. Representational harm would mean in this example that,
yecause only males were victims of fraud, future fraud would also involve male clients. The fraud
detection model does not cause this type of harm. For simplicity, we do not take representational
1arm into account in this study.

For rule-based systems in fraud detection, Allocational harm and Quality-of-Service harm
are relevant to analyze for protected groups. As the rule builders develop the fraud detection
model, it makes sense to let the rule builders analyze the model evaluate the developed model on
hese types of harm. Making the right choices requires knowledge about the context. By explicitly
ormalizing fairness in an iterative process with rule builders, it becomes easier for the rule builders
‘o discuss fairness of a rule-based model with other stakeholders. The rule-based systems might
seem transparent (i.e. the rules of the rule-based model are interpretable to the rule builders).
However, impact analysis on protected groups might lead to more insight on the effect of all rules.

 

4.4 Conclusion fairness notions in fraud detection

Unfairness is measured in this thesis by disparate impact; protected groups are treated differently
y the model. Whether or not this disparity is desirable should be analyzed by rule builders.
f this would not be considered, a systematic harm might remain, without anybody noticing it.
Fairness in fraud detection should be measured with two definitions: Quality-of-Service harm and
Allocational harm. Quality-of-Service harm: the model should not make more mistakes in one
rotected group as opposed to the other protected group(s). Allocational harm: the model should
rovide as much effort and services in every group to be fair. However, these types of harm can
still be represented in many figures and numbers. The basis for Allocational harm and QoS-harm
are the disparities in classifications of protected groups, which can be found in their confusion
matrices. Research on what figures and numbers to include in such an impact analysis can be
ound in Chapter 5.

 

 

Assessing fairness in anomaly detection 1

CHAPTER 7. CONCLUSIONS

 

is well-understood and justified. A complete fair model according to Allocational and Quality-of-
Service harm would be when you have the same proportion of mistakes, and the same proportion
of fraud alerts in every protected group. When it is difficult to predict fraud in a specific protected
group, the solution should not be to miss more frauds in another group to make this even. Decisions
and compromises have to be made about what is important to stakeholders. The direct harm of
False Positives for clients is not that noticeable because fraud analysts investigate the fraud alerts
before any decisions are made. On the other hand, generating many False Positives results in a lot
of time investigating non-fraudulent transactions, which is not ideal from a business perspective.
This framework can be used when fraud detection were to be an automatic decision-making model.
The context, the harms, and the implementation of this research would have to be revised.

7.3.2 Implementation Allocational harm and QoS-harm in fraud detec-
tion

Data set

The information presented in the tool could have been enriched with other data about the trans-
actions, such as; the transferred amount of money, type of fraud, etc. Adding this information
would assist the rule builders in drawing conclusions with the contexts of the transactions in mind.
The data set was a sample of the true data set, so the the prototype did not represent exactly
he data as it should do in practice. However, the rule builders understood how the tool could
e used in practice. Fraud alerts originate from single transactions, but can also be determined
yy a sequence of transactions. This was not included in this study. The results presented in the
ool may have become more accurate if this was taken into calculations. Including time series in
he tool might be a useful visualization technique that can also enhance understanding fraudulent
rends. Finally, more categories within protected groups could have been made. For example,
if you were to investigate on gender, you might also want to include age to get a better under-
standing of minority groups. All these additions might improve the tool as it is now, and could
e investigated in future work.

Rule-based model

n the tool, only individual rules were presented, but it might also be useful when the model
developer can select subsets of rules. This would help the model developers understand the impact
of the rule-based model more. Additionally, only crisp rules were used in this study. Analyzing
‘uzzy rules allows to work with thresholds, which can add a new dimension to the tool. The
decision of including a rule can become more nuanced by reducing or increasing the thresholds of
hese rules. This tool could also be partly used for black box models. This is interesting with the
rise of machine learning models in socio-technical systems. The analysis of the complete model
would remain the same. The individual rules part should be adjusted, because a machine learning,
does not have concrete rules. What might have potential for future research, is calculating SHAP
values over the attributes of the data set [37]. Model developers can then decide if the a protected
variable, or a proxy, should remain in the training the machine learning model.

 

7.3.3 Evaluation tool

This was an exploratory qualitative study. The results suggest that such a tool is useful for model
developers. However, this study does not demonstrate its usefulness in a quantitative manner.
Future work could take some of these ideas to design a tool and test the usefulness quantitatively.
A hypothesis could be about having a better understanding of fairness in fraud detection using
this tool. This would assume that the mental model of the model developer must be improved and
would cause better performance in carrying out their tasks [4]. A quantitative study, measuring
the task and mental efficiency can be measured [40]. However, in this industry, it is important to
consider the power of the designed experiments. Especially with a low amount of participants, it

 

40 Assessing fairness in anomaly detection

CHAPTER 5. IMPLEMENTING ASSESSING FAIRNESS IN FRAUD DETECTION

ROC curve distance as fairness metric

The ROC curve works well to analyze the trade-off between TPR and FPR. The distance between the points of
protected groups on the curve, represents the fairness metric Equality of Odds (visualized with the black dotted
lines).

The formula for FPR is FPR = FP/(FP+TN).

The formula for TPR is TPR = TP/(TP+FN).

ROC curve

© Particulier
© Zakelijk
@ all

 

TPR

0.4

0.2

FPR

Figure 5.11: Use case 1: ROC curve

Precision Recall curve distance as fairness metric

Since the fraud detection data set is dominated by the negatives, the ROC curve might limit the interpretation. A
large change in the number of false positives can lead to a smalll change on the ROC curve (because of the FPR),
while Precision captures the effect of the large number of false positives.

The formula for TPR is TPR = TP/(TP+FN).

The formula for Precision is PPV = TP/(TP+FP).
Precision Recall curve

© Particulier
© Zakalijk
08 © all

0.6

Precision

0.4

0.2

 

TPR

Figure 5.12: Use case 1: Precision Recall curve

Expectations use case 1

It is expected that all these metrics and visualizations would give a rule builder a better under-
standing in how well the model performs on the protected groups (Quality-of-Service harm), and
how often the model allocates services to each protected group (Allocational harm).

 

22 Assessing fairness in anomaly detection

CHAPTER 6. QUALITATIVE EVALUATION OF THE DESIGNED TOOL

 

6.2.6 Drawing conclusions fairness metrics

During the experiment, the participant were often in doubt whether the outcomes were fair or
not. They shared many considerations why it should be fair or unfair. They did add many times
that it depends for every sensitive attribute what is fair. For some groups it might be harder to
detect fraud. In other words, more information on the protected groups and relevant attributes
would be helpful in order to determine fairness in this model.

6.2.7 Perceived usefulness

The participants indicated that they now could discuss fairness and the experiment and tool gave
incentive to think more about fairness in their model. They could not form conclusive answers to
what is fair or unfair, but they were able to address this problem by using the fairness metrics of the
tool. The participants were asked about their thoughts on fairness in fraud detection at the start
of the experiment. In general, the participants did have at the beginning of the experiment an idea
of a fair fraud detection model, which consisted of fairness ideas that could not directly applied
to the fraud detection process. The statements were more abstract concepts instead of concrete
measurements. The same question was asked at the end of the experiment, and the participants
replied this answer involving concrete fairness metrics and other measures. Their opinion of
fairness has became more concrete. They mentioned that it is important to understand why a rule
is part of the model. Additionally, they mentioned the importance of understanding the protected
groups. In the experiment, business and private transactions were analyzed. Some differences in
performance can be explained by elaborating on the differences between these protected groups.
In other words, only relying on the fairness metrics in these tasks will not be enough in order to
draw conclusions on fairness. Disparities between protected groups can be justified. Future work
might look into using this justification, for example, considering conditional demographic parity
as proposed in the paper of Wachter et al. [39]. However, the tool does provide concrete points
to discuss and analyze fairness. The participants had ideas on how the rules work in practice, but
they mentioned that it was useful to get the confirmation of the actual effect on the model.

6.2.8 Interpretation of results

During the experiment, the participants commented on some aspects of the tool. In Section 6.2,
these comments were noted. Here, we discuss these results.

1. Intuitiveness: there were some suggestions to improve the intuitiveness, but in general the
tool seemed intuitive enough for the selected use cases.

e Completeness of tool: what information is missing in the tool? As can be read in
Section 6.2.2, there should be alterations in order to make the tool more intuitive.
Some improvements were; making the donut chart clearer on the small parts, adding
captions to the confusion matrices (TP, FP, FN, TN) and providing formulas for the
individual rules harm. Other suggestions warrant future research; adding the number of
true fraudulent transactions as a percentage of predicted fraudulent transactions should
not overload the user with information, investigating individual rules on the disparities
should be implemented with a whole new function, including the transferred amount of
money and types of fraud is also a whole new aspect in this study. These suggestions
can be investigated in future work. The suggestion of including False Negatives directly
in the fairness metrics does not add new information to the tool as this would be the
fairness metric False Negative Rate: FNR=1—TPR.

e Workflow: is the information presented in a logical manner? As can be read in Section
6.2.3, the information was presented logically to the participants. The participants
might have to spend more time to draw conclusions faster and find the relevant parts
of the tool easily. It would be an improvement to select a single rule and then get an

  

 

 

Assessing fairness in anomaly detection 35

Chapter 6

Qualitative evaluation of the
designed tool

This section covers the evaluation of the tool designed in Chapter 5. In this section we will answer
he question: To what extent does the developed tool assist model developers in understanding and
identifying violations of the notions of fairness in fraud detection? The notions of fairness are
Quality-of-Service harm and Allocational harm from Chapter 4. The goal is to evaluate what this
ool allows rule builders to do which would otherwise not be possible to achieve.

To understand and identify violations of QoS-harm and Allocational harm, the two use cases are
guidelines for determining the structure of the evaluation; reporting fairness on a global level and
uning rules.

In order to answer this question, we must distinguish which aspects of the tool are relevant.
For example: the tool can either work well, but does not include the visualizations to transfer
he information well. Or the tool has clear visualizations, but the rule builder does not know
10w to handle the tool’s functionalities. The following dimensions are important to evaluate: the
intuitiveness of the tool and the usefulness of the tool. The intuitiveness of the tool means that
he rule builders can find and understand all visualizations provided by the tool to execute the
use cases. The usefulness of the tool means that the rule builders can now report on the harms of
he complete fraud detection model and can find rules that cause these harms. These dimensions
can be divided as follows:

 

1. Intuitiveness

e Completeness of tool: what information is missing in the tool?

e Workflow: is the information presented in a logical manner?

e Understanding fairness metrics: how well do rule builders understand the information
presented?

2. Usefulness

e Analyzing visualizations: what information is analyzed by the rule builders?
e Drawing conclusions fairness metrics: how can the rule builders draw conclusions on
fairness in this context?

e Perceived usefulness: how useful do the rule builders perceive the tool?

6.1 Methods

The fairness tool is presented as a prototype as described in Section 5.5. In order to prepare for
the experiment, the experiment was piloted by a rule builder, which did not participate in the

 

30 Assessing fairness in anomaly detection

CHAPTER 7. CONCLUSIONS

 

is useful to add a qualitative part in a quantitative experiment. This way it is possible to find out
why some parts of the tool work or not. The qualitative part could focus on the understanding,
intuitiveness, satisfaction and impact.

 

Assessing fairness in anomaly detection 41

APPENDIX B. MOCK UPS

 

B.1.2 Screen 2

Analysis complete model

ren Step 2:

=z What type of errors are
made in the current. =
fraud detection model?
: How many (relatively)
ome correct fraud alerts are
= generated? What
causes the errors? =

Analysis false positives

 

  

Rules that caused the Rules that caused the
most false positives in oS most false positives in. [5
protected group: protected group: a

Female Male

 

=a
=
‘-is

 

Figure B.5: Version 0.0 - Screen 2

The goal of the second screen (Figure B.5) is to understand how well the fraud detection model
performs, what type of errors there are made, and which rules are responsible for false positives.

Visualization 1 of Screen 2

Protected group: Male
Protected group: Female

 

Figure B.6: Version 0.0 - Screen 2 - Visualization 1

What?
In Figure B.6, the true positives, false positives, and false negatives are presented in a circle dia-
gram. The true negatives are not included, because this would take up a large part of the circle
(fraud detection models deal with an imbalanced data set).

Why?
This visualization is to give the rule builder an indication if the model detects in a protected group
most fraudulent cases, or misses them. A difference between protected groups would mean that
the model works differently for these groups. When these diagrams would be similar, there would
be no quality-of-service harm of the complete model. This does not exclude the fact that there
might be discriminating rules included in the model, and that all errors of all rules together might
weigh each other out. This visualization is of exploratory nature, the rule builder cannot draw

 

50 Assessing fairness in anomaly detection
