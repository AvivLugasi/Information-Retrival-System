survey platform, and we were not confident respondents would understand the question.

As in the social media sample, we found that male respondents were statistically sig-
nificantly more likely than female respondents to favor including gender even if it would
lead to fewer women in science classes (Figure Ph. The mean gap between genders was
somewhat smaller 0.5 points as opposed to 1.1 points — possibly because respondents
were more likely to be in a hurry or to not understand the question, introducing noise.
Nonetheless, a meaningful gap emerged: male respondents were, for example, 41% likely
to give an answer of 4 or above, as opposed to only 29% for female respondents, an odds
ratio of 1.7.

3 Discussion

We identify statistically significant and practically meaningful gender discrepancies in be-
liefs about algorithmic fairness in two separate samples. A caveat to our results is that
one of our surveys is disseminated through social media, which will create a biased sam-
ple; we mitigate this concern by showing that gender differences in beliefs persist in a
more representative sample, collected using Google Consumer Surveys. Our finding is also
consistent with the results in [7], which identifies evidence of gender differences in beliefs
about which features are acceptable to use in criminal risk prediction, although they do
not assess statistical significance because their sample is small. Future work should assess
whether the differences we observe persist in other samples; if they do not, that discrepancy
is itself of interest. Future work should also seek to measure racial discrepancies in beliefs
about algorithmic fairness, a topic of pressing importance given the racial disparities in
algorithmic decisions.

One natural question in response to our results is whether these demographic differences
would still emerge if participants were given a chance to debate and reflect on the fairness
dilemmas. It is possible that, under such circumstances, respondents would converge in
their beliefs, an intriguing possibility for future work. Claims of such convergence, how-
ever, require evidence, given that participants initially start far apart and that academic
disagreements in algorithmic fairness (and moral philosophy more broadly) clearly persist

 

even after careful reflection.

To the extent that they exist, demographic differences in beliefs about algorithmic
fairness have two implications. First, disagreements about algorithmic fairness may stem
from fundamental aspects of background or life experience, and as such have more subjec-
tive answers than purely technical questions. Second, diversity in discussions of algorithmic
fairness is particularly pressing. It is worrisome that the race and gender groups least likely
to be involved in algorithmic discussions are also the groups most often harmed by algo-
rithmic disparities. If our demographics predict how we believe algorithms should behave,
we need our algorithm designers to be more demographically representative if algorithms
are to serve the will of the whole population.

Gender differences in beliefs about algorithmic fairness

Emma Pierson*

December 10, 2017

1 Introduction

The rapidly growing field of algorithmic fairness has produced many ethical dilemmas and
little consensus about how to resolve them. For example, it has been repeatedly shown
4 that different fairness constraints cannot all be satisfied simultaneously, both
in theory and in real data. Hluminating these conflicts is invaluable, but thus far there is
little agreement about what to do about them.

This disagreement likely occurs in part because the core debates in algorithmic fairness
are philosophical, not just technical. Building a classifier that can predict criminal recidi-
vism with high accuracy is a relatively straightforward technical problem with a clearly
defined success metric. But determining how to weigh maximizing accuracy against min-
imizing disparities requires value judgments about which people may disagree. A recent
review of the ethical considerations in data science argues, with respect to algorithmic fair-
ness, that “Favoring certain fairness properties over others could just as well have reflected
a difference in values” [I].

Previous work has shown that people’s moral judgments correlate with their demo-
graphic traits [5} [6], as do beliefs about the ethics of self-driving cars [2] and beliefs about
fairness and discrimination [10] [I2]. These findings raise the question of whether judg-
ments about algorithmic fairness also correlate with demographic traits. This is a question
of particular interest because computer science is extremely demographically skewed [9]. If
beliefs about algorithmic fairness correlate with demographics, and computer scientists are
demographically skewed, decisions made about algorithmic fairness may not reflect what
the population as a whole would want.

Here we present the results of two surveys showing that demographics do predict be-
liefs about algorithmic fairness. In an initial survey disseminated through social media and

“This is a writeup of a talk for AlterConf 2017, in response to requests to see the results. Thanks
to Shengwu Li, Sam Corbett-Davies, and Chris Olah for thoughtful comments. Feedback is welcome at
emmap1@cs.stanford.edu. Replication data and code are available at

References

1

or

 

 

[13

S. Barocas and D. Boyd. Engaging the ethics of data science in practice. Communi-
cations of the ACM, 2017.

J.-F. Bonnefon, A. Shariff, and I. Rahwan. The social dilemma of autonomous vehicles.
Science, 2016.

A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. FATML, 2016.

S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. Algorithmic decision
making and the cost of fairness. ADD, 2017.

L. M. Dawson. Women and men, morality and ethics. Business Horizons, 1995.

M. Fumagalli, R. Ferrucci, F. Mameli, $. Marceglia, $. Mrakic-Sposta, $. Zago, C. Luc-
chiari, D. Consonni, F. Nordio, G. Pravettoni, et al. Gender-related differences in
moral judgments. Cognitive processing, 2010.

N. Grgic-Hlaca, M. B. Zafar, K. P. Gummadi, and A. Weller. The case for process
fairness in learning: Feature selection for fair decision making. In NIPS Symposium
on Machine Learning and the Law, 2016.

J. Kleinberg, S. Mullainathan, and M. Raghavan. Inherent trade-offs in the fair de-
termination of risk scores. [TCS, 2017.

L. C. Landivar. Disparities in STEM employment by sex, race, and Hispanic origin.
Education Review, 2013.

S. Loughlin. Gender Differences in Perceptions of Discrimination. Barriers to
Women’s Career Progression: A Review of the Literature. State Services Commis-
sion. Available at: http://www.ssc.govt.nz/node/6602, 2000.

P. McDonald, M. Mohebbi, and B. Slatkin. Comparing Google Consumer Surveys to
existing probability and non-probability based internet surveys. Google White Paper,
2012.

K. Parker, J. Horowitz, and B. Mahl. On views of race and inequality,
blacks and whites are worlds apart. Pew Research Center. Available at:
http://www.pewsocialtrends.org/2016/06/27/on-views-of-race-and-inequality-blacks-
and-whites-are-worlds-apart/, 2016.

G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger. On fairness and
calibration. In NIPS, 2017.

academic listservs, we examine the effects of gender and academic background on people’s
beliefs about algorithmic fairness. We find that gender predicts people’s beliefs on several
questions, though we find no statistically significant association between beliefs and aca-
demic background. In a follow-up survey conducted using Google Consumer Surveys, we
replicate the gender effect.

2 Results

We disseminated an initial survey with four algorithmic fairness questions through social
media channel] We asked respondents for their gender, academic background (computer
science, other science or math, or humanities), and race / ethnicity. Our sample was not
sufficiently racially representative to give us statistical power to assess racial discrepancies
in responses: only 6% of respondents identified as black or Hispanic, a caveat which ought
to be kept in mind in interpreting our results. A second caveat is that, although we
had to present the questions as one-dimensional scales to allow for comparable numerical
responses, the questions had nuances which could not always be captured along a single
dimension, a fact some respondents commented on. In total 163 respondents filled out
our survey completely and identified as male or female: 72 women and 91 men; 49 from
computer science, 37 from other science / math, and 77 from humanities.

We found significant associations (p < .05, two-tailed t-test) between gender and re-
sponses to two of the four algorithmic fairness questions, with a third question close to
significant (Table . Male respondents were more likely than female respondents
to weight maximizing accuracy over minimizing racial disparities in criminal risk predic-
tion. Male respondents were also more likely than female respondents to favor including
gender in an education company’s algorithm that recommended classes to students if it
increased accuracy of class recommendations, even if that would make it less likely that
women were recommended science classes. We found no significant associations between
a respondent’s academic background and their responses to algorithmic fairness questions.
(While gender was somewhat associated with academic background — 29% of computer
scientists were female, as opposed to 52% of humanities respondents and 49% of other
science / math respondents — we verified that gender was still significantly associated with
algorithmic fairness beliefs when we controlled for profession).

Surveys conducted through social media study a selected sample (though, of course,
many survey populations have biases) and our initial sample size was somewhat small.
We therefore assessed whether we could replicate the significant gender associations in a
larger, more representative sample using Google Consumer Surveys In total our data
included 573 respondents identified as male or female: 282 females and 291 males. We
sought to replicate the “gender in course recommendations” significant association because
the “racial disparities versus accuracy” question required too much space to explain on the

 

 

 

‘Survey is available at https: //github.com/epierson9/algorithmic_fairness_survey/|

 

Use gender in course recommendations even if it reduces women in science classes?
Reduce racial disparities in criminal risk prediction at expense of accuracy?

Use computer algorithms (as opposed to human judges) in criminal justice at all?
Allow companies to keep details of criminal justice algorithms secret?

 

Pp

3.3e-05
4.2e-04
5.6e-02
1.0

Table 1: Results for social media survey. Answers are on a 7-point scale from 1 to 7, with

higher numbers indicating greater agreement.

Reduce racial disparities in criminal risk prediction
at expense of accuracy?
n= 163, p = 4.2e-04

 

Use gender in course recommendations
even if it reduces women in science classes?
n= 163, p = 3.3e-05

 

 
   

 

 

 

 

 

 

2 sa EE Males 2 Be = Males
§ 50 @mm Females § 50 @am Females
s st
40 € 40
° °
ae B30
£ e
= 2 = 2
° °
se 10 310
of °
> $ & 8 & F rr 5 6 7
Figure 1: Results of initial survey; higher numbers indicate greater agreement with the
question in the title. Left: male respondents were more likely than female respondents to

weight maximizing accuracy over minimizing racial disparities in criminal risk prediction.
Right: male respondents were more likely than female respondents to favor including gender
in an algorithm that recommended classes to students, even if that would make it less likely

that women were recommended science classes.

Use gender in course recommendations
even if it reduces women in science classes?
n = 573, p = 3.5e-03

 

60

50

40

30

20

% of respondents

10

 

Figure 2: Replication of gender-in-course-recommendations ef

Gam Males
@m_ Females

 

ffect using Google Consumer

Surveys. Effect size is smaller than in the social media sample, possibly due to noise
introduced by people taking the survey quickly or not understanding the question.
