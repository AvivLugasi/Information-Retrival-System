 

 

39

 

 

 

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

df_cd_CR = cd_race.compare(c_df_CR)
ce_df_CR[’two_year_recid’] = recid
ed_race[’two_year_recid’] = recid

3 #Transformation Visualizations

###DIR#HH

#GERMAN: Credit Amounts by Months, darker color is the new transformation, values
shouldve overlapped perfectly

plt.scatter(y= gd_df[’credit_amount’], x= gd_df[’month’], marker=’o0’, c=’ darkred’)

plt.scatter(y= g_df_DIR[’credit_amount’], x= g_df_DIR[’month’], marker=’*’, c=’
tomato’)

#COMPAS: Scatter plot of prior counts by ages , high counts were lowered and spread
out

plt.scatter(cd_df[’ priors_count’], cd_df[’age’], marker="*’, c=’purple’)

plt.scatter(c_df_DIR[’ priors_count’], c_df_DIR[’age’], marker=’o0’, c=’mediumpurple’
)

 

a4 #HHLFR#HH

#german

LFR_g = pd.crosstab(g_df_LFR[’age’], g_df_LFR[’credit’], rownames=[’age’], colnames
=[7eredib a)

sn.heatmap(LFR_g, annot=True, fmt=’g’, xticklabels=[’Bad’, ’Good’], yticklabels=[’
<5) 225) ])

plt .show()

#compas

LFR_c = pd.crosstab(c_df_LFR[’race’], c_df_LFR[’two_year_recid’], rownames=[’ race’
], colnames=[’reoffended’ ])

sn.heatmap(LFR_c, annot=True, fmt=’g’, xticklabels=[’No’, ’Yes’], yticklabels=[’
Black’, ’Caucasian’ ])

1 plt.show()

##HOPHHH

#german

OP_g = pd.crosstab(g_df_OP[’age’], g_df_OP[’credit’], rownames=[’age’], colnames=[’
eredit” ]))

sn.heatmap(OP_g, annot=True, fmt=’g’, xticklabels=[’Bad’, ‘Good’], yticklabels=[’<
25 > 257)

plt.show()

#compas

OP_c = pd.crosstab(c_df_OP[’race’], c_df_OP[’two_year_recid’], rownames=[’race’],
colnames=[’ reoffended ’ ])

sn.heatmap(OP_c, annot=True, fmt=’g’, xticklabels=[’No’, ‘Yes’], yticklabels=[’
Black’, ’*Caucasian’ ])

plt .show()

0 #HHCRHHH

21 #german

plt.scatter(y= gd_df[’credit_amount’], x= gd_df[’month’], marker=’o0’, c=’darkred’)

23 plt.scatter(y= g_df_CR[’credit_amount’], x= g_df_CR[’month’], marker=’*’, c=’tomato

)

#COMPAS: Scatter plot of prior counts by ages , high counts were lowered and spread
out
plt.scatter(cd_df[’ priors_count’], cd_df[’age’], marker="*’, c=’purple’)

7 plt.scatter(c_df_CR[’ priors_count’], c_df_CR[’age’], marker=’o’, c=’mediumpurple’ )

 

Shadiah Ricardo Lacouture 23

 

Justifying the use of fairness pre-processing algorithms

 

7

10.

11.

12,

13.

14.

15.

References

. Amsterdam Data Collective, a dutch company that helps people make their data/models fairer

based in Amsterdam. https://amsterdamdatacollective.com/insights/opinion-blog-add-fairness-to-
your-data-quality-framework/

. Andrus, M., Spitzer, E., Brown, J., & Xiang, A. (March 3, 2021). What We Can’t Measure,

We Can’t Understand: Challenges to Demographic Data Procurement in the Pursuit of Fairness.
https://arxiv.org/pdf/2011.02282.pdf

. Calmon, F., Wei, D., Vinzamuri, B., et al. (2017). Optimized Pre-Processing for Discrimination

Prevention. Advances in Neural Information Processing Systems 30 (NIPS 2017). https://krvarshney
.github.io/pubs/CalmonWVRV_nips2017.pdf

. Fazelpour, S., Lipton, C. (2020). Algorithmic Fairness from a Non-ideal Perspective.

https://arxiv.org/pdf/2001.09773.pdf

. Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015).

Certifying and Removing Disparate Impact. Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining.
https://doi.org/10.1145/2783258.2783311

. Friedler, S., Scheidegger, C., & Venkatasubramanian, S. (Sept 23, 2016). On the (im)possibility

of fairness Presentations. https://arxiv.org/pdf/1609.07236.pdf

. Hajian, S., Domingo-Ferrer, J. (July 2013). A Methodology for Direct and Indirect Discrimination

Prevention in Data Mining. https://www.researchgate.net/publication/258517824_A_Metho
dology_for_Direct_and_Indirect_Discrimination_Prevention_in_Data_Mining

. Kamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without dis-

crimination. Knowledge and Information Systems, 33(1), 1-33. https://doi.org/10.1007/s10115-
01104638

. Kasirzadeh, A., & Smart, A. (2021). The Use and Misuse of Counterfactuals in Ethical Machine

Learning. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Trans-
parency. https://doi.org/10.1145/3442188.3445886

Lee, B. (July 9, 2021). Escaping the "Impossibility of Fairness": From Formal to Substantive
Algorithmic Fairness. https://arxiv.org/pdf/2 107.04642.pdf

Lee, M. S. A., Singh, J. (May 8, 2021). The Landscape and Gaps in Open Source Fairness
Toolkits. https://dl.acm.org/doi/pdf/10.1145/3411764.3445261

Lee, M. S. A., Floridi, L., & Singh, J. (2020). From Fairness Metrics to Key Ethics Indicators
(KEIs): A Context-Aware Approach to Algorithmic Ethics in an Unequal Society. SSRN Elec-
tronic Journal. https://doi.org/10.2139/ssrn.3679975

Poel, Ibo & Royakkers, L.. (2007). The Ethical Cycle. Journal of Business Ethics. 71. 1-13.
10.1007/s10551-006-9121-6.

Powles, J. (Dec 7, 2018). The Seductive Diversion of ’Solving’ Bias in Artificial Intelligence.
https://onezero.medium.com/theseductivediversionofsolvingbiasinartificialintelligence890df5e5ef53

Yan, J., Gu, Z., Lin, H., & Rzeszotarski, J. (April 21, 2020). Silva: Interactively Assessing Ma-
chine Learning Fairness Using Causality. https://www.cs.cornell.edu/hubert/files/publications/silva
_chi.pdf

 

Shadiah Ricardo Lacouture 17

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

2 Related Literature

If we look at the situation currently, there are actual companies that offer to help businesses make their
models fairer, one such is based in Amsterdam and called ‘Amsterdam Data Collective’. Besides, the
algorithmic fairness field is not asleep in the meantime, another toolkit has been launched recently.
Yan, Gu, Lin, & Rzeszotarski introduced the new fairness toolkit called Silva in a paper in 2020. They
conclude that Silva is more efficient at removing bias than current systems, such as AIF360, with the
help of benchmark datasets. It should be easy to use and not require much skill, which means that Silva
might be more widely applicable, but it could struggle with scalability. Though, Silva is just in the
beginning stages and needs refinement before it can be truly used in general. The field seems to fork
in two directions, the first subsection mentions a few papers over the mathematical perspective and the
second section has the informal different approaches. This chapter is summarized in the third section.

2.1 Mathematical Perspective

Some papers look at algorithmic fairness from a mathematical perspective, also defined as a formalized
approach. Friedler, Scheidegger, & Venkatasubramanian (2016) define a decision, construct, and ob-
served space with math formulas. Then quantify the difference between spaces, understood as fairness.
They introduce the idea of two worldviews; one for group fairness to guarantee non-discrimination and
one for individual fairness which is really to guarantee fairness. The choice is between either assuming
we all are equal or assuming what you see is what you get. It is proved that these axioms are mutually
exclusive and can cause discrimination or unfairness to show up for individuals or groups when both are
applied. Though, this paper’s major contribution is the set of functions they offer, as a base for future
reference.

More papers support this approach, Calmon, Wei, & Vinzamuri (2017) are an example of a more special-
ized approach, mentioned later in Chapter 3 under Optimized Preprocessing. They explain what discrim-
ination and fairness metrics are in-depth and introduce various key points (discrimination control/group
fairness, distortion control, utility preservation) for the formulation of an optimization problem. They
propose a flexible, data-driven optimization framework for probabilistically transforming data to reduce
algorithmic discrimination. The focus is primarily on achieving group fairness while also accounting for
individual fairness through a distortion constraint. The reduction in discrimination comes at an accuracy
penalty due to the restrictions imposed on the randomized mapping. Moreover, our method is compet-
itive with others in the literature, with the added benefit of enabling an explicit control of individual
fairness and the possibility of multivariate, non-binary protected variables.

A similar paper with a formal approach, by Hajian and Domingo-Ferrer (2013) touch on how to develop
pre-processing techniques for algorithmic fairness. Their first step is to measure discrimination and
identify categories and groups of individuals that have been directly and/or indirectly discriminated
in the decision-making processes, the second step is to transform data properly to remove all those
discriminatory biases. Finally, discrimination-free data models can be produced from the transformed
data set without seriously damaging data quality. They show that the proposed techniques are quite
successful in removing discrimination and preserving data quality. The authors are careful and mention
though that ‘the perception of discrimination, just like the perception of privacy, strongly depends on
the legal and cultural conventions of society.’ A paper that explains this field well by Lee and Singh
(2021) shows exactly what the fairness toolkits offer and where they lack, this is done with the help
of focus groups of Data Scientists, reviews, interviews to ensure validity, and a survey. Each toolkit
they reviewed has a varying degree of user-friendliness, worldview, and is better equipped for different
models each time. Their results suggest that industry practitioners are still struggling with finding a way
to identify and mitigate potential unfairness in their models and systems. Only by keeping close to the
practitioners’ requirements and preferences can the open-source developers ensure widespread adoption
of their toolkits. Hence, Lee and Singh propose that this paper be a guide for practitioners and possibly

 

Shadiah Ricardo Lacouture 3

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

 

5
8

BeseEe BBB S

 

 

 

 

 

Figure 5.7: German CR Transformed Figure 5.8: Compas CR Transformed
Scatterplot of (Duration of Checking Ac- Scatterplot of Count of Prior Offenses by
count in) Months by Credit Amount Ages

It is clear that for all datasets at least a few features changed after transformations. For example, the
German dataset had clear changes over the Months, and Credit Amounts features. The Compas dataset
has clear changes to the Count of Prior Offenses, Ages features. The values increased and decreased
depending on the group they were in, it depends on the ratio the algorithm needs to achieve for fairness.

5.2 Results Changes

(3) Did the transformations change the results?

As mentioned before, the Disparate Impact metric is the division of groups divided by sensitive features
while looking at a positive versus a negative result. The disparate impact metric shows the direction in
which the results change. The table below holds the value for the metric before and after each transfor-
mation. Correlation Remover (CR) removed the sensitive feature column before transformation, so it is
not possible to calculate the metric. The same holds for Disparate Impact Remover (DIR), the sensitive
feature remains unchanged which leads to the same value for the metric after transformations. Learning
fair representations (LFR) was positively affected for the German dataset but declined for the Compas
dataset. In this case, the results changed in different directions. Lastly, Optimized Preprocessing (OP)
also seems to be positively affected after transformations on both datasets. The results moved in the
direction to favor the unprivileged group a bit more, though it is still less than the privileged ones.

In sum, the results that are visible through the metric for Learning Fair Representations and Optimized
Preprocessing show that some results values have been changed. A few have been relabeled to equalize
the results for unprivileged and privileged groups.

Table 5.1: Disparate Impact Metric Values

 

| Dataset German | Compas |
Original 0.82 0.84
Preprocessed | 0.79 0.78

 

 

 

 

DIR n/a n/a

LFR 1.0 0.65
OP 0.938 0,929.
CR n/a n/a

 

 

Shadiah Ricardo Lacouture 15

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

- 1600
1768
1400
1200
1000
800
} 600
No Yes

 

5.1 shows a decline.

<25

 

2 S00
400
4g 810 2
n 300 2
, 200 8
Bad Good
cedit reoffended
Figure 5.3: German Preprocessed Figure 5.4: Compas Preprocessed
Dataset, LFR Transformed Heatmap of Dataset, LFR Transformed Heatmap of
Sensitive Feature Sensitive Feature

The third row contains the distribution of sensitive features by the classification result after being trans-
formed by the OP algorithm. Note again that this algorithm was applied to the preprocessed datasets
which only contain numerical values.

As the row above, these graphs can be compared to figures 4.2 and 4.6.

Figure 5.5 offers slight changes to the German preprocessed dataset. The unprivileged group now has
12 fewer clients marked with bad credit, which moved to good credit. And the privileged group has 15
clients less marked with good credit, which moved to bad credit.

Figure 5.6 shows the changes to the Compas preprocessed dataset. The privileged group now has more
recidivists while the unprivileged group has fewer.

v

- 1600
500
1500
a 4 i611 1564
400 1400
1300
+ 300
+ 1200
: 515 200 a : =
| 100 + 1000
Bad Good .

Black

age

race

>25

Caucasian

   

credit reoffended
Figure 5.5: German Preprocessed Figure 5.6: Compas Preprocessed
Dataset, OP Transformed Heatmap of Dataset, OP Transformed Heatmap of
Sensitive Feature Sensitive Feature

The last row contains the transformations after applying CR to the datasets.

Figure 5.7 has the duration of owning a checking account on the horizontal axis by credit amounts loaned
on the vertical axis. The darker color represents the values before and the lighter color represents the
values after being transformed by the DIR algorithm. the values have not changed much, as they seem to
overlap nicely. Though there is a tighter concentration of dots for months below 20 and amounts below
5000 after transformations.

Figure 5.8 is a scatterplot with the count of prior offenses on the horizontal axis by the ages of previous
offenders. There seems to be a concentration of dots below 15 prior counts, which was more spread out
before. Though in general, the dots have moved the changes are not very pronounced.

 

Shadiah Ricardo Lacouture 14

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

8 Appendix

1 # Load necessary packages
2 from IPython.display import Markdown, display;
3 from warnings import simplefilter

# ignore all future warnings
simplefilter(action="ignore’, category=FutureWarning ) ;

import pandas as pd;
import numpy as np;
import seaborn as sn
import scipy.optimize as optim;
import matplotlib.pyplot as plt

3 from aif360.datasets import GermanDataset , CompasDataset

from aif360.metrics import BinaryLabelDatasetMetric

5 from aif360.algorithms import Transformer

from aif360.algorithms. preprocessing .optim_preproc_helpers.data_preproc_functions
import load_preproc_data_german , load_preproc_data_compas

from aif360.algorithms. preprocessing import DisparateImpactRemover , LFR,
OptimPreproc

from aif360.algorithms.preprocessing.1lfr_helpers import helpers as Ifr_helpers

from aif360.algorithms. preprocessing .optim_preproc_helpers.distortion_functions
import get_distortion_german, get_distortion_compas

from aif360.algorithms. preprocessing .optim_preproc_helpers.opt_tools import
OptTools

from fairlearn.preprocessing import CorrelationRemover

#loading the full datasets
gd = GermanDataset(protected_attribute_names=[’age’], privileged_classes=[lambda x:
x >= 25],
features_to_drop=[’personal_status’, ‘sex’])

cd = CompasDataset(protected_attribute_names=[’race’], privileged_classes=[[’
Caucasian’ ]],

features_to_drop=[’sex’])

#define groups for dataset

privileged_age = [{’age’: 1.0}]

3 unprivileged_age = [{’age’: 0.0}]
privileged_race = [{’race’: 1.0 }]
unprivileged_race = [{’race’: 0.0 }]

#define BinaryLabelDatasetMetric class

metric_gd = BinaryLabelDatasetMetric(gd, unprivileged_groups=unprivileged_age ,
privileged_groups=privileged_age)

metric_cd = BinaryLabelDatasetMetric(cd, unprivileged_groups=unprivileged_race ,
privileged_groups=privileged_race)

3 #loading the preprocessed datasets

gd_pre = load_preproc_data_german ([ age’ ])

5s ed_pre = load_preproc_data_compas ([ race’ ])

#define groups for dataset

privileged_age = [{’age’: 1.0}]
unprivileged_age = [{’age’: 0.0}]
privileged_race = [{’race’: 1.0 }]
unprivileged_race = [{’race’: 0.0 }]

3 #define BinaryLabelDatasetMetric class

metric_gd_pre = BinaryLabelDatasetMetric(gd_pre, unprivileged_groups=
unprivileged_age ,

 

Shadiah Ricardo Lacouture 19

84

85

86

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

privileged_groups=privileged_age)
metric_cd_pre = BinaryLabelDatasetMetric(cd_pre, unprivileged_groups=
unprivileged_race ,
privileged_groups=privileged_race)

#Visualize german preprocessed dataset
gd_pre_df = gd_pre.convert_to_dataframe () [0]

1 gd_cm = pd.crosstab(gd_pre_df[’age’], gd_pre_df[’credit’], rownames=[’age’],

colnames=[’ credit’ ])

2 sn.heatmap(gd_cm, annot=True, fmt=’g’, xticklabels=[’Bad’, ’Good’], yticklabels=[’

5) = 9501)

3 plt.show()

#Visualize compas preprocessed dataset

cd_pre_df = cd_pre.convert_to_dataframe () [0]

c_before = pd.crosstab(cd_pre_df[’race’], cd_pre_df[’two_year_recid’], rownames=[’
race’], colnames=[’reoffended’ ])

sn.heatmap(c_before , annot=True, fmt=’g’, xticklabels=[’No’, ‘Yes’], yticklabels=[
*Black’, ’ Caucasian’ })
plt .show()

1 #Visualize German Credit Dataset

#many categorical variables have been dummy-—coded

3 #Special attribute is Age above 25, Credit 2.0 is good and 1.0 is bad

 

gd_df = pd. DataFrame(gd.convert_to_dataframe () [0])

#graph features
sn.lineplot(data=gd_df, x="month", y="credit_amount")

sn. boxplot(data=gd_df, x="age", y="credit_amount")

gd_cm = pd.crosstab(gd_df[’age’], gd_df[’credit’], rownames=[’age’], colnames=[’
credit ~])

sn.heatmap(gd_cm, annot=True, fmt=’g’, xticklabels=[’Bad’, ’Good’], yticklabels=[’
S25 > 25) 1)

3 plt.show()

#Visualize Compas Recidivism Dataset, many categorical variables have been dummy-
coded

#Sensitive attribute is race == 1.0 (Caucasian), two_year_recid == 1.0 (did
reoffend in less than 2 years)

cd_df = pd. DataFrame(cd.convert_to_dataframe () [0])

»9 sn. lineplot(data=cd_df, x="priors_count", y="two_year_recid")

sn. boxplot(data=cd_df, x="race", y="priors_count")

3 ed_cm = pd.crosstab(cd_df[’race’], cd_df[’two_year_recid’], rownames=[’race’],

colnames=[’reoffended ’ ])

sn.heatmap(cd_cm, annot=True, fmt=’g’, xticklabels=[’No’, °Yes’], yticklabels=[’
Black’, °*Caucasian’ ])

plt .show()

#Checking Disparate Impact on datasets before transformations
print("German Disparate Impact: ", metric_gd.disparate_impact(),
"Compas Disparate Impact: ", metric_cd.disparate_impact());

\n

print("German Disparate Impact:
"Compas Disparate Impact:

,» metric_gd_pre.disparate_impact(), °\n
", metric_cd_pre.disparate_impact());
#Disparate Impact Remover

from BlackBoxAuditing.repairers.GeneralRepairer import Repairer

 

Shadiah Ricardo Lacouture 20

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

3.2 Ethical Accountability

The ethical cycle is used to aid in answering the second subquestion: ’When is it sound to alter reality
with a pre-processing technique?’ , which was defined by Van den Poel (2007). The ethical cycle is a
structured way to make a moral decision.

The first step is to state the question in such a way that the main actor and the conflict are clear. The
previous subquestion becomes:

‘When should a developer use pre-processing fairness techniques to alter the reality of the data, not
knowing the extent of consequences that could negatively impact some groups in the future?’

The question uses a developer as the main actor and the moral dilemma of trying to be fair while justi-
fying the means to get there.

The next step is problem analysis, one should look at the stakeholders, interests, values, extra infor-
mation, and uncertainties. This helps define the problem even better before moving on to deciding on
possible actions. Though Van den Poel states clearly that this can be an iterative process.

In this case, it is possible to generalize the main scenarios that developers might encounter before they
have to justify the use of these packages. For example, algorithms to grant loans for banks, university
applications, repeat offender prediction, and job applications. Issues that can occur with algorithms are
labeling errors where people might commit errors when labeling data in the past or some inconsistencies
create a bias. Another issue is that biases from the past are reflected in the history of the data collected
and it skews data in favor of a certain group.

These scenes have 3 main elements. (1) There are classes as input, for example, male or female. (2)
There is a ‘good’ and ‘bad’ decision as output, as in loan granted or denied. (3) The algorithm is used
to predict a ‘life-altering’ decision for a person of the given class. Though the life-altering degree is
different for each scene, a university rejection is bad but there are other universities. Job rejection is also
not the end of the world. But being classified as a likely repeat offender, consequently having leniency
revoked and being sent to jail is significant, unless they deserve to go to jail.

The main stakeholders are the (possible sample) classes or groups of people forming our society that are
processed by the algorithm. These are just normal people, as we mentioned before applying to college,
applying for a loan to start a business or buy a car, being processed through the judicial system awaiting
their final verdict, or just trying to get a job at a modern firm. The developer is also a stakeholder, she
(probably) makes these algorithms for a living, is concerned with the quality of her work, and is aware
of the overarching role her algorithms play in others’ lives.

It is also easy to imagine that developers are constrained by the time they have to work on projects and
the final product the employer desires. There is not always time to look in-depth at each possible group
for every situation and see the unique differences. Besides, it is possible that an employer does not care
much for fairness and just wants accuracy, take for example a firm developing a tool for selecting the
best job candidate. They can desire to be fair due to legal concerns, but the main interest is profit and that
can be guaranteed with the best candidate, whoever that might be. So, fairness might take an honorary
place.

In this situation, public welfare, justice, fairness, and integrity are also at play. For example, our society
can be gravely affected if a certain group is constantly denied loans, jobs, or studies. This could lead to
cases where they constantly have more difficulty opening businesses or it is harder to get a nicer house,
and upgrade their lifestyle Though, the algorithms’ main purpose besides being more accurate than using
manual labor, is to be fair. Moreover, the integrity of the developers is also at stake, they would not want
to lose their job or face repercussions for affecting the livelihood of people. All in all, we can use these
facts to help reformulate the question. We state the question with clear opposing morals to make it
easier to understand and also answer. What should a developer do to improve fairness, knowing on the
one hand that pre-processing fairness techniques alter the reality of the data and could affect the public
welfare, while on the other hand trying to maintain loyalty to their job and their professional integrity

 

Shadiah Ricardo Lacouture 7

Technische Universiteit
T U Q  Einchoven
University of Technology

Justifying the use of fairness pre-processing algorithms

 

 

 

 

 

 

 

 

 

17500 ‘ TeO0
15000 : ‘ 14000
12000
g 12500 2
3 : 5 10000
E 10000 | 3
3 7500 5, 8000
é 8 co0o
s000
4000
2500
2000
0
00 19 nD » ® 0 w 7
we month
Figure 4.3: German Dataset Boxplot Figure 4.4: German Dataset Linegraph of
Comparison of Credit Amount by Age Months by Amount of Credit
- 2000
— 1600
. 2080 1987 1800 x teal 1500
= 1600 2 1400
g a 1300
e } 1400 8 ‘i
Ss 1200 1100
5 s 1000
§ 1000 &
8 g 00
reoffended ie ba
reoffended
Figure 4.5: Compas Dataset Heatmap of Figure 4.6: Preprocessed Compas Dataset
Sensitive Feature Heatmap of Sensitive Feature

between privileged and unprivileged groups. The unprivileged group tends to have a higher count of
prior offenses. Figure 4.8 shows the same feature by the resulting recidivist classification. As the count

of prior offenses grows, the chances of being classified as a recidivist also grow. Though there is a big
dip at around 27 counts.

 

 

 

 

 

 

 

 

35 ' : 10
i: ’
30 ’ 08
is ' 2
g 2 g 06
2 6 5 04
a
10
5 02
0 00
00 10 0 5 10 1 20 3 Ei) 3s
race priors_count
Figure 4.7: Compas Dataset Linegraph of Figure 4.8: Compas Dataset Boxplot of
Count of Prior Offenses by the Recidivist Prior Offenses by Race (Sensitive Fea-
Classification ture)

4.2 Fairness pre-processing Algorithms

In this case, we will be looking only at pre-processing algorithms, even though in-processing and post-
processing algorithms are offered by these toolkits too. The algorithms are from AI Fairness 360 and
Fairlearn. These toolkits are well-documented and have been established for a while with an active
community that helps during implementation. Pre-processing is applied to the input data before it goes
into the chosen model. This is an intentional choice to keep the scope of the research question at

 

Shadiah Ricardo Lacouture 10

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

2.3 Summary

The literature implies that there is a division in the algorithmic fairness community, some argue for the
formalized approach, which indicates a limitation on the possible fairness computation. Others argue
for a more substantive approach, essentially, implying that algorithms can not be solely responsible
for fairness as bias is an issue that arises out of the way society is shaped and data is collected. The
mathematical fork tries to enhance datasets by adding fairness overall but none truly look at the changes
in-depth. Even though other approaches are more motivating than based on the exploration of datasets,
the idea of an alternative approach should be considered if fairness metrics are not truly reliable as
proposed by this latter subgroup. Still, if one were to choose to use these toolkits it must be thoroughly
reasoned. The purpose is to be fair but how can it be guaranteed if the data on which any model will
be based has been altered in an unseen way. In this field, statistics can be used to give an indication
but all repercussions are not known until they happen. It would be imprudent to not consider that any
model that had such high accuracy and was supposed to be fair could have impacted other individuals in
unexpected ways. This establishes the relevance of the main research question for this paper.

 

Shadiah Ricardo Lacouture 5

Table of contents

Technische Universiteit
Eindhoven
University of Technology

 

Title
Justifying the use of fairness
pre-processing algorithms

 

Where innovation starts

1 Introduction

2 Related Literature

3 Ethical Justification

4 Methodology

5 Results

6 Discussion & Conclusion
7 References

8 Appendix

13

16

17

19

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

3. Ethical Justification

As a short reminder, the subquestion guiding this chapter is, When is a fairness pre-processing technique
necessary from an ethical point of view? One must consider that small as it might be, each technique
applied has a consequence and one wouldn’t want to have an illusion of efficacy while the data is just
wrongly skewed. It is important to make a justified, well-argued choice as to make the eventual predic-
tions ‘truly fair’.

3.1 Counterfactual Misuse

This section considers the justification of choices before algorithm implementation by looking at coun-
terfactuals with the help of a paper by Kasirzadeh and Smart (2021). Counterfactuals are used a lot in
conjunction with algorithms and are sometimes used to validate fairness. As explained by Kasirzadeh
and Smart, imagine one has two events X and Y, where X happened before Y. Assume then that one
wants to predict the cause of Y based on X, so a counterfactual explanation would be to assume that if
X had not occurred then Y would not have occurred either. A more specific example is, say a job appli-
cation model that bases its choice on Age, Gender, Education Years, and Diplomas Obtained. Age and
Gender are both sensitive attributes in this case. Imagine the applicant is 50 years old, a woman with 6
years of education, and one diploma, and she is rejected by the algorithm. A counterfactual explanation
would be to say that if she were male she might not have been rejected.

Though Kasirzadeh and Smart, state that to be able to validate such a statement one needs to meet
certain criteria of ’... ontological, semantic and ethical choices and judgments’. Otherwise, it is not
possible to assume that the counterfactual explanation holds. These presumptions are based on different
perspectives. For example, an ontological perspective analyses the possible genders or social categories
(in essence, race). Then a presumption that should be answered is: what is gender (or race) for the
developers of the algorithm? These counterfactual assumptions on which the fairness of the algorithm
is based are not valid unless the choices are all explicit. For example, for the job application model to be
fair, one should be able to answer what the prediction of the model would be if for the same person only
the gender changed. But since it is not clear how gender was specified, it is hard to answer this question.

To bring it back into context, when choosing algorithms from fairness toolkits the first choice is where
to ’apply the fairness’. This can be done before the data is processed by the algorithm (pre-processing),
during training of the model (in-processing), or after receiving the predictions (post-processing). In
essence, if one chooses to in-processing the data, the model will be focused on improving a metric or
some other specific goal to improve fairness mathematically. Post-processing makes sure the predictions
of the model are recalibrated to again ensure some specific ratio, metric, or goal of fairness. These
algorithms are attached to the pipeline of the model to guide it in the chosen direction. Yet when
compared to pre-processing, this technique is not able to steer a model, since the model comes after.
The changes occur on the input data only, and pre-processing techniques have to transform the data
into a new perfect dimension where everyone is treated fairly. To tie it into the subquestion, "When
is it necessary to use a pre-processing technique?’ becomes ’When is it sound to alter reality with a
pre-processing technique?’

Hence, to use pre-processing algorithms is to choose to alter reality. The approach is to mathematically
transform values, the main consideration is to always remove the link to the sensitive features. Yet,
causal relationships between features are ignored. One assumes that in a fair algorithm all cases are to
be treated the same such that if one (sensitive) feature is changed for a chosen person and all else is
held the same, the result will be fair and predicted correctly. But this assumption attempts to assert that
the altered world is similar in all ways except for that one change. Though, the similarity of the dataset
representation in the altered world to the original is speculation. In this way, these models are misusing
counterfactuals to validate fairness, since ’no causal model captures absolutely objective relations in the
world’ (Kasirzadeh & Smart, 2021).

 

Shadiah Ricardo Lacouture 6

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

4 Methodology

 

This chapter considers the first subquestion; ’What is the effect of fairness pre-processing techniques
on input data?’. The chosen datasets, metrics, and pre-processing algorithms are all explained in detail
over the first three subsections, 4.1 to 4.3. The research needed to show the impact on input data when
one applies pre-processing techniques is explained in subsection 4.4.

4.1 Datasets

AIF360’s toolkit offers multiple datasets for use, most of them have been used in benchmark research
papers and are well-known. Two datasets with different sample sizes, the German Credit dataset, and
the Compas Recidivism dataset, will be used for analysis. Besides, preprocessed versions of the datasets
had to be used to implement some of the fairness algorithms since two of the algorithms only work with
numerical data. The preprocessed versions are slightly smaller but are also available to download from
AIF360’s modules.

The German Credit dataset is small, containing 1000 instances. The dataset has 20 attributes and clas-
sifies each person with good or bad credit risk based on these features. It has two protected attributes;
sex and age. Being a man and being older than 25 make an individual part of the privileged groups. The
chosen sensitive feature is age since it is also used in the benchmark dataset by Kamiran and Calders
(2012). The distribution of the result by the chosen sensitive feature is visualized in figure 4.1. It is also
offered with a preprocessed binary representation. The distribution of the preprocessed dataset is shown
in 4.2.

Besides, figure 4.3 shows a small comparison between sensitive features over the credit amount feature.
It seems that clients with a privileged age tend to have higher credit amounts. Last, figure 4.4 offers the
credit amount over the months one owns a checking account. The plot shows that as the months grow,
the credit amounts grow too. But they drop in the end, after more than 55 months. These plots are from
the original unprocessed German Credit Dataset.

The ProPublica Compas Recidivism dataset has around 6000 instances, the dataset has 400 attributes,
the majority are dummy variables. It has two protected variables, sex, and race. Being a woman and
Caucasian makes an individual belong to privileged groups. The dataset classifies all individuals as
either a possible recidivist or not. The chosen sensitive feature is race as it is more relevant. Based on
the findings from the past, it was shown that the Compas dataset was rigged since Caucasian offenders
were less likely than African-American offenders to be classified as reoffenders. This distribution is
visualized for the sensitive feature in figure 4.5. This dataset is also offered in a preprocessed binary
representation, the sensitive feature distribution is shown in 4.6.

The last figure 4.7 on the left is a boxplot showing the difference of the prior offenses count distribution

- 600

500 -500
v

- 400 + 400

8

| 300 300

+ 200 a0 ae }-200
nN

+ 100 } 100

Bad Good

gedit

 

credit

Figure 4.1: German Dataset Heatmap of Figure 4.2: Preprocessed German Dataset
Sensitive Feature Heatmap of Sensitive Feature

 

Shadiah Ricardo Lacouture 9

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

4.4 Analysis Structure

The detailed code can be seen in Chapter 8. First packages are loaded from AIF360, Fairlearn, and
graphing packages. The full version and the preprocessed version of the Compas and the German dataset
are loaded. The chosen sensitive feature for Compas is race and for German it is age. After which these
graphs are explored and some features are visualized. The datasets remain unchanged and thus are not
preprocessed or normalized since the values are not used for predictions. The purpose is only to see
the changes in the distribution. The chosen fairness metric, Disparate Impact from the AIF360 toolkit’s
metrics, is also calculated at this point before anything is changed. Then Disparate Impact Remover
algorithm is set up with repair level 0.5 and ran on both datasets. Learning Fair Representations has no
extra parameters set up, it is then run on both datasets. Right after, Optimized Preprocessing runs with an
epsilon of 0.05, which refers to the steps taken for converging. Lastly, Fairlearn’s Correlation Remover
was applied to the datasets. Though the result column had to be removed before the transformation. Note
that Reweighing does not alter data, it creates weights on the side so it does not apply to this question,
and it is not considered or applied here. After each transformation, the fairness metric is calculated
again to validate the fairness of the datasets. After the datasets have been transformed, the changes that
happened to the features are visualized.

The subquestion of this chapter is What is the effect of fairness pre-processing techniques on input
data?’. The main goal of the analysis is to look at the impact the pre-processing algorithms can have
on input data. There are many ways to categorize this impact. Some questions to consider that could be
answered in Chapter 4 are:

1. What features have changed after transformations?
2. How did the features change, did they increase or decrease values?

3. Did the transformations change the results?

Though the first two questions may have overlapping answers. Since they are specifically about the
features. The answers will help to form a possible solution to the first subquestion.

 

Shadiah Ricardo Lacouture 12

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

ease their introduction to toolkits.

The previous papers are offered for completeness of the related literature on pre-processing algorithms.
So, one has a clearer view of the algorithmic fairness landscape. Consequently, this fork is not explored
further as the main research topic questions the use of the approaches.

2.2 Critical Perspective

Truly opposing the previous papers, Lee (2021) argues that current formal approaches will not aid fair-
ness overall, and proposes a solution called the substantive approach. Lee mentions that algorithmic
injustice often results from accurate predictions that reproduce existing inequities rather than inaccu-
rate predictions that misjudge already-disadvantaged individuals. And so we must do away with social
hierarchies and try to alleviate the pain and injustices that arise from them.

This approach has three steps, but the essence is to keep algorithms as an enhancing tool and not the
main dish. (1) First, consider how hierarchies and existing policies enforce certain conditions. Then
(2) thinking on changes that can be applied to these social inequities and institutionalized policies. It
is in the final step (3) where we can analyze whether an algorithm would supplement the solution. To
conclude he reproaches that the field should look into more alternative theories and other manners of
using algorithms, with the purpose of actual change in society, as the algorithm should not act by itself.
In consequence, this will open up space for new implementations of algorithms and their evaluation.

An essay that truly captures this same spirit is The Seductive Diversion of ‘Solving’ Bias in Artificial
Intelligence by Powles (2018). This short Medium essay recalls the inherent issue with AI, which some-
times is introduced as a savior and other times as it is; biased. In short, this leads to the realization that
bias is a social issue, and trying to fix it through AI and obsessing with new theoretical and mathematical
functions to express and calculate it better will always fall short. The goal seems to stick around fixing
and improving A.I. systems, never to use a different system or not use a system at all. Powles concisely
states that AI might seem like an all-powerful force, but it is driven by people with power and money.
In turn, AI policies and major owners of data are self-regulated. They enforce a narrative and push the
idea of reliance on these systems, which might be difficult to reverse in the future. So, it calls for a
regrouping of sorts, rethinking the idea that AI is our panacea and facing the problem that a new and
more innovative idea is needed to solve bias.

A bold paper in the same trend by Fazelpour and Lipton (2020) states that much of the work in fairness
is impaired by all the idealistic ideas to try and formalize differences and justice when this world is
anything but. These fairness algorithms with ‘loose’ definitions could be claiming to do their job while
just causing more harm to disadvantaged groups. It is then best to have a non-idealistic approach. In their
own words “... our present decision-maker cannot through their actions alone bring about the immediate
end to all disparity...’. Hence, to apply fairness or minimize harms one should look at the broader sense
of the Machine Learning model one is fixing. Since a local static analysis will not cover all possible
challenges, it can and should be more than these simple modifications.

The last paper supporting the same idea by Andrus, Spitzer, Brown, & Xiang (2021) covers the issues
surrounding data collection and proposes 3 solutions. Data collection issues arise when practitioners do
not know exactly where the data is coming from for example if practitioners are buying data, it might
turn out to be unreliable. In turn, applying algorithmic fairness to this will only reduce the algorithmic
bias issue, but any such formal solution will not take into account where the actual discrimination could
be coming from and will not help in the end. The three solutions are; (1) clearer legal requirements
around data protection and anti-discrimination, (2) privacy-respecting algorithmic fairness strategies,
and (3) meaningful agency of data subjects. Finally, the authors reiterate once more groups are involved
in the collection and use of data it will lead to a better understanding to solve (uncovered) biases.

 

Shadiah Ricardo Lacouture 4

EINDHOVEN
eo UNIVERSITY OF
TECHNOLOGY

Eindhoven University of Technology

BACHELOR

Justifying the use of fairness pre-processing algorithms

Ricardo Lacouture, Shadiah M.

Award date:
2022

Link to publication

Disclaimer

This document contains a student thesis (bachelor's or master's), as authored by a student at Eindhoven University of Technology. Student
theses are made available in the TU/e repository upon obtaining the required degree. The grade received is not published on the document
as presented in the repository. The required complexity or quality of research of student theses may vary by program, and the required
minimum study period may vary in duration.

General rights
Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners
and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.

+ Users may download and print one copy of any publication from the public portal for the purpose of private study or research.
+ You may not further distribute the material or use it for any profit-making activity or commercial gain

Technische Universiteit
e Eindhoven
University of Technology

Department of Mechanical Engineering

De Rondom 70, 5612 AP Eindhoven P.O.
Box 513, 5600 MB Eindhoven

 

The Netherlands

www.tue.nl

Shadiah Ricardo Lacouture(1397648) Justifying the use of fairness
Responsible Lecturer pre-processing algorithms

Mykola Pechenizkiy, Hilde Weerts

Tansey 21,2022 Shadiah Ricardo Lacouture(1397648)

s.m.ricardo. lacouture@student.tue.nl

 

Where innovation starts

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

a reasonable depth. The established AI Fairness 360 toolkit offers four pre-processing options and
Fairlearn offers only one, namely;

1. Disparate Impact Remover: DIR (Feldman et al., 2015) works by editing values for the sake
of bettering fairness for all groups. In the documentation, it is mentioned that it does so while
maintaining rank order within groups. Note that it is possible to choose how much is ‘repaired’
by the algorithm, with help of an extra parameter.

In short, the mathematical idea is to calculate the marginal distribution on a single dependant
variable and the protected attribute, with which the cumulative distribution and an associated
quantile function are calculated. The latter function is used to rank the values from the marginal
distribution. After which, the algorithm is better able to choose higher-ranked values, which are
the ones that are the most similar to the original after being repaired.

2. Learning Fair Representations: LFR (Zemel et al., 2013) tries to find a hidden pattern for the data,
consequently, it attempts to represent the data in this manner while blurring the protected features.
The model achieves this by mapping the original dataset points to a secondary plane, while trying
to retain all the information on the independent features, except for the link to the sensitive feature.
Then it maps the points on the new plane to the dependent feature, here also making sure that the
mapping is as close to the original function.

3. Optimized Preprocessing: OptimPreproc (Calmon et al., 2017) learns a probabilistic transforma-
tion that edits the features and labels in the data with group fairness, individual distortion, and data
fidelity constraints and objectives. In the documentation, they note that this algorithm does not
use the prespecified (un)privileged groups, and instead tries to reduce statistical parity between all
possible groups in the dataset.

4. Reweighing: Reweighing (Kamiran and Calders, 2012) adds different weights for each entry com-
bination, this ensures fairness for all individuals before going to a later stage (e.g classification).
This transformation is thus not on the data itself but results in an accompanying array of weights
to be used together with the data.

5. CorrelationRemover (of Fairlearn): The computation is described as a linear transformation to
the normal features to remove their correlation to the sensitive feature column. It also tries to
retain as much of the original information as possible. This is done through a filter that allows the
practitioner to filter out as much or as little information as they desire. The retainer constraint is
measured with the least squared error metric.

The Fairlearn CorrelationRemover seems similar to AIF360’s LFR based on their concept of uncorre-
lating sensitive features from the rest of the features while making sure to encode data as efficiently as
possible.

4.3 Fairness Metrics

To aid in visualizing bias there are a few metrics available by the toolkits. These use ratios and com-
parisons per group or individuals to show how fair the data is overall. They are formalized to the grain
approaches that look at results. A useful metric is Disparate Impact, which occurs when a decision un-
intentionally has different outcomes for different groups. The datasets above have clear privileged and
unprivileged groups. So, the metric can clearly show the direction of the bias for these groups and should
help identify whether the rates are fair. The metric is defined in the paper by Feldman et al. (2015) it is
computed as the ratio between the rate of favorable outcomes for the unprivileged group to that of the
privileged group. The goal is a perfect 1 or 100% percent, in reality, there is an 80% minimum which is
ethically acceptable. If the ratio is higher than 1, that means that the unprivileged group is getting more
benefits than the privileged.

 

Shadiah Ricardo Lacouture 11

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

1 Introduction

Nowadays, many businesses implement Machine Learning (ML) as a way to improve decision-making
and predictions. This is applied in many sectors such as finance, banking, education, healthcare, and
even in private companies. The nature of Machine Learning is to self-improve an initial algorithm
without needing much human interaction and enhancing the overall process.

But, Machine Learning is a man-made tool and the data it receives will affect the results pervasively.
As such, when preparing a model one should keep in mind that the data available is an estimation of
what we wish to measure, for example, grades for intelligence. And it can be outdated or skewed, for
example, a company that only hired a specific ethnicity or gender regardless of capability. Any model
built on that data will want to follow the same biased pattern. This is why the algorithmic fairness
field is flourishing, we are aware that data is not always fair and we desire a solution. The focus is on
understanding where these biases come from and implementing mathematical solutions to correct them
back in a chosen direction.

In a simple sense, fairness means that every person gets what they deserve, it is recompense for merit.
In this case, though, algorithms are the base for mathematical fairness. Algorithms are trained on data,
and data can contain certain biases, it can be done on purpose, for example only letting people older
than a certain age take a test so the test results are only available from that age onward. Bias can also
be caused by accident, imagine a researcher that collects survey results only in certain neighborhoods.
Most neighborhoods have a concentration of specific family compositions or ethnicity, this can skew the
data as the sample the researcher uses is not a true representation of the population.

To correct these issues, mathematical fairness can take on multiple faces. It can mean that results have
the same ratio or are similar over all groups, that all individuals receive the same outcome, or that
outcomes differ based on specific values for individuals. Note that many of the definitions are mutually
exclusive.

Consequently, there are various ways fairness is applied in Machine Learning through specific toolkits.
There is a variety to choose from, at least 5 established and possibly more to come; Google’s What-
if tool, Aequitas, Fairlearn, IBM’s AI Fairness 360, and Sci-kit packages. The most established and
complete fairness toolkit AI Fairness 360 (AIF360) offers:

(1) A wide array of explanatory metrics and algorithms to test for bias. These are based on mathe-
matical definitions of fairness, though most are exclusive to each other, because of the contradicting
interpretation of fairness.

(2) Besides, the toolkit offers multiple datasets for bias mitigation testing. Which are made with the
purpose to help in many domains, such as finance, education, and healthcare.

(3) Their documentation offer explanations over these metrics, datasets, and algorithms.

In sum, the use of Machine Learning models is synonymous with not knowing exactly what it is doing
or how it is outputting predictions with what has been given. With the added calibration for fairness, it
is dubious that one knows what truly changed in the dataset, and important decisions rely on this. It is
then not an exaggeration to say models can be unfair, though this has not been a deterrent for their use.
Hence, the application of fairness by using toolkits to get the desired result is conflicting; on one hand
predictions and outputs have great accuracy and relatively fairer results, on the other, the data used to
train, test, and/or the results were adjusted, removed or relabeled.

Which raises the question: what is happening exactly to the data? It is not unusual to wonder if more
is happening behind the scenes, or better put if the changes to the data are influencing more than we
are aware of. Consider the fact that one might be testing and proving that these models work in a static
local environment. So, it is plausible that these models could cause more harm, for example, the fair
model could depreciate the main group in the quest of being fair to all or in some other way create

 

Shadiah Ricardo Lacouture 1

168

169

180

182
183
184
185
186
187
188

190

 

 

2 g_df_CR[’credit’] =
3 no_credit[’credit’]

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

Wiis fe [ls 07057750]

}
2 optim_options_c = {
"distortion_fun": get_distortion_compas ,
epsilon: 07055,
Vielis ty (0.9 oF 99 22995
"dlist": [.1, 0.05, 0]
}
class OP:
def __init__(self, unpriviliged_group, priviliged_group , dataset , optim_options
Ne
self.unpriviliged_group = unpriviliged_group
self.priviliged_group = priviliged_group
self.dataset = dataset
self.optim_options = optim_options
def df(self):
OP_object = OptimPreproc(OptTools , self.optim_options )
OP_object = OP_object. fit (self.dataset)
df_OP = OP_object.transform(self.dataset , transform_Y = True)
df_OP = self.dataset.align_datasets (df_OP)
return df_OP
c_OP = OP(unprivileged_race , privileged_race , cd_pre, optim_options_c).df()
g_OP = OP(unprivileged_age , privileged_age , gd_pre, optim_options_g).df()

c_df_OP = pd. DataFrame(c_OP. convert_to_dataframe () [0])

g_df_OP = pd. DataFrame(g OP. convert_to_dataframe () [0])

print("German Disparate Impact: ", metric(g_OP ,unprivileged_age , privileged_age),
Nai
"Compas Disparate Impact: ", metric(c_OP, unprivileged_race , privileged_race))

#Correlation Remover

credit = gd_df[’credit’]
no_credit = gd_df.drop(’credit’, axis=1)
CR = CorrelationRemover(sensitive_feature_ids = [’age’])

» gd CR = CR. fit_transform(no_credit)

recid = cd_df[’two_year_recid’]

no_recid = cd_df.drop(’two_year_recid’, axis=1)

CR = CorrelationRemover(sensitive_feature_ids = [’race’])
cd_CR = CR. fit_transform(no_recid)

5 #remove sensitive feature from original column, make sure both datasets have the

same shape, and compare
no_credit = no_credit.drop(’age’, axis=1)
g_df_CR = pd.DataFrame(gd_CR, columns = no_credit.columns)
g_df_CR.index = g_df_CR.index.map(str)
df_gd_CR = no_credit.compare(g_df_CR)

 

credit
= credit

#remove sensitive feature from original column

cd_race = no_recid.drop(’race’, axis=1)
c_df_CR = pd.DataFrame(cd_CR, columns = cd_race.columns)
e_df_CR.index = cd_race.index .map(str)

 

Shadiah Ricardo Lacouture 22

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

6 Discussion & Conclusion

In sum, Machine Learning models are used increasingly more in daily life seeing as we are becoming
more aware that data is biased. Formal approaches offer mathematical solutions to try to alleviate the
problem with the use of fairness toolkits. Yet, there is a dissent from the other side of the algorith-
mic fairness field, which considers fairness toolkits an ill-fitting patch-up when trying to solve societal
problems.

In Chapter 3, the second subquestion of the introduction chapter is answered, When is a fairness pre-
processing technique necessary from an ethical point of view?. It never seems necessary to use the
pre-processing techniques. Yet if one chooses to do so there are presumptions, choices, and judgements
that should be explicitly answered, and documentation to keep the process as thoughtful as possible.

Additionally, Chapter 4 mentions three smaller questions to aid in answering the first subquestion of
the introduction, What is the effect of fairness pre-processing techniques on input data?’. Chapter 5
offers the answer for the previous subquestion through analysis on the German Credit and the Compas
Recidivism datasets. The research uses 4 pre-processing algorithm from AIF360 and Fairlearn.

With the answers of the previous chapters, the main research question can be answered. It is noted again
for ease that the question is: ‘How do pre-processing fairness techniques impact input data and
when is their use justified from an ethical perspective?”

This study found that pre-processing fairness techniques alter values at seemingly random places, that
are based on mathematical functions. Moreover, multiple features and the results were changed after
transformations. When measured with the fairness metric Disparate Impact, it is clear that the balance
of positive results for both groups is trying to be restored but is not always achieved. It is clear that is
never truly necessary to use fairness toolkits, but when one does choose to use them some considerations
should hold. The validation of fairness will only be ethically accurate once presumptions, choices, and
judgements about the features are explicitly answered. Besides, an ethically accountable way to use
pre-processing fairness toolkits is by adding a well-founded documentation framework to the workflow.

It would be better if, in the future, the research is repeated with more datasets to find a clear trend.
Specifically, future research with more datasets could focus on finding out how exactly the algorithm
chooses which values to increase or decrease. It is also a possibility that larger datasets are needed
to notice a clear trend. This study was limited by the fact that four different algorithms were tested,
but a more specialized approach on one or two algorithms would be easier to handle. Besides, having
preprocessed datasets and full datasets made comparisons difficult. To add to the previous comment,
one should choose algorithms that can all work on the same dataset, this would help deliver a clearer
result. Another improvement for this paper is to visualize more or all features in-depth and compare
their absolute difference after being transformed.

Finally, the biggest contribution of this paper is the exploration of the way single features are changed
after transformations. This study is written in the hope that more research will be done to truly under-
stand the effect of fairness pre-processing algorithms, and that we keep a level-head when considering
the results. Simply because a metric says it is fair, does not mean one can’t explore further.

 

Shadiah Ricardo Lacouture 16

107
108
109
110
i

112
113
14
115
116
117
118
119

153
154
155

156

158

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

class DIR:
def __init__(self, unpriviliged_group , priviliged_group, dataset):
self.unpriviliged_group = unpriviliged_group
self.priviliged_group = priviliged_group
self.dataset = dataset

def df(self):
DIR = DisparateImpactRemover(repair_level =0.5)
df_DIR = DIR. fit_transform(self. dataset)
return df_DIR

def metric(dataset , unpriviliged_group, priviliged_group):
metric_DIR = BinaryLabelDatasetMetric(dataset , unprivileged_groups=
unpriviliged_group ,
privileged_groups=priviliged_group )
return metric_DIR. disparate_impact ()

c_df_DIR = pd. DataFrame(DIR(unprivileged_race , privileged_race , cd).df().
convert_to_dataframe () [0])

5 g_df_ DIR = pd.DataFrame(DIR(unprivileged_age , privileged_age , gd).df().

convert_to_dataframe () [0])

print("German Disparate Impact: ", metric(DIR(unprivileged_age , privileged_age , gd)
.df(), unprivileged_age , privileged_age) , ’\n’
"Compas Disparate Impact: ", metric(DIR(unprivileged_race , privileged_race , cd
).df() ,unprivileged_race , privileged_race));

#Learning Fair Representations

2 class LeFaRe:

def __init__(self , unpriviliged_group , priviliged_group, dataset):

self.unpriviliged_group = unpriviliged_group
self.priviliged_group = priviliged_group
self.dataset = dataset

def df(self):

LFR_object = LFR(unprivileged_groups = self.unpriviliged_group ,
privileged_groups = self.priviliged_group)

df_LFR = LFR_object. fit_transform(self. dataset)

return df_LFR

3 def metric(dataset , unpriviliged_group, priviliged_group):

metric = BinaryLabelDatasetMetric(dataset , unprivileged_groups =
unpriviliged_group ,

privileged_groups = priviliged_group)
return metric. disparate_impact ()

c_df_LFR = pd. DataFrame(LeFaRe(unprivileged_race , privileged_race , cd_pre).df().
convert_to_dataframe () [0])

» g_ df LFR = pd.DataFrame(LeFaRe(unprivileged_age , privileged_age , gd_pre).df().

convert_to_dataframe () [0])

print("German Disparate Impact: ", metric(LeFaRe(unprivileged_age , privileged_age ,
gd_pre).df() ,unprivileged_age , privileged_age), °\n’
"Compas Disparate Impact: ", metric(LeFaRe(unprivileged_race , privileged_race ,

cd_pre).df(), unprivileged_race , privileged_race))

#Optimized Preprocessing

optim_options_g = {
"distortion_fun": get_distortion_german ,
"epsilon": 0.05, # used to be 0.01, but takes too long
Veale (OE, I). 2 ee

 

Shadiah Ricardo Lacouture 21

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

new minorities to discriminate against. Besides, the widespread idea that there could be one cure to all
problems in Artificial Intelligence (AI) is unrealistic, namely, it rarely happens that man-made medicine
has no side effects. Here another question arises; when should fairness toolkits be used then?

To reiterate, the two subquestions are:

1. The first is on the impact of fairness pre-processing techniques on input data;
‘What is the effect of fairness pre-processing techniques on input data?’
The algorithms used are from two well-established fairness tools; AIF360 made by IBM and
Fairlearn.

2. The second handles the justification of their use;
‘When is a fairness pre-processing technique necessary from an ethical point of view?’
This question is still broad and therefore is better explained with the help of key scenarios that are
realistic and applicable to a variety of situations.

The questions mentioned above are joined into the main research question of this paper, which is:
“How do pre-processing fairness techniques impact input data and when is their use justified from
an ethical perspective?”

Finally, this paper is structured as follows; Chapter 2 handles related literature and will encompass
a broad range of papers with opposing perspectives on mathematical approaches and other solutions
for algorithmic fairness. Subsequently, Chapter 3 is on ethical justifications, this chapter describes the
sometimes incorrect validation of fairness in models and other ways to hold developers accountable. The
second subquestion is answered at the end of this section. Chapter 4 covers the methodology section,
wherein the research question is described more in-depth and the changes in data after using a fairness
toolkit are explored. Followed by Chapter 5 on the results, the transformed data compared to the original
is visualized in this section, which will aid in answering the first subquestion. After that, Chapter 6 is
for the discussion and conclusion section on the limitations and future possible work, the answer to the
main research question, relevance for the algorithmic fairness field, and summarizes the thesis. Chapter
7 holds the Appendix, where the visualizations and other results can be corroborated.

 

Shadiah Ricardo Lacouture 2

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

intact?

The third step is to consider the options for action. The developer can choose from different options;

1. The developer does not use a fairness toolkit
2. The developer removes bias by hand over each instance or for specific groups
3. The developer uses a fairness toolkit to remove bias

(a) With no added documentation

(b) And uses a framework to argue and document all her choices

Assuming that the developer is considering the fairness toolkit because they noticed some type of bias
for or against some groups, the first choice seems harsh, and the data would still be unfair in the end.
The second choice would go against the loyalty to company constraint, as this encompasses employer
demands and time constraints. Removing or relabeling values by hand for all instances could be too
time-consuming.

The last option is to use a fairness toolkit that has two sub-choices, either keep well-argued documenta-
tion of choices or just use it with no add-ons. The latter might seem enticing but it could lead to violating
the public justice and fairness values while having the former ensures that at least each choice was made
with care and their effect was as controlled as possible. From a certain perspective, it might seem ex-
travagant to have to justify each step or the choice of a random package. But note that we prevent the
consequence of negligence by making sure that everything used is strictly necessary. And having this
constraint will push developers to think more about their choices.

A paper by Lee et al. (2020) provides steps one can take to eventually come to an ethical decision. They
note that one needs to know more than what just fairness metrics tell, and propose Key Ethics Indicators
(KEI’s) as a solution. In essence, it is a documented balanced way to look at facts and make a decision
based on this trade-off.

3.3 Summary

In short, this chapter looks at counterfactual misuse and ethical accountability. Counterfactual misuse
refers to the fact that the fairness validation of causal models can be incorrect. As it is sometimes based
on assumptions that cannot be proven and are at best speculation. This study argues that pre-processing
is different from in-processing and post-processing techniques, as the former alters the real dimension
and the latter can only steer the direction of the model pipeline to get desired results.

Furthermore, the ethical cycle is used to propose a solution for holding developers ethically accountable
while using fairness pre-processing algorithms. Seeing that developers juggle many conflicting values
and morals, it is best to justify each step they take with documentation as this will promote thinking each
choice through.

Finally, to answer the original subquestion of the chapter When is a fairness pre-processing technique
necessary from an ethical point of view? It never seems necessary to use the pre-processing techniques.
Yet if one chooses to do so there are presumptions, choices, and judgements that should be explicitly
answered, and documentation to keep the process as thoughtful as possible.

 

Shadiah Ricardo Lacouture 8

TU / ONS ot technotoey Justifying the use of fairness pre-processing algorithms

 

16. Zemel, R., Wu, Y., Swersky, K., Pitassi, T. &; Dwork, C.. (2013). Learning Fair Representations.
Proceedings of the 30th International Conference on Machine Learning, in Proceedings of Ma-
chine Learning Research 28(3):325-333. Available from https://proceedings.mlr.press/v28/zemel
13.html.

 

Shadiah Ricardo Lacouture 18

TU / OC areeiy ortechnoory Justifying the use of fairness pre-processing algorithms

5 Results

 

This chapter handles the research done to show the impact of the algorithms on two datasets. Disparate
Impact Remover (DIR) and Correlation Remover (CR) were applied on the original datasets, but Learn-
ing Fair Representations (LFR) and Optimized Preprocessing (OP) were applied on the preprocessed
datasets. As mentioned before, the research shows the impact on the datasets after the transformations
with the help of three questions.

5.1 Feature Changes

This subsection tries to answer the two following questions mentioned in Chapter 4.
(1) What features have changed after transformations?
(2) How did the features change, did they increase or decrease values?

Different columns were visualized to check whether the distributions of features changed anywhere.
The next set of figures offers an insight into some of the features that were changed the most based on
summary statistics. The first column shows changes to the German Credit dataset and the second to the
Compas Recidivism dataset.

The first row is for DIR transformations. Figure 5.1 has the duration of owning a checking account on
the horizontal axis by credit amounts loaned on the vertical axis. The darker color represents the values
before and the lighter color represents the values after being transformed by the DIR algorithm. It can
be seen that for the first 30 months of owning a checking account credit amount values used to spread
much higher than after transformation.

To compare the effect of DIR on the Compas dataset, refer to figure 5.2. The scatterplot contains the
count of prior offenses on the horizontal axis by the ages of previous offenders. Counts between 15 and
25 were lowered and counts after 25 were spread out.

 

 

 

 

 

 

 

 

 

 

° 20
17500 e
rs000] . eS ie °| i.
“4 oe | 70 ngs 2 oa
12500 el ce: Cee “Hepa
10000 a 2 { a: = ba i fat” pe an 7
Lo ihe ae - 2 fate, oa
ry 9 age 2 | | - 5 . ea ite tas aK
5000 oft ‘ aay “8
2500} f i ele : = fat a *
ol? 2
mo 0 9 4 5S 6 7 o > x &
Figure 5.1: German DIR Transformed Figure 5.2: Compas DIR Transformed
Scatterplot of (Duration of Checking Ac- Scatterplot of Count of Prior Offenses by
count in) Months by Credit Amount Ages

The second row is for LFR transformations. Note that LFR was trained on the preprocessed version
of the datasets, which only contained numerical values. Figure 5.3 is a replica of figure 4.2 shown in
Chapter 3. The heatmap shows the distribution of the sensitive feature age by the classification of credit.
The dataset now marks all clients as having bad credit to achieve a fair ratio. Note from Chapter 3, that
LFR’s main goal is to make a new mapping plane where the features’ connection to the sensitive feature
is blurred. This might be the reason for the distribution of the groups.

Similarly, one can compare figure 5.4 to 4.6 from Chapter 3. The biggest change is the decrease for the
privileged group of past offenders as there used to be 882 and now it is 552. The part of the privileged
group that did not reoffend was 1281 but increased to 1551. While the opposite happened for the
unprivileged group. It is not a surprise that the disparate impact metric of LFR for the Compas Dataset

 

Shadiah Ricardo Lacouture 13
