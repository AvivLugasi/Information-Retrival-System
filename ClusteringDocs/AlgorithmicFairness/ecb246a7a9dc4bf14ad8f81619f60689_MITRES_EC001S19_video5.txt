 

Demographic Parity

¢ Demographic parity is the next step of the widely Known remedies to unfairness in

machine learning (Kusner et al, 2017) and is equivalent to independence of the outcome Y

with respect to the protected attribute (A):
p(Y|A =a) = p(V|A =a’), YLA,
where YLA denotes independence, anda anda’ are any couple of values of the attribute.

¢ [This definition expects the outcomes to be the same for groups, therefore the prediction

independent of the protected attribute group membership.

¢ Example: probability of being hired is independent of gender

 

 

Equalized Opportunity

¢ Equalized opportunity is concerned with treating fairly those who are determined to be
worthy of acceptance (Y=1). It is not concerned with rejecting people fairly across protected
groups. In otner words, the false positive rates and the true positive rates do not both need
to be equal across the protected categories. [he equalized opportunity principle states the

following condition for the probabilities: (Hardt et al., 2016)

p(¥ = 1|A=0,Y=1) = p(Y=1|A=1,Y = 1).

¢- In ahiring example, this would be Individuals deemed worthy of hiring by a human hiring

officer, whereas Y indicates those deemed worthy of hiring by the algorithm.

ge

gia
‘USAID
2 =e fl
“La <7
Ga eee” FROM THE AMERICAN PEOPLE

 

 

References

¢ Apfelbaum, E.P., Pauker, K., Sommers, S.R. and Ambady, N. (2010). In blind pursuit of racial equality?. In Psychological
Science, 21(11), pp.1587-1592.

¢ Chen, J., Kallus, N., Mao, X., Svacha, G. and Udell, M. (2019). Fairness under unawareness: Assessing disparity when protected

class is unobserved. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 339-348.

¢ Hardt, M., Price, E. and Srebro, N., 2016. Equality of opportunity in supervised learning. In Advances in neural information

processing systems, pp. 3315-3323.

¢ Kilbertus, N., Carulla, M.R., Parascandolo, G., Hardt, M., Janzing, D. and Schélkopf, B. (2017). Avoiding discrimination through

causal reasoning. In Advances in Neural Information Processing Systems, pp. 656-666.

¢ Kusner, M.J., Loftus, J., Russell, C. and Silva, R. (2017). Counterfactual fairness. In Advances in Neural Information Processing
Systems, pp. 4066-4076.

¢ Pleiss, G., et al. On Fairness and Calibration. In Guyon, |., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,
and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30 , pp. 5684-5693.

MITD-Lab ‘a, 4
ORE FROM THE AMERICAN PEOPLE

 

Fairness at the individual or group level

False

positives Unfair for the Low precision, unfair
individuals: for individuals,

High some favored possibly fair for the

without merit group

 
   
  
 
    
  

 
 
 

  

Unfair for
individuals: some
disfavored while
having merit

Low

 
 

 

Low High False
negatives

Se, mm,
_ a _ =o II ;
SaaS FROM THE AMERICAN PEOPLE

Base Case: Fairness Through Unawareness

¢ One approach Is fairness-through-unawareness (Kusner et al, 2017; Chen et al, 2019),

which leaves out of the model protected social attributes Such as gender, race, and other
characteristics deemed sensitive

¢ |The fairness-through-unawareness approach conceptually parallels the “color-blind”™

Strategy for promoting cultural diversity at work (Apfelbaum et al, 2010).

MITD-Lab USAID KK

 

Equalized Odds

¢ Equalizing tne odds = matching the True Positive Rate and False Positive Rate for different
values of the protected attribute (Hardt et al, 2016)
p(Y|A=0,Y=y)=p(YJAA=1LY=y), ye {0,1}

¢ [his is hard to do but if achieved is one of the highest levels of algorithmic fairness

- If you'd like to learn more, see Hardt et al, 2016: Pleiss et al, 2017; Kilbertus et al, 2017.

 

 

Acknowledgements

¢ This is joint work with Professors Lily Morse and Gerald Kane (Boston College) and Yazeed
Awwad (Research Assistant, MIT D-Lab).

¢ The authors thank USAID-MIT Grant AID-OAA-A-12-O0095 “Appropriate Use of Machine
Learning in Developing Country Contexts” and the Carroll School of Management at Boston
College for research funding. | thank Research Assistant Mariana Paredes (BC), Dr. Daniel
Brown (HBS), Dr. Daniel Frey (MIT), Kendra Leith (MIT D-Lab), Nancy Adams (MIT D-Lab), Dr.
Aubra Anthony (USAID), Dr. Shachee Doshi (USAID), Dr. Craig Jolley (USAID), Dr. Amy Paul
(USAID), Dr. Rich Fletcher (MIT D-Lab), Dr. Maryam Najatian (MIT CSAIL), Amit Gandhi (MIT D-
Lab), Lauren McKown (MIT CITE), and Dr. Sam Ransbotham (BC) for help with this work.

MIT OpenCourseWare
https://ocw.mit.edu

RES.EC-001 Exploring Fairness in Machine Learning
Spring 2019

For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms.

 

Demographic Parity

¢ Problems with demographic parity: what if we have people who are members of multiple
protected groups?

¢ While enforcing group level fairness (say, same hiring rate for females and males), this can
be unfair to the individual: it could force the algorithm to drop otherwise qualified individuals

just to achieve independence of outcome with the attribute.

¢ Furthermore, there could be differences in qualifications across a non-protected attribute,
Say empathy, programming skills, communication skills, analytics, which would be washed

off by forcing equality of probability of hire at the group level.

 

 

Fairness Criteria

Exploring Fairness in Machine Learning

Mike Teodorescu
Assistant Professor of Information Systems, Boston College
Visiting Scholar, MIT

 

Gre,

MITD-Lab <=)USAID
a == i
= a #&

a RES FROM THE AMERICAN PEOPLE

Potentially Sensitive Attributes in Machine Learning

Some countries have laws that protect specific groups of people from discrimination based
on certain individual attributes (often referred to as ‘protected attributes’), such as:

° race;

‘religion;

¢ national origin:

¢ gender:

¢ marital status:

© age:
* socioeconomic status.

MITD-Lab USAID Bi

 

 

Thank you

Mike Teodorescu
Assistant Professor of Information Systems, Boston College

Visiting Scholar, MIT

MIT USAID K&
ee” FROM THE AMERICAN PEOPLE

hmteodor@mit.edu

 

Confusion Matrix

TP+TN

Accuracy = ————_————__
¥~ TP+FP+TN+FN

The four cells are:

- TP = true positive (Correctly classified as Positive)

- TN =true negative (Correctly classified as Negative)
- FP = false positive (Incorrectly classified as Positive)

- FN = false negative (Incorrectly classified as Negative)

 

Ge, mn
USAID
a == ipl -
_ a % oom £
Qe” FROM THE AMERICAN PEOPLE

 

Confusion Matrix

Predicted

Negative Positive

True False
Negative Positive
(TN) (FP)

Negative

Actual

False True
Negative Positive
(FN) (TP)

Positive

 

 

MITD-Lab @ 3
ORE FROM THE AMERICAN PEOPLE

 

Review Questions

¢ What is “demographic parity”?

¢ What is “fairness through unawareness’ °

- Is fairness at the group level always the best?
« What is the ‘confusion matrix’?

¢ What is the “equality of odds’ criterion’

 
