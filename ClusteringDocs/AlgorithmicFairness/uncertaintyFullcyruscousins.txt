  

 

   

oe ee TTT Ty TOO oo

   

—— Vinu+ 2 verte Vinu
Vite

Ld = Ftinu 1
\/ gra
Fae Tene
V Anu

18+
1.6} : 4

14

 

      
       

      

 

    

 

r 1.5
L — Vinu+ ¥2uerfe ViInu
a2 a — Vinu +fuerfeVInu
r Vitnu 7} fF t+inu
1 Vi+inu 1 me Vi+inu
E seas esSeseee eee Po iit Diam 1
9 4 8 12 16 20 24 28 32 10° 101 10? 10% 10° 10? 10? 108 10*

(a) Natural axes plot. Note that the shaded (b) Semilog plot to emphasize asymptotic (c) Semilog plot, wherein each function is di-
region between the lower and upper bounds behavior. Note that even with a logarithmic vided by the lower bound /= + Inu to em-
is difficult to see without scaling, and its a-axis, the absolute gap between the lower phasize the decaying relative gap between
height is maximized at u = 1. and upper bounds remains quite small. lower and upper bounds.

Figure Al: Plots of various scalings of Vinu + ¥Zuerfe Vu and lower_and upper bounds thereof (y-axis), i.e.,
Vz +Inu < Vinu+ ¥ZuerfeVinu < /I+Inu, see proof of corollary against u > 1 (a-axis). The region
sandwiched by the lower and upper bounds is shaded in purple, is quite small for all wu > 1, and converges to 0 both
additively and multiplicatively.

 

M(.;w), it holds with probability at least 1 — 6 over a, y, and é that

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

M i OV/E[soh] —S; —2%@;w]< Min} E [soh]|—Si|;w] <M io cAE[soh] — 5; +2éj;w)], &
a Bi Yi, a
(14)
‘TRUE REGRET MALFARE LB PLuG-IN REGRET MALFARE ‘TRUE RecRe? MALPARE UB
Mis Ov] E [soA]—S|—2é;w}] < Mis |E[soh]—S3|;w] <Mlirncal E [soh]—S) + 2éi:;w] ,
Bi Vi, Di Bis Yi,r
(15)
LCB Estimate TRUE REGRET MALFARE UCB Estimate
thus if M(-; w) is \-Lipschitz-continuous w.r.t. some norm ||-|| 4, we have
M[{is| E [soh]-S|;w]-—M[in|E[soh] —Sj];w]| <2d]lélly - (16)
Bi Yiys Di
PLuG-IN REGRET MALFARE ‘TRUE REGRET MALFARE

Proof. This result follows essentially the same reasoning as theorem [3.4] The salient difference here is that we now bound
both the estimation error of h and each hj, which when summed yield the novel 2-factors. In particular, we have that

with probability at least 1 — 6, it holds for each i € Z and h € H (simultaneously) that

Elsoh]—S;

 

 

= (8, oh] — s) + (z [soh] — s:) - (8, [soh] — s) ALGEBRA

 

 

 

 

 

 

<| E fsoh]—S | +/E[soh]—- E [sod] +|Si -§; TRIANGLE INEQUALITY
Bis Yi, Di Ri Yi,

<| E [soh]-&)+é@:+é =| E [son]—S} +28, . DEFINITION OF &
Bi Yi, i Vi:

 

 

 

 

This result, paired with the assumed monotonicity of M(-;w), yields and (1
Lipschitz continuity yields (

 

and then applying the definition o:

 

 

 

We now show property [3.3] which approximates the reduction to the UCB resultant from drawing a single additional

26

URL

Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian Data Analysis. Chapman and Hall, second edition,
2004.

William M Gorman. The structure of utility functions. The Review of Economic Studies, 35(4):367-390, 1968.

Hoda Heidari, Claudio Ferrari, Krishna Gummadi, and Andreas Krause. Fairness behind a veil of ignorance: A welfare analysis for
automated decision making. In Advances in Neural Information Processing Systems, pages 1265-1276, 2018.

Hoda Heidari, Solon Barocas, Jon M Kleinberg, and Karen Levy. On modeling human perceptions of allocation policies with
uncertain outcomes. In EC ’21: The 22nd ACM Conference on Economics and Computation, Budapest, Hungary, July 18-23,
2021, pages 589-609. ACM, 2021.

Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association,
58(301):13-30, 1963.

Lily Hu and Yiling Chen. Fair classification and social welfare. In Proceedings of the 2020 Conference on Fairness, Accountability,
and Transparency, pages 535-545, 2020.

Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers?
In International Conference on Machine Learning, pages 2029-2037. PMLR, 2018.

George H John and Pat Langley. Static versus dynamic sampling for data mining. In Proceedings of the Second International
Conference on Knowledge Discovery and Data Mining, pages 367-370, 1996.

Rucha Kulkarni, Ruta Mehta, and Setareh Taki. Indivisible mixed manna: On the computability of MMS + PO allocations. In
Proceedings of the 22nd ACM Conference on Economics and Computation, pages 683-684, 2021.

Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without
demographics through adversarially reweighted learning. Advances in neural information processing systems, 33:728-740, 2020.

Jean-Samuel Leboeuf, Frédéric LeBlanc, and Mario Marchand. Decision trees as partitioning machines to characterize their
generalization properties. Advances in Neural Information Processing Systems, 33, 2020.

Gabor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. Foundations of
Computational Mathematics, 19(5):1145~-1190, 2019.

Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax Pareto fairness: A multi objective perspective. In International
Conference on Machine Learning, pages 6755-6764. PMLR, 2020.

Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample-variance penalization. In International Conference
on Computational Learning Theory. Springer, 2009.

John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the Royal Statistical Society: Series A
(General), 135(3):370-384, 1972.

Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust language modeling. arXiv preprint
arXiv:1909.02060, 2019.

Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathematical statistics, 33(3):1065-1076,
1962.

Arthur Cecil Pigou. Wealth and welfare. Macmillan and Company, limited, 1912.

Foster Provost, David Jensen, and Tim Oates. Efficient progressive sampling. In Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 23-32, 1999.

Kevin WS Roberts. Interpersonal comparability and social choice theory. The Review of Economic Studies, pages 421-439, 1980.

Esther Rolf, Max Simchowitz, Sarah Dean, Lydia T Liu, Daniel Bjérkegren, Moritz Hardt, and Joshua Blumenstock. Balancing
competing objectives with noisy data: Score-based classifiers for welfare-aware machine learning. In Proceedings of the 37th
International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of
Machine Learning Research, pages 8158-8168. PMLR, 2020.

Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics, pages
832-837, 1956.

Guy N Rothblum and Gal Yona. Multi-group agnostic PAC learnability. In Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages
9107-9115. PMLR, 2021.

Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International
Conference on Learning Representations, 2019.

Amartya Sen. On weights and measures: Informational constraints in social welfare analysis. Econometrica: Journal of the
Econometric Society, pages 1539-1572, 1977.

Mohammad Shahrokh Esfahani and Edward R Dougherty. Effect of separate sampling on classification accuracy. Bioinformatics, 30
(2):242-250, 2014.

20

MIXTURE SAMPLER.
(E12) €(*¥ xx Z)

   
     
 

       
   

  

JOINT SAMPLER

(BLM) (vx 9

@ (b) Mixture sampling model (c) Mixture sampling model (d) Conditional sampling model.

(mutual exclusivity). Cheap (combinatorial). Samples con- Herein LEARNER selects among
(a) Joint sampling model. Ex- samples contain data for a sin- tain data for a random (or ad- (variably-priced) groups, and
pensive samples each contain gle random (or adversarially se-_ versarially selected) nonempty then SAMPLER draws samples
data for all groups. lected) group. combination of groups. from the selected group.

 

Figure 2: Visualization of sampling models for group-centric fair machine learning. The data collected, its topology, and
which or how decisions are made are described for each sampling model. Note that the mixture sampling model is split
into the simpler mutually exclusive case (figure , wherein each sample pertains to a single group z € Z, as well as the
combinatorial case (figure , in which each sample pertains to some subset z € 2” of the set of all groups Z.

 

 

that considering only egalitarian regret may act as an enforcer of the status quo, if one group is particularly happy with
their hj and is thus aggrieved by any compromise — perhaps best summarized by the adage, “To those accustomed
to privilege, equality feels like oppression.” We mitigate this issue by summarizing regret with a power-mean malfare
function M,(-;w), instead of the egalitarian malfare, in order to lessen the impact of the most aggrieved group. In
particular, this class smoothly and nonlinearly interpolates between the worst-case (egalitarian) M\.o(-;w) regret and the
utilitarian M;(-;w) welfare or malfare.

Fascinatingly, we find that utilitarian regret minimization reduces to utilitarian malfare or welfare optimization, as all
terms involving per-group optimal sentiment can be factored into an additive constant from these objectives; observe

: : i — “ s= Mi(Ep, [0 h];w) —w-S*
$8 Rem(i) <8: (i-] gt60n si) = Sw Bbonl) ~w-s'| = s (Ep, [60 h}:-w) —w-S* (5)

i i s=u w-S*—Wi(Ep,[uch|;w) ,
namely S* appears only in the additive constant w-S*, which is independent of h. From this perspective, we conclude that
while the utilitarian regret is not particularly interesting, the power-mean malfare of regret interpolates between minimizing
largest regret, with its minority rule issues, and optimizing utilitarian welfare or malfare, where w parameterizes the

utilitarian objective, and p precisely specifies how the objective trades off between the two extremes.

2.2 Three Sampling Models for Populations with Multiple Groups

In order to study efficient sampling, we must first quantify the cost of a sampling-based estimation routine, which requires
a sampling model. Within a single-group population, methods like i.i.d. sampling, importance sampling, or sampling
without replacement are near-ubiquitous, and all can measure cost as sample size m € Z,, where Z, denotes the positive
integers; however, in group-sensitive settings, we must consider how samples from different groups are obtained, and what
the cost of collecting these samples is. In the context of this work, we don’t argue for a one-size-fits-all solution, but
rather we discuss three sampling models, and show that they fit key applications in the computer science domain and
beyond.

1) Joint Sampling: Each i.i.d. sample contains a piece of information for each of the g groups, with arbitrary dependencies
between groups. For example, per-group representatives could be shown a shared x € ¥ and asked for their feedback,
which would then be used to establish some J; for each group i. Thus each sample is in the space ¥ x Y% if the
&X components are shared between groups, or more generally in (¥ x Y)%. This setting also arises in multi-objective
reinforcement learning '2|, as well as various bandit problems and empirical
game theoretic analysis [Viqueira ct al] , where each query of an action or strategy profile yields a sample of the

utility values of each player, agent, or group.

  
 
 

 

2) Mixture Sampling: For each sample, the data are only relevant to a nonempty subset of groups z € 2*, thus samples

reduction of LINEARPSUTILITY(...), and similarly we can provide guarantees for function estimation via the logic of

LINEARPSESTIMATE(...).

Theorem 4.6 (Braided PS Guarantees). Suppose (h, jt, é, M**) — BrawepPSLoss(H, &(-,-),D, AES(...),,,¢,M(-;w), REG),
M(S;w) is continuous and strictly monotonic in S with (possibly infinite) Lipschitz constant Am w.r.t. ||-||,,, and the
schedules (s, 6) are ~7q$q,7j-uniformly-convergent w.r.t. ||:||,, and the additive error vector bound AEV(m, 6, x,y) —
(AES(m1, & +€1,Y1),-.-, AES(my, 5, @g,Yy))- Now take p to be the true objective value of h and * to be the true
objective value of the optimal h* (see theorem Then, with probability at least 1 — 6, we have

1) la-ulsése; & 2) MN <p <p Sates M 42.

 

5 Conclusion

This work generalizes existing theories of fair machine learning, with welfare, malfare, and regret objectives, thus
subsuming the minimaz fair learning [Martinez et al. Abernethy et al. } 2021] ahoti et al.
Shekhar et al. , multi-group agnostic PAC learning [Blum and ister 2020) |Rothblum and Yona| 2021], and

fair-PAC learning 1) settings, while enjoying rigorous statistical learning guarantees and the axiomatization

of cardinal welfare theory. In particular, we bound the generalization error and sample complexity of UCB-optimal

       

 

   

models, either given a fixed sample, or to meet a user-supplied ¢-d optimality guarantee via progressive sampling. Our
bounds leverage the specific character of the objective at hand, and our progressive sampling methods are tailored to three
realistic models of data generation. We stress that while training UCB-optimal models is analytically convenient, there is
also an important fairness impact to this decision, as fair malfare functions (e.g., egalitarian) place strong emphasis on

the most disadvantaged groups, which are often understudied minority groups notes that optimizing

 

empirical malfare M overfits to small numbers of sampled minorities, however we argue that training UCB-optimal models
(i.e., optimizing M"*) factors uncertainty into training, so that the needs of understudied groups (i.e., those with large €;
values) are better addressed.

Our active learning setting under the conditional sampling model is philosophically intriguing, as we find that optimally
investing sampling effort under uncertainty is challenging, depends on the objective at hand, and has important fairness

impact. In section we see that a host of factors involving the objective, function class H, and per-group distributions

 

Dig all interact to determine the sharpness of welfare, malfare, and regret bounds, and property [3.8] quantifies the
incremental UCB improvement of sampling each group. This analysis answers questions raised by
how sampling-error impacts fairness, and generalizes the analysis of|Shekhar et al

to arbitrary power-mean malfare functions. Algorithm [2] then incorporates these ideas into an active sampling algorithm,

  

as to

 

from the egalitarian special-case

 

which dynamically select groups to sample based on projected UCB improvement. Notably, algorithm [I] does use uniform
sample sizes under the joint sampling model, and uses whatever is available under the mixture sampling model, as these
are natural choices under these sampling models. In contrast, under the conditional sampling model, algorithm [2] is able
to make more intelligent decisions as to where to allocate sampling effort.

We thus conclude that (welfare-centric) fairness, statistical uncertainty, and sample complexity analysis are tightly
intertwined, and must all be considered to best allocate resources in service of the social planner. We are hopeful that
this analysis and algorithmic study will lead to a greater emphasis on sample complexity and finite sample analysis for
the social planner’s problem, which is traditionally analyzed in terms of the asymptotic Bayesian methods of classical
economics. In particular, we are hopeful that this analysis emphasizes and mathematically supports the call for greater

visibility of minority groups and the importance of incorporating diverse data into (fair) machine learning systems.

Acknowledgments

This work was supported in part by NSF award RI-1813444 and DARPA/AFRL grant FA8750.

18

 

Algorithm 2 Fair Learning with Braided Progressive Sampling under the Conditional Sampling Model

1: procedure BRAIDEDPSLoss(H, ¢(-,-), Pizg; C1:g, AES(..-), 8,6,€, M(-; w), REG) + (h, fi, é, M*)

2: input: Hypothesis class H, loss function é(-,-), per-group distributions D,,,, cost model Cj., € R{, additive error scalar bound
AES(m, 6, a, y), schedule s € Z¥ and 6 € [0,1)™, confidence radius ¢, weighted malfare M(-; w), and Boolean REG

 

 

 

3: output: Empirically UCB-optimal h, empirical malfare estimate ji, confidence radius é, and lower bound on minimal malfare M**
4: tig —1 > Initialize per-group timestep indices
5: WEE Zt (Wires, Yitser) ~ Di; i — (1+ Inec) AES(s1, 2, @i,:, yi) > Draw initial sample for all groups & bound error
6: loop ng > Loop over braided algorithm iterations
tm Vig €ZtEZ,: EO | é if ifj else én] so } > Estimate of é; after sampling group i for t more iterations
A aay
s WeZ: &e (int E [£0 i) if ReG else 0 > Set regret baseline of per-group minimal empirical risks (or 0 if ~REG)
hEH wi: Yi,
9: he argmin Mire cA E [@oh]—Si+ éi;w) > Compute UCB-optimal h
hen Bi,
to: = MS + inf Mi OV E [Coh]—S,-&; w) > Lower-bound optimal MA\*
hen Bi Yi

ne (AV) (nc HOV E oh|-S-é;w),M(incA E oh]-Si+ és w)) > LCB and UCB on h (regret) malfare

 

Bindi, Bi sYi,
a: if AY < M*! + 2¢ then > Check if desired error guarantee is met (termination condition)
13: (ae (2° +i"), 2c" - A) > Symmetric estimate of ji of malfare or regret of h
return (h, ji,é, M**)
end if 1 ace ss z
i <argmax sup [7 (av — Mi recA E [€oh]—Si+ é');w)) > Maximize improvement:cost ratio
i€Z teZy Ci(Se4t; ae St;) Bi VE,
ee ,

RECIPROCAL Cost EsTIMATED (REGRE’ [ALFARE IMPROVEMENT

214, 8

17: (Virtaryserpe, Ysltacy aise, ) ~ DEH; tH ti +1; Ei & (1+ Inne) AES(8e,,
1s: end loop

19: end procedure

 

5t,

3 Bi, Yin) > Sample group i & bound error

 

 

M(S; w) is continuous and monotonic in S with (possibly infinite) Lipschitz constant Am w.r.t. ||-||,;, and the schedules
(8,6) are saw N
h and y* to be the true objective value of the optimal h’, i.e., if REG = FALSE, take p = M(i +> Ep,[¢0 hj; w)
b= M(i +> Reg;(h);w) and
yw = infnen M (i+ Reg;,(h);w). Then, with probability at least 1 — 6, the output (h, ,é,M**) obeys

1) |a-pl<é<e; & 2)M* <p Sp ptéesM" 42.

uniformly-convergent w.r.t. AEV(...) and ||-||,,. Now take y to be the true objective value of

  

and pw” = infpey M(i +> Ep,[€0 h];w), or if Rec = TRUE, take (see equation |:

 

 

 

4.3. The Braided Progressive Sampling Algorithm

Under the joint and mixture sampling models (algorithm progress is linear (i.e., sequential, as no decisions are made

 

except when to terminate); we begin with (at least) s; samples per group, and advance until we reach a sufficient sample
size to terminate with the desired guarantee. For the conditional sampling model, we present braided progressive sampling

(algorithm which is actively making decisions, thus linear analysis is not applicable. At each iteration (line [6) of

 

algorithm [2 a group index i € Z is chosen (line

 

to optimize an estimate of knowledge-gain via logic similar to

that of section [3-3] (due to space limitations, the details are deferred to appendix|B.2), and group i is sampled for one

 

additional timestep (line , ie., the sample associated with group 7 is extended from size sz, to 814¢,, where t; denotes

 

the current timestep for group 7. The remainder of algorithm P]is essentially the same as algorithm [I] after sampling, we
optimize (line[9) a UCB-optimal h, bound the objective (lines [LO}f11), and terminate if the user supplied guarantee is
met, otherwise we continue.

There is thus a lattice of possible sample size vectors m, i.e., the possibilities are the Cartesian product s x --- x s.
To avoid a union bound over this (exponentially large) lattice, we analyze the method as a braid, in that g progressive
sampling sequences are concurrently active, and at each iteration we select some group i, and advance the schedule

by one timestep for only group i (thus we have g independent strands, advancing and intertwining in some random
ot;

order). Consequently, we must use (line the additive error scalar bound €; <— AES(mi, : Pa Bas), i.e., we operate

 

 

on one group at a time, rather than over all groups as in the linear algorithm (algorithm [I] line . Similar analysis

 

is employed for multi-armed bandits, where a union bound is taken over all timesteps and each arm being sampled.

With algorithm 2] explained, we now show a correctness result, analogous to theorem for the linear algorithm. Note

 

that, as with algorithm [I] we can generalize algorithm [2] and its guarantees to utility and welfare functions, using the

17

A Proof Appendix

  

We now present proofs of all mathematical claims in the paper body. We first show in appendix he estimation

guarantees, sample complexity, and incremental-sampling knowledge-gain bounds of section ‘We then derive in

 
 

the results of section [3] i.e., those related to sampling schedules and progressive sampling.

A.1 Statistical Estimation Guarantees for Malfare, Welfare, and Regret

In this section, we show all bounds related to uniform convergence of sentiment values, and estimating welfare, malfare,
and regret. We begin with theorems[3.1]and [3.2] and property 3.3} which bound single-group uniform convergence rates in

the form of additive error scalar bounds, i.e., AES(...), under various conditions.

Theorem 3.1 (Uniform Convergence for Bounded Finite Hypothesis Classes). We may bound the distribution-free
AES(m, 6), the distribution-dependent AESp(m, 6), and the data-dependent AES(m, 6, x, y) scalar additive error as

Bennett ;&
2Ve,y[s oh] In 2H
Mews ili Cousins and Riondato

(m-1)
Proof. First note that items [I] and 2] hold via union bound over the lower and upper tails of the sentiment of each

h € H, hence the In ae! term, and item

2] AHI
23r ina
m

AH!
vee rin=3 sup
3m hen Wi

H\|+1
gee Cs ao -f:SUB,
3(m — 1) hen

lbet

  
 
   

  
    

 

 

 

respectively.

 

  
 
 

as a union bound over one additional tail, which is used to bound the

olds via Hoeffding’s |Hoeffding

inequality (technically the sub-gamma form of Bennett’s sub-Poisson bound, a.k.a. Bernstein’s inequality),

and item [3] holds via the supremum variance bound of|Cousins and Riondato}{2020| theorem 2], followed by Bennett’s

inequality. Note that here VJ denotes the unbiased (Bessel-corrected) empirical variance, though the empirical supremum

supremum variance. In particular, item

 

nequality, and item 2] via Bennett’s

 

variance is still an upward-biased estimate of the supremum variance. O

 

 

Theorem 3.2 (Uniform Convergence with Rademacher Averages). Suppose hypothesis class H and sentiment function

 

 

s(-,:), take (x,y) ~ D™ and o ~U"™ (+1), ie., o is uniformly distributed on (+1), and define the Rademacher average

Rin(soH,D) and Bousquet variance proxy Win(s oH, D) [see [Bousquet|

 

as

Rn(soH,D) = E | lee

 

25's ° nies)o, >» Vnl(soH,D) = sup V[soh] + 4rB®in(soH,D) . (7)
mi heHP

 

ay, t ; T
We may then bound AESp(m, 65) as € + 2Rim(soH,D) +" Ing + (Pe .

3m m

Proof. This result follows via the Rademacher symmetrization inequality 2013} lemma 11.4], which

upper-bounds the expected supremum deviation as twice the Rademacher average, followed by Bousquet’s boun:

Property 3.3 (Practical Bounds on Rademacher Averages). 1) Suppose H has Vapnik-Chervonenkis (VC) dimension

d eae fe ae 71), and £(g,y) = 1—1,(%) is the 0-1 loss. Then for some absolute constant c,
Rmn(CoH,D) < \/, which implies bounds for linear classifiers, bounded-depth decision trees [2020],
and many classes of neural network 2 is
2) Suppose VY = {# € R® | |||, < R} is the R-radius £2 ball in R“, H = {#4 W- Z| ||w||, < y} is a y-regularized linear
class, Y = [—Ry, Ry], and ¢(-,-) is a A-Lipschitz loss function s.t. €(y,y) = 0. Then r < 2\Ry and Rn(€oH,D) < BE.

ym
This implies bounds for (kernelized) SVM, generalized linear models |Nelder and Wedderburn|

, and bounded linear
regression.

 

 

on the supremum deviation, i.e., on the quantity AESp(...). O

 

 

 

 

 

 

22

Proof. This result is a collection of standard results in statistical learning theory, most of which are fully cited in the
property statement. We now carry out our bibliographic duties with regards to the remaining results. These bounds are
often stated for the empirical Rademacher average, Rn(C oH, (a, y)) which conditions on the sample (a, y), but we state

bounds that hold over any possible distribution by considering worst-case realizations of these samples.

We first show item [I] The bound &,,(H,D) < 4/ el follows via Dudley’s chaining or entropy integral arguments,

along with Haussler’s bound on the £2 covering number of Vapnik-Chervonenkis classes, see, e.g., Boucheron et al.||2!

 

theorem 13.7]. The specific classes mentioned in the theorem statement are of bounded VC dimension, hence their

inclusion.

We now show item First note that each of the cited model classes is Lipschitz-continuous, so their bounds follow
from the general statement. Now, note that the range of H is [—Ry, Ry], which follows directly from the Cauchy-Schwarz
inequality. Composition with ¢(-,y), for y € [—Ry, Ry], then maps this range to a subset of [0,2\Ry], by Lipschitz
continuity and nonnegativity of ¢(-,-), and the fact that ¢(y, y) = 0, hence we conclude r < 2ARy.

We now bound the Rademacher average. In particular, |Shalev-Shwartz and Ben-David) lemma 26.10] show that

Rin(H, Dix) < ae, where D)x denotes the marginalization of the label space Y from the instance distribution D, i.e.,

 

they bound the Rademacher average of the linear hypothesis class on the unlabeled distribution over Y. Technically, their
definition of Rademacher average contains no absolute value inside the supremum, but this is immaterial, as [Cousins]
2020} lemma 5] show that the same bound holds with or without the absolute value. We then apply the

Ledoux-Talagrand contraction principld’] lemma 11.6] to compose the hypothesis class with a

A-Lipschitz loss function, which yields Rn(¢0H,D) < 2\®m(H, Dix) < ea oO

 

We now show theorem: and

 

5|and corollary which bound tails and expectations of malfare and welfare

   

values in terms of additive error vector bounds, i.e., in terms of AEV(...).

Theorem 3.4 (Welfare and Malfare Tail Bounds). Suppose sentiment function s(-,-): Y’ x Y — Ro+, per-group
probability distributions D1:g, sample size vector m € Z{, samples (w,y) ~ Df”! x «++ x Dj’, failure probability
6 € (0,1), and additive error bound AEV(...), and let € + AEV(m,6,x,y). Then for all h € H and all monotonic
aggregator functions M(-; w), it holds with probability at least 1— 6 over a, y, and é that

 

 

M (i HOV Elsohl = aw) <M (: ed ony, fone) <M (: ad eA Elsohl +éiw) , & (9)
TRUE LB PLUGIN EXTMATE M TRUE UB
M (: HOV 2,8 oh] — i; w) <M (i 4 Els oh]; w) <M ( Hen oy. [soh] + &; w) ; (10)
LCB Estimare M+ TRUE A CGREGATESM UCB Esrimare Nit
thus if M(-; w) is \-Lipschitz-continuous w.r.t. some norm ||-||,,, we have
M (: 4 a oni) —-M ¢ 4 Ebonhw) <Alléllar - (11)

 

PLuc-IN EsTIMATE

Proof. We first note that, by the definition of AEV(...), with probability at least 1 — 6, for all h € H and groups i € Z,
it holds that |Ep, [so h] — S3| < é;. Given this, note that ® and follow directly from the monotonicity assumption

on M(-;w). Similarly, ( follows from the monotonicity assumption and the definition of Lipschitz-continuity.

 

 

 

 

Theorem 3.5 (Welfare and Malfare Expectation Bounds). Suppose as in theorem and assume also that AEV(m, 6, x, y) =

 

AEV(m, 6) is a deterministic distribution-free or distribution-dependent (but not data-dependent) bound. Then

|M-E[M]| <E[IM—MI] < Af JABV(m, HII y dd

SIt is of course possible to remove the leading 2 factor, if we are willing to work with the Rademacher average definition without the
absolute value.

 

23

show tail bounds for the estimation of welfare and malfare in terms of their plug-in, LCB, and UCB estimates, and we
then bound the bias of M. We note that this approach may seem backwards, as often tail bounds are given in terms of
expectations. In this setting, due to the bias of each estimator, we employ tail-bound integration methods to bound

expectations, hence the primary position of tail bounds.

Theorem 3.4 (Welfare and Malfare Tail Bounds). Suppose sentiment function s(-,-): Y’ x Y — Ro+, per-group
probability distributions D1.,, sample size vector m € Z4, samples (x,y) ~ D7”! x --- x Dj”, failure probability
6 € (0,1), and additive error bound AEV(...), and let € < AEV(m,6,x,y). Then for all h € H and all monotonic
aggregator functions M(-; w), it holds with probability at least 1— 6 over a, y, and é that

 

 

M (i HOV Elsohl = ecw) <M (: ed ony, fone) <M (i ad eA Elsohl +é0w) , & (9)
TRUE LB PLUGIN ESTATE M TRUE UB
M (: HOV «8,6 oh] —éi; w) <M (i = Eb oh; w) <M ( Hel one, [soh] +6; w) : (10)
LCB Estimare M+ TRUE A CGREGATESM UCB Esrimare Nit
thus if M(-; w) is )-Lipschitz-continuoug§] w.r-t. some norm ||-||,,;, we have
M (: Fa E [so hive) -M (i 4 Eb ° nw) S<AMElly (11)
VS

Piucin WsTiare TRUE AGorecare

we see that minimizing M' (or maximizing M") is in some sense a safe choice, as w.h.p. we can bound

 

the true aggregate value in terms of the UCB or LCB. This idea is reminiscent of the sample variance penalization

algorithm of |Maurer and Pontil

with variance-dependent bounds, but here the bound depends on the structure of the malfare or welfare objective at hand.

 

, Wherein ERM is supplanted by minimizing an upper-bound on risk; in that case

It should also be noted that while the final Lipschitz form is concise and convenient for all Lipschitz-continuous

aggregator functions (e.g., all p > 1 power-mean malfare functions, see theorem [2.3] item 3p, it can be quite loose. For

 

 

 

 

 

example, under + uncertainty intervals, the egalitarian welfare W—..((4+1,9+8); w)=min(4+1,9+8) must be on the

 

interval 3

  
 

is convenient for intuition and analysis, when

From Tail Bounds to Expectations While theorem [3.4] gives high-probability bounds on the gap between empirical

 

and true welfare or malfare, it does not actually bound the expectation (and thus the statistical bias) of the plug-in

estimator. Unlike many simple large deviation bounds, the expectation of the plug-in estimator M does not even appear

 

in the theorem. Nevertheless, we now bound the integral over a worst-case distribution of possibilities for both the lower

and upper confidence bound estimators. This bounds the bias of the plug-in estimator, and then corollar; derives a

 

particularly convenient form for these bounds using Bernstein-type variance-sensitive bounds.

Theorem 3.5 (Welfare and Malfare Expectation Bounds). Suppose as in theorem[3.4] and assume also that AEV(m, 6, x, y) =
AEV(m, 6) is a deterministic distribution-free or distribution-dependent (but not data-dependent) bound. Then

a a 1,
| M—E[M| < E[IM- Ml] < Af |AEV(m,6)|], 46

The above theorems give general recipes for bounding tails and expectations, so for demonstrative purposes, we

instantiate them with theorem [3.1] for malfare estimation. Similar bounds can be derived for learning with theorem

 

Corollary 3.6 (Bernstein-Type Malfare Bounds). Suppose as in theorem and also per-group sample size m

 

(Le., m = (m,...,m)) and p > 1 power-mean malfare function M,(-;w). Now, let variance proxy v be defined
in three cases as v = Mij2(v;w) = (SL, wii)” for p = 1, v = w-v for p € (1,2], or v = |lul|,, for p > 2.
Then for all 6 € (0,1), we have

 

*In other words, if for all S,S’, it holds that |M(S;w) — M(S’;w)| < Al|S — 8’ ||).

10

 

      
  
  
 
  

 

   

 

 

 

I—— N(0,1),mi = 16
----N(0, 1), m1 +1=17
a N(1,1),m2=4
i H--- N(1,1),m2+1=5
vo
5 Wu 20 lower tails
2 a
3
3
a
°
2
a
-1

 

Uncertainty over Per-Group Risk Values

Figure 3: In this example, rather than using the general tail bounds of section we use a simple frequentist® Gaussian
analysis, where loss values for each D; are distributed M(1:, 1), for unknown means pt = (1,2). Here we obtain ~ 95%
confidence intervals with 20 tails, and plot the lower tails of groups 1 and 2, of sizes m = (16,4). Letting A; denote
the improvement to bound radius for group 7, and assuming equal sampling cost C; = C2, we see that the reduction in
uncertainty for the unweighted utilitarian malfare objective is optimized by sampling group 2, with a reduction of |
and the reduction for the egalitarian objective is optimized by sampling group 1, with a reduction of Ao.

© We can of course relax the assumption of known variance via confidence estimation through the Student’s t-distribution. Note also that

similar analysis is possible in a Bayesian framework, e.g., if we can assume a Gaussian conjugate prior [Di

    

Note that all such analysis is necessarily heuristic, as we fundamentally cannot answer this question without more
information: it is precisely because we are trying to estimate unknown means that we can’t know how the samples we
draw will impact the empirical means. For now, we heuristically assume that our estimated expectations are reasonably
accurate, and consider what will happen as tail bounds sharpen with additional samples. The strategy we thus employ is
to make a reasonable guess as to how sampling might impact the UCB by assuming that the empirical mean will not be

strongly affected, and all confidence intervals over m samples will contract at a @,/+ rate.

Property 3.8 (Incremental Gain of Sampling). Suppose w-weighted power-mean malfare M,(-;w), sample (x, y) with

 

group sample sizes 1.9, and let x’, y’ extend x, y to sample sizes m’, where m’ = m +1), i.e., group i has one additional

sample. Now, let é + AEV(m, 6, #,y) and é — AEV(m’',6,2',y’), and take h = argmin,, <7, Mp (i + Es, [00h] + é5 w),
M=M, (i ad Eo, ous [Co hj; w), AN =M, (i ad Ex,..yi le oh]+éi; w), and AM = infnewM, (i Dad Ey ut (oh|+é; w).

Then the incremental impact of sampling from group i on the UCB is approximately

Ato At Eiwi Ex, bYi, [oh] +&  SiWi Ex, Yi, [oh] mo
~ Imi t+? na ~ Imi M

 

(17)

Observe that characterizes the knowledge gain of sampling from group i. This gain is proportional to the current
bound radius é;, the group weight w;, and the (p— 1)th power of the ratio of the UCB risk of group i to the UCB malfare,
ie., (Bass, 2a, [€0 hl + &/M), and inversely proportional to the amount of effort mi already put forth into studying

as to where sampling should occur, but it

 

group 7. These terms line up with the soft arguments at the top of section

is only via precisely studying sample complexity and estimation error that we gain quantifiable mathematical insight. In

 

particular, the weight term w; appears directly, and £+ captures both the difficulty of estimating this group, and also

m
the diminishing incremental improvement produced by further sampling. The ratio between the risk of group i and the
malfare then captures how important group 7 is relative to the other groups, and this term being raised to the (p — 1)th
power nonlinearly adjusts its impact; higher p saturate high-risk groups, tending towards egalitarianism, wherein the
most disadvantaged group becomes the most important, whereas in the p = 1 (utilitarian) case, this term is 1. Finally,
for optimization problems, the dependence on h captures other dependencies; namely the behavior of M(-;w) near the
optimal h € H is what matters.

This analysis parallels concerns in the stratified sampling regime, wherein subpopulations are sampled individually,

13

3.1 Uniform Convergence Bounds for Mean Estimation

In this work, the common functional form of our additive error (AE) bounds is data dependent uniform convergence,
vectorized to operate over samples from multiple groups, rather than on a single-group sample. Occasionally, we are
interested in the scalar form AES(m, 6, x,y): Z; x (0,1) x ¥™ x Y™ — Rox, which operates on a single group, but unless
otherwise stated, we refer to the vector bound AEV(m, 6, x, y): Z& x (0,1) x (4@™! x---x¥™)x(Y™ x---xV™) > Rox.
In particular, given a sample (x,y) ~ D7”! x --- x Dg", we require a random functior] AEV(...) such that

é¢ AEV(m,6,a2,y) => P| maxsup| E [soh]—Efsoh]|-—é>0] <6. (6)
wy é\ eZ poy [iin Di

Section explores how AEV(...) can be used to bound malfare, welfare, and regret, and the remainder of this subsection

 

is dedicated to showing non-trivial bounds of this form for machine learning applications. All of our additive error
bounds assume bounded sentiment range r = supy’cy/yey s(y’,y), but this can usually be relaxed if we instead assume
a moment condition, e.g., each s oh is sub-exponential, sub-gamma, sub-Poisson, or sub-Gaussian [Boucheron et al.|
. We often assume for the sake of intuition that uniform convergence rates exhibit convergence in probability to
AES(m, 6,...) € ©\/ini/m, which agrees with approximate bounds from many central limit theorems, though section J]

requires less restrictive convergence rates to show correctness of our estimation routines.

  

 

Data-Dependent Bounds Data-dependent uniform convergence bounds, i.e., those of the form AES(m, 6, x, y), are
invaluable for studying a population about which little is known. However, they can’t be evaluated until data are available,
thus we cannot determine a priori how much data will be required to meet a given confidence radius. This contrasts
distribution-dependent bounds, which take the form AESp(m, 6), and depend on the distribution D (but not the data
(x, y)), and therefore must make often-problematic or unrealistic assumptions about the data-distribution. Even further
in this direction are distribution-free bounds, which depend on neither the distribution nor the data, but must thus yield
worst-case dependence on the data distribution. These three classes of bounds may be related, in the sense that each
RHS can be used to bound each LHS, as

AES(m,6) << sup) AESp(m, 6) < sup AES(m,6,x,y) -
D over Xx (w.y)E(%xyyr
In section [4] when constructing schedules for progressive sampling, we often assume knowledge of AES(m, 4), but this is

usually possible via this worst-case RHS bound.

Learning and Uniform Convergence We first present simple bounds for bounded finite hypothesis classes, which

depend on the sentiment range r, hypothesis class size |?1|, variances V[-], and empirical variances V{-].

Theorem 3.1 (Uniform Convergence for Bounded Finite Hypothesis Classes). We may bound the distribution-free
AES(m, 6), the distribution-dependent AESp(m, 5), and the data-dependent AES(m, 6, x, y) scalar additive error as

2172 In AHL
1) eo 1/42
V m
H|

rin ae sup
3m heu

AH|+1
3) é— Ts | sup
3(m __ 1) hEeH

Note that supremum variances and empirical variances are properties of the distribution and sample, respectively.

     
 
  
 

2
yea m

2Ve,y[s 0h] In
(m—1)

     

 

Cousins and Riondato} {2020} ; respectively.

Dependence on variance is necessary (similar terms appear in mean-estimation lower-bounds [Devroye et al.

Lugosi and Mendelson
?Note that AEV(...) can be a randomized algorithm (e.g., involving Monte-Carlo Rademacher averages [Cousins and Riondato] [2020],
s on the data

bootstrapping, permutation testing, or other such methods), thus in general, its output € is a random variable that depen

x,y. Going forward, we present only scalar bounds, but it is to be understood that given additive error scalar bound AES(...) and a finite
group count g, we may construct the additive error vector bound AEV(m, 6, x,y) — (AES(m1, 4, Bry Y1y:)reeey AES(m,, 2, ag 1Yo,:))
via the union bound.

 

9]), however the In|H| union bound terms are loose, and the bounds are vacuous for infinite

 

 

and simplify to get

   

6(6 — 1) SEE ABOVE
84m) 2 —aory
plese ~ DEFINITION
ad(B — 1)
= ALGEBRA
B(m +1)
B-1 1
SS a >
= Bm +1) CEG

 

Now, observe that || AEV(. 2 Whar is monotonic in m and 6 by regularity condition [3] and thus, by the specified limit
condition, may be taken arbitrarily close to 0 for sufficient values of m at timestep t(m). We thus conclude that the

double-geometric schedule is 0-uniformly-convergent. O

 

 

We now show the correctness results for algorithms [I] and 2} namely theorems

 

Theorem 4.5 (Linear PS Guarantees). Suppose (h, ju, 2, M**) — LinearPSLoss(H, &(-,-),D, AEV(...), 8,6,€, M(-;w), REG),
M(S;w) is continuous and monotonic in S with (possibly infinite) Lipschitz constant Am w.r.t. ||-||,;, and the schedules
(s,6) are sa Hinw) h
h and y* to be the true objective value of the optimal h*, ie., if REG = FALSE, take p = M(i + Ep,[é0 hj; w)
= M(i +> Reg,(h); w) and

uniformly-convergent w.r.t. AEV(...) and ||-||,;. Now take pz to be the true objective value of

 

and yw” = infpex M(i ++ Ep,[é0 h];w), or if REG = TRUE, take (see equation

 

pw = infnen M (i+ Reg;(h);w). Then, with probability at least 1 — 6, the output (h, ji,é,M**) obeys
1) |A-pl<é<e; & 2) M* <p SS ptesM* +42.

   

Proof. We first show that algorithm|1]is guaranteed to eventually terminatd??] after reaching some timestep ¢. Note that
at timestep t, under the joint sampling model, AEV(...) is evaluated with sample sizes m = (s;,..., 8), and under the
mixture sampling model, sample sizes are at least that large, ie., m > (8:,...,84), which by the monotonicity regularity

on AEV(...), implies that the bounds with larger sample sizes are at least as sharp (in the worst case;

 

for most reasonable bounds, increasing sample sizes yields improvement).
ae is

By the definition of JwGtiney Ilha

accurate estimate (i.e., € is sufficiently close to 0) such that by the Lipschitz guarantees of theorem 3.4] or theorem|3.7] the

 

-uniform-convergence (definition for finite A, we eventually obtain a sufficiently

algorithm will terminate (line|19). Furthermore, for infinite A, limy—,.. € = 0, which again implies eventual termination,

 

now under only the continuity assumption. In particular, both M*+ and Mt are eventually ¢-estimated, for a total error

of < 2e, which yields termination.

To see the correctness of this result, we first observe that by union bound over t € Z,, all tail bounds of AEV(...) hold
simultaneously with probability at least 1 — }7°°, 6; = 1—6 (recall that 6 is defined as such by regularity condition 2}.
Under the joint sampling model, this union bound is a simple bound over AEV(...) evaluated at sample size vectors

(81,..., 81), (82,...,82),..., and failure probability values 61, 62,.... At first glance, this strategy seems invalid for the

 

mixture sampling model, as the sample sizes are actually not known a priori, since they depend on the groups sampled,
however a subtle conditioning argument circumvents this objection. The simple trick we employ is to condition the
algorithm on the infinite sequence of z € 2* samples drawn on line [10] and establish the schedule after performing this
conditioning operation, thus the sample sizes are fixed from the perspective of the schedule, and the samples themselves
remain conditionally i.i.d. given this order.This technique is entirely valid, because the sample sizes depend only on these
z values, and are conditionally independent from the actual samples x and y.

We henceforth assume that we are in the probability at least 1 — 6 case where all € bounds taken over the course
of the schedule hold. With this established, items [I] and [2] follow via theorem [3.4] if ReG = TRUE, and via theorem [3.7]
holds, as he H, and é was computed with AEV(...), we get |f — yu] < é, and by the
observe that it is implied by item [1] coupled with the bound M** < p’*,

otherwise. In particular, iter

 

termination condition, é < ¢. To see item

 

 

 

which holds for similar reasons.

 

Technically, under the mixture sampling model it is possible that the loop (lines[9|[12) runs infinitely, but so long as each group is
sampled with nonzero probability, this is a zero probability event.

29

2) We introduce various statistical estimators for regret, welfare, and malfare. We then bound the tails, expectations,
and statistical biases of these estimators through a unified analysis in terms of uniform convergence bounds.

3) We introduce three practical models of sampling (data collection) for populations consisting of multiple groups, with
philosophical connections to stratified sampling. We then show that the knowledge-gain in optimizing or estimating our
fairness concepts depends intimately on the model class, per group distributions, and the fairness concept at hand.

4) We present two progressive sampling algorithms, tailored to our sampling models, to optimize our fairness objectives.
We introduce novel technical conditions under which progressive samplers can estimate not only Lipschitz-continuous

objectives, but also strictly-monotonic continuous objectives, which is of independent interest beyond the fairness sphere.

2 Learning Framework and Objectives

In this section, we introduce the functional form of the objects and random spaces that we operate over, and we define
our learning objectives. In particular, section presents the welfare, malfare, and regret objectives, which compile

2)reifies this

 

per-group sentiment values into a cardinal objective value that can be optimized and analyzed, then sectio

   

abstract mathematics with three realistic models of data-collection, each of which requires its own statistical treatment
to efficiently learn from data, i.e., to optimize and bound objectives, while minimizing the cost of obtaining said data.
We henceforth assume a supervised learning setting, where ¥ is the domain and Y is the codomain. We also assume
either a loss function? | €(-, ): x Y > Roy or a utility function u(-,-): Y’ x Y + Rox, which map predictions and labels
onto negatively connoted loss or disutility, or positively connoted gain or utility, generically termed a sentiment function
s(-,-): Y’ x Y + Rox. In most supervised learning settings, a single probability distribution D over V x Y suffices, but
we assume a set Z of g groups, and we model the experiences and conditions of each group as its own distribution, i.e.,
we have D,,...,D,. For convenience, we often compose the sentiment function with a predictor or model h(-) : ¥ > Y’,

taking (so h)(x,y) = s(h(x),y), thus we quantify model performance for group i as Ep, [so h].

2.1 Fair Learning with Malfare, Welfare, and Regret Objectives

Here we define the welfare, malfare, and regret objectives. While the details differ, each of these is a function of the
expected utility or loss (generically sentiment) of some h: XY — Y for each of the g groups, and we are interested in

selecting the model or hypothesis h from some hypothesis class H C ¥ — Y’ that optimizes the given objective.

Malfare and Welfare A welfare function W(S; w) measures overall positive utility S across a population weighted by
w, whereas a malfare function A\(S;w) measures overall disutility S, and generically, we say an aggregator function
M(S;w) measures overall sentiment S. The prototypical example is the utilitarian (or Benthamite) aggregate, defined as
Mi(S;w) = S-w, which simply averages sentiment across the population (e.g., welfare as per-capita income, or malfare
as per-capita medical expenditure), and the second-fiddle is the egalitarian (or Rawlsian) welfare W_.(S; w) (minimum)
or malfare M.o(S;w) (maximum), which summarizes a population’s sentiment as that of its most disadvantaged member.
We assume throughout that w € (0,1)% is a probability vector, thus ||w||, = 1, and S € Ré, is nonnegative. Ab initio, our
first objective is, via the social planner’s problem, to maximize welfare
, or by extension (e.g., in chores manna or harm allocation
[Feder otal] 7, or in machine learning ) to minimize malfare, i.e., we seek to

approximate

 
       

 

h* = arguin M (i E[€0h]; w) or h* = argmax W (i+8 E[uo hk]; w) : (1)
hen hen

Intuitively, the utilitarian case seeks to optimize overall or average sentiment, whereas the egalitarian case instead seeks

to lift up the most disadvantaged group, and thus promote equality, perhaps at the expense of overall (total) utility.
1Often Y’ = y, such as in standard classification and regression settings, but this is not universally the case. For instance, in probabilistic
classification ar sgerossion ie., conditional density estimation), )”/_is a space of distributions over ), either parametric [Nelder and
Cousins ad Riondato||2019] or nonparametric |Rosenblatt! 1962], and in interval estimation, Y" is a
rom which one is to be recommended, and

‘space 01 ert sets over Y. Similarly, for recommender systems, Y” can be a set of tems,
Y can be a subset of these items that a given user would like.

 

   
   

(continuous) H. We now state results using Rademacher averages |Bartlett and Mendelson] 2002 1{s halev-Shwartz and

[Ben-David] [2014] 2014] that tolerate infinite H, while preserving the variance-dependence of item [2]

Theorem 3.2 (Uniform Convergence with Rademacher Averages). Suppose hypothesis class H and sentiment function

 

 

 

s(-,:), take (x,y) ~ D™ and o ~U"™(+1), ie., o is uniformly distributed on (+1), and define the Rademacher average

 

 

 

Rm(soH,D) and Bousquet variance proxy Wn(s oH, D) [see [Bousquet| as
Rn(soH,D) = E lee|2 a oh(xi)oi >» VYn(soH,D) = sup Vis oh] +4r&,,(soH,D) . (7)
wye | hen nen P

 

3m m

rin 4 m (8! nt
We may then bound AESp(m, 6) as € — 2&m(soH,D) + 4 ea 4 yf WaeonDied :

  
 

Data-dependent analogues of theorem
asymptotic cost
theorem [3.1] item P] however it improves when correlations exist between elements of H, because the effective size of H
(7). The abstract inequalities

are quite opaque, so we now provide concrete bounds on the Rademacher averages of practical infinite

re possible using empirical Rademacher averages and variances at no

In the worst case, theorem [3.2] performs comparably to union bounds, i.e.,

 

is smaller for the purposes of realizing the supremum in the Rademacher average, see

of theorem |3

 

 

hypothesis classes. The below results hold for any distribution D, and are thus distribution-free, although similar

distribution-dependent or data-dependent bounds are possible.

Property 3.3 (Practical Bounds on Rademacher Averages). 1) Suppose H has Vapnik-Chervonenkis (VC) dimension
d 1], and (9, y) = 1 — 1,(9) is the 0-1 loss. Then for some absolute constant c,
Rm(LoH,D) < a which implies bounds for linear classifiers, bounded-depth decision trees
and many classes of neural network :

2) Suppose ¥ = {7 € R° | ||z||, < R} is the R-radius Lz ball in RY, H = {FZ W-#| ||wl|, < y} is a y-regularized linear
class, Y = [—Ry, Ry], and &(-,-) is a \-Lipschitz loss function s.t. ((y,y) = 0. Then r < 2\Ry and Rn (CoH, D) < AZ.

ym
This implies bounds for (kernelized) SVM, generalized linear models |Nelder and Wedderburn)

, and bounded linear
regression.

 

     

 

 

3.2 From Mean Estimation to Welfare, Malfare, and Regret Bounds

We now adapt the additive error bounds of section [3.1]on expectations to bound malfare, welfare, and regret in terms of
empirical estimates thereof. In particular, the strategy for each is to combine tail bounds for mean-estimation with the

monotonicity axiom (definition [2.1]item[I) to bound the tails and expectations of our desiderata. We use the uniform

  
   

convergence bounds of section o bound the error of these estimates, thus we need only propagate this uncertainty
through the appropriate aggregator functions. In general, aggregator functions are nonlinear, and optimizing over H
results in estimation bias, thus the plug-in estimator is biased, however, we still obtain tail bounds on our objectives
via AEV(...). Because the plug-in estimator is biased, we also consider various LCB-and-UCB-style estimates, which
when optimized yield safer function choices and partially control for overfitting. Finally, in some cases, integrating over
worst-case uncertainty from the tail bounds of AEV(...) yields convenient bounds on the expectation (and thus the bias)

of the plug-in estimator.

Welfare and Malfare Due to the lack of an unbiased estimator for welfare and malfare, we study the simple plug-in

estimator M, as employed by {Cousins and Riondato

In particular, we take

 

, and introduce a pair of lower and upper estimators (M*,M").

 

Stem (i E jen) at em (1-+0v E jones) & MT =M Gx E jon +esw)

i Vis ig oUt i, Way:
ee“. “> SS —— Tee
Piuc-In ESTIMATE LCB EstiMare UCB Estimate 8)

where V and A are the (minimum precedence) infix binary maximum and minimum operators, respectively. By

monotonicity (axiom it holds that M* <M <M". The lower and upper confidence bound estimates are convenient,

 

both to show high probability bounds, and to sandwich the plug-in estimator, which we use to bound its bias. We first

9

 

is estimated to within +9 additive error, there is no further benefit to improving their estimate. Thus the optimal (as
measured by sufficient sample size) sampling strategy depends on the true expected utilities, the difficulties of estimating
utilities for each group, and the objective in question, and in no way resembles the naive uniform or proportional so-called
“fair sampling strategies” described above. We argue that such naive strategies are dangerous, as they introduce subtle
biases and fairness issues, but the rationale for alternative sampling strategies is only apparent through the lens of sample
complexity.

We now ask the questions, “Given a sample, what do we need to obtain sharper bounds?” and “How much will
bounds improve with a larger sample?” We begin with a soft discussion as to why samples from different groups may
contribute more or less information to an estimate, which we measure as the improvement to tail bounds that additional
samples may yield. In particular, for malfare, we discuss the improvement to upper bounds, but the entire discussion can
be directly translated to welfare and lower bounds in the usual manner. We then quantify these factors mathematically,

and we develop these ideas further in section

 

where they are used to adaptively choose from which group to sample.
Answering these questions tells us what fundamentally is needed to solve an estimation or optimization task, which
allows us to intelligently guide scientific inquiry by directing limited resources towards studying the most informative

populations, where informativeness is measured by the amount of improvement to tail bounds.

Philosophical Discussion We now discuss the three main factors driving heterogeneity in sampling impact.
1) Variable estimation difficulty or overfit potential: Often it is inherently more difficult to give bounds on the expected

sentiment for some groups than for others. This can be due to differences in variances (see theorem or in uniform

 

convergence bounds (see theorem

 

, and in general, occurs when é + AEV(...) has €; < €; for some group i # j,
even while m; ~ m,;. There are many possible causes of such asymmetries, but we now give two examples in the
machine learning sphere:

A) The class H can be less complicated when projected onto group i than on group J, e.g., if H is linear classifiers, and

group i exists in a lower-dimensional subspace than group j (see item

 

B) Group i may itself be more self-similar (less diverse) than group j; for instance with tabular data, there may simply
be no members of group i that attain certain feature values, which would for instance, impact bounds for decision trees.
It is entirely possible for minority groups to exhibit either more or less intra-group diversity than a majority group,
thus this effect can work in either direction.

2) Variable task difficulty: Some groups may be inherently easier or harder to satisfy than others; e.g., regression and
classification problems are generally easier for groups with labels that are more homogeneous, and regret varies with
the optimal expected sentiment S;. Similarly, for recommender systems, if a group is generally satisfied by a larger

number of options, they will generally have lower risk. This is crucial, because most malfare and welfare functions are

 

o high-risk or low-utility groups, thus the ease of satisfying a group effects their impact on malfare and
welfare values.

3) Aggregator function interactions: Complicated interactions also occur through the malfare or welfare function. When
learning over H, the set of near-optimal functions is more relevant than those that are clearly bad choices overall, and
groups that tend to be mutually satisfied (i.e., are correlated) are less impactful to the overall objective. Weight values

in malfare or welfare functions may also differ between groups, and higher-weighted groups are usually more impactful.

Quantifying the Incremental Value of Sampling We measure the impact of sampling by asking the question,
“What is the incremental value of a single sample drawn for some group?” In particular, we quantify the value of the
sample as the reduction in uncertainty, as measured by the infimum UCB (over H), and although this is inherently a
discrete question, we approximate the answer for the power-mean malfare with tools from the calculus of infinitesimals.
We consider the power-mean malfare family for its simplicity and convenient differentiability properties, but similar
analysis is possible for welfare or regret bounds. For the sake of intuition, we lead by presenting a parametric Gaussian

example in figure

 

 

4In particular, this holds for all p 4 1 power means, and is axiomatically justified by the Pigou-Dalton transfer principle (defini-
EA

tion[21]item f-

12

Intuitively, definition captures the idea that no matter how unlucky we are with the sampled a, y, if AEV(...)

 

bounds tails once for each timestep t of the schedule, with per-group samples of size at least s; and failure probability 6;,
then at some point an ¢-estimate of the objective will be produced. Note that neither data-dependent AEV(...) bounds
on sentiment values, nor sufficient per-group error radii to estimate the objective, are known a priori, thus it is not always

possible to select a sufficient static sample size, however, definition is more flexible, as it requires only the existence

 

of a (possibly unknown) sufficient sample size. Even when a sufficient sample size is known, unless it is also necessary,
progressive sampling is usually more sample-efficient, often terminating closer to the necessary sample size.

With this definition in hand, we now construct finite ¢-, and infinite 0-, uniformly-convergent schedules. In the context

 

still be used. Both are based on geometrically-increasing sample sizes, which are efficient because they never “overshoot”

any sample size by more than a constant factor, yet they cover an exponentially large range of sample sizes in a linear

number of timesteps.

Definition 4.2 (Geometric-Uniform Schedule). Suppose optimistic size a > 1, common ratio 8 > 1, and schedule length
T €Z,. The geometric-uniform schedule then takes (geometric) s; = [a3] and (uniform) 6; = $1),...r(t).
Definition 4.3 (Double-Geometric Schedule). Suppose optimistic size a > 0 and common ratio 8 > 1. The double-
geometric schedule then takes (geometric) s: = [a8] and (geometric) 6: = ae,

Lemma 4.4 (Sufficient Conditions for Uniformly-Convergent Geometric Schedules). Suppose as in definition

assume also that

 

sup
(w,ylE(XXY)ETXI

 

 

AEV ((sr,.

 

Then the geometric-uniform schedule (s, 6) is e-uniformly-convergent.

Furthermore, suppose as in definition |4.3] a > $ and assume that

lim sup
moo (w,y)E(XXY)™*I

 

 

AEV ((m,...,m), sfaty,2,9)|| | =0 : (20)

Then the double-geometric schedule (s, 6) is 0-uniformly-convergent.

 

The initial and final sample sizes of the geometric-uniform schedule are s; = [a3] and sr = [a7], and often one
can set 1/3 and sr to minimal sufficient and maximal necessary sample sizes (as a function of T, the objective, and other
parameters). To maximize statistical efficiency while controlling the value of 8, we may select the minimal T such that

ae
[logs ao =o
bounds of theorerr

      

n particular, assuming a A-Lipschitz objective, the Hoeffding (item
) of length T € O(log ar), For the double-geometric

schedule, we may similarly set *1/3 to a minimal sufficient sample size, and here there is no T’ parameter (the schedule is

and empirical Bernstein (item

  

imply ¢-uniformly convergent schedules via

 

infinite), thus we may simply select 6 as desired. This yields 0-uniformly convergent schedules, since each of the bounds

  

, as do those of theorem so long as limy,—..Maxjez®m(s oH, Di) = 0.

 

Both of the above schedule types are efficient, in the sense that for the smallest (per-group) static sample size m*
at which we obtain the bound e*, some ™m < [8m*] is contained in the schedule at some timestep t, and the bound
é + AEV((m,...,7),6,...) exceeds e* only because it uses a smaller 6 value, i.e., because 6; < 6. In particular,

assuming all bounds are asymptotically @/u for u = In 4, we have for each group i that

¢<0,/ “= ¢ See “ . (21)

 

éi log(T) + u Ei log(m*) + u
for the geometric-uniform and double-geometric schedules, respectively. Note also that log(T’) € @ (log log PA), whereas

log(m*) € O(log ), thus {

€

 

shows us that the geometric-uniform schedule is preferable to the double-uniform

 

schedule, unless m* is exponentially smaller than the above bound, e.g., if \ = oo, or if a nonlinear objective is more

stable to perturbations of each S; about its optimum than the Lipschitz constant A would indicate.

 

7 The base-6 logarithm arises intuitively, as the number of times the sample size must increase by a factor 8 to reach size sp from 81.

 

03 geo

0.2

0.1

 

Pid didi did dd F
% 4 8 12 16 20 24 28 32 199 10? 103

 

(a) Natural axes plot. Note that the shaded (b) Semilog plot, wherein each function is (c) Semilog plot, wherein each function is
region between the lower and upper bounds scaled by u to emphasize their asymptotic scaled by (u + 4) to better illustrate the
is difficult to see without scaling. @(4) behavior. gap between lower and upper bounds.

and lower and upper bounds thereof (y-axis), i.e., wie S1-4/a
2

ave see , against u > 1 (a-axis). The region sandwiched by the lower and upper bounds is shaded in purple,
and we note that even when scaled by wu, this region rapidly converges to 0 — in particular, the gap is bounded as

Figure A2: Plots of various scalings of 1 —

   

 

1 i eg 3
Qu+V2 Qu+s > tu

Furthermore, suppose as in definition i, and assume that

AEV ((m,...,m), s@aty,@,9)||

 

lim sup
moo (@,y)E(XXY)™*I

 

 

=0. (20)

M
Then the double-geometric schedule (s, 6) is 0-uniformly-convergent.

Proof. Recall that, via of definition the goal is to show

   

j su)
af ieahcreesixt | AEV((s:,..., 8+), 64,2, 8)||y Se -

We now show the results for the geometric-uniform and double-geometric schedules in separate parts.

The result for the geometric-uniform schedule holds by the regularity conditions assumed on AEV(...), where the

infimum occurs, possibly nonuniquely, at t = T. In particular, this is implied by condition ] thus we have

inf sup = sup
tees (ey e(ex YI [ABV Cts scent Seth (wy)e(xxy)er*9

 

 

ABV((81,.-.,87), #529) Conpition [3]
<e. By ASSUMPTION

We may thus conclude that the uniform-geometric schedule is e-uniformly-convergent.

The result for the double-geometric schedule uses the regularity conditions on our bound AEV(...), and also derives
a bound on 6; in terms of s;, which allows us to reason over only sample sizes, rather than the schedule itself. Let t(m)
denote the smallest timestep t for which a sample of size m is guaranteed to be drawn. In particular, ignoring the ceiling
operator, and the fact that not every possible sample size m € Z, is in the schedule s, the inverse of m = a! gives
us t(m) > log, . However, the sample size m may be up to a factor § larger due to the schedule discretization, and
AGF):
a

rounded up, thus we have the matching upper-bound t(m) < log, . With this result in hand, we now substitute

28

geometric welfare through the reduction of the LINEARPSUTILITY(...) procedure of algorithn Our solution is to

 

maximize not just the improvement:cost ratio of sampling for a single additional timestep, but over any number t of
timesteps. For reasonable malfare functions, when the empirical risk is unchanged after taking the minimum with c (i.e.,

the expression cA Ez, ..y;,[€0 h] takes the value Ey, .y,,,[£0 h] on lin

 

of algorithm

 

we expect diminishing returns
(in improvement:cost ratio) to further sampling, thus the t = 1 step greedy optimal choice should usually be selected.

In particular, given per-group uniform convergence bounds €)., and assuming each group is on timestep t1:, of their

 

sampling schedule, we estimate the UCB improvement |for sampling from group 7 for t timesteps as as

    
 

St; In a
1 fori=j (23)

ee; fori#g, & Ere; aie
a Bye;

3

We then estimate the malfare improvement as Aj, < MT -M (i Ho cNEe,. wi (lo hl + \);w), and

compute the cost of sampling from group 7 for t timesteps as C;(s:42; — s¢;). We then select the group with maximal
A

projected improvement to sampling cost ratio, i.e., we select 1 + argmax sup ——* —

weZ teZy Ci(si+t; — St; )

Note that even if the estimated improvement is not accurate, we still gain information; either the upper bound

 

(see line]16).

decreases more than expected, which decreases the relative value of further sampling from this group (in which case we
are less likely to sample from this group in the future), or it decreased less than expected (likely due to selection bias), in
which case we may decide to sample it again, but now more work is required to get the same reduction in confidence

radius (increased cost), so sampling from another group may now be optimal.

Further Notes on Long-Term Planning Optimizing over timestep count t, as well as group i, does seem like it
may create some issues for the algorithm, however the impact is philosophically, computationally, and practically very
small. In general, as long as some improvement to sampling a group is projected to happen eventually, evaluating the
supremum over t exhaustively will eventually reach some t such that the cost is so large that no larger t can be optimal.
For example, if the current (regret) malfare UCB is N, and improvement Aj, > 0 is projected after some timestep count
t, then for cutoff timestep t', defined as the smallest integer t* > t such that
at nif
M Ait q f
a ee oo or equivalently, s,+ > St, + (S142; — 8t;)=— 24
Ci(sit ye, — Sti) ~ Cilseye; — 8t;) 7 eer Se Rn (24)
the improvement:cost ratio is never maximized after more than t’ additional timesteps. We thus conclude evaluating the

supremum over t in group selection (line is not computationally intractable.

 

Note also that even if sampling group 7 for t timesteps yields an optimal improvement:cost ratio at this iteration,
the algorithm may change course on the next iteration. For example, if more improvement than projected occurs, then
sampling from another group j, which now has a greater impact on the malfare (e.g., group 7 is no longer maximal, and
thus inconsequential to egalitarian malfare) now optimizes the improvement:cost ratio, or if less improvement occurs
than projected, then projected improvement of group i at the next timestep may decrease, after which another group
may have the optimal projected improvement:cost ratio. In other words, despite some element of long-term planning
with this supremum over ¢ in group selection, the algorithm does not commit to sampling from a group for more than a
single iteration, which is important, because long-term projections are likely to be inaccurate, as they are made with less

information than the greedy-optimal group selection at each iteration.

 

"Note that we can’t simply assume ,/1 rates, as 6 may also be changing. Even incorporating the In } terms characteristic of exponential

tail bounds is not always entirely accurate; and when the schedule and bound are fully specified, in some cases it may be possible to produce a
. : : 26 6

better estimate of the bound improvement. In particular, if AES(...) is not data-dependent, we can simply take é") — AES(s:+¢,, Sttti),

32

are elements of ¥ x Y x 2%. This generality is useful for studying concepts like intersectionalism and multicalibration

Rothblum and Yona|

race), and is in some sense more data-efficient than associating each sample with a single group, but the case of mutually

, where individuals may belong to multiple groups, (e.g., at the interface of both gender and

 

exclusive groups (i.e., each sample belongs to exactly one group, thus samples are in ¥ x Y x Z) is also computationally

and philosophically convenient |Dwork et al.
ii.d., where the group identities of the sample are left up to chance (i.e., roughly proportional to group frequencies),

 

. This model naturally represents a mixed population being sampled

and is thus the most appropriate model for learning from

3) Conditional Sampling: Here we actively choose from which group to sample, in contrast to the mixture sampling model,

 

existing datasets with group identity features

 

where we simply cast our net and “get what we get.” In particular, we sample iid. (¥,)/) pairs conditioned on some

group z € Z, thus we may select sample sizes m1:, € Z{, and draw a sample (x,y) € (Wx )™! x +--+ x (¥@ x Y)™.
This is a natural model in active sampling |Abernethy et al.

guide further study and resource expenditure, and similar conditional sampling structure arises in stratified sampling

and scientific inquiry settings, where initial results

 

settings.

Each of our three sampling models, as well as how they interact with learning or estimation routines, is illustrated
in figure [2] In mixture sampling, we generally assume unit cost C = 1 per sample, and in joint sampling, we assume
constant cost C > 1 per joint-sample, as it is more expensive to set up a properly controlled joint sampling distribution.
On the other hand, in conditional sampling, some groups may be more difficult or costly to study than others, so we
assume a cost model C1:4 € R4., where C; is the per-sample cost for group 7, thus the total cost of drawing a sample with
per-group sizes my., is m-C. Note that the extra control of the conditional sampling model is extremely convenient
and very powerful, however it is generally more expensive than mixture sampling. These costs are entirely application
dependent, so we take no stance on which is preferable, and rather focus on developing efficient learning algorithms under

each sampling model.

3 Statistical Analysis and Estimation Guarantees

In this section, we discuss the statistics of estimating malfare and welfare functions. In particular, we assume a set Z of g
groups, and we want to estimate the malfare, welfare, or regret of per-group expected loss or utility of some h: ¥ > Y’,
ie., . .
Me M(in Else hj;w) , or M&M(i++ Reg,(h);w) ,

where Dj., are distributions over V x Y, s(-,-) : Y’ x V > Rox, (soh)(z, y) = s(h(x), y), and M(-; w) generically represents
some aggregator function. Estimating the expected loss or utility of one group is a well-studied sampling problem, but
generalizing to the welfare, malfare, or regret of multiple groups introduces some subtleties. We start by noting that
while the empirical mean is an unbiased estimator of expected utility or loss of a single group, in general there is no
unbiased estimator of welfare or malfare (essentially due to their nonlinear nature, much like with the standard deviation).
Thus rather than unbiased estimators, we seek additive error (AE) bounds of the form P(|M — M| < €) > 1-64, where €
is the confidence radius (a.k.a. the margin of error), and 6 is the failure probability (or, by alternative convention, 1 — 6 is
the level of confidence).

In machine learning, it does not suffice to estimate the welfare or malfare of a single function h(-): ¥ > Y, as we
optimize over a hypothesis class H C X¥ — ’, thus we seek some sample-dependent h €H with true objective value
within € of that of the optimal h* € H. At times, we are also interested in related statistics, like the objective values of h
and h*, and in general, tools to bound the deviations between the empirical and true objective values for any h € H are

sufficient to bound these quantities. The rest of this section pursues such bounds, assuming a fixed failure probability

 

6 and sample size m; for each group i € Z. In particular, section eviews known results for uniformly estimating

 

expectations across H, section builds upon these results to uniformly estimate malfare, welfare, and regret values,

 

and sectio: then studies how varying per-group sentiment values and confidence radii impacts these bounds, and

 

quantifies the incremental value of sampling from each group as a function of these quantities.

Uncertainty and the Social Planner’s Problem: Why Sample Complexity Matters

Cyrus Cousins

August 8, 2022

Abstract.

Welfare measures overall utility across a population, whereas malfare measures overall disutility,
and the social planner’s problem can be cast either as maximizing the former or minimizing the latter.
We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and
regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each
group. In particular, we consider estimating these quantities for individual functions (e.g., allocations
or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error
We then study

algorithmic fairness through the lens of sample complexity, finding that because marginalized or

over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages.

 

minority groups are often understudied, and fewer data are therefore available, the social planner is
more likely to overfit to these groups, thus even models that seem fair in training can be systematically
biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample
sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated
by these conclusions, we present progressive sampling algorithms that efficiently use data to optimize

various fairness objectives.

Keywords

Algorithmic Fairness Minimax Fair Learning } Multi-Group Agnostic PAC Learning Fair PAC Learning
Social Planner’s Problem } Welfare Estimation } Malfare Estimation } Sampling Methods

1 Introduction

Machine learning systems in settings like facial recognition |Buolamwini and Gebru| 2018} |Cook et al. | 2019||Cavazos et al. |
2020] and medicine [Ashraf et al. | 2018) 8 Beerge et al. | 2022| | Chen et al. | 2018] exhibit differential accuracy across race,

gender, and other protected groups. This can lead to discrimination: for example, facial recognition in policing yields
disproportionate false-arrest rates , and machine learning in medicine can lead to inequity of health
outcomes {DeCamp and Lindvall 0}, both of which exacerbate existing structural and societal inequalities impacting

minority groups. In recent years, researchers have proposed welfare-centric fair learning models, which constrain or

optimize welfare
or malar
Diana et al. [2021 Cousins [2021] Shekhar et al.| [2021] to promote fair learning across all groups, as well as regret-based
methods {Blum and Lykouris| {2020} Rothblum and Yona| [2021], which similarly promote fairness by minimizing the

maximum dissatisfaction of any group, relative to their preferred outcome, i.e., how much excess risk is incurred, or utility

 

 

  

 

 

 
 
 

 
 
 

      

 

 

is lost, to any group by compromising on a shared solution.
We study sampling and learning problems in the optimization of welfare, malfare, and regret objectives. In particular,

our setting subsumes the minimax fair learning |Martinez et al. | 2020) | Abernethy et al. | 2020) | Diana et al.}|2021} |Lahoti

Shekhar et al. — also known as Group Distributionally Robust Optimization (Group DRO) [Hu et al.| [Hu et al}

— and the fair-PAC learning settings, by considering arbitrary
malfare or welfare functions, as well as the multi-group agnostic PAC learning |Blum and Lykouris Rothblum:

fand Yona [2021] setting, by considering arbitrary malfare functions — rather than just the maximum — of per-group

regret values. This extension naturally and smoothly interpolates between minimizing utilitarian (i.e., weighted average)

    

     

 

and egalitarian (i.e., maximum) malfare of risk or regret. Crucially, this allows for fine-grained control over the desired
fairness concept, and mitigates the minority rule issues of minimax methods, while remaining axiomatically grounded in
cardinal welfare theory. We bound the generalization error of optimizing welfare, malfare, and regret objectives, and find

that while the power-mean malfare is always easy to estimate, due to Lipschitz-continuity (as studied by|C

 

our learning algorithms work for any malfare, welfare, or regret objective that is continuous and monotonic in per-group
(dis)utility values.

We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or
minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit
to these groups. Consequently, even models that seem fair in training can be systematically biased against such groups.
Section [3] shows that this effect can be mitigated with sufficient per-group sample sizes, and section [4] presents progressive
sampling methods, which dynamically sample until a near-optimal model (w.r.t. some fairness objective) is learned. Our

analysis rigorously addresses issues raised by, e.g.,|Chen et al. , who ask how sampling-error impacts fairness, and

 

suggest using learning curves to draw sufficient per-group sample sizes so as to ensure small sampling-error, and |Shekhar|

, who study the problem of optimally allocating sampling effort for the special case of egalitarian malfare.

 

Our bounds leverage the specific character of the objective at hand; for example, utilitarian welfare is sensitive to
the average confidence radius across groups, whereas egalitarian welfare is more sensitive to the confidence radii of
disadvantaged (i.e., low-utility or high-risk) groups. Furthermore, our progressive sampling methods are tailored to three
realistic models of data generation: in the joint sampling model, each sample contains a piece of information for every
group, in the mixture sampling model, samples are annotated with (sets of) group labels, and in the conditional sampling

model, we are allowed to choose from which groups to sample. In section [4

 

specific fairness objective at hand, the optimal decisions as to where to invest sampling effort based on partial information
are highly non-trivial, and of great import to fairness. While our settings and modelling assumptions are practically

motivated, this is a highly theoretical paper, and all novel results are meticulously proven in appendix[A]

Contributions We summarize the contributions of this work as follows.
1) We generalize the regret objective, and unify it with the welfare and malfare objectives for fair machine learning.
All three are cardinal objective families, which may be computed or estimated to measure the fairness of a system, or

optimized in machine learning or economics settings to create fair systems, i.e., they address the social planners problem.

generally to produce an improved mean estimator. In particular, we suggest a form of disproportionate allocation, i.e.,
per-group sample sizes are not necessarily proportional to their population frequencies. Rather than simply considering
variances to estimate means, we holistically consider the objective and uncertainty over various quantities, thus our
sample-size selection-strategy is a variant of the minimax sampling ratio
method. |Chen et al. also suggest disproportionate allocation in fair machine learning, albeit only for bounding
absolute differences of per-group fairness statistics. Similar concerns also arise in optimizing minimax-fair models, wherein
Abernethy et al.

though it is unclear whether such methods generalize beyond the egalitarian case.

   

 

present an algorithm that takes gradient steps to improve a model for the highest-risk group,

4 Progressive and Active Sampling Algorithms

 

considers fixed sample sizes M1:g and failure probabilities 6, and bounds the confidence radius ¢. In this section,
we want a fixed ¢-6 additive error guarantee, but we are willing to let an algorithm select the sample size m (or per-group
sample sizes m1:,). In particular, due to the cost of sampling and processing data, we want our algorithm to minimize m
(or cost measured as some function of m), while constraining ¢ and 6 to user-supplied levels. Some cases are simpler than
others; the joint sampling model yields a standard progressive sampling method with a fixed sampling schedule, and
the method under mizture sampling is similar, except a subtle conditioning argument allows us to use variably-sized
per-group sample sizes based on the order groups are sampled in. For the conditional sampling model, we develop an
active sampling approach, which makes cost-sensitive decisions as to which group to sample at each iteration. More

details on sampling schedules and other aspects of our progressive sampling algorithms are given in appendix [B]

4.1 Convergence and Sampling Schedules in Progressive Sampling

We can’t simply draw samples one-by-one, compute bounds using €é + AEV(m, 6, x, y) after each sample, and terminate
when a sufficiently sharp bound is available, because the possibility of early termination leads to the multiple comparisons
problem, wherein by chance the desired confidence radius is met at some timestep. Progressive sampling algorithms
correct for this by establishing, usually a priori, a sampling schedule s and failure probability schedule 6, which usually
dictate that, at timestep t, we take a tail-bound with 6 = 6: and sample size s:, while ensuring that all bounds hold
simultaneously (by union bound) with probability at least 1 — 6. Due to this union bound, it is statistically inefficient to

take bounds after drawing every sample|°} Furthermore, for technical reasons, we henceforth assume a few mild regularity

 

conditions:
1) The sampling schedule s € Z$ is a strictly monotonically increasing sequence, i.e., for all t € Z,, $1 < $141;
2) The failure probability schedule 6 € [0,1)°° is a sequence that sums to some 6 € (0,1), i-e., 7, 6+ =/4]], = 5; &

3) The distribution-free bound®| sup, y||AEV(m, 5, x, y)|| is monotonically decreasing in my:, and 6 for any norm |\-||.

 

In order to prove that a progressive sampling algorithm produces a (probabilistically) correct answer, it is crucial to
show that it does not loop indefinitely. We now introduce ¢-convergent schedules, which require all sentiment values to
eventually be ¢-d estimated w.r.t. some norm ||-||,,, which we then translate into guarantees on welfare, malfare, or regret,

as per theorems and

  

Definition 4.1 (e-Uniformly-Convergent Schedule). For any ¢ > 0, a sampling schedule s and failure probability schedule

6 are e-uniformly-convergent w.r.t. AEV(...) and some norm |]-||,, if

i suj
ant loser pees || AEV((se, see , 81); 6¢,2,Y)|| 4 Se. (18)

5Indeed, in some of the earliest progressive sampling work, did employ arithmetic schedules, i.e., those with
constant differences between successive sample sizes. We discuss only the more efficient geometric schedules introduced by
1999]. Early work in progressive sampling had no failure probability schedules 6, considering statistical error and multiple comparisons only
through heuristic convergence estimation techniques, thus the statistical cost of performing this correction was not y ly appreciated.

®Note that in the case where é is a random variable, as in (6) and the related discussion in the prologue of sectio we assume this
property holds with certainty, which can be achieved by combining the radomized data-dependent bound with a worst distribution-free
bound. Alternatively, if it does not hold with certainly, but rather almost certainly, or even in probability, the subsequent analysis largely
applies, albeit with these inherited probabilistic guarantees.

 

 

 

 

 
 

14

Notably, welfare maximization generalizes utility maximization to multiple groups, and malfare minimization likewise
generalizes risk minimization, and the well-studied minimax fair learning framework arises as the special-case of egalitarian
malfare minimization. In general, we assume only monotonicity and continuity of aggregator functions; however, there

are a set of relatively standard axioms that, when taken together, restricts the class of interest to the power-mean family

 

This is convenient, as all power-mean malfare functions are Lipschitz-continuous, which in section [4]

leads to stronger estimation guarantees and more efficient sampling algorithms than ¢-6 limit-continuity.

Definition 2.1 (Axioms of Cardinal Welfare and Malfare). Suppose an aggregator function M(S;w). For each item,

assume (if necessary) that the axiom applies for all S,S’ € Rf, scalars a, 8 € Roz, and probability vectors w € (0, 1)%.

1) Strict Monotonicity: S' #0 =» M(S;w) < M(S+S’;w).

2) Weighted Symmetry: Suppose g’ € Z,, S’ € Ré,, and probability vector w’ € (0, 1)", such that for all u € Ro, it
holds that 30; .4. s,2u Wi = Li gt. sf We Then M(S;w) = M(S’; w’).

3) Continuity: M(S; w) is a continuous function (in the standard ¢-6 limit-continuity sense) in both S and w.

4) Independence of Unconcerned Agents: M((S1:9—-1, 0); w) < M((Sj.g—1, 0); w) => M((Si.g—1, 8); w) < M((Si.g—1, 8); w)-

5) Multiplicative Linearity: M(aS;w) = aM(S;w).

6) Unit Scale: M(1;w) = M((1,...,1);w) = 1.

7) Pigou-Dalton Transfer Principle: Suppose p = w-S = w-S’, and for all i € Z: | — Sj| <|u — S|. Then for utility
and welfare, W(S'; w) > W(S;w), and for disutility and malfare, M(S’; w) < M(S; w).

 

These are the axioms employed by

 

in the construction of the fair-PAC learning framework, and we
briefly argue they are quite natural, though henceforth we only require monotonicity and at times continuity
; modified to include the

(multiplicative linearity)

 
 

strengthens the traditional independence of common scale axiom, and ensures that the units of welfare or malfare must

match those of sentiment, and axiom [6] (unit scale) merely specifies a multiplicative constant. Taken together, axioms

 

strengthen the Debreu-Gorman theorem , to uniquely characterizes all aggregator functions
as weighted power-means. Finally, axiom ton} |1920), characterizes

fairness in the sense of equitable redistribution of utility (welfare) or disutility (malfare).

 
 
 

Theorem 2.2 (Aggregator Function Properties theorems 2.4 and 2.5]). Suppose aggregator function
M(S;w), and assume arbitrary sentiment vector S € R§, and probability vector w € (0, 1)%. The following then hold.

1) Power-Mean Factorization: Axioms[1}[6] imply dp € Rs.

can MtSian) Sepa oy _ 5, gt wi p=0_ fo(x) = In(x)
M(S;10) = Ma(S:0) = Sp" (DirwufolS.)) =nno {/Zemns? , with {t 40 fla) =senlp)e” .

2) Fair Welfare and Malfare: Axioms|I]{7]imply p € (—oo, 1] for welfare and p € [1, 00) for malfare.
3) Lipschitz-Continuity: For all p > 1, it holds that |Mp(S;w) — Mp(S’; w)| < M,(S — S’|;w) < maxiez|Si — Si}.

In closing, we note that utilitarian philosophy is often criticized for permitting great inequality by ignoring the
needs of smaller or less visible groups, whereas egalitarian philosophy is criticized for ignoring the masses in favor of
outliers and disadvantaged groups, and its inherent susceptibility to minority rule. Concretely, utilitarian aggregates
only weakly satisfy the Pigou-Dalton principle, thus do not incentivize equitable redistribution (of wealth or suffering),
and egalitarian aggregates satisfy only weak (i.e., not strict) monotonicity, thus only incentivize gains in the most
disadvantaged group(s). Power-means provide a spectrum of intermediaries, so exactly how tradeoffs should be made may
depend on the application, as well as the culturosocietal values of the social planner and their society. They are also
statistically convenient, as many of our estimation guarantees hold in terms of generic Lipschitz-continuity assumptions,

and thus apply to any power-mean malfare function. Figure[]] illustrates the behavior of the power mean function around

 

the utilitarian p = 1, as well as the egalitarian limits of p > too.

The Malfare of Regret We now discuss regret, which is a property of the hypothesis class H. and per-group distributions
Dig. Intuitively, regret measures the relative dissatisfaction of group i with some h € H, relative to their preferred

outcome h} € H. In particular, we define the (per-group) preferred outcome hj as the model that group i would select

Now suppose p € [1,2]. Then

M,(e;w) < Mo(e; w) PowER-MEAN INEQUALITY

DEFINITION OF Mo(-;w), €

 

 

ALGEBRA

 

+
m m

m vn

2 - wi/ 2vi In 72 2w + vin 22
3m

 

 

 

JENSEN’S INEQUALITY

 

FACTORING

 

 

Finally, for p € [1,00], we have

ax nt 2v; In 22 me 2|v|| , In 22
M,(e;w) < Moo (Ee; w) max + . .

We now show the expectation bounds of items [2] and B} Via theorem [3.5] we seek to bound integrals of the form

 

 

 

rin $
dé ,
3m ;

 

 

 

1 1 ‘UV
E[M-— Al < J, Mp(AEV((m,...,m), 6); w) 46 <f, aus

for the various v as defined above. Note that we will encounter either In 7 or In § terms, depending on whether we

employ 2-tailed or 1-tailed bounds, in items 2] or [3] respectively. The difficult part of these integrals is

f yng dé = $/7g erfc(/Ing) + +VJing INTEGRATION
In(eg) . Vu>1: Vinut+ugVrerfe(VInu) < /In(eu)

Note that the integration is exact, and the looseness of the erfc(-) (complementary error function) bound is quite minuscule,
both multiplicatively and additively, as Vu >1: 0 < \/In(eu) — (v Inu+ us erfc(VIn u)) <1- ae see figure

visualization. Observe also that the logarithmic terms in the fast-decaying summands integrate (improperly) as

 

‘in a5 =1 *néd5 = 1 1=1
J m2a5=1(g) ~ f° msd5 = In(g) +1 =In(69) «

Substituting the values of v derived above into these indefinite integrals then yields the desiderata. In particular, item
directly to bound E].M — AA|], and item

(i.e., one considering single-sided error, rather than absolute error) to upper-bound E[A], paired with the observation

 
 

substitutes into theore: ubstitutes into a directed variant of theorem

    

that, by Jensen’s inequality (as p > 1 power means are convex), M < E[A]. oO

We now show theorem |:

 

which provides regret bounds analogous to the aggregator function bounds of theorem [3

 

Theorem 3.7 (Regret Estimation Bounds). Suppose sentiment function s(-,-): J” x Y + Ro+, per-group probability
distributions D,.,, sample size vector m € Z4, samples (x,y) ~ Df"! x +++ x Dj", failure probability 6 € (0,1), and

additive error bound AEV(...), and let € < AEV(m,6,a,y). Then for all h € H and all monotonic malfare functions

References

Jacob Abernethy, Pranjal Awasthi, Matthdus Kleindessner, Jamie Morgenstern, Chris Russell, and Jie Zhang. Active sampling for
min-max fairness. arXiv preprint arXiv:2006.06879, 2020.

Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 2009.

Ahmed Ashraf, Shehroz Khan, Nikhil Bhagwat, Mallar Chakravarty, and Babak Taati. Learning to unlearn: Building immunity to
dataset bias in medical imaging studies. Machine Learning for Health Workshop at Advances at Neural Information Processing
Systems, 2018.

Kristine Barge, Torbjorn Gundersen, Edmund Henden, and Kjetil Rommetveit. Can medical algorithms be fair? Three ethical
quandaries and one dilemma. BMJ Health & Care Informatics, 29(1), 2022.

Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of
Machine Learning Research, 3(Nov):463-482, 2002.

George Bennett. Probability inequalities for the sum of independent random variables. Journal of the American Statistical
Association, 57(297):33-45, 1962.

Avrim Blum and Thodoris Lykouris. Advancing subgroup fairness via sleeping experts. In Innovations in Theoretical Computer
Science Conference, volume 11, 2020.

Stéphane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence.
Oxford university press, 2013.

Olivier Bousquet. A Bennett concentration inequality and its application to suprema of empirical processes. Comptes Rendus
Mathematique, 334(6):495-500, 2002.

Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In
Conference on fairness, accountability and transparency, pages 77-91. PMLR, 2018.

Jacqueline G Cavazos, P Jonathon Phillips, Carlos D Castillo, and Alice J O’Toole. Accuracy comparison across face recognition
algorithms: Where are we on measuring race bias? IEEE Transactions on Biometrics, Behavior, and Identity Science, 2020.
Irene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? Advances in Neural Information

Processing Systems, 31, 2018.

Cynthia M Cook, John J Howard, Yevgeniy B Sirotin, Jerry L Tipton, and Arun R Vemury. Demographic effects in facial recognition
and their dependence on image acquisition: An evaluation of eleven commercial systems. IEEE Transactions on Biometrics,
Behavior, and Identity Science, 1(1):32-41, 2019.

Cyrus Cousins. An axiomatic theory of provably-fair welfare-centric machine learning. In Advances in Neural Information Processing
Systems, 2021.

Cyrus Cousins and Matteo Riondato. CaDET: Interpretable parametric conditional density estimation with decision trees and
forests. Machine Learning, 108(8):1613-1634, 2019.

Cyrus Cousins and Matteo Riondato. Sharp uniform convergence bounds through empirical centralization. Advances in Neural
Information Processing Systems, 33, 2020.

Cyrus Cousins, Kavosh Asadi, and Michael L. Littman. Fair e3: Efficient welfare-centric fair reinforcement learning. In 5th
Multidisciplinary Conference on Reinforcement Learning and Decision Making. RLDM, 2022.

Hugh Dalton. The measurement of the inequality of incomes. The Economic Journal, 30(119):348-361, 1920.

Gerard Debreu. Topological methods in cardinal utility theory. 1959.

Matthew DeCamp and Charlotta Lindvall. Latent bias and the implementation of artificial intelligence in medicine. Journal of the
American Medical Informatics Association, 27(12):2020-2023, 2020.

M DeGroot. Optimal statistical decisions. McGraw-Hill, New York, 1970.

Luc Devroye, Matthieu Lerasle, Gabor Lugosi, Roberto I Oliveira, et al. Sub-Gaussian mean estimators. The Annals of Statistics,
44(6):2695-2725, 2016.

Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, and Aaron Roth. Minimax group fairness: Algorithms and
experiments. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 66-76, 2021.

Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in
Neural Information Processing Systems, 34, 2021.

Virginie Do and Nicolas Usunier. Optimizing generalized Gini indices for fairness in rankings. In 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval, 2022.

Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Mark D. M. Leiserson. Decoupled classifiers for group-fair and efficient
machine learning. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York,
NY, USA, volume 81 of Proceedings of Machine Learning Research, pages 119-133. PMLR, 2018.

Clare Garvie, Alvaro Bedoya, and Jonathan Frankle. The perpetual line-up. Unregulated police face recognition in America, 2016.

19

Proof. First note th:

                                         

ave
|M —E[M]| < E[/M—M|]
thus all that remains to be shown is
= 1
E[M-Ml] < af |AEV(m,6)|),, 46

By the definition of Lipschitz-continuity, note that for any S,S’, it holds that

    

M(S;w) — M(S‘;w)| < As = S|,

Now, we conclude the desideratum as

E ||M—M{] = \, P((M—M|>e) de PROPERTIES OF EXPECTATION (HAZARD FORMULA)
<f, P(AS— Sil >) de [M-M| <AI]S — Sllau
1 n
= J, inf (« 20: P(AIS—Slly >e) < ) dé INTEGRAL OF INVERSE FORMULA
1
<A] ||AEV(m, 5)]|,, dd - DEFINITION OF AEV(...), LipscHiTz-CONTINUITY
0 M O

 

Corollary 3.6 (Bernstein-Type Malfare Bounds). Suppose as in theorem Bl] and also per-group sample size m

 

(Le., m = (m,...,m)) and p > 1 power-mean malfare function M,(-;w). Now, let variance proxy v be define
in three cases as v = Mi,(v;w) = (7 wile)” for p = 1, v = w-v for p € (1,2), or v = |lv||,, for p > 2.
Then for all 6 € (0,1), we have

29 29
rin 2uln 3

3m m

2) E [ax - A|| < < Tin@eg) / 2v In(2eg) . &
3m m

r In(eg) 2v In(eg)
TONED ny PUES,
3m m

<6;

 

 

 

3) M<E[M] < M+

Proof. We first show the tail bounds (item [ip, which we then use to show the expectation bounds (items 2]and In

[3.1]item Pj to the singleton function family H = {h} (thus |H| = 1), with a union bound

 

particular, we apply theorem

 

over all g groups to bound per-group confidence radii, i.e.,

rIn 22 2Vp, [so h] 1 r In 22 2v; In 22
ei + sup |
7m hen me

 

 

 

all other cases to these three via monotonicity.
We first show the case of p = 1. Observe that

 

 

 

 

1
Mi(e;w) = So wi = . DEFINITION OF M,(-;w), €
om
#1
rin 22 2 2In 22 rn 22 2 Iw; i) In 22
= aon + Sowiver a aaa + 2 eer ives) Toe ALGEBRA

24

 

   

—1. M,(1,2)3)
vrs Magoo (1, 2,3)

Mi; =2

 

 

 

 

| | | | |
—20 —10 0 10 20

 

 

(a) Natural axes plot of the power mean, with p € [—25,25]. (b) Sigmoidally transformed plot of the power mean. The a-axis
Behavior around p = 0 is clear, and behavior as p — +o is is transformed by the tangent function to emphasize behavior
suggested (but not confirmed) by observing extreme values of p. around p ~ 0, as well as for p — +oo. Minor ticks are linearly
The utilitarian and geometric welfare are also visually depicted. spaced on the [—1, 1] region, and powers of 2 outside this region.

Figure 1: Plots of the unweighted power mean of (dis)utility values (1,2,3), as a function of the power p. Observe
that the limits as p > +00 are the minimum and maximum, and these and other significant values are marked on the
plots. Figure|la]presents a natural axes plot, which necessarily is limited to a finite region of p, and figure [Ib] presents a
sigmoidally transformed plot, which shows the entire spectrum of p € [—00, ox].

 

for themselves, i.e.,

hj =argmin E[6oh] or hj =argmax Efuch] , (2)
hen Di hen Pi

for loss or utility, respectively, and we let S; denote the optimal expected sentiment for group i, i.e., S} = Ep, [so hij].

We now formally define the regret of group i on some outcome or model h € H as

 

Reg;(h) = Eléo h]-—Sj , Reg,;(h) = Sj - E [uch] , or generically, Reg;(h) = |E[s oh|-S} (3)

Intuitively Reg,(h) is nonnegative by construction (hence the absolute value in the generic form), and it quantifies the

amount by which group i prefers their optimal h; to h.

Several authors {Blum and Lykouris Rothblum and Yona

the selected h, and the statistical and computational questions that arise are studied under the umbrella of “multi-group

minimize the worst-case (over groups) regret of

   

agnostic PAC learning.” We generalize this notion, optimizing not just worst-case (i.e., egalitarian), but arbitrary malfare
functions, of per-group regret values, which allows for greater flexibility and resistance to the usual issues of egalitarian
malfare. In particular, we seek

h* = argmin M (i+ Reg;(h); w) = argmin M (i +4|E[soh] —S}| ;w) 5 (4)

hen heH Di

Curiously, since we seek to measure overall regret, and regret is a nonnegative quantity with negative connotation, we
always summarize it with a malfare function M(-;w), even when we began with a utility function. Intuitively, this is
because we can never hope to select a shared function h that group 7 prefers to hj, thus excess dissatisfaction is always

positive in both the loss and utility cases. In some sense, the malfare of regret thus measures the price of sharing in a
society, as the shared model his naturally compared |Dwork et al.

 

to letting each group select their own model hy.

Why Consider the Malfare of Regret? Previous work summarizes regret across groups by taking the largest regret
amongst them. This is analogous to game-theoretic regret (i.e., the maximum over agents of utility differences between

adjacent profiles), but even there, any malfare function could reasonably aggregate per-group regret values. We argue

rn 22 2u In 28

 

 

MM > a) es.

DP | pee Al > 3m ™m <6;

2) e [lms] < Tea) , [Benen ;
3m m

3) M< EA) < m4 Dine), /2etnles)
3m m

Estimating the Malfare of Regret Regret is difficult to bound, as it depends both on the expected sentiment of the

selected h, and also on H through the (unknown) per-group optimal sentiments Sj.,. We thus introduce the estimators

S:= inf E [oh], or S;=sup_ E [uch] , (12)
hEH @ 5: ,Yi,: hEH Fi, Vi

for loss or utility (note that these are downward and upward biased estimates), respectively, cf. By analogy with

   

the plug-in estimator for the regret malfare minimizer is then

h= argmin M [it
hen

E [soh]—S;

Bi Ve

jw). (13)

 

 

The following theorem bounds the difference between the true and empirical malfare of regret.

Theorem 3.7 (Regret Estimation Bounds). Suppose sentiment function s(-,-): J” x Y + Ro+, per-group probability
distributions D;.,, sample size vector m € Z%, samples (a, y) ~ D7"! x --- x Dj", failure probability 6 € (0,1), and
additive error bound AEV(...), and let € + AEV(m,6,a,y). Then for all h € H and all monotonic malfare functions
M(-;w), it holds with probability at least 1 — 6 over a, y, and é that

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

M tS OVIE[soh|—Si —2é;;w)/<Mlivn}] E [soh]|—S|;w] <M tecNElsoh]-S +2€;;w], &
a Bi, Vi, a
(14)
TRUE REGRET MALFARE LB PLuG-IN REGRET MALFARE ‘TRUE REGRET MALFARE UB
Mis Ov] E [soh]—S,|—2é;w] < Mio Bieoal-si| im <MlivncA| E [soh]—S,|+2éi;w
Bi Vi: Dy Bi, Vi:
(15)
—_—__
LCB Esrimare ‘TRUE REGRET MALPARE UCB Estimate
thus if M(-; w) is \-Lipschitz-continuous w.r.t. some norm ||-|| 4, we have
Is isn] E [soh]—SijJ;w] —M[ is] E[soh]—Si];w]] <2d]élly - (16)
Bi, Vi: Dy
PLuG-IN REGRET MALFARE TRUE REGRET MALFARE

Note that similar bounds on the expectation of the regret plug-in estimator can be shown along the lines of corollary

 

mutatis mutandis for regret. Note also that theorem

 

up to a 2-factor attached to the confidence

radius, thus in some sense regret is “about twice as difficult” to estimate as malfare or welfare.

3.3 Information Asymmetry and Where Best to Sample

An intuitive notion of fairness would suggest that we should draw equally-sized samples for each group, or perhaps
samples proportional to population frequencies. If the goal is to optimize or bound welfare, malfare, or regret, such
intuitive notions should be rejected, as they are critically flawed. We now discuss the ways in which samples drawn from
one group or another may be more or less valuable to for the purposes of estimating or optimizing these objectives.

As a brief thought experiment, suppose we want to estimate the egalitarian welfare of a population consisting of two
groups. Suppose also that their utilities are similarly difficult to estimate, and their expected utilities are (1,10). In

such a setting, nearly all sampling effort should be invested in estimating the utility of group one, as once group two

11

Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge University
Press, 2014.

Shubhanshu Shekhar, Greg Fields, Mohammad Ghavamzadeh, and Tara Javidi. Adaptive sampling for minimax fair classification.
Advances in Neural Information Processing Systems, 34, 2021.

Umer Siddique, Paul Weng, and Matthieu Zimmer. Learning fair policies in multi-objective (deep) reinforcement learning with
average and discounted rewards. In International Conference on Machine Learning, pages 8905-8915. PMLR, 2020.

Till Speicher, Hoda Heidari, Nina Grgi¢é-Hlaéa, Krishna P Gummadi, Adish Singla, Adrian Weller, and Muhammad Bilal Zafar.
A unified approach to quantifying algorithmic unfairness: Measuring individual & group unfairness via inequality indices. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2239-2248,
2018.

Vladimir Vapnik and Aleksei Chervonenkis. The uniform convergence of frequencies of the appearance of events to their probabilities.
In Doklady Akademii Nauk, volume 181, pages 781-783. Russian Academy of Sciences, 1968.

Vladimir Vapnik and Aleksei Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities.
Theory of Probability and its Applications, 16(2):264—281, 1971.

Enrique Areyan Viqueira, Cyrus Cousins, and Amy Greenwald. Improved algorithms for learning equilibria in simulation-based
games. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, pages 79-87, 2020.

21

Theorem 4.6 (Braided PS Guarantees). Suppose (h, jt, é, M*+) — BrawepPSLoss(H, £(-,-),D, AES(...), 8,6,¢, M(-; w), Rec),
M(S; w) is continuous and strictly monotonic in S with (possibly infinite) Lipschitz constant Am w.r.t. |\-||,,, and the
schedules (s, 6) are IO sina Uniformly-convergent w.r.t. ll: and the additive error vector bound AEV(m, 6, x,y) <—
(AES(mi, £,a1,y1),...,AES(m
objective value of the optimal h* (see theorem
1) |A-pl<é<e; & 2) M* <p SS ptesM* +42.

; 5,@9,Yq))- Now take pz to be the true objective value of h and pL” to be the true

 

 

Then, with probability at least 1 — 5, we have

Proof. Proof of theorem

 

is quite similar to that of theorem

 

The primary difference is that the union bound is now
over g individual schedules and all timesteps, i.e., we now have the total failure probability of all possible tail bounds

(taken on line is no greater than

 

t

a|>
eI

Sy sala =0.
i=1 t=1 tI

Note that while the order these bounds are taken is random, the braided structure takes no more than one bound for
each (i, ti) pair consisting of a group i € Z and a time index ti € Z;, and said bounds do not depend on the order in
which they are taken. As the bounds themselves are always taken at the same sample sizes, we need only sum over failure
probabilities of all possible tail bounds that may be taken by the algorithm to union-bound the total failure probability.
As in the proof of the linear algorithm, we conclude that all tail bounds that could possibly be taken by the algorithm
(now over all groups i € Z and all timesteps t; € Z,) hold simultaneously with probability at least 1 — 6.

We now condition on the event that all bounds taken are correct (which holds with probability at least 1 — 4). In this
case, as in the linear algorithm, so long as the braided algorithm terminates, it produces a correct answer. The reasoning
here is identical to that of theorem [4.5] as both algorithms share a termination condition.

Despite their similarity, it is not so straightforward as in the linear algorithm to show that the braided algorithm
the

appropriate uniformly-convergent schedule is sufficient to guarantee termination eventually, this would not be so in

is guaranteed to terminate (i.e., it does not loop indefinitely). In particular, consider that while in algorithm

 

algorithm P| if it were to select groups arbitrarily at each iteration, e.g., sampling from the same group at every iteration
could loop indefinitely. However, this does not occur, due to the group selection logic on line [16] We consider first the
0-uniformly convergent schedule case, and then the general case, concluding eventual termination in both.

We first show that given a 0-uniformly convergent schedule, the algorithm will not loop indefinitely without sampling
each group an infinite number of times, thus if the algorithm does not first terminate, any per-group time index vector t
will eventually be exceeded (componentwise). To conclude this, it is sufficient to show that for each group i, the algorithm
will either iterate until it either terminates, or group 7 is selected. This is because group i will eventually have the optimal

improvement:cost ratio, and thus be selected to sample on line{16| which we now show. Consider that, by the strict

 

monotonicity assumption, there is always nonzero (positive) projected improvement gain to improving the error bound é;,
and thus to selecting any group 7 to sample. However, selecting other groups ad nauseam will take their cost terms to oo,
and thus their improvement:cost ratios to 0 (in the limit). It thus follows that, if the termination condition were never
met, each group 7 would be sampled from an unbounded number of times, during which their error bound would converge
as €; ~ 0, and thus é » 0 by the continuity assumption. Consequently, it holds that I M(h*) and M** ~ M(h"*),

which implies termination on line

 

The case of an Ine -||,y-uniformly-convergent schedule follows similarly, except now groups may cease sampling

EG
if their error bounds are nonzero, but sufficiently small so as to ¢-estimate the objective. If this holds for all groups
(which again happens eventually if termination does not occur first), we again meet the termination condition of line [12]

hence we conclude guaranteed termination in this case.

With termination shown in all cases, we now conclude that algorithm |2} produces a correct answer with the stated

 

 

 

probability, by the same reasoning as in the analysis of algorithm

 

e., theorem

 

B.A Traveller’s Handbook to Progressive Sampling

In this appendix, we provide deeper intuition for our progressive sampling algorithms, with an emphasis on how they differ

from standard progressive sampling approaches. We also describe how decisions are made as to which group to sample in

30

the braided algorithm (algorithm e., under the conditional sampling model. Note that this material is provided

 

purely to supplement understanding of the methods; all proofs related to these algorithms are detailed in appendix

 

B.1 Tricks of the Trade: the Magician’s Secrets, Revealed

Understanding progressive sampling algorithms essentially comes down to understanding how union bounds are taken,
and which tail bounds may be taken and when, since correcting for the multiple comparisons problem is the central
technical issue in such methods (e.g., an algorithm that computes bounds after every sample while using a union bound

is extremely inefficient, because of the excessive union bound cost). For the most part, algorithms

 

progressive sampling methods, however there are some subtle details that obscure the simple reasoning at their core.
In particular, under the mixture sampling model, sample sizes are not always known a priori, and in the conditional
sampling model, decisions are actually made dynamically that determine the order in which tail bounds are taken. Both
of these decisions have ramifications that impact the core logic of a static alternating series of sampling and bounding
steps, but as always, there is no real magic here: merely a few logical flourishes that permit these slight modifications to
the standard flow of progressive sampling algorithms.

Under the joint sampling model, the linear progressive sampling algorithm is quite straightforward, as the total
number of samples drawn b each timestep is completely known a priori (i.e., 8, from each group at timestep t), so a
simple union bound over a deterministic schedule suffices. Under the mixture sampling model, there is some subtlety to
why the algorithm works as it does, since the sample sizes at which tests are run are in fact a random variable dependent
on the order in which groups are sampled. A naive approach would be to consider a union bound over possible orders of
sampling, or otherwise correct for the fact that multiple outcomes are possible, however this is ultimately not necessary,
and such approaches would induce harmful corrective terms to probabilistic error-bounds and sample complexities. The

simple trick we employ is to condition the algorithm on the infinite sequence of Z samples drawn on line and establish

 

the schedule after performing this conditioning operation, thus samples drawn under the mixture sampling model are in
fact samples from individual groups in a known order. Of course, we don’t “really” know this order, nor do we perform
this conditioning anywhere in the algorithm; this is an analytical technique, and the simple fact of its existence suffices to
show correctness. This analysis does not actually require a randomized order of Z samples, and in fact the algorithm
works even if group identities are adversarially selected, as nowhere in the analysis do we actually assume sampled group

identities (line

 

are random. However, it must be true under adversarial Z selection that each (4’, Y) pair drawn is
conditionally independently given each group in the set Z, thus the adversarial mixture sampling analysis is most useful
in the context of mutually exclusive groups (i.e., singleton z samples).

Because of these design decisions, all three algorithms are able to use simple progressive sampling parameters, such as
a single sampling schedule s and failure probability schedule 6, albeit in slightly different ways. They also enjoy the
characteristic sharpness of standard progressive sampling algorithms, with only an extra $ factor attached to failure
probabilities (6 values) for the conditional sampling model, to accommodate the union bound over g simultaneous

(braided) linear progressive sampling instances.

B.2 Selecting Where to Sample in the Braided Algorithm

We now discuss how the decision as to which group to sample from is made. Our reasoning here largely parallels that of

8] we want to maximize the improvement made to the UCB-optimal h. However, rather than apply the linear

 

subderivative approximation, which is accurate for smallchanges (e.g., adding a single sample), we consider the impact of
advancing the sampling schedule of each group by one timestep; for geometric schedules, this is a multiplicative — rather
than an additive — change to the sample size. We must also include the cost of sampling when selecting where to sample,
so this too enters the equation through the linear cost model C}.,. Intuitively, the idea is essentially to select the group i
for which the ratio of projected improvement to cost is maximized.

Unfortunately, there is a slight wrinkle in the algorithm: at each iteration, we can not simply greedily maximize the
improvement:cost ratio from a single timestep of sampling group 7, because this leads into a failure mode, wherein if a
group’s risk is near c, i.e., near maximal, it will never be sampled. There is no simple workaround, as the malfare function

may not even be defined for inputs larger than c (for instance if AA(S; w) is actually the function ec — Wo(c — S; w), i.e.,

31

 

Algorithm 1 Fair Learning with Linear Progressive Sampling under the Joint and Mixture Sampling Models

1: procedure LINEARPSLoss(H, f(-,-),D, AEV(...),8,6,¢, M(-;w), REG) > (h, ji, 2, M*4)
2: input: Hypothesis class H C X — Y’, loss function &(-,-) : ¥’ x Y > [0,¢], joint or mixture distribution D, additive error vector bound

 

    

 

 

AEV(m, 6,a@,y), schedule s € Z° and 6 € [0,1)*, confidence radius ¢, weighted malfare M(-;w), and Boolean REG
3: output: Empirically UCB-optimal h, empirical malfare estimate ji, confidence radius é, and lower bound on minimal malfare M**
45 Micg — 0; Lig (), caitig ())s Yirg — (), chen ()) > Initialize per-group sample counts, empty per-group sample lists
5: fort€1,2,...do > Progressive sampling timesteps
: if D is joint sampler then
Tr: (@1:g,91-141:8¢, Yiigay1tiiar) ~ D884; Vie Zi mi & 8 > Sample from joint distribution (assume so = 0)
8: else if D is mixture sampler then
9: while min; m; < s; do
10: (z,y,z)~D > Draw X x Y x 2 triplet (domain, codomain, groups)
1: Wiez: m; +m; +1; (@im,,Yim;) « («7 y) > Increment counts and store samples for each group i associated with (, y)
12: end while
end if
€1.g — (1+ rec) AEV(m, 61, @, y) > Bound additive error of per-group supremum deviations (w.h.p.)
iw «Wie 2: Se (int, E [Eo h)) if ReG else 0 > Set regret baseline of per-group minimal empirical risks (or 0 if 7REG)
we he argmin M(irecA ' E [oh] — Sibu w) > Compute UCB-optimal h
iM © inf Mi OV E [€oh]-S -&; w) > Lower-bound optimal M*
hen Bi Yi,

ws (AV AT) (anti HOV E oh|-S&—- éi;w), M(i eA E (oh]-—S 4 ew) > LCB and UCB on h (regret) malfare
Bia, BisYi,

 

19: if AA’ < M* +26 then > Check if desired error guarantee is met (termination condition)
20: (4,8) (20° a5 MM’), can" ee wm) > Symmetric estimate ji and confidence radius é of (regret) malfare of h
21: return (h, ft, é, Mv") > Return UCB-optimal h, é-estimate of A\(-;w), and lower-bound on optimal malfare A\**

22: end if
23: end for

 

nd procedure .

25: procedure LINEARPSUTILITY(H, u(-,-),D, AEV(...), 8,6,¢, M(-; w), REG) > (h, fi, 2, M*")

26: input: Utility function u(.,-) : Y’ x Y > [0,¢], weighted aggregator function M(-; w) (malfare if REG, otherwise welfare), see lin
27: output: Empirically LCB-optimal h, empirical welfare ji, confidence radius é, and UB on maximal welfare M** (or similar for regret)
(h, fi, 2M") — LinrARPSLoss(H, ¢ — u(-,:),D, AEV(...), 8,6, €, (2I ree — 1)M(Si 4 ¢— Si;w),REG) > Negate to flip inf and sup
return (A, r — fi,é, (21peq — 1)M**)

nd procedure

procedure LINEARPSEsTIMATE(h, s(-,-),D, AEV(...), 8,6,¢, M(-; w)) > (
input: Hypothesis h(-) : ¥ — Y’, sentiment function s : (Y’ x Y) > [0,
output: Empirical aggregator function estimate fi and confidence radius é

(_, #,€, __) + LinearPSLoss({h},s(-,-),D, AEV(...), 8,6,¢, M(-; w), FALSE) > Estimate the malfare or welfare of so h

  
   

    

)

eighted aggregator function M(-; w), see line’

  
 

   

2 return (/i,é)
36: end procedure

 

4.2 The Linear Progressive Sampling Algorithm

is quite simple. At timestep t = 1, we guess that a sample of size 81

 

The core of linear progressive sampling (algorithm
for joint sampling,
. if

erminates, otherwise, our guess was incorrect, so we

 

for all groups will be sufficient to <-d optimize the objective, we draw at least such a sample (lin
or lines [9]{12] for mixture sampling), compute tail bounds (lin , then determine the UCB-optimal h (line
our bounds indicate that h is provably near-optimal, algorithm
increment t, draw at least s, samples (per-group), and repeat. The basic principle is quite flexible, so algorithm [I] can

maximize welfare or minimize malfare of risk or regret via the LINEARPSLoss(...) and LINEARPSUTILITY(...) routines.

  
   

Furthermore, in addition to learning and optimization tasks over some class H, algorithm [I]can be applied to estimation
tasks: given a single function h, it can estimate the malfare or welfare of so h via the LINEARPSEsTIMATE(...) routine.
Theorem [4.5]shows that algorithm [I] learns an optimal h € H to within user-specified ¢-d additive error. We require

only monotonicity (axiom [Ip and continuity (axiom |i

 

of M(-;w), though the power-mean malfare family is convenient,
NB this
result generalizes to welfare objectives, mutatis mutandis (flipping infima and suprema), via the negation reduction of
LINEARPSUTILITY(...), i-c., lines [25}[30] and to function estimation via LINEARPSEsTIMATE(. .. ).

 
 

as Lipschitz-continuity (theorem item[3} permits efficient ¢-uniformly-convergent schedules (definition

 

Theorem 4.5 (Linear PS Guarantees). Suppose (h, fi, é, M**) —LinearPSLoss(H, &(-,-),D, AEV(...), 8,6,€, M(-; w), REG),

16

sample for some group 7.

Property 3.8 (Incremental Gain of Sampling). Suppose w-weighted power-mean malfare M,(-;w), sample (x, y) with
group sample sizes m1:,, and let x’, y’ extend x, y to sample sizes m’, where m’ = m+ 1i, i.e., group i has one additional
sample. Now, let é + AEV(m, 6, x, y) and é + AEV(m’, 6,2’, y’), and take h = argminyc7,Mp (i ad Be, iui: [Coh] +é; w),

M=M, (i ed Ee, oy [£0 iw), A =M,(i red Ee, x: [eo Al+éiw), and A\' = infren Mp Dad Ey wy, [f° h]+éiw).

Then the incremental impact of sampling from group i on the UCB is approximately

x Bn on pel A “ 1
AN mt we 2 (Se [eoh| tay wy EiWi (= Vi, en : (17)

~ Imi + 3 Mw 2m; M

 

Proof. We first assume that for each h € H, i € Z, it holds that Ee, wi, (lo h] & Ey yi [£0 hj, and we use this
approximation throughout. The result then follows directly from three observations.

p-1 ~
First, note that ao Mp (S;w) = Maw thus for any UCB-optimal h, we have the subderivti |
i MP yw

a a —1
é 4 . Es, y: l2oh]+é\"
Ooh ane M, (- E (of +s) 3 wi (aS vislle }
Osub€i hEH Bi Vins M

Now, assuming © asymptotic behavior of €;, observe that

 

 

 

awe Mi ig (@ -e) e: (1 mi ~ & , &
ie i i) Ei mi +1 ~Omi+ 3 Im; ’

and note that this approximation is quite sharp (see figure |A‘

 

as

1 u 1
Vu>1: <1-,/—— < ——_.. 22
a= 2u+3— u+l~ Qu+vV2 (22)

Finally, using the subderivative approximation, the impact of the additional sample on the malfare is approximately

 

ot at ~ 2, Osub . ‘ o yo

5 Mi Ee,., 4 pl +é
REE (- ) ty (Sanat -
m+1

 

FINITE DIFFERENCE APPROXIMATION
éi E [€oh]|e E [€oh]
i ehett

 

aYa,

 

SEE ABOVE

 

 

x a —1
ei Be.,.yz, [00h] + é \"

ww! ay, Boca [60 ] + & SEE ABOVE

2mi tg nw

. x ~)\ pol 1 ~ 1
_, SiWi (= vio ") mtOC) ~ mi
2 SS : Siti Si

2mi M M, (Stew) ~ M, (Siw)

 

 

A.2 Uniformly-Convergent Sampling Schedules and Progressive Sampling Guarantees

In this subsection, we show all results relating to sampling schedules and progressive sampling. Before showing the

correctness of algorithms[1] and [2] we begin with the uniformly-convergent schedule analysis of lemma [4.4]

Lemma 4.4 (Sufficient Conditions for Uniformly-Convergent Geometric Schedules). Suppose as in definition and

  

assume also that
sup

(w,y)E(XXY)ETXI

 

 

AEV ((sr,..., 87), ,y)|| <e. (19)

M

Alo

Then the geometric-uniform schedule (s, 6) is e-uniformly-convergent.

 

°Technically, infrew Mp (i + Bo; vu; [0 Al + ew) is not always concave in é, thus this quantity is not always a subderivative, but
it is at least a reasonable linear approximation. Furthermore, a sufficient condition for convexity is for Mp (i 4 Ee, ,.y;,[60h] + éi; w),

which is always convex in é, to also be convex in the space of h € 2. (as is quite common in machine learning and optimization contexts).

27
