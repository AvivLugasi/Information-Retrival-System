Objectives

The data producer computes the representation
given the fairness criteria and input data.

There are a variety of methods for representation
learning with individual fairness or group fairness
constraints, which, in turn, can be label
(in-)dependent.

e = =Inputs:. x, Y
o ©Data
o Fairness criteria v(f, Y|S)
Alternatively e(X:, X;)
e Output:
o Learned representation

g:X,YHOZ

 

Google Al
gy Brain Team

Data
Producer

Computes the fair
representation given data
regulator criteria

e INPUT: Fairness criteria
e OUTPUT: Representation

P26

Google Al
gy Brain Team

Some tradeoffs when comparing algorithmic fairness approaches

Ease of Scalability Ease of auditing Fairness / Generalization
implementation Performance
and (re-)use tradeoff

Pre-processing, e.g.,
representation
learning

Post-processing,
e.g., threshold
adjustment

In-processing, i.e.,
joint learning and
fairness regulation

 

P59

Google Al
gy Brain Team

Group fairness/performance tradeoff on Adult dataset

 

 

  

 

 

 

 

 

 

0.85 a 0.850 * LAFTR-DP a
wre LAFTR-EO
0.848 * LAFTR-EOpp
0.84 @ MLP-Unfair
= >, 0.846
@ 0.83 3
= 5 0.844
Oo Oo
9 9
< 0.82 <t 0.842
meee LAFTR-DP
‘ s**) LAFTR-EO
0.81 s ++ © LAFTR-EOpp 820
. see) DP-CE
@  MLP-Unfair 0.838
0.80

0.025 0.050 0.075 0.100 0.125 0.150 0.175
DP

(a) Tradeoff between accuracy and Anp

0.02 0.04 0.06 0.08 0.10 012 014 0.16
Aro

(b) Tradeoff between accuracy and Aro

Flexibly Fair Representation Learning by Disentanglement,

Madras et. al., ICML 2018

 

0.850 * LAFTR-DP a
* LAFTR-EO
0.848 | mmm LAFTR-EOpp
| @ MLP-Unfair
0.846
Bout
Vv
© 0.844
a
3 0.842! |
~ sol
0.840 |
0.838 5
0.836 [4

 

 

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08
Aropp

(c) Tradeoff between accuracy and Azopp

Pal

(Im-)possibility Results for
Group-Fair Classification

Classifier statistics are not arbitrarily
flexible!

E.g. binary classification statistics have
two degrees of freedom,

thus can match at most two
independent statistics across groups
(c.f. Kleinberg et. al., 2017;
Chouldechova, 2017)

Beyond binary classification, the degrees
of freedom grow quadratically with
number of classes

Cisse and Koyejo (2020, In Prep)

Google Al
gy Brain Team

 

Figure showing number of classes vs.
degrees of freedom

More independent constraints can be
enforced when there are more classes.

Google Al
gy Brain Team

References

e Cisse M., Koyejo, S., 2020. Representation learning and fairness. In Prep.

e Chouldechova, A., 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2),
pp.153-163.

e Creager, E., Madras, D., Jacobsen, J.H., Weis, M., Swersky, K., Pitassi, T. and Zemel, R., 2019, May. Flexibly Fair Representation
Learning by Disentanglement. In /nternational Conference on Machine Learning (pp. 1436-1445).

e Dwork, C., Hardt, M., Pitassi, T., Reingold, O. and Zemel, R., 2012, January. Fairness through awareness. In Proceedings of the 3rd
innovations in theoretical computer science conference (pp. 214-226). ACM.

° Goodhart, C.A., 1984. Problems of monetary management: the UK experience. In Monetary Theory and Practice (pp. 91-121). Palgrave,
London.

° Hardt, M., Price, E. and Srebro, N., 2016. Equality of opportunity in supervised learning. In Advances in neural information processing
systems (pp. 3315-3323).

e Hiranandani, G., Boodaghians, S., Mehta, R. and Koyejo, O.0., 2019. Multiclass Performance Metric Elicitation. In Advances in Neural
Information Processing Systems (pp. 9351-9360).

e Ilvento, C., 2019. Metric Learning for Individual Fairness. arXiv preprint arXiv:1906.00250.

e Jung, C., Kearns, M., Neel, S., Roth, A., Stapleton, L. and Wu, Z.S., 2019. Eliciting and Enforcing Subjective Individual Fairness. arXiv
preprint arXiv:1905. 10660.

e Kearns, M., Neel, S., Roth, A. and Wu, Z.S., 2018, July. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup
Fairness. In International Conference on Machine Learning (pp. 2569-2577).

e Kleinberg, J., Mullainathan, S. and Raghavan, M., 2017. Inherent Trade-Offs in the Fair Determination of Risk Scores. In 8th Innovations
in Theoretical Computer Science Conference (ITCS 2017) (Vol. 67, p. 43). Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik.

P69

Google Al
gy Brain Team

A Framework for Fair
Representation Learning

 

88 Google Al

Google Al
gy Brain Team

An Adversarial Approach for Learning Fair Representations

e Representation: g:X,YwHZ
Adversarially trained neural network autoencoder

e Prediction: f:ZHY
Feedforward neural networks

e Fairness Measure: v(f, Y|S)
Statistical parity, equal opportunity, equalized odds
Specialized adversary loss functions for each fairness measure

Provide bounds on the fairness violation of any subsequent classifier using
the learned representation

Flexibly Fair Representation Learning by Disentanglement,
Madras et. al., ICML 2018

Google Al
gy Brain Team

Generative Adversarial Representations

“GAP leverages recent advancements in adversarial learning to allow a data holder to learn
universal representations that decouple a set of sensitive attributes from the rest of the
dataset”

  
   
 
   

  

Distortion

Constraint

Raw data i
Generative Adversary
Decorrelator

- - Deep Neural Net
ae Private/fair

representation

  
   

Noise

Adversarial loss

Learning Generative Adversarial RePresentations (GAP) under Fairness and Censoring Constraints
Liao et. al., 2019 P43

Google Al
gy Brain Team

Alternative approaches for implementing algorithmic fairness

Pre-processing

Representation learning,
feature adjustment,
metric learning

Individual fairness, e.g.,

Cisse et. al., 2020
Group fairness, e.g.,
Zemel et. al, 2013

In-processing

Joint learning and
fairness regulation

Individual fairness, e.g.,

Dwork et. al., 2012
Group fairness, e.g.,
Hardt et. al., 2016

 

Post-processing

Adjustment of potentially
unfair model e.g.
threshold adjustment

Individual fairness:
Dwork et. al., 2012
Group fairness e.g.,
Hardt et. al., 2016

Google Al
gy Brain Team

Back to the story ...

 

88 Google Al

Google Al
gy Brain Team

Group Fairness

Data Regulator

 

88 Google Al

GS Saree,
A manager oversees several teams, all are using the same data to build

predictive models for different products. The manager seeks to ensure both
fairness and accuracy across the products.

Each team is solving a different prediction task.
There is no company policy on fairness, thus no shared guidelines.

e Team alpha is fully focused on accuracy, but is oblivious (neighbors say they are
apathetic) about fairness issues.

e Team beta, team nu and team gamma are all interested in fairness. Each team is
really excited to implement this and has read the literature, but each team has
selected different fairness definitions.

e Team zeta would like to improve the fairness of their predictions, but has no
idea how to incorporate or measure fairness.

e The manager has decided to independently verify that all released products
are fair Per

Google Al
gy Brain Team

Pros of incorporating fairness using representation learning

e Often much more efficient than alternatives, especially with re-use
e Can be employed when the data user is untrusted, or apathetic about

fairness
o Data user is (mostly relieved) of the burden of directly reasoning about
fairness

e Inherits other good properties from representation learning
o  Interpretability (in some cases)
o Transportability (across datasets, institutions, ...)
o Some robustness properties, some (weak) privacy properties
e Audits can be much more efficient (especially when only auditing the
representation)

Google Al
gy Brain Team

Individual Fairness: Advantages and Challenges

Advantages:

e Intuitive and easy to explain to the data producer (and to non-experts)

e Individual fairness implies generalization (c.f. Xu & Mannor, 2012)

e Individual fairness implies statistical parity given regularity conditions
(Dwork et al., 2012)

Challenges:

e Regulator must provide a metric or a set of examples to be treated similarly.
Constructing a metric requires significant domain expertise and human insight.

e Fairness of the representation heavily depends on the quality of the metric
chosen by the regulator.

e Optimizing and measuring individual fairness is generally more computationally
expensive than other measures

Google Al
gy Brain Team

The Data Regulator: Measuring (Un-)fairness

e Regulator must choose how to measure (un-)fairness
o For individual fairness: must choose the distance metric
o For group fairness: must choose the classifier statistics to equalize

e However, remember that there are no magic metrics or measures;
Measurement 101: all measures have blind spots

“When a measure becomes a target, it ceases to be a good measure.”

e For ML, we generally specify all measures apriori and optimize them

e However, all metrics will have failure cases, i.e., unusual situations with
non-ideal behavior

e One productive approach is to select measures that best capture tradeoffs
relevant to the context

Goodhart’s Law; Goodhart, 1981; P22
Strathern, 1997

Google Al
gy Brain Team

Data Producer

—

Representation

Learning /

 

 

 

Google Al
gy Brain Team

Data User

ML Model

 

 

Metric Elicitation

Determine the ideal
evaluation metric by
interacting with users,
experts (Hiranandani et.
al., 2019).

Ongoing extension to
eliciting group fairness
metrics.

Complementary work on
eliciting distance metrics
for individual fairness
(IlvVento, 2019; Jung et. al.,
2019)

Google Al
gy Brain Team

Figure from Hiranandani et. al (NeurlPS 2019)

  
    
 
   
 
   
  

 
 

Classifier A vs Classifier B

Elicitation Procedure | (confusion Matrix A vs Confusion Matrix B)
Constructs queries to

elicit metrics using
minimal feedback
Relative Preference Feedback

Output

Standard Oracle’s
Classification Performance
Metric

Classifiers
(Confusion Matrices)

Data

Poster # 226; Wednesday

P23

Google Al
gy Brain Team

Cons of incorporating fairness using representation learning

e Less precise control of fairness/performance tradeoff
o Should expect worse fairness/performance tradeoff than joint training
o See the “cost of fairness in representation learning”
(McNamara et. al., 2019)

e May lead to fairness overconfidence
o Data user may act adversarially when optimizing for the performance
metric of interest
o Data user can still violate fairness, e.g., by violating data agreement

e Startup costs can be high:
o Representation learning can be expensive, especially with multiple
fairness constraints

One of the key tasks of the data regulator is
determining the fairness criteria

The most common algorithmic fairness criteria are
individual fairness and group fairness...

Google Al
gy Brain Team

Individual Fairness: Similar individuals treated similarly

Data Regulator: Which individuals are similar?
equiv., which individuals should be treated similarly?

One approach:

e Define a partition of the space into disjoint cells
such that similar individuals are in the same cell.

e Individuals in the same cell should be treated
similarly even if they are apparently different
(e.g. dots with different colored attributes).

 

Data Regulator

 

 

Sensitive See

p
attribute S || stat. v(f, vis)

 

 

 

px, X,) Fairness
on Criteria _/

EEE

 

 

 

 

 

 

 

Data 1)

 

Data Producer

giX,YHZ

Representation
Learning

 

 

Google Al
gy Brain Team

Data User

ML Model

 

Individual Fairness:
Metric Learning Approach

Regulator (to the data producer):
Provides sets of examples which should be
treated similarly (e.g., similarly labeled points).

Producer: Learns a distance metric such that
individuals which should be treated similarly are
closer to each other.

Find a metric p such that V(x, 22, £3):
%1,%2 € C;, and x3 € Gyy - i)
=> p(x1, 22) < p(x1, x3)

Cisse and Koyejo (2020, In Prep)

Google Al
gy Brain Team

© Similarly labeled
BB Differently labeled
BB Differently labeled

Image Source: Weinberger and Saul, 2009.

 

P29

 

 

 

 

 

 

 

 

 

 

 

Distortion 0 0.003 | 0.0045 | 0.005 | 0.006 | 0.007 | 0.008 | 0.01
Apdemp(white) | 0.061 | 0.055 | 0.04 0.03 | 0.03 | 0.02 | 0.02 | 0.01
Apdemp(black) | 0.109 | 0.021 | 0.02 0.05 | 0.03 | 0.05 | 0.03 | 0.03
Apbemp(Asian) | 0.14 | 0.082 | 0.07 0.07 | 0.06 | 0.07 | 0.06 | 0.03
Apemp (Indian) | 0.031 | 0.006 | 0.01 0 0.01 0 0.01 | 0.01

 

a

Google Al
Brain Team

Table 6: The demographic parity fairness (indicated by Apemp(-)) of ethnicity classification on the

UTKFace dataset.

Learning Generative Adversarial RePresentations (GAP) under Fairness and Censoring Constraints

Liao et. al., 2019

Google Al
gy Brain Team

Learning Controllable Fair Representations

e Representation: g:X,YwHZ
Variational autoencoder

e Prediction: f:ZHY
Feedforward neural networks

e Fairness Measure: v(f, Y|S)
Statistical parity, equal opportunity, equalized odds
Information-theoretic approximations to the fairness metrics,
combined with variational approximations for efficient estimation
Recovers approximations to existing methods as special cases

Learning Controllable Fair Representations
Song et. al., AISTATS 2019

GS Saree,
A manager oversees several teams, all are using the same data to build

predictive models for different products. The manager seeks to ensure both
fairness and accuracy across the products.

Representation learning to the rescue!

e Representation learning can be used to centralize fairness constraints, by
moving the fairness responsibility from the data user to the data regulator

e Learned representation can simplify and centralize the task of fairness
auditing

e Learned representations can be constructed to satisfy multiple fairness
measures simultaneously

e Learned representations can simplify the task of evaluating the
fairness/performance tradeoff, e.g., using performance bounds

GS Saree,
A manager oversees several teams, all are using the same data to build

predictive models for different products. The manager seeks to ensure both
fairness and accuracy across the products.

Each team is solving a different prediction task.
There is no company policy on fairness, thus no shared guidelines.

e Team alpha is fully focused on accuracy, but is oblivious (neighbors say they are
apathetic) about fairness issues.

e Team beta, team nu and team gamma are all interested in fairness. Each team is
really excited to implement this and has read the literature, but each team has
selected different fairness definitions.

e Team zeta would like to improve the fairness of their predictions, but has no
idea how to incorporate or measure fairness.

e The manager has decided to independently verify that all released products
are fair P3

Google Al
gy Brain Team

References

Liao, J., Huang, C., Kairouz, P. and Sankar, L., 2019. Learning Generative Adversarial RePresentations (GAP) under Fairness and
Censoring Constraints. arXiv preprint arXiv:1910.00411.Louizos, C., Swersky, K., Li, Y., Welling, M. and Zemel, R., 2015. The variational
fair autoencoder. arXiv preprint arXiv:1511.00830.Locatello, F., Abbati, G., Rainforth, T., Bauer, S., Schdlkopf, B. and Bachem, O., 2019.
On the Fairness of Disentangled Representations. In Neural Information Processing Systems

Lum K, Johndrow J. A statistical framework for fair predictive algorithms. arXiv preprint arXiv:1610.08077. 2016 Oct 25.

Madras, D., Creager, E., Pitassi, T. and Zemel, R., 2018, July. Learning Adversarially Fair and Transferable Representations. In
International Conference on Machine Learning (pp. 3381-3390).

McNamara, D., Ong, C.S. and Williamson, R.C., 2019, January. Costs and benefits of fair representation learning. In Proceedings of the
2019 AAAI/ACM Conference on Al, Ethics, and Society (pp. 263-270). ACM.

Quadrianto, N., Sharmanska, V. and Thomas, O., 2019. Discovering fair representations in the data domain. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (pp. 8227-8236).

Song, J., Kalluri, P., Grover, A., Zhao, S. and Ermon, S., 2019, April. Learning Controllable Fair Representations. In The 22nd
International Conference on Artificial Intelligence and Statistics (pp. 2164-2173).

Stock, P. and Cisse, M., 2018. Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases. In
Proceedings of the European Conference on Computer Vision (ECCV) (pp. 498-512).

Strathern, M., 1997. ‘Improving ratings’: audit in the British University system. European review, 5(3), pp.305-321.

Xu, H. and Mannor, S., 2012. Robustness and generalization. Machine learning, 86(3), pp.391-423.

Wang, X., Li, R., Yan, B. and Koyejo, O., 2019. Consistent Classification with Generalized Metrics. arXiv preprint arXiv:1908.09057.
Weinberger, K.Q. and Saul, L.K., 2009. Distance metric learning for large margin nearest neighbor classification. Journal of Machine
Learning Research, 10(Feb), pp.207-244.

Zemel, R., Wu, Y., Swersky, K., Pitassi, T. and Dwork, C., 2013, February. Learning fair representations. In /nternational Conference on
Machine Learning (pp. 325-333).

P70

Objectives

The data regulator determines which fairness criteria
to use, and (optionally) audits the results.

When training:

e = Input: interaction with
users/experts/judges/policy to determine
fairness criteria

e Output: fairness criteria

When auditing:
e Input (for auditing the data producer):
o _Learned representation
e Input (for auditing the data user):

o Data and model predictions
e Output:
o Are fairness criteria satisfied?

 

Google Al
gy Brain Team

Data
Regulator

Determines fairness criteria,
determines data source(s),
audits results

e INPUT: Data

e OUTPUT: Fairness criteria
AUDITING

e INPUT: Models

e OUTPUT: Satisfactory?

P2

Google Al
gy Brain Team

Individual Fairness: Similar individuals treated similarly

Basketball (23%) Basketball (50%) Basketball (28%) Basketball (73%) Basketball (15%) Basketball (21%)

 

Ping-pong ball (73%) Rugby Ball (18%) Baseball player (69%)

Pairs of | port classified differently.

Stock and Cisse, 2018 P12

Algorithmic Robustness Implies Generalization
(> Individual Fairness Implies Generalization

 

If a dataset D consists of n i.i.d. samples and the algorithm Ap is
(B, e(D))-Individually Fair, then for any 5 > 0, with probability at least 1 — 6:

(Ap) — tp(Ap)| < €(D) + M- 2B m2 2in(t/9)

where I(Ap) (resp. /p(Ap)) is the risk (resp. empirical risk ) of Ap.

Challenge: Individually fair models with low training error + generalization

Cisse and Koyejo (2020, In Prep) pe

Google Al
gy Brain Team

Individual

Fairness o
Data Producer a

 

8 Google Al

Objectives

Data producer computes the machine learning
model given the sanitized representation.

Most of the fairness responsibility is with the
data producer and regulator. The data user need
only remain compliant with the pre-specified
expectations, e.g., avoid adding new features that
can result in fairness violations.

e = Inputs:
o Sanitizeddata 7, Y
e = Output:

o MLmodel f: 72> Y

 

Google Al
gy Brain Team

Data User

Computes ML model given
sanitized data

e INPUT: Sanitized data
e OUTPUT: ML model

P52

Keep calm and
earry ML on

Data user may be
ignorant of the fairness
concerns in the
system.

Data user trains ML
models as usual

 

a

© PyTorch

 

Google Al
Brain Team

Google Al
gy Brain Team

Group fairness with representative prototypes

e Representation: g:X,YwHZ
Via prototypes, defined by parameterized mixture model that stochastically
maps data to prototypes

e Prediction: f:2~Y
Parameterized mixture model that stochastically maps prototypes to labels

e Fairness Measure: v(f,Y|S)
Statistical parity TPs + FPs = P(f(Z) = 1S)
i.e., group averaged label probability across groups
Trained to minimize the weighted average of data approximation,
prediction quality, and statistical parity

Learning Fair Representations;
Zemel et. al, ICML 2013

Google Al
gy Brain Team

Data User

Computes ML model
given sanitized data
Data Regulator

Determines fairness
criteria, determines data

source(s), audits results
Data Producer

Computes the fair
representation given
data regulator criteria

 

McNamara, Ong and Williamson (AIES 2019) pz

Fairness and Representation
Learning

Moustapha Cisse & Sanmi Koyejo

Google Al
gy Brain Team

Conclusion

e Representation learning is a promising approach for implementing algorithmic
fairness

e Fair representation learning can be implemented with modular separation between

tasks/roles:
o Data regulator: determines fairness measure(s), audits results
o Data producer: learns the fair representation
o Data user: agnostically learns the ML model

e Some new-ish observations and results:
© Connections between individual fairness and robustness, generalization
o Distance metric learning for individual fairness and representation learning
© Elicitation for selecting fairness measures
o Group fairness impossibility results depend on the number of classes

 

P66

Dear colleagues, the story you are about to hear is
true. Only the names have been changed to protect
innocent computer scientists...

Beyond algorithmic fairness

e Fairness is a nuanced and challenging issue with many open
problems, e.g., incorporating user agency, metric selection, ...

e Feedback loops are common in deployed systems, i.e., predictions
leading to (user) actions, which are collected as new data.

e Inappropriate data is often the source of bias, e.g., labels which are
correlated with sensitive attributes due to sampling effects,
non-causal data collection, systematic undersampling of
sub-populations ...

e Collecting additional data may be the best way to improve both
performance and fairness

Google Al
gy Brain Team

Group fairness, and individual fairness with ambient metric

Min. Discrimination Max. Delta

 

 

7] LR
Gm FNB
Mmm ALR
Ga LFR

 

 

 

 

6
Accuracy Discrimination Accuracy Discrimination

German German

 

 

German Adult Health

Figure 2. Individual fairness: The plot shows the consis-
tency of each model’s classification decisions, based on the
yNN measure. Legend as in Figure 1.

 

 

 

* os, - 00 . ees .
Accuracy Discrimination Accuracy Discrimination

Adult Adult

 

Learning Fair Representations;
Zemel et. al, ICML 2013

P33

Google Al
gy Brain Team

Lots of open questions!

e For the data regulator:
o How does one pick appropriate fairness definitions, what is the role of metric elicitation?
o What are some best practices for auditing the results?
e For the data producer:
o Can we further improve algorithms for learning fair representations?
o Canone construct algorithms for individually fair representation learning?
e For the data user:
o What is the cost of fairness via representation learning?
o What are some best practices for avoiding fairness leakage?
e And many more algorithmic questions:

o How do these ideas apply beyond (binary) classification problems?
o How do these ideas apply to continuous variables e.g. age?

 

P67

When do
disentangled
representations
imply fairness?

Thursday, Poster #34

On the Fairness of Disentangled Representations
Locatello et. al., NeurlPS 2019

Ge le Al
gy Brain Team
Downstream
Performance
Fairness

Figure 8: If disentanglement is a causal
parent of downstream performance and
fairness and there are no hidden con-
founders, then the former can be used as a
proxy for the latter.

—T
Disentanglement

a

Google Al
gy Brain Team

Individual Fairness: Similar individuals treated similarly

Data Regulator: Which individuals are similar?
quiv., which individuals should be treated similarly?

   
   
  

An algorithm Ap is (B, e(D))-individually fair if Y can be
partitioned into B disjoint subsets denoted {C;}2_, such that Vr, € ¥:

(01,2) EC; > |l(Ap, 21) — (Ap, 22)| < €(D)

Remark: Individual fairness implies algorithmic robustness (c.f. Xu & Mannor 2011).

Dwork et al., (2012)
Cisse and Koyejo (2020, In Prep)

Accuracy

Group Fairness, Performance Tradeoffs

 

 

 

 

 

0.85
0.80 meen FFVAE
* FactorVAE
0.75 eae (rune
0.70 o: BVAE
© MP
0.0000 0.0005 0.0010 0.0015 0.0020 0.0025
Aop
(a) a = Scale

Google Al
Brain Team

as

 

 

°
ie
8

Accuracy
°
&
&

°
2
S

 

 

 

 
   
   

 

 

 

  
    

 

 

 

 

1.00} smn FFVAE 2 Pi 1.00) sms FFVAE
= FactorVAE 0.95 *=* FactorVAE
ons = CVAE = CVAE
+ BVAE 0.90 « B-VAE
0.90
o e MLP o MLP
© 0.85
3 0.85 3
m= FFVAE g S 0.80
0.80
=** FactorVAE < <<
= CVAE oes 0.75
» B-VAE O70
e MLP 0.70
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.00 0.05 0.10 0.15 0.20 0.25
App App App
(b) a = Shape (c) a= Shape A Scale (d) a = Shape V Scale

Figure 2. Fairness-accuracy tradeoff curves, DSpritesUnfair dataset. We sweep a range of hyperparameters for each model and report
Pareto fronts. Optimal point is the top left hand corner — this represents perfect accuracy and fairness. MLP is a baseline classifier trained
directly on the input data. For each model, encoder outputs are modified to remove information about a. y = XPosition for each plot.

Flexibly Fair Representation Learning by Disentanglement,

Creager et. al., ICML 2019

P47

Google Al
gy Brain Team

Fairness of Disentangled Representations

e Representation: g:X,YwHZ
Disentangled representations (independent of the fairness metric)

e Prediction: f:ZHY
Feedforward neural networks

e Fairness Measure: v(f, Y|S)
Statistical parity TPg + FPs = P(f(Z) = 1|S)
No explicit regularization for fairness measure.
Sensitive attributes are unknown during representation learning

On the Fairness of Disentangled Representations
Locatello et. al., NeurIPS 2019 pas

GS Saree,
A manager oversees several teams, all are using the same data to build

predictive models for different products. The manager seeks to ensure both
fairness and accuracy across the products.

Challenges:

e Some teams do not have the expertise (or interest) to design fairer
models.
Different teams use different definitions of fairness.
Incorporating fairness can have different impacts on the performance of
the models across products.

e Auditing all the predictive models for fairness can be challenging when
each team has its own recipe.

P4

Google Al
gy Brain Team

Fairness of Disentangled Representations

“Analyzing the representations of more than 12 600 trained state-of the-art disentangled
models, we observe that several disentanglement scores are consistently correlated with
increased fairness, suggesting that disentanglement may be a useful property to encourage
fairness when sensitive variables are not observed.”

 

dSprites Color-dSprites Noisy-dSprites Scream-dSprites SmallNORB Cars3D Shapes3D
' 1 ' ' 0.84 ' ‘ 0.6- ' - 0,404 ' z= ' i 0.82 t +. ' ' F
0.75- - 0.72- -
= 07
3 _ 0.6- -  0.4- ‘Sie, -  0,32- - 0.90-
S 0:60; r 0.64- 3
Ss
Ee a 0.6-
fo | [ 0.4- - 0.2- - 0.24 5 0.75-
6 0-45 ane! b -
0.5- nt
030-4 t | r 0.24 ' ' c  0.0- } j—- 0.165 ; — | | ae 60- | ;—
0.09 0.12 0.15 0.18 0.09 012 0.15 0.18 0.125 0.150 0.175 0.150 0.165 0.180 0.08 0.10 0.12 0.00 0.08 0.16 0.24 0.00 0.04 0.08
Unfairness Unfairness Unfairness Unfairness Unfairness Unfairness Unfairness

Figure 4: Unfairness of representations versus downstream accuracy on the different data sets.

On the Fairness of Disentangled Representations

Locatello et. al., NeurlPS 2019 P49

Data Regulator

 

 

((Senstve Group
attribute S || stat. v(f, Y|S)

px, X,) Fairness
os Criteria _/

 

 

 

oe

Data 1)

 

 

 

 

as

Google Al
Brain Team

Group Fairness: Similar Classifier
Statistics Across Groups

Regulator: Which statistic v(f, Y |S) should
be equalized across the groups?

Commonly used measures are straightforward
functions of classifier performance statistics ,
é.g.,
e Eq. of Opportunity (Hardt et. al. 2016)
TPs = P(Y = 1,f =1|S)

e Equalized Odds (Hardt et. al. 2016)
{TPs; FPs}

e Statistical parity (Dwork et. al. 2012)
TPs + FPs = P(f(Z) = 15)

Google Al
gy Brain Team

relevant elements

false negatives true negatives

 

selected elements

Image Source: wikipedia; Sensitivity_and_specificity P19

Another key task of the data regulator is to audit the
learning system (e.g., Madras et al., 2018)

The most efficient approach is to audit the learned
representation, I.e., the data producer

For complex label-dependent settings, or for an
adversarial data user, the data regulator must audit
the final model, i.e., the data user

This tutorial will outline how representation
learning can be used to address fairness problems,
outline the (dis-)advantages of the representation

learning approach, discuss existing algorithms and
open problems.

Google Al
gy Brain Team

Conclusion

 

88 Google Al

Google Al
gy Brain Team

Individual

Fairness o
Data Regulator a

 

8 Google Al

Google Al
gy Brain Team

Group Fairness: Advantages and Challenges

Advantages:

1. Efficient to compute, measure and enforce for the data producer and regulator.
2. Often easier to explain to policy-makers (as in terms of population behavior)
3. Much more existing work, strategies for representation learning

Challenges:

1. Data regulator must determine which classifier statistic(s) to equalize.

2. Fairness of the representation depends on the quality of the fairness metric chosen
by the regulator.

3. Group fairness can lead to (more) violated individual fairness, e.g., intersectionality
can lead to fairness gerrymandering (Kearns et. al., 2018), and other issues
(McNamara et. al., 2019)

Google Al
gy Brain Team

Individual Fairness:
Metric Learning Approach

Regulator (to the data producer):
Provides sets of examples which should be
treated similarly (e.g. similarly labeled points).

Producer: Equivalently, learn a representation
such that individuals which should be treated
similarly are closer to each other in the induced
euclidean metric: © Similarly labeled
BB Differently labeled

Find a metric p such that V(x1, 22, 73): i srecaivanae
iiferently iabele

41,72 € C; and x3 € C39 oe t)
=> ||21 — 2a|l2 < ||z1 — zalle

 

where z; = La; and p(2;,2;) = 27 L7 La;.

. . Image Source: Weinberger and Saul, 2009. P30
Cisse and Koyejo (2020, In Prep)

Mutual
Information
measures can
approximate
fairness
quantities

The relationship between
mutual information and
fairness related quantities

Learning Controllable Fair Representations,

Song et. al., AISTATS 2019

Google Al
gy Brain Team

 

 

 

 

 

German
0.65 ‘ o2f--- PCAFeatures
g é '
U 2. 2 @ Learned Features
ed ee eee
< 0.60 e@ &
2 . ee .°? a 0.1
Fosste e & x" ° ee ce
ss eo
© 0.0 ee%e* fe
T T T T T T T
10 15 20 25 0.0 0.2 0.4 0.6
Ig(x, Z|) Ig(z, u)
Adult
o2 | ~~~ PcAFeatures
Q07O frees ie . e@ Learned PeBnures |
3 Sw
< te *$ 8 ee
4 0.65 4 os t, Jol #08 | as
f |p” ° ote
0.60 +
T T T T T T T T
25 50 75 10:0 12:5. 15:0 17:5 0.0 0. 1 02 0.3

Ig(x, Zu)

1q(Z, u)

P38

Google Al
gy Brain Team

Generative Adversarial Representations

e Representation: g:X,YwHZ
Adversarially trained neural network autoencoder

e Prediction: f:ZHY
Feedforward neural networks

e Fairness Measure: v(f, Y|S)
Statistical parity
Implemented by constructing representations that are robust against the
optimal adversary

Learning Generative Adversarial RePresentations (GAP) under Fairness and Censoring Constraints
Liao et. al., 2019 P42

Google Al
gy Brain Team

Group Fairness

Data Producer

 

88 Google Al

Google Al
gy Brain Team

Controllable Fair Representations

“Our method encourages representations that satisfy the fairness constraints while being
more expressive, and that our method is able to balance the trade-off between multiple
notions of fairness with a single representation and a significantly lower computational
cost.”

max I, (x; z|u) | } Ar | ro |
—_ Zemel et al. (2013) 0 | A,/Az
s.t. I,(z;u) < Edwards and Storkey (2015) | 0 a/B
Madras et al. (2018) 0 71/8
Louizos et al. (2015) 1 B

Learning Controllable Fair Representations
Song et. al., AISTATS 2019 P37

Google Al
gy Brain Team

Semi-supervised variational autoencoder + MMD fairness

 

e Representation: g:X,YwHZ 9
Variational autoencoder

e Prediction: f:7ZWHY
Logistic regression, Random forests

 

e Fairness Measure: v(f, Y|S)
Statistical parity TP; + FPs = P(f(Z) = 1|S)
Implemented by penalizing MMD of stochastic
embeddings across groups

 

te

e Trained using variational inference + MMD regularization.

The Variational Fair Autoencoder
Louizos et. al, ICLR 2016

Lipschitz Continuity implies Individual Fairness
If ¥ is compact w.r.t metric p and (Ap) is L(D)-Lipschitz continuous i.e.:
\l(Ap, 21) —l(Ap, t2)| < L(D) - p(a1, 22) V01, 42 € X

Then algorithm A is (N (7/2, 4, p), L(D)y)-individually fair for all ~ > 0 where
N (4/2, ¥, p) is the covering number of 4%.

Good news: One can achieve fairness through Lipschitz regularization.
Bad news: Data is non-Euclidean (e.g. images, graphs): p ¥ || - ||2.

Challenge: Can we learn a representation of the data such that p = || -||2is a
good metric to compare instances ?

GS Saree,
A manager oversees several teams, all are using the same data to build

predictive models for different products. The manager seeks to ensure both
fairness and accuracy across the products.

Challenges:

e Some teams do not have the expertise (or interest) to design fairer
models.
Different teams use different definitions of fairness.
Incorporating fairness can have different impacts on the performance of
the models across products.

e Auditing all the predictive models for fairness can be challenging when
each team has its own recipe.

Google Al
gy Brain Team

Fair Representations using Disentanglement

e Representation: g:X,YwHZ
Variational autoencoder

e Prediction: f:ZHY
Feedforward neural networks

e Fairness Measure: v(f, Y|S)
Statistical parity TPg + FPs = P(f(Z) = 1|S)
Implemented by penalizing mutual information of sensitive and
non-sensitive feature representations, which encourages disentanglement

Flexibly Fair Representation Learning by Disentanglement,
Creager et. al., ICML 2019 P45

Representation learning is the task of estimating a
concise and informative data summary, usually
implemented as a low-dimensional data
transformation.

gi X,YHZ

Approaches in common use include PCA and
non-linear autoencoders.

Google Al
gy Brain Team

Adversarially Learning Fair Representations

“We frame the data owner's choice as a representation learning problem with an adversary
criticizing potentially unfair solutions”

Classifier Adversary
9(Z) h(Z)

Encoder Decoder

f(X) k(Z, A)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Flexibly Fair Representation Learning by Disentanglement,
Madras et. al., ICML 2018

$ Google Al
Brain Team

Performance vs. group fairness

 

 

   
   

 

 

 

Random chance $
HE Random Forest (RF)
HEE Logistic Regression (LR)

HE Discrimination 8
lB Discrimination prob. S || 0.7

- Random chance Y
HE Model accuracy Y

 

 

 

 

 

 

 

 

 

oO
oe
oo¢ °
£
oO
~

   
 

  
     

x LFR VAE VFAE ~* x LFR VAE VFAE x LFR VAE VFAE

(b) German dataset

HM DiscriminationS
HE Discrimination prob. $ ||

   
  

 

 

[= Random chances
Random Forest (RF)
HEE Logistic Regression (LR)

 

  

“= Random chance Y
0.8 ME Model accuracy Y

 

 

 

|
|
| 0.0.

 

x LFR VAE VFAE ~* x LFR VAE VFAE x LFR VAE VFAE

(c) Health dataset

The Variational Fair Autoencoder

P35
Louizos et. al, ICLR 2016

Google Al
gy Brain Team

Fair Representations using Disentanglement

“Can we... learn a flexibly fair representation that can be adapted, at test time, to be fair to a
variety of protected groups and their intersections?”

 

 

 

 

 

 

 

 

y

target label

 

 

 

 

 

 

 

 

 

Z_ we

Cin” ET modified sens. latents

 

 

 

 

 

 

 

x

, ,
non-sensitive latents sensitive latents

(eo ee hLhUDlUhLe
q A

non-sensitive observations

 

 

sensitive observations

t
>

 

 

 

 

 

 

 

 

Flexibly Fair Representation Learning by Disentanglement,

Creager et. al., ICML 2019

er a

 

 

P46

Google Al
gy Brain Team

Thank you for
your attention!

Questions?

 

88 Google Al

Google Al
gy Brain Team

Data User

Computes ML model
given sanitized data
Data Regulator

Determines fairness
criteria, determines data

source(s), audits results
Data Producer

Computes the fair
representation given
data regulator criteria

 

McNamara, Ong and Williamson (AIES 2019) P5S
