and X fp.

Z ~ Bernoulli(pa)
X° = fyo(Z)

Y= fy(X4)

O! ~ Bernoulli(Y')

xen min{X'+ Xip,Cmax} if O' =1,D' =1

max{Cmin, X' — Xpp} if O' =0,D' =1

Synthetic Gaussian In the second example, we extend a previous loan application where
the feature variable has only 1-dim. The initial feature distribution X° is sampled from a
group-specific 2-dim Gaussian distribution. The target variable is the sigmoid of a linear
transformation of the feature vectors with weight vector /. The 7-th feature positively

contributes to the target variable ([V fy (X)]; > 0) if the i-th component in MM is positive.

Z ~ Bernoulli(pa)

x? ~ Naltez, uz)
1
t
Y= [pex

ttt X'+X, if O'=1,D'=1

X'—X,, if O'=0,D'=1

4.5.1 Simulation Environment
4.5.1.1 Simulation environment setup

In this part, we briefly outline how the simulation environment using the causal Markov

decision process is set up. The state consists of a state vector (X,Y, Z) and a set of struc-

92

algorithm should deliver a accurate and fair decision D1 (admit/not admit) to applicant 1.

Observe Feedback After applicant 1 has been admitted, the true label Y' (whether this
applicant succeed in the first semester of study) would be revealed at the end of the aca-
demic semester. Meanwhile, the admission committee needs to keep evaluating the next
applicant.

This is a typical setup for many human-in-the-loop decision making process. While
many fairness solutions attempt to optimize a classifier, the goal here is to find a fair and

accurate algorithm to combine the decisions from multiple experts or classifiers.

3.1 Online Binary Classification

3.1.1 Unique properties about the problem setting

online setting In this problem, we study the online setting where examples arrive sequen-
tially. This is in contrast to batch learning setting where the entire dataset is available all
at one. The online setting is useful when decision makers need to make decisions when
data becomes available in a sequential order. The algorithms in online setting are usually
adaptive to new data points and is usually for making decisions based on limited amount of

data.

black box experts We assume access to a set of black box experts, which could either
be machine learning models or human experts. Many applications with fairness concerns
are high-stakes, and usually have human decision makers in the loop. The assumption
of black box experts allows us to deliver fair decisions without any assumptions on the

performances of the black box experts.

aggregation algorithm _In this setting, we study online binary classification where the
black box experts makes binary predictions on the example. In the case that the predictions
from black box experts are continuous, an additional threshold might be learned to convert

the continuous predictions to a binary prediction, and could be left as an interesting future

45

bound in the sense that the bound depends on the expert with lowest FPR, which is

equivalent to the best expert on instance (z, —).

The second term crpr is a constant that depends on the maximum difference of
|F PR4 — F PRa| for an individual expert (€) and 7. We don’t put any assumption

on the value of € and treat it like a property of the black-box experts.

The last term is a function hrppr(q) of g. The numerator a.4,¢4,- is expected cross-
instances cost for instance (A,—), and the denominator p4ji4T is the expected
number of examples from group A with — label. This ratio can be interpreted as the
additional expected error rate for instance (A, —) due to selecting a wrong instance
of MW. Since errors made on instance (A, —) are false positives for group A, this
is equivalent to the additional FPR for group A due to selecting a wrong instance
of MW. Thus hrpr(q) is the difference in FPR for group A and group B due to
selecting a wrong instance of MW. Since a4, pa, a,-T and ag,_, pp, fup,-T can
all be estimated on the fly, the value of function hypp(q) can be set to zero by

choosing q4,- and qz_.

3.4.3 Implication of the theoretical result

The fairness bound shows the asymptotic result that after the optimization step converges,
the absolute difference of FPR/FNR between groups can be bounded by constants crpr and
Crwnr - In appendix, we show that these constants depend on factors intrinsic to the prob-
lem: properties of the distribution and the fairness of the base expert («,F PRy-,F.N Ry-).
In the appendix, we also compare the theoretical bound of EqOdds with the achieved value
of EqOdds in experiments to get a sense of the tightness of the bound under different dis-

tributions.

62

tural equations f,, governing the dependencies between variables. The structural equations
implicitly specify the directed edges in the DAG. When we update a state, we first do a
topological sort of all the nodes within the graph, and then update with respect to the topo-
logical orders. In this case, parent nodes always get updated before child nodes, and the

causal structure is preserved.

 

Algorithm 2 Simulation using causal Markov decision process
Given DAG G and structural equations f,,
Given initial state 5° = (x°, y°, z°)
Given fairness constraint C
fort =1,...,7 do
d' + get_decision(s',C) > Find optimal decisions by solving Eq-4.1 through PGD

 

r' & get_reward(s', d‘) > Get reward
for v € topological_sort(G) do > Update according to the causal mechanism
sly] = fi (st, d’)
end for
end for

 

Solving the constrained optimization The sequential decision making process modeled
by graphical model provides a natural computation graph. Instead of resolving Eq-4.1 at
each time step, we use take projected gradient descent steps at each time step based on

previous step’s optimal solution. Specifically,
tl = Proje(7? — a’ VU(r"))

where Proje is the projection onto constraint set C specified by a fairness constraint, and a‘

is the learning rate.

4.5.1.2 Evaluation Metrics

Throughout the experiment section, we used different metrics to measure the impact of
threshold policies. Each evaluation metric is averaged over 10 simulation runs. In each
simulation run, we sample 50000 individuals from the distribution and run 200 steps. The

evaluations metrics are:

each round. For example, during the early rounds, since the algorithm hasn’t converged
yet, we might want to set A for equalized FPR and equalized FNR to be smaller to penalize
the algorithm less for unfairness. Another scenario is a shifting distribution, where G-

FORCE can be adaptive to the distribution with different 2.

3.5. G-FORCE for delayed feedback

In many real world applications, true labels are not instantly revealed and an algorithm often
needs to work with delayed feedback. For example, during the college admissions process,
the performance of a student is generally evaluated at the end of each term, while colleges
typically offer admission decisions in mid-year. Similarly, when an individual applies for a
loan, the bank often needs to wait for some time to know whether the applicant will default
or not. The duration of the delay could be a constant, a random variable, or a function of
time.

In the constant delay setting, we assume that the delay duration is a constant determined
by the sensitive attribute and the true label. Therefore, for a example of group z and label
y arrives at time t, the algorithm will make a prediction at time t, but the true label will be
only revealed at the end of time t + 7. (where T,,, is some delay duration). For simplicity,
we assume t + Tz < T. Thus the indices of feedback at time t is a set DL, = {t': t! <
t,t + Tzy = t}. In contrast, for the non-delayed setting, the indices of feedback at time t is
a singleton Dt, = {t}.

We next present how G-FORCE ’s theoretical bound changes when the feedback is

delayed.

3.5.1 Theoretical Result Under Delayed Feedback

Under the construction of the G-FORCE , both regret and fairness bound are a small mod-

ification of the original theoretical result. We leave the details of the proof to appendix.

Theorem 7 (Regret Bound). Let f* be the best expert in hindsight. Let Dax = MaX;j,z,y |Di,

be the maximum cardinality of the feedback set of all MW instances.The cumulative ex-

64

notions of fairness such as calibration, which requires that outcomes are independent of
protected attributes conditional on estimates. It is also generally accepted that there is often
a trade-off between predictive accuracy and fairness [Corbett-Davies et al., 2017]. Kearns
et al. [2018] argue that statistical parity constraints could lead to fairness gerrymandering,
where a classifier that satisfies fairness in each group could violate the constraint on groups

that are combined with combinations of protected attribute values.

2.3.2 Biases mitigation in machine learning

Fair classification Classification is an important task in supervised machine learning and
is used in various applications that directly impact humans such as loan applications and
college admission. As we talked about in the last chapter, biases for classification prob-
lems can be addressed during model training (in-processing) or after model training (post-
processing). Zafar et al. [2017] incorporate equalized odds as a constraint while solving
optimization problems, while Hardt et al. [2016] remove discrimination at post-processing

steps.

Fairness from a causal perspective There are also some concurrent works studying
long-term fairness from a causal inference perspective. Chiappa [2019] consider the case
where a sensitive attribute affects the decision through both fair and unfair pathways. They
propose to use the latent inference-projection method to disregard effects along the unfair
pathways. Creager et al. [2020] frame the dynamic process as a changing causal struc-
tural model to evaluate different policies. Algorithmic recourse [Karimi et al., 2021] uses
counterfactual analysis to propose the set of actions resulting in the desired output from the
model. A recent line of work [Karimi et al., 2021] explores providing favorable outcomes

to individuals from the disadvantaged group through minimal intervention on the features.

2.3.3 Fairness in Sequential Decision Making

Fairness in online learning setting There has been recent interest in studying fairness

in an online setting, particularly the bandit setting. Gillen et al. [2018] consider a bandit

39

from one-step constrained optimization subject to some fairness constraints. We in-
vestigate whether these fair policies could have a disparate impact on shaping the

target variable distributions of different population groups.

* Lastly, in section-4.6, we conclude with key takeaways and considerations in design-

ing policies that align with long-term fairness.

4.1.1 Related Work

Recently, a few works have studied the dynamics of algorithmic decisions and the un-
derlying distributions [Liu et al., 2017, D’ Amour et al., 2020a, Zhang et al., 2020]. Liu
et al. [2017] first use loan application as an example to study how the variable of interests
(credit scores) change as a result of decisions in a simple one-step feedback model. They
demonstrate theoretically that under the one-step model, unconstrained optimization never
decreases group-wise average credit scores while common fairness criteria could lead to a
decrease in the group-wise average credit scores.

Later, D’ Amour et al. [2020a] extends the one-step theoretical model to multi-step sim-
ulation using MDP. They show that multi-step simulation gives qualitatively different con-
clusions compared to the previous one-step analysis because of edge effects. In particular,
constrained optimization could decrease group-wise average credit scores when there is a
maximum cap on credit scores.

Most recently, [Zhang et al., 2020] models the dynamics under the partially observed
Markov decision process (POMDP) where the hidden variable represents the binary quali-
fication state. They study the equilibrium of qualification rates in the long term. However,
in many real-world applications, the target variable could be continuous. For example, the
target variable could be the probability of repayment for a loan application or the prob-
ability of re-offense in recidivism prediction. In this type of setting, the dynamics of the
distribution target variable are more complex and cannot be simply captured by equilibrium
analysis on the qualification rate.

In this chapter, we propose a setting that is more realistic and suitable for real-world

applications. Our setting is different from the previous work in the following ways:

715

Algorithmic Fairness in Sequential Decision Making
by
Yi Sun

Submitted to the Institute for Data, Systems and Society
on October, 2022, in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy in Social and Engineering Systems

Abstract

Machine learning algorithms have been used in a wide range of applications, and there
are growing concerns about the potential biases of those algorithms. While many solu-
tions have been proposed for addressing biases in predictions from an algorithm, there
is still a gap in translating predictions to a justified decision. Moreover, even a justified
and fair decision could lead to undesirable consequences when decisions create a feedback
effect. While numerous solutions have been proposed for achieving fairness in one-shot
decision-making, there is a gap in investigating the long-term effects of sequential algo-
rithmic decisions. In this thesis, we focus on studying algorithmic fairness in a sequential
decision-making setting.

We first study how to translate model predictions to fair decisions. In particular, given
predictions from black-box models (machine learning models or human experts), we pro-
pose an algorithm based on the classical learning-from-experts scheme to combine predic-
tions and generate a fair and accurate decision. Our theoretical results show that approxi-
mate equalized odds can be achieved without sacrificing much regret. We also demonstrate
the performance of the algorithm on real data sets commonly used by the fairness commu-
nity.

In the second part of the thesis, we study if enforcing static fair decisions in the sequen-
tial setting could lead to long-term equality and improvement of disadvantaged groups
under a feedback loop. In particular, we model the interaction between algorithmic de-
cisions and underlying distribution using Markov Decision Model with general transition
functions. We propose a new metric that measures the distributional impact of algorithmic
decisions as measured by the change in distribution’s center, spread and shape. This metric
categorizes the impact into within-group impact and between-group impact, where within-
group impact measures how policies impact the distribution within a group, and between-
group impact how policies impact the distributions of two population groups differently.
Our results show that there is generally a trade-off between utility and between-group im-
pact for threshold policies, and common fairness constraints could lead to "backfire effects"
where the impact on groups could be disparate.

Thesis Supervisor: Kalyan Veeramachaneni

MW instance associates a weight to a classifier f for group z and label y; e.g. the weight
of classifier f for group A and negative label examples is denoted as wy,4,_.

In figure-3-5, we illustrate how G-FORCE selects a MW instance to use. For an exam-
ple from group A, G-FORCE follows the path Z = A and goes to the right branch. At this
point, the label is not known but G-FORCE needs to select between (A, +) and (A, —). For
this purpose, G-FORCE constructs a meta selection probability ¢, which it uses to select
instance (z,y) with probability g.,, at each round . The figure illustrates the case where
(A, —) gets selected.

We show that it is possible to bound regret, FPR and FNR as a function of the meta
selection probability g. Moreover, the bounds can be further optimized by choosing the op-
timal meta selection probability that balance between regret, FPR and FNR. For the sake of
clarity, the rest of the proposal we consider binary classification with two sensitive groups,

though the algorithm can be easily extended to multi-group and multi-class problems.

3.4.1 G-FORCE mechanism

We use an example to illustrate the mechanism of G-FORCE for one round. The mech-
anism of G-FORCE is explained in Figure 3-6. At each round, G-FORCE takes in an
example (a, z). G-FORCE works in three steps: optimization step, prediction step and

the update step. The pseudo code for the algorithm is presented in 1.

Optimization Step G-FORCE first selects an appropriate MW instance to use. While
group attribute z is known, at this point G-FORCE doesn’t know the label yet, and has to
choose between instance (z, +) and instance (z, —). G-FORCE constructs a meta selection
probability q to select between the two instances, where q,,, and q.,_ are the probability of
selecting (z,+), and (z, —) respectively.

In the case that G-FORCE selects the wrong instance (for example, true label is — but
(z, +) is selected), we refer to the additional losses as cross-instances cost a... (formal
definition in next section). This meta selection probability allows us to explicitly construct
a upper bound on regret, FPR, and FNR as three functions of g. We later show q.,, and qz,—

can be explicitly set to tighten this bound by solving an optimization problem that balances

55

Thus, using 6.29, we have:

 

 

 

 

 

E z be
What t t we t
=a. OL ota SoS a = Oe

t=1 SEF fEeF 7

ML pert DG a _-
t

and,

 

 

 

 

E[L.] = y(n) Lp +Qig—-4 a + Yoon *04,,)
t

Finally, the total expected error is lower bounded by:

E[L] > y(n) Ly +. (6.16)
6.1.2.3 Fairness Bound

Proof We assume group A arrives with probability p, group B arrives with probability
1-p, that is, P(z = A) = p. The expected mean label of group A is defined as 4. =
P(y = +|z = A) and mean label of group B is defined as wg,, = P(y = +|z = B). Each

individual classifier is €-fair, thus:

 

 

 

 

 

 

 

 

 

 

 

 

[Ey ls Pian coe | PB I<ev/
Der iy = -H1{z = A} Der Hy = —}1{z = B}
(6.17)
which represents the cardinality of the selected subset of samples.
The absolute difference of FPR between group A and group B is:
La Lp
|FPRa — FPRp| = |Exyz 7 S 7 3 | |
dnail{z=AHy=—-} dias dt{z = BHy =—}
(6.18)

119

Sampling

State of the World

 

Dataset

Training

Figure 1-1: A typical machine learning cycle contains five stages: the state of the world describes
the true underlying distribution; data is sampled from the state of the world; a machine learning
model learns patterns from the training dataset; the model makes predictions on new instances; these
predictions are transformed into decisions about an individual; an individual could take actions and

change the state of the world.

 

Feedback

Machine Learning

Model

24

O

Decision-making
Individual
(1

Prediction

Inference

 

 

 

 

 

 

 

 

 

 

0.6] —— Group A 0.6] —— Group A 0.6] —— Group A
— Group B — Group B — Group B
0.4 0.4 0.4
0.2 0.2 0.2
0000 05 0500S 0 OOD 025 O50 OS «1.00 "G00 02500 OT «1.0
Repay Probability Repay Probability Repay Probability
0.6] —— Group A 0.6] —— Group A 0.6] —— Group A
— Group B — Group B — Group B
0.4 04 0.4
0.2 0.2 0.2
0.04 0.04 0.04
0.00 050075 100 "900 025 05007 1.00 "000 025. 050075 1.00
Repay Probability Repay Probability Repay Probability
0.6: 68
— Group A — Group A — Group A
— Group B — Group B — Group B
0.4 04. ad
0.2 0.2 0.2
0.0 | 0.0 0.0
0.00 0250.50 0751.00 0.00025 iHSC*iCL 0.000.250 H«*LOO

Repay Probability

(a) MaxUtil.

Repay Probability

(b) DemoPar.

Repay Probability

(c) EqOpp .

Figure 4-16: Histogram for the final distribution for repaying probability after different policies.
The unfilled bars indicate the initial distribution and the filled bars indicate the final distribution.
Top row: forgiving setting. Middle row: neutral setting. Bottom row: harsh setting.

101

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

1.00
White 0.06 04
0.75; 0G Black
0.04
0.50
0.2:
0.25 0,02
0.005 00 0.25 050 075 1.00 ow 400 600 800 0.0 00 0.25 050 075 1.00
Group Ratio Credit Score Repay Probability

Figure 4-9: The initial distribution for the FICO score dataset. Left: The initial distribution for
the group ratio. Middle: The initial distribution for the features (credit score). Right: The initial
distribution for the target variable (repay probability).

We categorize the simulation settings into three regimes based on the relative value of

5 _ _ ta , Ky 5
the X,, and X yp. In this case Xj, = Xn = 0, and the cost ratio g = XqixX" Cost ratio
specifies the feature change value for a false positive relative to the full feature value range.
If X;, is greater, the cost ratio will also be greater. The cost ratio can be interpreted as the

relative impact of a false positive on features.
* Forgiving setting : X;, = 150, Xp = 75;
* Neutral setting: X,, = Xs») = 75;

* Harsh setting: X;, = 75, Xf, = 150.

4.5.2.2 Fixed threshold policies

In this section, we discuss the impact of repeatedly employing a fixed threshold policy on
the center and spread of the target variable distribution. Center and spread are two impor-
tant summary statistics to describe a distribution. Center describes a typical value of the
distribution, and spread describes the variation of the data. We use mean-WGI to measure
the impact on the center of the distribution, and var-WGI to measure the impact on the
spread of the distribution. We show how the utility and impact change under different fixed
thresholds across different simulation settings (forgiving, neutral, and harsh). We show the
experiments for one-step simulation as well as multiple-step simulation. In the multi-step
simulation, we run the simulation for 200 steps. The multi-step simulation captures the

long-term dynamics between the thresholds and the target variable.

95

: . . YX OF) O2VT
where the last inequality follows the fact that a‘, < 1. Since ma IF ary.

When T’ — oo, the estimation errors are sub-linear and thus go to zero. Therefore,

 

 

 

 

 

 

 

| [eae — se ga Spy, an Late)
LY 2

| |< |Q+n-(n)) PPRpt+e(14 oe

Ca- Ca, pal—Has)P pal we+)T
SS
QFPR
(6.20)

Similarly, for the absolute difference of FNR between group A and group B, we have:

 

 

 

 

Cas Caa

 

 

 

Ee Lpy
LY

. yt : yt
| |<|Q+n-4(n)) FNRy;-+e(14 n+(S Dae BB+ tee |

PAbasl PBB sl
QPNR

(6.21)

where F PR;«(F.NRy-) is the FPR(FNR) of the best classifier f* on the best sensitive

group 2*.

6.1.3 Proofs for Delayed Case
6.1.3.1 Proof of Lemma 1

Let = So fer wy, ,- Note that in the delayed feedback setting, the weight updated
will incur a delay, but the loss will still be updated every round. Thus the expected loss on
group z, + at each round is the same as before:

U

wt wee
Blt) = da Do Get Gat tH GEO Gat (6.22)
fEeF Or feF 9%

Then, the weight transition would be different, where multiple update step could happen

in each round. On the other hand, if D; is a empty set, there will be no update for round t.

122

Thus, equation 6.19 becomes:

 

 

 

 

 

 

 

 

 

 

 

 

Lp+(B,-),B,— Ly+(B,-),B—
< f*(B,-),B, — tn VE f*(B,-),B,
<|(1+7) (Ban Cp +e y(n) L,Y ,Z Ces. +
Dy ds . aly ye dp, . an LE Ind |
Ca Cz SUE nCa,—
Law 58)
<|(149—7()) Bae | PB | 4 (1 +
»~ qs . aly »~ dp, . ag LE Ind |
Ca Cz Ae nC a,
Devin: jp)
S|(149=9(0)) Baie |-~PPE| 4 (1 +a)
( ida 04- de Op, ) Ind
pal—paw)T papas) ) * np = pa4)T
Ye a oy yi dp, * p- Ind
<|(l+n- (n)) FPR +e(1+7 +( 2 : : :
IC (n) tel +n) pal—pas)T pad-pes)T/ np pa+)T
wa 4 2th“ Oh)
<|/(l+yn- 9 FPRe«+eée1+n)+ | oo OOOO
<|(L4 n= 9(0)) FPRp + e(L bn) + (Se ae Sarin)

where F’'PRy- is the FPR of the best classifier f* on the best sensitive group z*. In the
fourth line, when T — oo, the inequality follows from the fact that the last term goes to

zero since its numerator is a constant .

Let qa, and qz,_ indicates the converged true value q', _ and qj, _ respectively, where

ga,— = limp+o0 ds and gp = limp4o dp Let Oy = ds — qa and 6, =
dp — qp,— be the estimation errors at round t. By the classical central limit theory, the
estimation errors converge at the rate of O(%). Therefore,

DE. da . aly _ 2 ds, . aR

pal—pfa+)T pal - wa 4)T
_ wi(e,- —44,-): any (G5 — 4p) an qa, or aly dp 1 aly

 

pa(l — pa4)T pal — we 4)T pall —pfas)T pal — pe+)T
$$ $< —__$___
QFPR
_ ar Oy . aly wee OB . aly
TT Qrpr

~ pal= pas)? pa pe4)T
— y08) D0)
pall—pas)T pa — ue4)T

 

Tv Qrpr

121

ple, some assumptions in classical machine learning might not hold:

¢ Average performance: The goal of minimizing average loss incentives the model to
find the best parameters that fit the majority group well. This often leads to disparate

performances in the majority group and the minority group.

Static data distribution: In one shot prediction problem, it is common to assume the
data distribution is independent of the predictions. However, predictions could have
the power to change the underlying distribution D due to the feedback loop. This of-

ten leads to problems when the decision maker needs to make repetitive predictions.

The research community has come up with many ways to evaluate the biases as a result
of model predictions. In the next section, we briefly introduce a few most well-adopted

metrics for fairness that are based on model predictions.

2.2. Fairness Metrics

Fairness can be considered on an individual level or a group level. At the individual
level, fairness can be intuitively defined as "similar individuals should be treated similarly"
[Dwork et al., 2012a]. As discussed in Dwork et al. [2012a], one challenge of working
with individual fairness is that the distance metric is difficult to specify. Group fairness can
be defined as balancing some metrics across different demographic groups (such as gender
groups, racial groups, etc.).

At the group level, the definitions can be roughly grouped into three categories: (1)
statistical metrics that are solely based on the statistical relationship between the predictions
and outcome variables, (2) causal metrics that involve all variables, and (3) others that are
beyond prediction fairness. We introduce these metrics under a binary classification setting,

although extensions are often available.

2.2.1 Statistical Fairness Metrics

We will next introduce four mostly well-used statistical fairness metrics for fairness: (1)

Demographic Parity (DemoPar) requires a predictor that is independent of the sensitive

34

measure the geometric segregation in the feature space of population groups after applying

algorithmic decisions [Heidari et al., 2019].

Economic inequality In economics, the Gini coefficient [Yitzhaki, 1979] is the statistical
dispersion metric that measures the inequality of income within a social group. We use x;
to indicate the the income for an individual 7 and Z to indicate the average income, and
r= as to indicate the inequality ratio for individual 7. Perfect equality is achieved when
the inequality ratio r; = = equals 1 for everyone, which happens when everyone’s income

is equal to the average income. A more general measure of inequality can be defined as:
Inequality = > pjh(r;)
j

where p; is the weight of the population, and h(r;) is a function of the deviation of each
individual’s r; from the point of equality. The Gini coefficient is a useful metric for mea-

suring within-group impact as a result of algorithmic decisions.

4.5 Case Studies

Next, we use two case studies to empirically illustrate the impacts of threshold policies. In
both cases, the group distribution is time-invariant and sampled from Z ~ Bernoulli(pa)

where py is the probability that an individual comes from group A.

Loan Application Example The loan application example was first proposed by Liu
et al. [2018] to study the one-step feedback effect of fairness constraints. We first frame the
loan application example in the format of a dynamic SCM. The variables in the SCM are
as follows: Z € {0,1} is the binary sensitive attribute, X € [Cmin, Cmax] is the credit score
, D € {0,1} is the binary loan approval/rejection decision, and Y € [0, 1] is the probability
of repaying. The initial feature distribution fxo and the repay probability fy are estimated
from the dataset.

Since the data is not sequential in nature, we use a synthetic structural equation for the

feature update function fy, where we experiment different feature update parameters X;,

91

3.1.1 Unique properties about the problem setting... .......... 45

8.1.2. (Notations: . ss amaow ss ok mm yk se mH ee ow 47
3.1.3. Metric for evaluating accuracy ...............0000. 49
3.1.4 Metrics for evaluating fairness... 2... 2 ee 50
3.2 Online Algorithms .......... 2... 00.0. eee 51
3.2.1 Multiplicative weights algorithm (MW) ............... 51
3.2.2 Group-aware MW algorithm ..................00. 52
3.3. Motivation forour work... 2... 2... ee 53
3.3.1 Need to use distinguish error types... ............0.0. 53
3.3.2 Need to care about label imbalance .............0.2.. 53
3.3.3. Need to consider delayed feedback... ...........0.0.. 54
3.4 G-FORCE algorithm ..............0.. 2.0.0... ..0.0008. 54
3.4.1 G-FORCE mechanism ...........0-...2...0.000.0. 55
3.4.2 Theoretical Analysis of G-FORCE ................. 57
3.4.3. Implication of the theoretical result... 2... .......00.. 62
3.5 G-FORCE for delayed feedback ....................00. 64
3.5.1 Theoretical Result Under Delayed Feedback ............ 64
3.6 Empirical evaluation of G-FORCE ...................00. 65
3.6.1 Case study: Synthetic Datasets... 2.2... 2. ee 66
3.6.2 Case study: Real Datasets... 2... 2... ee. 68
3.7 Conclusion ... 2... 2... 70
Study of Fairness with Feedback Loop 73
4.1 Introduction... 2.2... 2.2... 0.000.000.0000 0 000000000. 74
4.1.1 Related Works seas esse ee eS eae sew eRe EEE s Ee 75
4.2 Motivating Example ................. 2.0.0.0... 000. 76
4.3. Formulation and setting... 2... 2.2... 2. ee ee 82
4.3.1 Background: Markov decision process... ............. 82
4.3.2 Modeling the feedback loopas MDP. ................ 83
43.3: Threshold.policies au... 5 we 6 ee ee 84

130

3-7

3-8

3-9

43

4-4

4-6

4-7

The size of each color block is proportional to the number of examples in
that group-label subset. Imbalanced setting is created with p4 = 0.9, ia. =
0.7, 48,4 = 0.3 and balanced setting is created with pa = 0.5, 44 =

0.5, pe HOB eee 66

The achieved accuracy on group-label subsets for imbalanced setting (p4 =
0.9, a4 = 0.7, 4,4 = 0.3) and balanced Setting (p4 = 0.5, a4 =
0.5, p+ = 0.5). Left: GroupAware. Right: G-FORCE . The vertical
black line denotes the standard deviation. The red dashed line is the overall

ACCULACY: wa gett RHR SELES HRB ELS RHO BH EEE aw 67

G-FORCE shows a clear improvement over GroupAware on both equalized

FPR (bottom left) and equalized FNR (bottom right) on adult dataset. ... 70

An overview of the feedback loop in the loan application example. .... . 77

Initial credit scores distribution of group A (advantaged group) and group

B (disadvantaged group)... 2... ee ee 78
Outcome when using a policy that has the same threshold regardless of group. 79

Outcome when using a demographic parity policy that issue loans to the

same percentage of people in both groups. .................-. 79

Outcome when using equalized opportunity policy. ............. 80

The dynamic data generation process unrolled by time. Z is the sensitive
attribute, X is the features, Y is the target variable, and D is the decision
applied by the agent. The purple arrow indicates a policy function that
maps from the features X* to a decision D*, and the red arrow indicates the

feedback effect from decision D' to features X**!. 2... 0. 82

Parameters defined in terms of the confusion matrix. ............ 84

14

| Group A | Group B

 

Applicant a, ag a3 a4 a5 Mean] b; bo bz by bs Mean

Credit Score X 4

Decision

New Expected X’ | 4

6 8 9 6.4 en! 4.2

6 86 98 6.68 5 74 4.28

5 2 3 4
0 0 0 1 1 04,0 0 0 0 #1 0.2
5 2 3 4

Table 4.4: Outcome when using an equalized opportunity policy. The bank issues loans to the same
fraction of qualified applicants (50%) in both groups.

 

 

 

 

Notation Meaning

D Underlying distribution where the dataset is sampled from

Z Protected group attribute such as gender or race

xX Feature attributes the other than protected attribute

Y Ground truth target variable

Ss State S consists of (Z, X,Y)

O O ~ Bernoulli(Y). An instantiation of the target variable.

(z,x,y) An individual sampled from the distribution is a tuple of the protected at-
tribute, feature attribute, and ground-truth label

Py The CDF distribution of target variable Y.

G A DAG representing the dependency between state variables

fol-) Structural equations for node v

V f(x) Derivative of structural equations f, evaluated at x

St State at time t¢ consists of (Z, X‘, Y*)

D' Decision at time t

ut Utility for the decision maker at time t

Te Policy function for group z

Te Threshold used for group z at time t

Lip Feature value increase for a true positive

Lfp Feature value decrease for a false positive

Lin Feature value increase for a true negative

Lin Feature value decrease for a false negative

Utp Utility increase for a true positive

Ufp Utility decrease for a false positive

Utn Utility increase for a true negative

Urn Utility decrease for a false negative

g Distance metric

ot Within-group impact for group z at time t

A‘ Between-group impact of group A and B at time t

 

Table 4.5: Notation table for the terms used in this chapter.

81

Chapter 3

G-FORCE : Achieving Fairness in

Online Decision Making

In this section, we investigate how to combine predictions from models to produce fair
decisions for individuals. This chapter will be structured as follows: we first use college
admission as a motivating example to describe the our setting. We then introduce unique
properties of this problem setting and notations. We then map this setting to the mathemat-
ical framework of online classification with fairness constraints. Next, we briefly introduce
the current progress and the motivation for our work. In the next section, we present our
algorithm, G-FORCE , and its theoretical guarantees. Finally we present G-FORCE ’s

performances on synthetic and real datasets.

 

1, # c

State of the World Data Collection Model Prediction Individual

 

Figure 3-1: From predictions to fair decisions.

We first use an example on college admissions to illustrate the nuances in sequential
decision making with fairness concerns. Suppose a college is trying to admit students for
a program on a rolling basis where the admission committee has to make decisions as

they go. In addition to the applicant’s qualification, the admission committee also aims

43

Thus by the recursive function, we have

 

 

ar
T41 1 What ot
ot < 6! [Ja- n> ot Cry)
ea fF 2+
ig
=d][a-1>- niet 4). (6.2)
t=1 fEeF et

Following the updating rule of the MW algorithm, we have

witt =whe(l- noir ene Mut}

— (1 _ num Pog (6.3)
where w}, . = 1, as all the weights are initialized.

Using 6.23 and 6.24,

 

whae = (1- pebtinn <aet < aT] 1-1 That, (6.4)
EF e+

and taking the logarithm of both sides, we have

 

In(L = n) Lee <ind +o hn( in a =)

 

 

t=1 fEeF Wat
w
In(1—n)Lpz4 <Ind—n>> hat B (6.5)
FEF UBT
we, Ind
ee fst Crag S (LA ML Gag + (6.6)
t=1 feF Of Uf

Equation 6.26 follows because if 7 < 1/2, we can use the inequality In(1 — 7) < —n. This
is intuitive as if we always choose weights for positive examples, it reduces to the same

bound as in the original MW algorithm.

We now assume that the expected error on group z,+ when wrong weights wy, are

115


 

pected loss E|L] of G-FORCE can be bounded by:

 

 

 

Ind
E[L] < (1+ n)Lyp + -—s + D”™ hrac(q), (3.12)

where hrgc(q) = > > Gey Coy:

2e{A,Bhye{+—} ¢
This upper bound shows that the expected cumulative loss has an additional multi-
plicative factor of Dax on the function hrzc(q) compared to the non-delayed setting.
Nevertheless, the order of regret is still the same. This result is consistent with Joulani
et al. [2013], in the sense that delay feedback normally increases regret in an additive way

for stochastic setting.

Theorem 8 (Fairness Bound). Let F PR «(FN Ry-) be the classifier achieving lowest ex-
pected cumulative loss on subset {z,—}({z,+}), Vz € {A, B}. For G-FORCE, we have:

|FPR, — FPRp|
<|(1+n- (mn) FPR +e1+n)+

(“— Lie DBE IB- oe |
pa(l — Has)T pa(l = Bp+)T

(3.13)

 

hepR_delay(@)

where Di” = max,|D', _| and DY = maz,|Di _|

For the fairness bound, compared to the original G-FORCE’s bound, the function hrpr

also depends on the maximum feedback cardinality D7“, DR“.

3.6 Empirical evaluation of G-FORCE

In this section we present G-FORCE ’s performance on real and synthetic datasets. G-
FORCE keeps three statistics that are necessary to compute parameters for functions hrpr
and hryr: (i) the probability of a sample coming from group z, denoted by pz, (ii) the base
rates of outcomes, denoted by j1,,,, and (iii) the cross-instance costs a, which is estimated

as differences of expected loss between using a right instance and a wrong instance. All

65

stance, if certain jobs historically employ more males than females, those positions might
attract more males to apply, worsening the data bias. In some cases, it can also be diffi-
cult or unethical to collect more data from under-represented groups. There is also work
that tries to transform the dataset such that the underlying biases are removed [Bellamy
et al., 2018]. The idea is to learn a new representation of the dataset, removing information
correlated to the sensitive attribute and preserving the information of features as much as
possible.

However, in many real-world scenarios, a pre-collected large dataset might not be avail-
able. For instance, if anew company wants to recruit employees, it often needs to start with
limited data and make decisions sequentially. This requires new approaches for mitigating

biases in sequential decision-making.

1.2.2. Model Bias

After data collection is over, a machine learning model needs to digest and learn from the
collected dataset (training examples). Unfortunately, a machine learning model sees the
training examples differently than humans do. The sole goal of most machine learning
models is to learn a mapping from input to output to minimize empirical risk (training
errors). Unfortunately, minimizing training errors leads models to recklessly absorb all the
correlations found in the training data. Many of the extracted correlations are spurious, in
the sense that although they reduce the training errors, they appear completely random and
uninformative to humans.

As a thought experiment, consider the problem of classifying profile pictures of males
and females. If males are more likely to wear ties in the training data set, a naive model
would pick up this spurious correlation between gender and tie. Later, if that model en-
countered a picture of a female with a tie, the model would fail unexpectedly. As humans,
we can realize that tie-wearing does not definitionally mean a male picture, yet it would be
difficult for an algorithm to differentiate correlation from causation.

Mitigating biases during the model training stage is perhaps the most well-studied solu-

tion for addressing biases. These methods can be grouped into in processing techniques and

26

Implication of cross-instances cost Note that how large cross-instances cost is depends
on the performance of black box experts and is not known in advance. In practice, since
G-FORCE keeps track of weights 7;,,.,,, cross-instances cost can be estimated on the fly. At
the end of each round, the true label is revealed and the weights are updated. The estimation
for a is updated at each round after the MW weights are updated. Let us rearrange the terms

of cross-instances cost as following:

to t t t
Oy = So say — Tey) “fay
SCF

This rearrangement enables us to analyze this cross-instances cost in detail. Here we

explain each component in the definition of cross-instances cost :

¢ For a single expert f, (They! - Th oy) is the difference in probability, where the first
term is the probability of choosing the expert f when algorithm picks the wrong
instance (z,y’) and the second term is the probability when the algorithm picks the
correct instance (z, y). After revelation of the label at the end of the round ¢ this can

be calculated. We calculate this after updating the weights.

et

My = GALZ = z}I{Y* = y} is the loss of an expert f at round ¢ when the

example comes from group z with label y. If expert f is a good expert for instance
(z,y), ly,2,y would be small (equal to zero in binary classification). On the other hand,
if the expert f is a bad expert for instance (z, y), lz, would be large (equal to one

for binary classification).

3.4.2.1 Regret Bound

Since we have a separate MW instance for each combination of group and label class
(z,y), we can first develop regret bound for each MW instance separately. We use E[L. |

to indicate G-FORCE ’s cumulative expected loss on MW instance (z, y).

59

1.3.1. Summary of Contributions

In the first part of the thesis, we study how to translate model predictions into fair decisions.

We make the following contributions:

¢ We propose a meta-algorithm, G-FORCE (Group-Fair, Optimal, Randomized Com-
bination of Experts), which combines black-box predictions into fair and accurate
decisions in an online setting. We measure fairness using the strictest metrics based
on classification parity (equalized odds), which require both equalized false positive

and false negative rates among population groups.

¢ The algorithm re-weights experts based on their past performance in terms of accu-
racy and fairness. Under this framework, we show that the algorithm’s performance
on regret and fairness can be upper bounded. We demonstrate the performance of the
algorithm on real data sets commonly used by the fairness community, as well as on

synthetic datasets to test its performance under extreme scenarios.

¢ We also extend the theoretical analysis for the delayed setting, where the true label is
not instantly revealed at each time step. We demonstrate how the previous algorithm

can be adapted in this setting.

In the second part of the thesis, we study how decisions made on people could change the

state of the world through the feedback effect. We make the following contributions:

¢ We first propose a new metric that measures the distributional impact of algorithmic
decisions as measured by the change in distribution’s center, spread and shape. Un-
like previous work that has focused on the disparity of the group mean, this metric
allows us to characterize the change of target distribution shape in a more fine-grained
way. This metric categorizes the impact into within-group impact and between-group
impact, where within-group impact measures how policies impact the distribution of
a group, and between-group impact how policies impact the distributions of two pop-

ulation groups differently.

¢ We conduct experiments with general a set of well-used group fairness constraints on

synthetic Gaussian distribution and real-world datasets. We demonstrate that previ-

30

Acknowledgments

Iam very fortunate to have received valuable guidance and support throughout this journey.

First and foremost, I would like to express my sincere gratitude to my advisor, Dr.
Kalyan Veeramachanenni, for his invaluable guidance and support during my Ph.D. study.
You have always motivated me to think big and think outside the box. I’ve learned not only
research skills but also many life lessons from you.

Next, I would like to thank my thesis committee members, Professor Alberto Abadie,
Professor Caroline Uhler, and Professor Alfredo Cuesta-Infante for their words of encour-
agement and valuable feedback on the thesis.

I would also like to thank all my collaborators and labmates, whose knowledge, insight,
and generosity has always been a source of inspiration for me. I would like to thank all
members of my lab: Dongyu, Ivan, Micah, Ola, Sarah, Nassim, Sara, and many others. It
has been a true pleasure to brainstorm and collaborate with many of you on various projects.

I would also like to appreciate the support from IDSS. I would like to thank Munther
Dahleh, Ali Jadbabaie, John N. Tsitsiklis, and Fotini Christia for taking the lead in creating
a collaborative and intellectually stimulating environment. I am also thankful to the MIT
staff: Elizabeth Miles, Michaela Henry, Gracie Gao, and Brian Jones.

Next, I would like to thank all my friends at MIT and in Boston, old and new, whose
paths crossed with mine in the last few years. Ph.D. is a long journey and thank you for
always being there for me throughout the ups and downs.

Last I would like to express my immense gratitude to my parents for their unwavering
support and unconditional love. Thank you for always believing in me and encouraging me

to pursue my passions and dreams.

Center In figure-4-10, we plot the mean-WGI for one-step simulation. The red dashed

line indicates the threshold for MaxUt il, which maximizes utility without any constraint

Ufp

and is achieved at Tyfaxutit = mia

 

The mean-WGI metric measures how the center of the target variable distribution changes
compared to the initial distribution for each group respectively. A positive mean-WGI in-
dicates the policy exerts a positive impact on the center of the distribution. Across different
cost ratio settings, the sign of mean-WGI changes as we switch from the forgiving setting to
the harsh setting. Specifically, mean-WGI is positive for the forgiving setting and negative
for the harsh setting. On the other hand, as the threshold value increases, the magnitude
of mean-WGI decreases. This implies that the sign of the mean-WGI depends on the cost
ratio, yet lower thresholds increase the magnitude of the mean- WGI. When the cost ratio is
lower (as in the forgiving setting), a lower threshold amplifies the positive impact. On the
other hand, when the cost ratio is higher (as in the harsh setting), using a lower threshold

will amplify the negative impact.

Mean-BGI measures how a policy shifts the center of the two distributions differently
and is represented by the gap between the two group-wise mean-WGI lines. As shown
in the figure, as the threshold increases, the mean-BGI decreases. This suggests that for
a lower cost ratio setting, there is a trade-off: a higher threshold leads to lower positive
mean-WGI respectively, but also lower mean-BGI. On the other hand, for a higher cost
ratio setting, a higher threshold is always more desirable (lower negative mean-WGI, lower

mean-BGI).

In figure-4-11, we plot the mean-WGI for one-step simulation. Multi-step simulation
generally shows the same trend for mean-WGI as in one-step simulation, but with an am-
plification effect on the magnitude for mean-WGI. However, the threshold for MaxUtil,
which maximizes average utility, appears at the lower spectrum of threshold values at
around + = 0.15. This implies that for long-term simulation, there is a trade-off between
utility and mean-BGI. Lower thresholds lead to higher average utility but also a higher

disparity between the two groups as measured by the mean of the target variable.

96

Theorem 4 (Regret Upper Bound). Let f* be the best expert in hindsight.

Ind
E[L.,| < (1 + n)L pr zy + 7 + S: tg s ay (3.2)
t

The overall cumulative expected loss E[L| of G-FORCE can be bounded by:

 

 

 

 

Ind
E[L] < (1+ m)Ly + Fa + hrec(q) (3.3)

where hrgc(q) = S: Yoda, Oly.

2€{A,B}ye{+,-} ¢

Implication of the regret bound This upper bound shows that the expected cumulative
loss is upper bounded by the summation of three terms: (1) the cumulative loss of the best
expert in hindsight, (2) the constant term aid, (3) the function hrrg of meta selection

probability g. Here we breakdown the components in the function hrc:

* The cross-instances cost at round t is the difference in expected loss between select-
ing right instance (z, y) and wrong instance (z, y). For MW instance (z,y), 70%
is the cross-instances cost of instance (z, y) weighted by the meta selection probabil-

ity of choosing instance (z, y).

The function hrrg is the cross-instances cost summed over all MW instances. The
value of hrc can be minimized by choosing proper values of meta selection prob-
ability g. For instance with higher cross-instances cost , we might want to assign a

lower meta selection probability q.

In order to show the bound for differences in FPR across groups (i.e. for EqOdds), we

also provide a lower bound on the expected cumulative loss of G-FORCE.

Lemma 5 (Lower Bound). Let f* be the best expert in hindsight. Then, G-FORCE’s ex-

pected cumulative loss is lower bounded by:

E{L] = y(n)» Lys + hrec(q). (3.4)

60

Decisions could change the population distribution First, decisions could change the
population distribution. Consider again the example of predicting crime rates in a com-
munity. If innocent people living in this community know that a model has predicted high
crime rates there, they may move out. This leads to a self-fulfilling feedback loop that
changes the future population distribution, such that the crime rate could further increase

in this area.

Decisions could change the distribution of the features and outcomes Decisions could
also change the outcome distribution. For example, say a bank takes affirmative action to
approve loans at a lower threshold for people coming from less advantaged socioeconomic
groups. And then say those people might have trouble paying back the loan later, which
decreases their credit scores and credentials in the future. In this case, even well-intended

actions may create an accidental feedback loop.

Strategic classification Algorithmic decisions could change distributions unintention-
ally. Individuals could also strategically react to the decision-making rule [Hardt et al.,
2016, Milli et al., 2019, Ghalme et al., 2021]. For instance, if applicants know which fea-
tures are used in a loan approval decision, they might be incentivized to manipulate those
features to get approved. This might lead to strategic behaviors such as holding multi-
ple credit cards or moving to a different zip code, changing their loan eligibility without
necessarily affecting their ability to repay it. Such tension between decision-makers and
individuals can be modeled in a game-theoretic setting [Zhang et al., 2022, Keswani and

Celis, 2022].

Most of the fairness solutions focus on one-shot classification or regression problems,
and there is a gap in addressing fairness in a sequential and dynamic environment. This
motivates us to address fairness concerns under the sequential dynamic environment and

investigate the long-term impact of particular solutions.

28

ated to a given expert, restricted to samples from group z, while ¢;.,, represents the loss
function with the same information as before but also restricted to samples from label class
y. These subscripts are substituted with a specific value when needed. For instance, ¢,..—
represents the same as before but specifying that all samples with negative labels. The lack
of subscripts represents the generic variable.

In this section, we formally describe the setting in the language of online binary clas-
sification. As in the typical online learning setting, the algorithm runs through round t =
1,...,7'. We assume access to a set of black box experts F = {f1,..., fa}, which could
be human experts or machine learning algorithms. At each round t, one expert f’ € F is
selected to estimate the label for the input example, §° = f'(2', z'). Then, at the end of the

round, the true label y’ is observed, producing a loss ¢(g*, y’).

Instant Feedback We first assume that we can instantly observe the true label after a
decision has been made. In the college admission example, this means that whether a
student would succeed in the first semester is instantly known after the student is admitted.
Below, we relax this assumption and try to tackle the problem where the feedback of the
true labels is delayed. The decision making process runs through rounds t = 1,...,7'. At
each round t:
* A single individual (a*,z') € R” arrives, where x! € 4 is a set of features and
z' € Z is the group attribute .
¢ Each expert i makes a prediction jf = f;(x',z'). According to the aggregation
algorithm , a final decision 9° = f(z‘, z‘) is assigned to the individual .
* The true label y’ is revealed after the decision is made.
The goal here is to find an algorithm that combines the experts’ predictions accurately and

fairly.

Delayed Feedback Next we also formally describe the setting when the feedback is de-
layed: As before, the decision making process runs through rounds t = 1,...,7. At each
round t:

* A single individual (a*,z') € R” arrives, where x! € 4 is a set of features and

48

able to use some dynamics policies rather than a fixed policy designed for a single fairness

constraint.

105

4-16

4-17

4-18

4-19

5-1

6-1

Histogram for the final distribution for repaying probability after different
policies. The unfilled bars indicate the initial distribution and the filled
bars indicate the final distribution. Top row: forgiving setting. Middle row:
neutral setting. Bottom row: harsh setting. ..............00-
Gini- WGI and Gini-BGI for different fairness policies. The dashed line in-
dicates the advantaged group, and the solid line indicates the disadvantaged
SYOUP; a ewe ee EER HERS REESE RES ELE ETA OR EEE EGE
Initial distribution for the Gaussian 2d. Left: initial feature distribution.
Right: initial target variable distribution... ..............004
Synthetic Gaussian Example. Top: MM, = [1,1]. Middle: M, = [1,—1].
Bottom: M3 =[-1,l]). .. 2... ee ee

Different types of distributional shifts in sequential decision making. . . . .

Pareto Curve for the synthetic dataset with imbalanced setting. x-axis is
the regret and y-axis is the average value of Equalized FPR and Equalized

FNR. The pair indicates (Aregret, AFairness) Where AFairness = AFPR =
Threshold for different fairness policies as a function of cost ratio. The

dashed line indicates the threshold for advantaged group, and the solid line

indicates the threshold for disadvantaged group. .............--

16

102

Notation

Meaning

 

 

 

D Underlying distribution where the dataset is sampled from

Z Protected group attribute such as gender or race

xX Feature attributes the other than protected attribute

Y Ground truth target variable

Ss State S consists of (Z, X,Y)

O O ~ Bernoulli(Y). An instantiation of the target variable.

(z,x,y) An individual sampled from the distribution is a tuple of the protected at-
tribute, feature attribute, and ground-truth label

Y The CDF distribution of target variable Y .

G A DAG representing the dependency between state variables

fol) Structural equations for node v

Vfuo(x) Derivative of structural equations f, evaluated at x

St State at time ¢ consists of (Z, X*, Y‘)

Dt Decision at time t

ut Utility for the decision maker at time t

Te Policy function for group z

7 Threshold used for group z at time t

Ltp Feature value increase for a true positive

Lfp Feature value decrease for a false positive

Lin Feature value increase for a true negative

Lyn Feature value decrease for a false negative

Utp Utility increase for a true positive

Ufp Utility decrease for a false positive

Uin Utility increase for a true negative

Ufn Utility decrease for a false negative

g Distance metric

ot Within-group impact for group z at time t

Abe Between-group impact of group A and B at time t

 

Table 3.1: Notation table for the terms used in this chapter.

46

Setting Description Examples

 

online setting An online setting where data Admit students on rolling
becomes available sequen- basis.
tially.

black box experts black box experts where the | Committee Members.

function that generates pre-
dictions cannot be modified.
aggregation algorithm An algorithm that combines Head of the committee.
predictions from black box
experts

 

Table 3.2: Unique properties of the setting.

work. In this section, we focus on designing an aggregation algorithm that combines the

binary predictions from black box experts in order to produce a fair and accurate decision.

3.1.2 Notations

We start with binary classification problems, with a positive and a negative class, i.e., Y €
{+,—}. Each example (also referred to as individual) in the data set consists a pair (a, z) €
IR”, where x € X is a vector of features attributes and z € Z is the group attribute . We
also assume that the group attribute is binary can be partitioned into Z € {A,B}. Let
F ={fi,..., fa} bea finite set of black box experts; and let 7 = f(a, z) be the prediction
of an expert on an example (x, z). We denote the group rate p, as the probability that
an individual comes from group attribute z, where p, = P(Z = z). We denote the base
rate j1,,, as the probability that an example comes from group attribute z has label y,where
be = PY = y|Z = 2).

We use superscript ¢ to denote the time index or round t; for instance, y’ is the true
label associated to the individual arrives at round ¢, ie. (x’, 2’). Superscript * denotes
optimality; for instance f*(z, y) represents the best expert on group attribute z with label
class y.

Throughout this thesis, it is often necessary refer to an expert f, to the group attribute z,
to the true label y or a combination of them. We indicate such a combination with a list of

subscripts at the right of the variable. Thus, for instance, wy, denotes the weight associ-

47

the distributional impact of algorithmic decisions on the target variable. In particular, we
characterize the long-term impact on the center and spread of the target variable distribution
as a result of threshold policies. The theoretical results provide useful guidance on choosing

the best threshold policies when balancing different considerations.

Better metrics for long-term fairness Fairness constraints ensure that the decisions as-
signed satisfy some statistical parity in a myopic way that is oblivious to dynamics, yet
these decisions could impact features and target variables in an undesired way. In practice,
there are often trade-offs between myopic decision fairness and long-term fairness.

In addition, the fairness constraints we discussed are all defined in terms of error metrics
that are based on the average outcome and are thus ignorant of the distributions. When risk
distributions differ, these error metrics could be poor indicators of inequality. Decisions
based on classification parity metrics could lead to dichotomies on the target distribution
even when the average outcome remains the same. The within-group and between-group
impacts are useful metrics for measuring the dynamic and distributional impacts of algo-

rithmic decisions.

Mitigating the backfire effects Designing fair policies requires careful consideration
of interactions between the decisions and the underlying distributions. In this work, we
characterize the optimal threshold for maximizing utility, maximizing within-group impact,
and minimizing between-group impact.

Our simulation results suggest that there is generally a trade-off between utility and
between-group impact. A higher between-group impact indicates the policy leads to a
higher disparity between two groups as measured by some metrics. In specific, a lower
threshold generally leads to higher utility, but also higher between-group impact as mea-
sured both by the mean and variance of the target variable. This trade-off suggests that
mitigating backfire effects requires careful consideration and balance between different
desiderata. For fairness policies that are computed based on constrained optimization, a
different threshold could be used at each round. This further increases the complexity of

comparing pros and cons of different fairness policies. In practice, it could be more desir-

104

0.20

 

=——— Group A
0.15; —— Group B

0.10

0.05

=5 5 | aati litt
. 000000 «0.25 «(050 0.75 1.00

x1 Target Variable

   
  

 

 

 

 

Figure 4-18: Initial distribution for the Gaussian 2d. Left: initial feature distribution. Right: initial
target variable distribution.

the right. The feature update is X;, = [0.02,0.01] and X;, = [0.01,0.02]). We simulate
with two feature contribution matrices M, = [1,1] and Mp) = [1,-—1], where the first or
second feature is a "bad" feature (negatively impacts the target variable) respectively. In

Figure 4-18, we plot the initial features and target variable distribution.

Feature segregation In Figure 4-19, we plot the final distribution of the features under
different structural equations fy. In the top row, both features positively contribute to the
target variable (fy(X) > 0). In the middle row, the first feature negatively contributes to
the target variable; and in the third row, the second feature negatively contributes to the
target variable. This shapes the feature spaces differently even though the feature tran-
sition equation X‘+! = fy(X‘) is the same. Even when the target distribution is close
enough, certain features could be segregated more than is desired for the groups. In real-
world applications, the structural equation between features and target variable fy (X) is
rarely known and is in fact what most machine learning models are trying to predict. This

interplay between features and target variables adds more complexity to the analysis.

4.6 Conclusion and key takeaways

In this chapter, we model the interactions between decision-makers and individuals using

MDP where the transitions could be general structural equations. This allows us to analyze

103

6.2 Appendix for Chapter 4: Fairness with Dynamic Feed-
back

6.2.1 Additional Experiments Results
6.2.1.1 Thresholds for each policy as cost ratio increases

In figure 6-2, we plot the average thresholds of each policy as a function of the cost ratio.

Regardless of the group, MaxUtil’s threshold only depends on the parameter for the utility

Ufp

function ([; F
‘fp utp

 

), which is set as 0.5 in the experiment.

For DemoPar policy, it consistently overcompensate for the disadvantaged group by assigning a
lower average threshold for the disadvantaged group. Remember that demographic parity constraint
equalize the rate of possible decisions, and if the disadvantaged group has a lower target variable,
the threshold will also be lower.

The case with EqOpp is a little bit more complicated. As cost ratio increases, EqgOpp switches
from lower threshold for disadvantaged group to lower threshold for advantaged group. The reason
is that equalized opportunity requires both group have equalized false positive rates. In the loan
application case, this means the decision maker should issues the same percentage of people loans
among those who can repay. Higher cost ratio means the stake of defaulting (false positive rate) is
higher for an individual. When the stake of false positive rate is high for an individual, and will lead
to the greater target variable decreases. By assigning a lower threshold for the advantaged group,
EqOpp intentionally increases the false positive rate of the advantaged group in order to match that

of the disadvantaged group.

128

This doctoral thesis has been examined by a Committee of the Institute of
Data, Systems and Society as follows:

Professor Alberto Abadie.... 0.0... ccc ccc cc cee cece ene eeee
Chair, Thesis Committee
Professor of Economics

Professor Caroline Uhler......... cee cee cece ene enn eens
Member, Thesis Committee
Professor of Electrical Engineering and Computer Science

Professor Alfredo Cuesta-Infante .........0. 00.0 ccc cc cee ccc eee eee
Member, Thesis Committee
Professor of Computer Science

Term Meaning

 

State of the world The state of the world refers to the underlying ground-truth distribu-

tion.

Sampling Sampling is the process of taking measurements from the state of
the world.

Dataset A dataset is a collections of data points sampled.

Training Training is the process of learning patterns from the dataset.

Model A model takes the training data and optimize for some objective
function.

Inference Inference is the process of using a model to make predictions on
new data points.

Prediction Prediction is some scores or labels generated by the model during

the inference process.
Decision-making Decision-making is the process of transferring model predictions to
an actionable decision.

Individual An individual is a data point where the decision is made on. In the
context of fairness, an individual refers to a person.

Feedback Feedback is the process where decisions could impact the state of
the world.

 

Table 1.2: A glossary of the terms used in the machine learning cycle. The bold terms are the nodes
in the cycle, and the italic terms are the edges in the cycle.

In the next section, we show that biases can manifest in any part of the typical de-
ployment cycle of machine learning models. These range from biases caused by using an
unbalanced dataset, to biases that come from spurious correlations between demographic
information and predictions in model representations to biases that occur during the process

of transforming a model prediction into a decision.

1.2. A Full Taxonomy of Biases in Machine Learning

We next showcase how biases can arise through different development stages of a machine
learning model. We use Figure 1-1 to illustrate a typical development cycle of machine
learning models, and use this to show how biases could arise and be mitigated in each

stage. The terms used in the figure are explained in Table 1.2.

23

attribute (2) Equalized Opportunity (EqOpp) [Hardt et al., 2016] requires a predictor that
is independent of the sensitive attribute given that the label is positive, and (3) Equalized
Odds (EqOdd) requires a predictor is independent of the sensitive attribute given the true
label, and(4) Calibration [Verma and Rubin, 2018] (test fairness) requires that outcomes

should be independent of protected attributes conditional on the risk score.

Demographic Parity The most intuitive definition is demographic parity, which requires
the probability of positive prediction should be equalized for different groups. In other
words, the prediction Y should be independent of the sensitive attribute 7. This metric

could be achieved when the model ignores the group attribute.

Definition 2.2.1 (Demographic Parity (DemoPar)). A predictor Y satisfies demographic

parity if
P(Y =1|Z = A) =P(Y =1|Z = B)

One issue with demographic parity is that it ensures the acceptance rate is equal regard-
less of whether an individual is qualified or not. If the target variable Y is correlated with
group attribute 7, demographic will rule out the perfect predictor Y = Y [Hardt et al.,
2016].

Equalized Error Rates Accuracy parity, or equalized error rates, improve on demo-

graphic parity by bringing the true qualification target variable Y into the definition.

Definition 2.2.2 (Equalized Error Rates (EqERR)). A predictor Y satisfies equalized error

rates if

P(Y AY|Z=A)=P(Y 4Y|Z =B)

However, equalized error rates don’t distinguish between the error types, and the cost

of false positives and false negatives could be very different in many applications.

Equalized Opportunity Equalized opportunity requires that the predictor Y is indepen-
dent of the group attribute 7 conditional on the positive outcome Y = 1. For example,

in the loan application example, this requires that among all people who could pay back

35

t _ Jet t @ Group
Aap= 62-4 53-8| @® Group B

t
62-4

¢
Bhs )

Density Density

 

Target variable Target variable

t=0 ———————_-> tt=t

Figure 4-8: An illustration of the backfire effects of a policy. 6,4 and 6, measures the impact
of decisions on the orange group (group A) and blue group (group B) respectively. Here group B is
the disadvantaged group since its target variable distribution lies on the lower spectrum. Compared
to the initial distribution at time t = 0, decisions lead to backfire effects in terms of WGI for group
B (the center is decreased and spread is increased. Decisions also lead to backfire effects in terms
of BGI where group A and group B’s distributions are further apart.

¢ Between-group Impact(BGI): Between-group impact measures the absolute differ-
ence between two groups’ within-group impact. Between-group disparity captures

whether the within-group impact is different among different groups.

Definition 4.4.1 (Within-group impact (g-WGI)). Let Y! be the group z’s target variable at
time t and Py be the distribution of Y!. The within-group impact is defined as the change
in the distribution as characterized by a function of the distribution with respect to t = 0
for group 2, i.e,

5! = g(Pyz, Pys) (4.2)

where g(-) is some distance metric.

The choice of the distance metric Previous works studying the long-term impact of fair-
ness decisions [D’ Amour et al., 2020b][Mouzannar et al., 2019][Zhang et al., 2020] have
focused on how decisions change the outcome in the average sense, where g is the abso-

lute difference of the mean. Here we allow g to stand for general functions that measure

88

3.4.3.1 Optimal balance between regret and fairness

In this section, we show that hype and hryr can be set to zeros by solving the following

set of functions:

 

 

t _ t
~ A, YO | qA— | 0, (3.8)
parha--T pp: be--T] | gp

_ t t C
DQ _ Der Oe | ac | _ ig, (3.9)
paras *T pe: p++ T

B+

In addition, the upper bound for regret in Eq. (4) can also be tighten by adding the

following constraint:

[Sek Leak Cra, Crake] | | =o (3.10)

Given all these equations, constraints and inequalities we can define the following op-

timization step.

Optimization step At each round, we are led to solve three functions of q where function

parameters are determined by the equations (3.8), (3.9) and (3.10) defined above.

q* = Aihrec(q) + A2hepr(q) + Ashenr(q) (3.11)

where A = [Aj, Ag, A3] is a vector balancing the importance of regret, equalized FPR,
equalized FNR that can be provided on a case-by-case basis for different applications. In
our experiments, we solve (3.11) by using a Sequential Least Squares Programming method
(SLSQP) and setting Ay = Ap = A3 = 1.

In practice, G-FORCE can accommodate different use cases by setting different A at

63

20

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

State of the World P(X,Y,Z)

7 1
= @® © @ @ &
xample * Se r

¥} y? ys Y} Ys
Experts Predictions 8 ws ws ae
va| |yz| jee) lel dye
‘ y ‘ ¥v ‘
Algorithm
v v v ¥v v
Action y) y? ys y+ ys
| T=2
Feedback Yi ¥3
T=1
[—)> ¥2

 

 

Figure 3-4: A figure depicting online learning w.

 

ith constant delay with 74 = 2 and tg = 1.

jim Regret(T)/T = 0. This means that as round goes on, the average regret goes to
00

zero and the algorithm converges to the best ex:

3.1.4 Metrics for evaluating fairnes

pert in hindsight.

Ss

In addition to regret, we also evaluate the fairness on the online algorithm. We introduce

two metrics: Equalized error rates (EqERR ) and Equalized Odds (EqOdds).

Definition 3.1.2 (EqGERR and c-ERR). A randomized algorithm satisfies EgERR if:

EY AY|Z=A]=

A randomized algorithm satisfies €-ERR if:

 

 

ie

 

 

[YY 4Y|Z=A]—

 

 

 

 

EqERR requires that the algorithm makes
rate) for all groups. In this metric, different ty:

tives) are not distinguished.

 

 

 

YAY|Z=B]

 

[V £Y|Z = Bl <e

equal percentage of errors (equal accuracy

pes of errors (false positives and false nega-

50

Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-
supervised photo upsampling via latent space exploration of generative models. 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pages 2434-2442, 2020.

Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai.
Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In
NIPS, 2016.

Richard S. Zemel, Ledell Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. Learning
fair representations. In JCML, 2013.

Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational
fair autoencoder. CoRR, abs/1511.00830, 2016.

Flavio du Pin Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
Kush R. Varshney. Optimized pre-processing for discrimination prevention. In NPS, 2017.

Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya
Kannan, Pranay Kr. Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema
Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan Saha, Prasanna Sat-
tigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. Ai fairness 360: An exten-
sible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. ArXiv,
abs/1810.01943, 2018.

Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision
making and the cost of fairness. In KDD, 2017.

Richard A. Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie H.
Morgenstern, Seth Neel, and Aaron Roth. A convex framework for fair regression. ArXiv,
abs/1706.02409, 2017.

Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In
NeurIPS, 29, pp. 3315-3323, 2016.

Smitha Milli, John Miller, Anca D. Dragan, and Moritz Hardt. The social cost of strategic classifi-
cation. Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.

Ganesh Ghalme, Vineet J. Nair, Itay Eilat, Inbal Talgam-Cohen, and Nir Rosenfeld. Strategic clas-
sification in the dark. ArXiv, abs/2102.11592, 2021.

Xueru Zhang, Mohammad Mahdi Khalili, Kun Jin, Parinaz Naghizadeh Ardabili, and M. Liu. Fair-
ness interventions as (dis)incentives for strategic manipulation. In JCML, 2022.

Vijay Keswani and L. Elisa Celis. Addressing strategic manipulation disparities in fair classification.
ArXiv, abs/2205.10842, 2022.

Yi Sun, Ivan Diaz, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Towards reducing biases
in combining multiple experts online. In JJCAI, 2021.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness
through awareness. ArXiv, abs/1104.3913, 2012a.

132

2.3. Related Work

2.3.1 Fairness Metrics

Individual Fairness Fairness can be considered individually or collectively. At the in-
dividual level, fairness can be defined as "similar individuals should be treated similarly"
Dwork et al. [2012a]. yet it is often challenging to specify suitable distance metrics to

measure similarity between individuals.

Group Fairness At the group level, fairness can be defined as balancing some statistical
metrics approximately across different demographic groups (such as gender groups, racial
groups, etc.). Equalized odds [Zafar et al., 2017], or "disparate mistreatment," requires
that no error type is disproportionate for any one or more groups. This could be achieved
by equalizing false positive rates, commonly referred to as equal opportunity [Hardt et al.,
2016], or equalizing classification errors. In addition to statistical parity, another line of
research focuses on defining fairness from a causal perspective. Kusner et al. [2017] first
defines counterfactual fairness as requiring a decision to be the same in the counterfactual
world where the individual belongs to a different group. For a more comprehensive list of
the fairness definitions, we refer the readers to the survey paper [Verma and Rubin, 2018].

Lastly, people have also proposed some metrics that are beyond prediction problems.
These approaches often are rooted in other domains such as economics, law, and psy-
chology. For one, Heidari et al. [2019] proposes an effort-based measure of fairness and

quantify how algorithmic policies would reshape the underlying populations.

The incompatibility of fairness metrics Despite the numerous definition of fairness,
many of them could be inherently incompatible both from a mathematical perspective and
also from a conceptual perspective. First, individual fairness and group fairness could be
conflicting where satisfying group fairness can yield harm for people belonging to those
groups [Dwork et al., 2012b, Corbett-Davies et al., 2017, Green, 2020].

Even within group fairness metrics, recent work shows that it is impossible to simul-

taneously achieve equalized odds [Chouldechova, 2017, Kleinberg et al., 2017] with other

38

plies here as well: lower thresholds lead to higher average utility but also a higher disparity

between the two groups as measured by the variance of the target variable.

—— Group A
0.10

—— Group B

--- Utility

 

©
S
a

ee
0.00 ———

 

\
\
\
\
\

var-WGl One-step

 

1.00

0.75
=

0.505

5
0.25

 

0% 080 076
Forgiving Setting

Figure 4-12: One-step simu

—— Group B

\
\
\
\
\

025° 050 0.75
Forgiving Setting

\
1.00

0.00

 

 

var-WGl One-step

—— GroupA =—— Group B

0.10:

--+ Utility

1.00

 

©
3
a

©
3
8

 

 

 

 

050
Neutral Setting

0.25 0.75

—— Group B

---U

0.05} »

0.00

var-WGl One-step

 

028 080 075
Harsh Setting

   

tility
1.00

0.75
=
0.50

0.25

ation on var-WGI under different fixed threshold values for the forgiv-
ing (left), neutral (middle), and harsh settings (right) respectively. The red dashed line indicates
TMaxUtil for each setting.Here the utility is the average utility over the simulation steps.

==> Utility

1.00

0.75

-/0505 F

5
0.25

 

\
1.00

0.00

tility

 

—— GroupB

 

0.25
Neutral Setting

050° 0.75 1.00

-=+ Utility

1.00

0.00

—— Group A

—— Group B

== UI
0.20

   
 

0.10

oO =.

7 Saeee.
= 0.00) / ae wn
5 / \
7.0.10

 

\
\
\

-0.20

0.00 0.25 050 0.75

Harsh Setting

\
1.00

tility
1.00

Figure 4-13: Multi-step simulation on var-WGI under different fixed threshold values for the for-
giving (left), neutral (middle), and harsh settings (right) respectively.

4.5.2.3 Fair policies lead to backfire effects

In this section, we compare policies that maximize utility subject to fairness constraints

(DemoPar, EqOpp) with policy that only maximizes utility (MaxUt il). Besides MaxUtil pol-

icy, all other policies use a different threshold at each time step based on the solution from

the optimization problem in eq-4.1. This adds complexity in quantitatively characterizing

the backfire effects of fair policies. Instead, we use the simulation results to provide some

insights on enforcing fair policies. In this section, we investigate the impact of fair policies

on mean of variance of the target variable distribution.

Impact of fair policies on mean In Figure 4-14, we plot mean- WGI and mean-BGI as

a function of the cost ratio. As shown in the theoretical result, mean-WGI monotonically

decreases as the cost ratio increases. This is showcased in figure-4-14: as the cost ratio

increases, all policies exhibit negative impacts on both groups.

98

Results For each dataset, we repeat the experiments 100 times, each with 10000 sam-
ples from a specific distribution setting. For imbalanced setting, the results in Figure 3-8a
shows that for GroupAware algorithm, the larger subsets {A,+} and {B, —} have nearly
100% accuracy while {.A, —} and {.B, +} have around 50% accuracy. The GroupAware al-
gorithm, which runs only one MW instance per group attribute z, promotes selecting the
perfect classifier for the larger group-label subset within each protected group. This leads
to high error rates on the remaining subsets since their associated perfect classifiers are

unlikely to be picked.

GroupAware GFORCE

 

1.0

0.8 | IFPRa- FPRg|=0.53

|FNRq- FNRg|=0-52

FPR ,- FPRg|=0.13
0.6

|FNRa- FNRg|=0.17

0.4

0.2

  
   

  

0 —— 0.0
FPR, FPRe FNRa FNRe FPR, FPRe FNRa FNRe

(a) Imbalanced Setting.

GroupAware GFORCE

1.0 1.0

 

0.8 0.8

IFPR a- FPRg|=0.17 IFPRa- FPRg|=0.0

0.6 IFNRa- FNRgi=0.15 | g 6 IFNRa- FNRg|=0.02

0.4 0.4

0.2 0.2

 

0 - 0.0
FPR4a FPRg FNRa FNRg FPRa FPRg FNRa FNReg
(b) Balanced Setting.

Figure 3-8: The achieved accuracy on group-label subsets for imbalanced setting (p4 = 0.9, wa4 =
0.7, 4B,+ = 0.3) and balanced Setting (p4 = 0.5, v4.4 = 0.5, wp.4 = 0.5). Left: GroupAware.
Right: G-FORCE . The vertical black line denotes the standard deviation. The red dashed line is the
overall accuracy.

Even for the perfectly balanced setting, G-FORCE achieves a more balanced accuracy

in each subset and a more stable behavior compared to GroupAware as in Figure 3-8b.

67

¢ When we know that the distribution shift is purely caused by the model’s decisions
(as is assumed to be the case in backfire effects), can we create a more efficient
algorithm where frequently retraining and re-balancing different fairness constraints

is not necessary?

110

Theorem 3. GroupAware Equalized Error Rate (EqERR ) ([Blum et al., 2018])
Let ERR, be the error rates on group z, and z* be the group with the lowest error rates.

Let < 1/2,

Ind
|ERR4— ERRp| < 57 ERRy-(2+) + —

where f*(g*) is the best expert on the group with the lowest error rate.

The above theorem shows that the equalized error rates of the algorithm is also upper

bounded by the equalized error rates of the best expert.

3.3 Motivation for our work

3.3.1 Need to use distinguish error types

One potential drawback of the group-aware algorithm is that it only bounds the perfor-
mance of the overall algorithm errors for each group, without a guarantee of how the errors
will distribute across the label classes. In many real life applications, false positive rates
and false negative rates could have very different implications and costs. In the COMPAS
example Angwin et al. [2016] shown in the first chapter, the algorithm has approximate
the same accuracy (error rates) for black and white defendants. The algorithm is biased
towards black defendants since it has a much higher false positive rate for black defen-
dants where they could be mis-classified as being high risk and arrested. This showcases
that equalized error rates is not a suitable metric to measure fairness if FPR and FNR have
different implications. In this work, we use equalized odds as the fairness metric, which

balances both FPR and FNR across groups.

3.3.2 Need to care about label imbalance

Beyond the scope of fairness, unbalanced label class is a fairly common phenomenon in
many machine learning applications. For a highly imbalanced distribution, even if the

aggregation algorithm performs badly on the minority label class, and the regret or accuracy

53

4.1 Introduction

The first part of this thesis considers whether it is possible to produce fair decisions from
black-box predictions in an online setting. One important assumption in this study is that
a past decision will not impact future distributions of features. As we saw in the first
chapter, decisions could create a feedback loop that nudges feature distributions of different
groups in different ways. The change in feature distributions will in turn change the target
variable distribution. In classical machine learning settings, the goal is to create a model
that minimizes empirical risk with respect to a dataset. In this setting, fairness constraints
can be enforced through a constrained optimization. However, these predictions could lead
to consequential decisions, because the predictions of the model could have long-lasting
effects on target variable distribution beyond a single step.

In this chapter, we study whether enforcing fair decisions closes the gap of target vari-
able distribution between advantaged and disadvantaged groups. The chapter is structured

as follows:

* In section-4.2, we first use an example of loan application to showcase how fair

decisions could shape underlying distribution undesirably.

¢ In section-4.3, we present our setting for modeling interactions between decision-
makers and underlying distribution as Markov Decision Process. We focus on thresh-
old policies, i.e. policies which assign positive decisions when features or target

variable is above some threshold.

* In section-4.4, we first formally propose a metric to measure the distributional im-
pact of algorithmic decisions on the target variable distributions. We identify the
backfire effect — i.e. when policies result in a disproportionate impact on a protected
group over the long term. Specifically, we can categorize the backfire effect into two
scenarios: (1) within-group impact measures how a sequence of decisions shifts the
distribution of the target variable of a group, and (2) between-group impact measures

the absolute difference between two groups’ within-group impact.

¢ In section-4.5, we investigate the impact of fair threshold policies, which are derived

74

12

6.1.2.1 Proof of Lemma 1

Let ! , = 0 ;<~ wy... We start computing the expected loss on group 2, +:

Elt..J= dom. 4 Ay=+}

 

 

SEF
=> dea: teh +¢ 7 le Lf Wty = +}
ie SEF
_. pt What t _ t =
=¢4°>, a, elty= +}+q_- ye a -E Ay = +}
SEF SEF
we we
whet fe
De ee (6.1)
fer ~2t feF

The overall expected loss is composed by two terms: the former, which is the expected loss
on group z,+ when their associated weights wy, are selected, and the later, when the
wrong weights wy, are selected. Both terms are weighted by their respective estimated

rates q! , and q!_.

Then, we have the following inequality:

t+1 __ t+1
ey = eye

 

feF
= > wt (1- tty}
SEF
< SO win — 0G Afy = +})
{EF
= > whey ~ n>, Wher lpet
SEF SEF
wi, +
74 LS at)
FCF ot ob

114


—— MaxUtil —-— DemoPar —— EqOpp

 

 

 

 

 

 

 

 

 

—— MaxuUtil —-— DemoPar —-— EqOpp

0.3

5 o

$ om 0.2

: Fo -

o

€ E / a
9 ——

0.30 0.45 0.60 0.75 , 0.30 0.45 0.60 0.75
cost ratio cost ratio

Figure 4-14: Mean-WGI and mean-BGI for different fairness policies.The dashed line indicates the
advantaged group, and the solid line indicates the disadvantaged group.

Across different cost ratios, MaxUt il always exerts a more positive WGI for the ad-
vantaged group compared to the disadvantaged group. On the other hand, the relative WGI
for the groups for DemoPar and EqOpp swaps when shifting from a low-cost ratio to a
high-cost ratio regime. Under a low-cost ratio regime, both policies have more positive
impacts on the disadvantaged group; and under a high-cost ratio setting, both policies have
more positive impacts on the advantaged group. Comparing the three policies, while the
WGlis fairly similar for the advantaged group, MaxUt il exhibits the least negative impact
and DemoPar exhibits the most negative impact on the disadvantaged group.

The mean-BGI metric measures how the average change in the target variable differs
between two groups. A high mean-BGI indicates a policy increase in the disparity of the
target variable between two groups in the average sense. In terms of mean-BGI, EqOpp
results in the highest mean-BGI when the cost ratio q is lower, and DemoP ar results in the

highest mean-BGI when the cost ratio is higher.

Impact of fair policies on variance In Figure 4-15, we plot mean-WGI and mean-BGI
as a function of the cost ratio. As shown with the theoretical result, the direction of var-
WGI doesn’t monotonically increase as cost ratio increases. This is illustrated empirically
in figure-4-15. As cost ratio increases, var-WGI increases for disadvantaged group and
decreases for advantaged group.

For var-BGI, MaxUtil leads to higher var-BGI for high cost ratio settings, and fair

policies (DemoPar and EqOpp ) leads to higher var-BGI for low cost ratio settings.

99

Markov Decision Process A Markov decision process (MDP) is a tuple (S,.A, P, R) in
which S is a finite set of states, A is a finite set of actions, P is a transition function defined

as P:S x Ax S —+ [0,1], and R is a reward function defined as R: S x A —> R.

4.3.2 Modeling the feedback loop as MDP

In this section, we show that the feedback loop of algorithmic decisions can be modeled
using MDP. At each time step, the state contains three variables: S = (X,Y, Z), where
X € R¢isa set of features, Z € {0,1} is the time-invariant group attribute such as race
or gender, and Y € [0,1] is the target variable representing the probability of a positive

outcome.

¢ Initialization: The process is initialized with a time-invariant group attribute Z, a
set of observed features X‘, and the target variable Y*. The initial group distribution
is time-invariant and sampled from Z ~ Bernoulli(po) where pp = P(Z = 0)
is the probability that an individual comes from group Z = 0. The initial feature

distribution X° is sampled from the initial distribution P(X°|Z).

Decision and Outcome: The target variable Y* is a function of the features X‘, i.e.,
Y!' = fy(X*). At time step t, a binary decision D' € {0,1} is generated from a
policy function 7 based on state S’, i.e, D' = 7(S*). After applying the decision, a
binary outcome is observed. We use an auxiliary variable O' ~ Bernoulli(Y‘) to
indicate the outcome variable, which is sampled from a Bernoulli distribution with

Y* as the parameter.

Transition: Based on the realized outcome O', the features X' for each individual
will be updated based on the decision and the outcome, where X'+! = fx(X‘, D', O°).

The target variable Y‘ will be updated accordingly.

Utility: The decision maker’s utility is a function of the decision and the realized

outcome, ie., Ut = fy(O', D*).

In Figure 4-6, we illustrate the dynamic environment unrolled by time, where the purple

83

4.4 Measuring the Long-term Impact of Decisions

In this section, we propose a new metric for measuring fairness through the long-term
impact of decisions. We first discuss the shortcomings of current fairness metrics in a
sequential decision-making environment. These gaps motivate us to design better metrics

to assess the fairness of decisions in sequential and dynamic environments.

4.4.1 Filling in the gaps for long-term fairness metrics

Decision Fairness vs Outcome Fairness Existing fairness metrics are defined for de-
cision fairness, which ensures the decisions satisfy classification parity (accuracy, false
positive rates [Hardt et al., 2016] etc.) at the time of decision-making. However, under the
feedback loop, even fair decisions can potentially impact outcomes or the target variable

unfairly.

Average Fairness vs Distributional Fairness In many real-world scenarios, such as with
loan applications, the target variable distribution is often skewed or heavy-tailed. Conclu-
sions drawn from decisions made using only metrics defined as oblivious to the distribu-

tions could be insufficient.

4.4.2 The distributional impact of algorithmic decisions

We introduce a novel fairness metric for measuring the impact of algorithmic decisions on
the distribution of the target variable. We first categorize the impact as within-group impact

and between-group impact:

¢ Within-Group Impact(WGI): Within-group impact measures how a sequence of al-
gorithmic decisions shifts the distribution of the target variable of the group. Within-
group disparity happens when decisions following a policy lead to a negative impact
on the group, such as further increases in inequality or dichotomy within a population

group.

87

|

GroupA —— GroupB  —=- Utility —— GroupA —— GroupB =~ Utility ——GroupA —— GroupB == Uflty

 

 

    

 

 

 

 

 

 

 

§ 0.10 100 $0.10 7 100 § 1.00
% % %
& 0.05 0.75 & 0.05 075 6 6@ 0.75
6 26 26 2
tm (0.00: i 0.505 zm 0.00 7 0.505 @ 0 0.505
g i ‘ 5 g i \ 5 g 5
@ 0.05 i \ [0.25  E-0.05 | \ j025 E+ 0.25
3 | \ 3 | \ 3 | \
E10 025 080 076 1.00°° EM o59 025 050 075 1.00°° E950 035 050 075 1.00°°
Forgiving Setting Neutral Setting Harsh Setting
Figure 4-10: One-step simulation on mean-WGI under different fixed threshold values for the for-

giving (left), neutral (middle), and harsh settings (right) respectively. The red dashed line indicates
optimal threshold for MaxUtil.

 

 

 

 

 

 

 

 

 

 

 

 

 

—— GroupA —— GroupB —=- Utility —— GroupA —— GroupB == Utility —— GroupA —— GroupB === Utility
0.10 1.00 0.10 1.00 0.10 1.00
@ 0.05: — 075 6005} 075 @ 0.75
= fa SSSStoenn 2s SS 2s
© 0.00] ” = 0.50 ¢ 0.00 Y — |0505 ¢ Tha [0.505
5 \ = § Z Bs — =
a \ Po \ > 9 \ eS
E -0.05. \ jo25 £-0.05 \ jo25 £ oe \ [0.25
\ . \
A a ig A
°.105 90 035 080 0.75 1.00% °105 50 025 060 0.75 1.00% 0 0o0 035 080 0.75 1.00°
Forgiving Setting Neutral Setting Harsh Setting

Figure 4-11: Multi-step simulation on mean-WGI under different fixed threshold values for the
forgiving (left), neutral (middle), and harsh settings (right) respectively. Here the utility is the
average utility over the simulation steps.

Spread In figure-4-12, we plot the var-WGI for one-step simulation. Var-WGI measures
how the threshold policies change the variance of the target variable distribution changes.
A positive var-WGI indicates the policy increases the spread of the target distribution. In
the loan application example, this indicates the inequality of the repaying probability dis-
tribution within a group is increased. Contrary to mean-WGI, the relationship between cost
ratio setting and var-WGI is not linear. This is reflected as the neutral setting leads to lower
var-WGI than either the forgiving setting or the harsh setting. For a fixed cost ratio, a higher
threshold is always more desirable as it leads to lower var-WGI. As shown in figure 4-12,
all threshold policies lead to a non-negative var-WGI.

Var-BGI measures how the threshold policies shift the variance of the two distributions
differently. For each setting, as the threshold increases, var-BGI decreases. This suggests
that for var-BGI, a lower threshold is always more desirable. Across different simulation
settings, as the cost ratio increases, the gap between var-WGI (var-WBI) decreases.

In figure-4-13, we plot the var-WGI for multi-step simulation. Multi-step simulation
generally shows the same trend for var-WGI as in one-step simulation, and also with an

amplification effect on the magnitude of var-WGI. The same trade-off for mean-WGI ap-

97

 

| Group A Group B Positive label #ofrounds pa flay PB +

Adult | White non-White income exceeds 50k/yr 24421 0.851 0.26 0.16
German Male Female good credit score 300 0.853 0.73 0.50
COMPAS | White non-White low risk for recidivism 1584 0.398 0.54 0.39

 

Table 3.4: Summary statistics of datasets. Here p, is the percentage of group A, ju4+ is the
percentage of positive labels in group A, and ju, is the percentage of positive labels in group B.

Since the label distribution is balanced, {A,—} and {.A,+} have the same accuracy when
classifying an example from group A. GroupAware arbitrarily chooses between perfect
classifier for {A,+} or {A, —} when classifying examples from group A, which leads to
large deviations when considering errors on each more fine-grained subset (same analogy
for group B). On the contrary, in both settings, G-FORCE is able to track the performance
of the EqOdds on each group-label subset and compensate their differences in terms of
accuracy. In the plot, the red dashed line represents the overall error rates of the algorithms.
As shown in the theoretical results, compared to GroupAware G-FORCE has a slightly

increase in regret, and is reflected as the slight increase error rates in the experiment.

3.6.2 Case study: Real Data sets

Datasets We use the Adult, German Credit and COMPAS datasets, all of which are
commonly used by the fairness community. Adult consists of individuals’ annual income
measurements based on different factors, and the goal is to predict whether someone’s in-
come exceeds 50k/yr based on census data. The group attribute is race, and the two groups
are White (Group A) and non-White (Group B). In the German dataset, people applying
for credit from a bank are classified as “good” or “bad” credit based on their attributes. The
group attribute is gender, where the two groups are male (Group A) and female (Group B).
COMPAS provides a likelihood of recidivism based on a criminal defendant’s history. The
group attribute is again race, where the two groups are White (Group A) and non-White

(Group B).

Creating Black-box Experts The set of black box experts¥ that form the black-box
experts are: Logistic Regression (LR), Linear SVM (L SVM), RBF SVM, Decision Tree

68

Committee Predictions

Qualification Distribution (Black-box Predictions)

(State of the world) Pass/Fail Classes

(Feedback)

 

 

Admit/Not Admit

Applicant:
RPT een (Action)

(Sampled Example)

 

 

 

 

Percentage ay

 

   

Qualification score

 

 

 

 

Figure 3-2: An example on college admission with experts.

to making the decision process fair to people of different races, genders, socioeconomic
groups. Specifically, for all the applicants that are qualified, the committee should offer
admissions to the same percentage of people regardless of their population groups. In
figure 3-2, we illustrate the process and demonstrate how it fits into the machine learning

cycle..

State of the world The state of the world consists of a joint distribution over group
attribute Z (race, gender etc) and observed features X (SAT score, GPA, etc). In particular,

students coming from different groups might have a different distribution of features.

Sampled Student At each round, a student sampled from the population applied for the
program. For this motivating example, we assume the population can be split into group A

(orange) and group B (blue).

Committee Predictions For each incoming applicant, each member on the admission
committee will evaluate this person’s qualification, and predict whether the applicant will
succeed in the first semester of study. We will call the prediction of committee member i

on applicant 1 as Y/.

Decision (Admit/Not admit) Admission decision is made based on committee member’s
predictions. In order to aggregate predictions from admission officers with different expe-

rience levels, the head of the admission committee need an aggregation algorithm . The

44

* Utility: Utility is averaged over a decision maker’s utility u; for making a decision

on an individual 7. The higher the utility, the better.

¢ Within-group impact (g-WGI): Within-group impact is measured using equation-4.2
with respect to different g function. For all the WGI metrics on center or shape, a
higher number indicates a better WGI. For all the WGI metrics on spread, a higher

number indicates a worse WGI.

¢ Between-group impact (g-BGI): Between-group impact is measured using equation-

4.3. For between group impact, the lower the better.

We listed the evaluation metrics and their meanings in the following table.

 

 

 

Metrics Expression Range
Utility 5 ui (0, 1]

Mean-WGI 47" yt — 17" Ly? [-1,]]
Med-WGE iyi WP [-1, 1]
var-WGI plat FP - Fee - PP LY
Gin-WGI Gt — G9, Gt = Sah [-1, 1]
e-BGI Io, — 0 (0, 1]

Table 4.8: Evaluation Metrics.

4.5.2 Simulation result: Loan application
4.5.2.1 Simulation setup

The initial group distributions Z°, initial credit score distributions X°, and initial repay
probability Y° are estimated from the FICO score dataset. In this experiment, we set uj) =
Uyp = Land Cmin = 300, max = 850. We also set the feature value change and utility
change when not issuing a loan as 0 (uin = Ufn = Xin = X fn = 0). The initial distribution
of Z°, X°, Y° is shown below. The groups are White (Group A) and Black (Group B), and

we refer to group A (White) as the disadvantaged group.

94

The statistical parity metrics are often oblivious to the underlying risk distribution, and in
some cases could cause harm to the group they are trying to protect [Corbett-Davies et al.,
2017]. Another drawback of statistical definitions is that they largely ignore all attributes
of the classified subject except for the sensitive attribute Z. We next turn our attention to

causal fairness metrics, which consider the relationship between all variables.

2.2.2 Causal Fairness Metrics

Before diving into causal fairness metrics, we first briefly introduce the definition of a

causal model. A causal model is a triple (U, V, F’) such that:
* U is the set of exogenous variables determined by factors, not in the model.
* V is the set of endogenous variables {Vj, ...., Vn}.

* Fisa set of functions {f1,..., fr} called structural equations, one for each V; € V,
such that V; = fi(pai,Upa,), pai C V \ Vi and U,,, € U, where pa; refers to parents

of V; in the causal graph.

Let Yz.—2(U =u) = y denotes the value of Y for given U = wif Z had taken the value

of z.

Definition 2.2.7 (Counterfactual Fairness). Let Z, X,Y represent the protected attributes,

remaining attributes, and true label respectively. Predictor Y is counter-factually fair if
P(¥z_2(U =u) =y|X = 2,2 =z) =P(Vze2(U =u) = |X =2,2 =2)

for all y and z.

Specifically, counterfactual fairness [Kusner et al., 2017] requires that changing the
sensitive attribute Z while holding things that are not causally dependent on Z constant
will not change the distribution of the prediction Y. In order words, a causal graph is coun-
terfactually fair if the predicted outcome Y in the graph does not depend on a descendant

of the protected attribute Z.

37

Age Age or generation

Race Caste, race, color, ethnicity, national origin

Gender Gender, gender expression, sexual orientation

Religion Religion, ideology, politic preferences, membership to guilds-unions-political
parties

Table 1.1: A list of protected attributes.

the likelihood of a defendant becoming a recidivist. While COMPAS’s overall accuracy
is similar for white and black defendants, it has been shown that black defendants were
more likely to be misclassified as being at high risk for violent recidivism [Angwin et al.,
2016]. In particular, black defendants who did not recidivate were nonetheless incorrectly
predicted to re-offend at a rate of 44.9%, while white defendants were only incorrectly

predicted to re-offend at a rate of 23.5%.

In health care, a widely used algorithm [Obermeyer et al., 2019] for predicting risk
scores of extra healthcare needs has been shown to underestimate risks for black patients.
Specifically, the algorithm assigns risk scores to patients, and patients at the 97th percentile
of the risk score are enrolled in the extra healthcare program. However, at this percentile,
black patients have 26.3% more chronic illnesses than white patients. This biased predic-
tion would lead to sick black patients not receiving the extra care they need. This happens
because the model uses health costs as a proxy for healthcare needs. Because black pa-
tients tend to spend less money than white patients at the same level of healthcare needs,

the model underestimates their health needs.

Broadly speaking, algorithmic decisions based on machine learning models shouldn’t
recommend disparate treatment or predict disparate impact based on people’s protected
attributes [Verma and Rubin, 2018], which include age, race, color, religion, national origin,
sex, marital status, and political preferences. A comprehensive list is shown in table 1.1. As
shown in the previous two examples, even when demographic information is not directly
used in the decision-making process, biases can still manifest because of proxies in the
dataset. When algorithmic decision-making is put into practice, many different factors
must be carefully considered throughout the design and deployment of a machine learning

model.

22,

L. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair
machine learning. ArXiv, abs/1803.04383, 2018.

A. D’Amour, Hansa Srinivasan, J. Atwood, P. Baljekar, D. Sculley, and Yoni Halpern. Fairness is
not static: deeper understanding of long term fairness via simulation studies. Proceedings of the
2020 Conference on Fairness, Accountability, and Transparency, 2020a.

Hussein Mouzannar, Mesrob I. Ohannessian, and Nathan Srebro. From fair decision making to
social equality. Proceedings of the Conference on Fairness, Accountability, and Transparency,
2019.

Min Wen, Osbert Bastani, and Ufuk Topcu. Algorithms for fairness in sequential decision making.
In AISTATS, 2021.

Jiechuan Jiang and Zongqing Lu. Learning fairness in multi-agent systems. In NeurIPS, 2019.

X. Zhang, Ruibo Tu, Y. Liu, M. Liu, Hedvig Kjellstrom, Kun Zhang, and C. Zhang. How do fair
decisions fare in long-term qualification? ArXiv, abs/2010.11300, 2020.

Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-
algorithm and applications. Theory of Computing, 8:121—164, 2012.

Pooria Joulani, A. Gyorgy, and Csaba Szepesvari. Online learning under delayed feedback. In
ICML, 2013.

Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil.
Empirical risk minimization under fairness constraints. In NeurIPS, 31, pages 2791-2801, 2018.

Alexander D’Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D. Sculley, and Yoni
Halpern. Fairness is not static: Deeper understanding of long term fairness via simulation studies.
In Proc. of the Conference on Fairness, Accountability, and Transparency, page 525-534, 2020b.
doi: 10.1145/3351095.3372878.

Shlomo Yitzhaki. Relative deprivation and the gini coefficient. Quarterly Journal of Economics,
93:321-324, 1979.

Matja Perc. The matthew effect in empirical data. Journal of the Royal Society Interface, 11, 2014.

Juan C. Perdomo, Tijana Zrnic, Celestine Mendler-Diinner, and Moritz Hardt. Performative predic-
tion. ArXiv, abs/2002.06673, 2020.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2018.

Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
ArXiv, abs/1607.02533, 2017.

134

Group A Group B

Applicant | a, a2 a3 a4, a5 Mean} b,; by bs b4 bs Mean
CreditScoreX | 4 5 6 8 9 6.4 2 3 4 5 7 4.2
Decision] 0 1 1 1 1 0.8 Oo 1 1 1 1 0.8

New Expected X’| 4 5 62 86 98 672 |2 26 38 5 74 4.16

po

 

Table 4.3: Outcome when using a demographic parity policy. The bank issues loans to the same
fraction of people (80%) in both group.

In table-4.3, the expected average score of group B will be decreased to 1g = 4.16, and
the difference of averages between the two groups are increased to A’ = 2.56. Although
the second bank tries to be fair, it actually further make group B’s average credit scores

worse and further segregates the two groups’ credit score distributions.

Equalized Opportunity The third bank adopts a more constrained fairness metric called
equalized opportunity. This metric requires that among those who can payback the loans,
the bank should issue loans to the same percentage of people (Equalized false negative
rate). For all applicants with credit score X > 5, their repaying probability is Y > 0.5.
We call this set the qualified applicants, and this includes 4 individuals in group A and 2
individuals in group B. The bank decides to issue to the top 50% of qualified applicants,
which will be a4, a5 and bs.

threshold

 

 

: a ie » wa
eam @@e® @@
‘ ~~ - _ -: 1"
| Group B (a) (&) () &) '.( bs )
— “AS oo
= >
2 3 4 5 6 7 8 9
bp = 4.28 ba = 6.68

Figure 4-5: Outcome when using equalized opportunity policy.

As shown in Table-4.4, the difference between the two groups is again A’ = 2.4, which
is the same as the first bank that only maximizes profit. Recall that the initial group disparity
is A = 2.2, and again the equalized opportunity bank makes the average credit scores of

the two groups more disparate.

80

States of the world

 

Group

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Attributes 2
~
Features x Tq 4 xX? xX®
=
1 Policy 4) | Feedback Ay | at
Target
Variable Ls LY" LY?) Y°

 

 

 

 

 

 

 

 

 

 

Decision DZ D'z DIZ

Figure 4-6: The dynamic data generation process unrolled by time. Z is the sensitive attribute, X
is the features, Y is the target variable, and D is the decision applied by the agent. The purple
arrow indicates a policy function that maps from the features X* to a decision D*, and the red arrow
indicates the feedback effect from decision D! to features X'*".

 

 

 

 

4.3 Formulation and setting

The previous motivating example showcases one-step feedback of decisions on underlying
distributions. In this section, we formulate the long-term feedback of decisions through the

lens of the Markov Decision Process (MDP).

4.3.1 Background: Markov decision process

We assume the target variable is a function of the features, and the target variable is a func-
tion of the features. MDP can be leveraged to characterize long-term inter-dependencies
of features, the target variable, and the decisions as a graphical model. It characterizes
the dependencies between variables at each state and also models the temporal transition
of the underlying distribution. Although the ground truth dependencies are rarely known
in real life, knowledge of the causal dependencies that generate the data could be useful
when comparing different policies. This framework also naturally constructs a computa-

tion graph, where gradient flow over a long horizon can be easily computed.

82

1.3. Thesis Summary and Contribution

In many real-life situations, including job and loan applications, decision-makers must
make justified and fair real-time decisions about a person’s fitness for a particular oppor-
tunity. In this thesis, we focus on studying algorithmic fairness in sequential decision-
making settings where the data comes on the fly. Within the cycle of machine learning,
many solutions have been proposed for auditing and mitigating model unfairness in terms
of predictions. However, there is still a gap in addressing the biases that arise after the pre-
diction stage in the machine learning cycle. We focus on the last two stages of the machine

learning cycle and study fairness beyond prediction time.

We first study if it is possible to translate model predictions to fair decisions. In particu-
lar, given predictions from black-box models (machine learning models or human experts),
we propose an algorithm based on the classical learning-from-experts scheme to combine
the predictions and generate a fair and accurate decision. We measure the accuracy of the
algorithm using regret, which measures the difference in the algorithm’s accuracy com-
pared to the best expert. For fairness, we adopt the equalized odds metric, which requires
equalized false positive and false negative rates. Our theoretical results show that approx-
imate fairness can be achieved without sacrificing much regret. We also demonstrate the

performance of the algorithm on real data sets commonly used by the fairness community.

In the second part of the thesis, we investigate how decisions made on individuals could
change the state of the world. Can enforcing fair decisions in a sequential setting lead to
long-term improvement of welfare when the feedback loop is taken into account? In partic-
ular, we study the long-term impact of repeatedly enforcing different fairness constraints at
each decision time on shaping the underlying population under Markov Decision Models.
We propose a metric to measure the distributional impact of algorithmic decisions on the
target variable distributions in terms of within-group and between-group impact. Our re-
sults show that fairness constraints could lead to "backfire effects" which further entrench

distributional disparities between population groups.

29

their loan (Y = 1), they should have an equal probability of getting the loan regardless of
their group. In the context of the confusion matrix, this metric could be defined in terms of

equalized false positive rate.

Definition 2.2.3 (Equalized FPR (EqFPR)/Equalized FNR (EqFNR)). Let FPR, = P(Y
1|Z = z,Y = 0) and FNR, = P(Y = 0\Z = z,Y = 1) be the False Positive Rate
(FPR) and the False Negative rate (FNR) for group z respectively. A predictor/classifier is
said to satisfy Equalized FPR and Equalized FNR on group A and group B respectively if
FPR, = FPRg and FNRy = FNRz.

Definition 2.2.4 (Equalized Opportunity (EqOpp)). A predictor exhibits equalized oppor-
tunity if P(Y =1|Z = AY =1) =P(Y =1|Z = B,Y =1). nother words, it satisfies
eqFPR.

 

 

Equalized Odds _ A stronger notion of fairness that is defined based on the confusion
matrix is Equalized odds. Equalized odds require that the predictor achieves both equalized

FPR and FNR.

Definition 2.2.5 (Equalized Odds (EqOdd)). A predictor exhibits equalized odds if it achieves
both an equalized FPR and an equalized FNR.

As seen from the above, equalized odds is the most strict metric among those that are

based on statistical parity of outcomes and predictions.

Definition 2.2.6 (Test Fairness). A classifier f is perfectly calibrated if for any score s €
[0,1], P(Y = 1|/f(X) =s] =.

Calibration is well-used in practice, which requires that when conditioning on scores or
risk estimates, the true label should be independent of the group attribute. Essentially this
requires the scores from a classifier should carry the same meaning for both groups.

First, there are often contentions and trade-offs between them. For example, previous
work has shown that equalized odds and calibration can not be achieved at the same time
[Chouldechova, 2017, Kleinberg et al., 2017]. How to synthesize or characterize the trade-

offs of these incompatible metrics in real applications remains an open research problem.

36

—— Maxutil —-— DemoPar —— EqOpp —— MaxuUtil —— DemoPar —— EqOpp

 

 

 

 

 

 

 

 

Ps 0.3

= 0.2 — =
a a 3°? ee
0.1 et = —
= ne 501 ————

0.0 geeemeeesesee eal 4

30 0.45 0.60 0.75 a0 0.30 0.45 0.60 0.75
cost ratio cost ratio

Figure 4-17: Gini-WGI and Gini-BGI for different fairness policies. The dashed line indicates the
advantaged group, and the solid line indicates the disadvantaged group.

MaxUtil DemoPar EqOpp

 

Mean-BGI 0.011 0.005 0.024
Median-BGI 0.057 0.192 0.159
Var-BGI 0.014 0.094 0.090
Gini-BGI 0.031 0.107 0.063

W1-BGI 0.002 0.092 0.087

Table 4.9: Between-group impact (g-BGI) when measured using different g function (forgiving
setting). The bold number indicates the policy that results in the biggest g-BGI. Using different
metrics g leads to different conclusions.

ferent conclusions on which policy leads to the biggest backfire effect. This raises the con-
cern that using an average metric (such as groupwise average outcome) to evaluate fairness
could lead to an unfair comparison between policies, and comprehensive characterization

of the distributional impact of decisions is essential.

4.5.3 Simulation results: synthetic gaussian (2d)

We use the synthetic gaussian dataset to study the effects when the features are multi-
dimensional. This allows us to create a more realistic dependency between features and the

target variable, where the structural equation fy is more complex.

4.5.3.1 Simulation setup

The initial features for the two groups are sampled from N (tio, 1) and N (11, J) respec-

tively, where jig = [0,0]” and jy = [1,1]”. The initial feature distributions are shown on

102

ous work measuring disparity in group mean could be insufficient, and using a more
fine-grained metric could lead to different conclusions from the previous simulation
works. In particular, our results show that there is generally a trade-off between

utility and between- group impact for threshold policies.

This thesis is based on the following papers published:

1.4

Towards Reducing Biases in Combining Multiple Experts Online. [Sun et al., 2021]
Preliminary version appeared at Neurips 2019 Al for Social Good. The final version
appeared at Proceedings of the 30th International Joint Conference on Artificial In-
telligence (IJCAI 21). This was joint work with Dr. Ivan Ramirez who was an equal

contributor to this work.

The Backfire Effects of Fairness Constraints. [Sun et al., 2021] The preliminary ver-

sion appeared at ICML 2022 Responsible Decision Making in Dynamic Environment.

Thesis Outline

The thesis in organized as follows:

In chapter 2, we introduce the background. We introduce some commonly used met-
rics for fairness and briefly introduce related work on achieving fairness in sequential

settings.

In chapter 3, we formally describe the setting of online learning with fairness. We
then introduce our method of combining black-box classifiers’ predictions to deliver

fair decisions.

In chapter 4, we study the long-term impacts of algorithmic decisions. In particu-
lar, we study under which scenarios would algorithmic decisions lead to a further

disparity between population groups.

In the last chapter, we conclude with potential future directions.

31

32

6.1 Appendix for Chapter 3: Fairness in Sequential Deci-
sion Making

6.1.1 Additional Experiment Results

6.1.1.1 Additional Experiment Results on Synthetic Dataset

Pareto Curve on Synthetic Dataset To clearly illustrate the trade-off that can be achieved
in the optimization step, we plot the pareto front by varying A defined in the optimization

step 3.11. The Pareto curve is in 6-1.

Pareto Curve synthetic fpr_equal_fnr
(1.0,0.0)

fairness
°
zm
a

 

Figure 6-1: Pareto Curve for the synthetic dataset with imbalanced setting. x-axis is the re-
gret and y-axis is the average value of Equalized FPR and Equalized FNR. The pair indicates
(Aregret AFairness) Where A Fairness = AFPR = AFNR-

Different simulation distributions We summarize the experiments for Synthetic

data in Tables 6.1 and 6.2, where we fix pa = 0.9,pp = 0.1,14,4 = 0.7 and varies

LB +-

112

founders, etc. They also focus on studying the dynamic system’s equilibrium behavior but

do not support counterfactual inference.

41

 

Algorithm 1 GFORCE Algorithm
Initialize Whew ~ 1 for each f € F,z € {A, B},y € {+,-}.
Initialize q;,, = 3 foreach z € {A, B},y € {+,-}.
Initialize 7 < 3.

 

fort < 1,...,7 do
A new example (x*, 2’) comes in

Obtain 9} = f(x", z*), foreach f ¢ F
Optimization step:

Obtain the optimal meta selection probability q*
Selection step:

t _ Ww Bs. * eee
Tet = su with probability q.,+

Select expert f with 7), =
, Th. = Fe with probability ¢.,—

fEF

Obtain loss ¢% = ¢(}, y*) for each classifier f ¢ F
Update step: Update the weights table according to the exponential rule:

wit _ wi ey (1 _ ni lZ=2FY <9}

end for

 

the three functions. The parameters of the three functions depend on statistics pz, Jlzy, Vz,y,
which can all be estimated on the fly. We refer to these statistics as G-FORCE Statistics,

and the optimal solution of the optimization problem as q*.

Prediction Step Suppose instance (z, +) is selected, G-FORCE uses normalized weights

Whit

That = Sup en to sample an expert f, and adopts f’s prediction for this round.

 

Update Step After the prediction, the true label y is observed and each expert f produces

loss £%. ,, = (f(a, z),y). G-FORCE only updates the weights for instance (z, y) with the

“fzy
exponential rule

t+1

— apt — new
Whey = fey (l= n) few,

56

Chapter 6

Appendix

111

4.4 Measuring the Long-term Impact of Decisions. ............... 87

4.4.1 Filling in the gaps for long-term fairness metrics .......... 87
4.4.2 The distributional impact of algorithmic decisions ......... 87
4.4.3 Disparity ina broadercontext .................0.. 90
4:5 Case Studies. 2s ss 4 ee ge ees ee OEM HEE EER wR ERR Eee 91
4.5.1 Simulation Environment ...................00.. 92
4.5.2 Simulation result: Loan application ..............00.. 94
4.5.3 Simulation results: synthetic gaussian (2d) ..........0... 102
4.6 Conclusion and key takeaways ..........-..-0.2.00 00000. 103
Conclusions and Future Work 107
5.1 Conclusion ... 2... 2.00... 0000000000 00000200848 107
5.2 Connections to machine learning models in a sequential setting... ... . 108
5.3. FutureWork. 2.2... ee 109
Appendix 111
6.1 Appendix for Chapter 3: Fairness in Sequential Decision Making. .... . 112
6.1.1 Additional Experiment Results... ..............0004 112
6.1.2 Proofs for Non-delayed Case... 2... 2. ee ee. 113
6.1.3. Proofs for Delayed Case... 2... 2.2 ee ee 122
6.2 Appendix for Chapter 4: Fairness with Dynamic Feedback ......... 128
6.2.1 Additional Experiments Results ..............2.2004 128

11

setting that learns from the feedback of a fairness oracle and returns all pairs of individu-
als for which the individual fairness constraint is violated. Joseph et al. [2016] study fair
online classification in the contextual bandit setting, where fairness is defined as a worse
candidate is never favored over a better candidate by the algorithm. Liu et al. [2017] con-
sider satisfying calibrated fairness in a bandit setting. Bechavod et al. [2019] consider the
problem of enforcing the equalized opportunity constraint at every round under a partial
feedback stochastic setting where only true labels of positively classified instances are ob-
served. Blum et al. [2018] specifically shows that it is impossible to achieve equalized
odds under an adversarial setting when an adaptive adversary can choose the label for an

instance.

Long-term fairness in interactive and dynamic environment Several works have stud-
ied the dynamics between algorithmic decisions and long-term population qualifications.
One of the first works that touch on this topic is Liu et al. [2018], which considers the
one-step feedback model and shows that enforcing common static fairness metrics in con-
strained optimization does not in general promote average group scores. Later, D’ Amour
et al. [2020a] extends the previous one-step analysis to multiple-step using simulation and
argues that long-term dynamics may lead to different conclusions from the one-step anal-
ysis. Mouzannar et al. [2019] study whether enforcing demographic parity could lead to
equality of qualifications. Wen et al. [2021] model the feedback effects as Markov decision
process and proposes learning fair decision-making policies through cross-entropy opti-
mization. There are also some works studying fairness in multi-agent systems [Jiang and
Lu, 2019].

Most related to our work, Zhang et al. [2020] study the problem under a partially ob-
served Markov decision problem setting and characterize the impacts of fairness constraints
can have on the equilibrium of group qualification rates. One thing that has been missing
from previous work is that the analysis only focuses on the average qualifications of groups,
yet an algorithm or policy could have a more profound impact on the shape of the popu-
lation beyond the group mean. In addition, the simulation setting is often too stylish and

often ignores the nuances in the data generation process such as causal relationships, con-

40

Title: Principal Research Scientist

and taking the logarithm of both sides, we have

 

In(1 — n)L p24 < d ln ou What So hea

SCF ® + TED

 

 

In(1 — ML par Snd—n>) a What 0 Gat (6.26)
feF 4 TEeDt
we, Ind
va fect SS) Ga4< (L+ m)L pea += (6.27)

feF ® + rEDt

Equation 6.26 follows because if 7 < 1/2, we can use the inequality In(1 — 7) < —7. This
is intuitive as if we always choose weights for positive examples, it reduces to the same

bound as in the original MW algorithm.

As before, we assume that the expected error on group z, + when wrong weights wy, 2,—

are selected is bounded as:

 

 

a whan oy < What Bey tat_ (6.28)

Let D4" = max; |D,| be the maximum cardinality of feedback set, we have:

 

 

yee, >" Ge as See. So Geet Gia 6.29)

f6F Be reDt [€F Bet reD reD

where af _ < 1 is the difference of loss in expectation made when using the incorrect

weights of the MW algorithm on group z, + (Cross-Instance Cost). Then

 

 

 

 

 

 

 

 

 

T

d= 3 (ho DP > Gee tat wee, >. Se .) (6.30)
t=1 fEF 24 TeDt SEF o.- TED,
T

E[L-4]< >> What YL Goet tL Gove (6.31)
t=1 feF “>t rede TED

124

Contents

1

2

Introduction
1.1 Algorithmic Faimess ........ 0.000 ee ee
1.2 A Full Taxonomy of Biases in Machine Learning ..............
1.2.1 Dataset Collection Bias... 2... 2.2... 0.0.00... .000.
1.2.2 ModelBias.......... 2.0.00. .0.0 0000000000.
1.2:3. Decision Bias: see e ess ss MBS TEER wwe eka
1.2.4 Feedback Loop .......... 0.00.00 00 ee eee ee
1.3. Thesis Summary and Contribution ...................0..
1.3.1 Summary of Contributions ..................000.
14 Thesis Outlines. .¢ 6 wee pee ea He ee Owe Ee 8
Background
2.1 Preliminaries .. 2.2.2...
22, Fairmess Meitics:: :s2aeee ei ks BBS eae ewe EEE see
2.2.1 Statistical Fairness Metrics... 2... 2 ee ee
2.2.2 Causal Fairness Metrics ... 2.2.0... .2...2. 0000000.
23 Related Work : iis neee eres BBS eee ewe ELE see
23:1 Faimess Mettics oa 52s 5s wow gy ee ea ww RE ES es
2.3.2 Biases mitigation in machine learning ..............0..
2.3.3. Fairness in Sequential Decision Making ...............
G-FORCE : Achieving Fairness in Online Decision Making

3.1 Online Binary Classification ... 2... 0.0... 00000 eee eee

21
21
23
25
26
27
27
29
30
31

33
33
34
34
37
38
38
39
39

43

Model

oo f-- Qe.
7 “ky Strategic

State of the World Individual

Natural

 

Figure 5-1: Different types of distributional shifts in sequential decision making.

5.2 Connections to machine learning models in a sequen-

tial setting

One of the common and essential assumptions in machine learning models is that the train-
ing distribution and the distribution on which the model is deployed are the same. Yet
much recent work has shown that this assumption is often violated in real-world applica-
tions [Perdomo et al., 2020]. Predictions and consequential decisions can lead to changes
in distribution, especially when there are humans in the loop. The interplay between pre-
dictive models and underlying distributions occurs in many applications. Recommendation
systems predict users’ preferences and provide suggestions, and these suggestions could
shift users’ preferences in turn. In traffic prediction, the predicted best route might attract
more vehicles, making that very route less desirable. Here we list a taxonomy of scenarios

under which the training and testing distribution could be disparate.

Distribution Shift Distribution shift is a general phenomenon in which an underlying
distribution changes over time. The cause of this shift could be an exogenous factor or fac-
tors unrelated to the decision, such as a change in the weather. It could also be endogenous

factors that arise as an artifact of model predictions and consequential decisions.

108

Chapter 1

Introduction

1.1 Algorithmic Fairness

The past decade has witnessed tremendous advancements in machine learning models. In
image classification, machine learning models based on deep neural network first surpasses
human-level accuracy [He et al., 2016]. The list of advancements continues to grow, as
these models become able to perform more tasks such as text classification [Johnson et al.,
2017] and game-playing [Silver et al., 2016]. Machine learning models are able to learn
and extract patterns from much larger amounts of data than humans can. Recently, machine
learning models have been put to use in fields that are considered to be high-stakes and are
traditionally left to humans, such as predicting healthcare needs [Obermeyer et al., 2019],
accessing creditworthiness for loan applications, and predicting criminal recidivism for law
enforcement [Dressel and Farid, 2018].

Recently, there have been growing concerns about potential bias and discrimination in
these machine learning models. Models could propagate stereotypical and historical biases
reflected in the training data. For example, image searches for professions such as CEO
produce fewer images of women [Kay et al., 2015], and word embeddings used in natural
language processing could encode gender biases [Caliskan et al., 2017].

The consequences are especially alarming when machine learning models are used in
high-stakes applications. For example, Correctional Offender Management Profiling for

Alternative Sanctions (COMPAS) is a commercial algorithm used by U.S. courts to predict

21

72

For the sake of notation we define

T

Ca = 9 1{y = -}1{z = A} and Cp_ = 0 1{z = BHy = -}.

t=1

Using Lemmas | and 2, we have:

 

 

 

 

 

t=1

 

 

 

La. ts.)
E, z * ~ :
Baas [ZS - Z|
<|E (L+mLp(A,-)Am ehh Lee — Le de Ue,
ne Ca_ Ca Ca_ Cre Cae
Lp(a), A Ly(e,-),B,-
=|(1 ] E, z —— = E, Zz - ——
[1-4 Beye LSM] — 4B [-R P| +

 

 

 

 

eae yp)
t Ca Cz,

Using equation 6.37, we have:

 

 

 

 

Ca

Lp (B-),A— Lp(B)B,
“2 ,Y,Z a — Egy ee

 

Ind
eee 6.19
v= [oon | | G19)

Moreover, without loss of generality we assume that {* makes the smallest average loss on

group B. This is,

Lp(a) A— LyB_) A
| pels | £ Baya | Pols | <

Ca Cy

120

 

 

 

 

°
EiY.z

Lj(B-),B-

Ge.

]+e

6.1.2.2 Proof of Lemma 2

Using the same process as for the upper bound, we have:

t+1 _ t+1
Oe a » D fae

 

fF

“ult nyt
SOF

>So wh. (- n(l + ny fy = +}
foF

=P hea 4 Denali
{GF —

=o! (1-n tm) i hea)

2+

fEF

Thus, by the recursive function, we have

 

oft >), [] 040 yo Unt Ceo)

 

fEF fp
7
=d][Q-» +n ys Whack Cy 24)
t=1 SEF O24

Let f* be the best expert in hindsight in terms of achieving lowest false positives, we

117

6.1.3.3 Fairness Bound

Proof We assume group A arrives with probability p, group B arrives with probability
1-p, that is, P(z = A) = p. The expected mean label of group A is defined as 44. =
P(y = +|z = A) and mean label of group B is defined as pp. = P(y = +|z = B). Each

individual classifier is €-fair, thus:

 

 

 

Lea Lye
[Ex,y,2 T Be Re y,z T LB <e,Vf
daw L{y = —}1{z = A} dia Ly = —}1{z = B}
(6.37)
which represents the cardinality of the selected subset of samples.
The absolute difference of FPR between group A and group B is:
La Lp
|FPRa — FPRp| = |Exyz | = a 7 = |
dai l{z=A}Hy=—-} Liar d{z = BHy =—}
(6.38)

For the sake of notation we define

Ca, = D> 1{y = —}1{z = A} and Cp_ = $0 1{z = BH{y = -}.

t=1

Using Lemmas | and 2, we have:

126

 

 

 

0.4 -

0.2 - —e- Maxutil
—e— DemoPar
—e— EqOpp
0.0 - ' 1 I ' 1
0.3 04 0.5 06 0.7

Figure 6-2: Threshold for different fairness policies as a function of cost ratio. The dashed line indi-
cates the threshold for advantaged group, and the solid line indicates the threshold for disadvantaged
group.

129

have

t t
o.= S: What

feF

t
Sd Masts +

t
Wet

=d-max(1— nein fr Mua}
SEF

=d-(1- nyo Coe A=}

Therefore we have:

 

T t
T it . ad Ww 32
de (Lahr See Md > of > d-T] nd +0) 0 a = Graal
t=1 fEF 2,

Taking the log of both sides:

 

T
In(1 — )Lyp 24 > yon (: — (1+) : atc fae] (6.13)

 

 

t=1 feF 2+
* Whack
In(1 — 9) Lye24 > In (1 — n(1 +7) > Cee (6.14)
t=1 feF 4
Ww 2.
rw fat Co > Wn) Lgret (6.15)
t=1 foF Oo.
where ¥(7) is defined as:
In(1 — 7)

a(n) = In(1— (14+ 7))

using that In(1 — n(1 + y)x) > In(1 — (14+ n))a for all x € [0,1] and 7 © (0,7"*”),

max ais

where 77 which does not restrict the range of 7 € (0, 0.5).

118

 

 

 

 

 

 

La Lp
nae |

 

 

 

 

 

 

 

Cae Cp
<|E (a Mera AM on, DRE La ol,
=> EY ,Z Ca T Cu, T Ca
y(n) Lyp(p),p,- DE dp: 5-1
Cx Ox.
Ly(A,-),A- Ly+(B-),B,-
=|(1 E, ,;/—eeee| - Key. | —————
(1+ Bags [RAE] — 9B [RPE | +
DE gah, DEM at ~ob_ LE ind |
Ca Cz, reer Ca

Using equation 6.37, we have:

Ly (B,-),A- Ly (B-),B,-
Reyiz [AS _ fir,y,z Ge. | — €

 

 

 

 

Moreover, without loss of generality we assume that {* makes the smallest average loss on

group B. This is,

Lp*(A,-),A,— Ly*(B,-),A,— Ly*(B,-),B,—

Thus, equation 6.19 becomes:

Lye_).B,— Dag 9 5 -
1040 (Bane |-AE AE] +) —reBane [AGE] +

(== ye da, . oly DB ys dp . “hs LE Ind
1 Wea yz

 

 

 

 

Ca Cp.- nCa,—

DR Yd DB YI dp
<|(l+n—(n)) FPRp +e(1 +n) 4 : ; ; ; ;
<|Q+n—9@) p+ e(.+n) ( pA(l— pa,4)T pa pp4)T |

127

—— MaxUtil —-— DemoPar —— EqOpp

 

 

 

 

 

 

 

 

 

—— Maxutil —— DemoPar —— EqOpp
0.3

0.10
oO © 0.2
s om
«0.05 5
$ > 0.1 ——

0.00 ol :

0.30 0.45 0.60 0.75 , 0.30 0.45 0.60 0.75
cost ratio cost ratio

Figure 4-15: Mean-WGI and mean-BGI for different fairness policies. The dashed line indicates
the advantaged group, and the solid line indicates the disadvantaged group.

4.5.2.4 The hidden story behind average outcome

In Figure 4-16, we plot the final distributions of the target variable under three different
settings. Compared to the initial distributions, repeatedly enforcing a policy changes the
shape of the final distributions in a way that cannot not be captured simply by the group
mean. In particular, all policies create dichotomies and the Matthew effect [Perc, 2014] on
the target variable distribution such that "the rich get richer and the poor get poorer."

This phenomenon is further showcased in Figure 4-17, where we plot the gini-WGI
and gini-BGI as a function of the cost ratio g using the Gini coefficient as the g function.
A positive gini-WGI indicates that the policy increases the inequality of target variable
distribution within a group. On the other hand, a negative gini-WGI indicates the policy
decreases the inequality within a group. As shown in the left plot, as the cost ratio increases,
the gini-WGI increases for all three policies. The dynamics on the advantaged group are
fairly similar among the three policies. For the disadvantaged group, MaxUt il is the only
policy that doesn’t increase the gini-WGI, while gini-WGI decreases drastically with the
cost ratio for the other two policies.

As for between-group impact, DemoPar leads to the highest gini-BGI consistently. In
general, as the cost ratio increases, the gini-BGI also increases for the fair policies.

In general, the comparison of WGI and BGI between different policies highly depends
on the simulation setting and metric g. In table 4.9, we evaluate g-BGI using different g

function. Depending on the simulation setting parameter and the metric g, we may get dif-

100

MB + MW _— GroupAware G-FORCE

0.1 0.016 + 0.013 0.4944 0.009 0.305 + 0.020
0.3 0.018+0.013 0.487+40.012 0.182 + 0.014
0.4 0.026+0.011 0.475+0.019 0.148 + 0.032
0.5  0.024+0.018 0.283+40.162 0.110 + 0.030
0.6 0.011+0.008 0.019+0.022 0.032 + 0.017

 

 

 

 

Table 6.1: Equalized FPR by fixing p4 = 0.9, pp = 0.1, wa,4 = 0.7

 

LBs MW _— GroupAware G-FORCE

0.1 0.473 + 0.060 0.509+ 0.055 0.304+ 0.028
0.3 0.490+0.031 0.486+0.022 0.194+ 0.018
0.4 0.508+0.018 0.488+0.019 0.146+ 0.018
0.5 0.488+0.013 0.296+0.162 0.111 + 0.030
0.6 0.495+0.020 0.022+0.019 0.046 + 0.010

 

 

 

 

 

Table 6.2: Equalized FNR by fixing p4 = 0.9,pp = 0.1, w44 = 0.7

6.1.1.2 Additional Experiment Results on Real Dataset

 

#ofrounds pa fla+ [B+

 

Adult 24421 0.851 0.26 0.16
German Credit 300 0.853 0.73 0.50
COMPAS 1584 0.398 0.54 0.39

 

Summary statistics of real data sets

6.1.2 Proofs for Non-delayed Case

We define the cumulative loss of classifier f on group z as Ly. = Soy .. The cumu-
lative false positive of f on group z is the cumulative loss made on the negative examples;
and its expression is Ly. = ny & Ay = —}. Similarly, we defined the expected loss
on group z as E[L.] = wy rer nl, . and the expected false positive on group z is

[Lz] = eo Diver ml Ly =—}.

113

 

 

 

 

where using 6.27, we finally obtain:

 

 

 

 

 

 

 

 

 

7 Ind r
B[L.4] <(L+m)Ly24 + — ae (6.32)
i t=1 _
<(14+n)Lye4 +04 + pee ya (6.33)
s VE f.z+ n q,,— Oz, — .
Similarly,
[Le] < (1+ n)Ly2- ee Dre dit de M8, (6.34)

 

 

 

The expected total errors on group z is, adding the two equations above:

 

 

 

A[L.] < (14 nbs + 2 + (Di Dad at + DE Dadi Aa)

 

. Let D™” = max; D In the same way, the expected total errors (considering z =

ZY
A, B) is:

Ind
E[L] < (1+ n)Ly+ - +aDp™™ (6.35)

where all the Cross-Instance Costs are condensed in:

a= > ey > a y-
t

2€{A,B} ye{—,+}

6.1.3.2 Proof of Lemma 2

The proof of the lower bound for the loss is largely the same as the non-delayed case, and

therefore we only present the final result here:

E[L] > y(n) Lye +aD™™. (6.36)

125

Performative Prediction The concept of a performative prediction was first proposed by
Perdomo et al. [2020], which describes scenarios in which predictive model-based deci-
sions may influence the outcome that the model tries to predict. In this sense, the machine
learning model is not only predictive of the target but is also performative of the target.
It’s reasonable to assume in this case that the distributional shift is benign or at least pre-
dictable. The backfire effects of fairness constraints we discussed previously provide an

example of performative feedback.

Strategic Classification When model predictions are converted to decisions made about
people and applied downstream, the distributional shift could be strategic or even adversar-
ial. After decisions informed by models are applied to people, individuals could strategi-
cally react to the models by nudging their features [Hardt et al., 2016][Milli et al., 2019].
For example, an attacker of a machine learning system can adversarially alter an image
[Madry et al., 2018][Kurakin et al., 2017] to intentionally cause the model to predict a

wrong label.

5.3. Future Work

Unified framework of studying the interaction between models and humans

¢ As we discussed above, there are many different ways that an underlying distribution
could change over time, and many solutions have been proposed for each of them.
Could there be a unified framework for studying interactions between models and

humans and their fairness implications?

¢ Can we design effective interventions that make the interactions between models and

humans fairer?

Efficient ways of finding optimal policy When a machine learning model needs to be
repeatedly deployed, one common practice is to frequently retrain the model on a new

dataset. This is certainly not ideal and leaves room for a lot of interesting future work:

109

post processing techniques. In processing approaches usually involve enforcing some fair-
ness constraints at model training time [Corbett-Davies et al., 2017] through constrained
optimization or modification of the objective function [Berk et al., 2017]. Though easy
to implement, these methods can lead to drops in model performance. In addition, many
machine learning models are black boxes, and it is sometimes impossible to change the
training paradigm of the models. Post-processing techniques treat the model as a black box

and adjust the model predictions to remove biases [Hardt et al., 2016].

1.2.3. Decision Bias

Even with a perfectly fair machine learning model, things can still go wrong during decision-
making time. Machine learning models only generate predictions, not actionable decisions.
In many high-stake systems with humans in the loop, there are gaps in translating a model
prediction into a justified decision in practice. For example, suppose there is a machine
learning model that predicts crime rates in a community, and police officers make deci-
sions based on these predictions. If a model predicts that certain communities are more
likely to have a higher crime rate, law enforcement in that area may tend to lower their
threshold and arrest more people.

Very few works have been proposed to address the unfairness that arises in this stage.
This gap motivates us to design algorithms that can deliver fair decisions when working

along predictions from humans or black-box models.

1.2.4 Feedback Loop

It is also important to bear in mind that delivering a fair decision is not always the end of
the story. In many cases, decisions carry big and profound consequences. Unlike common
machine learning applications such as image classification, here the data comes from peo-
ple, and people could react to decisions made about them. Decisions that affect individual
people often create a feedback loop and change the state of the world from which the data

is sampled.

27

Chapter 5

Conclusions and Future Work

5.1 Conclusion

In this thesis, we study the fairness of machine learning algorithms in a sequential decision-
making setting. When considering fairness in machine learning, many complexities arise
because the data comes from people, and decisions are applied to people. This often creates
a feedback loop that involves interactions between the predictions/decisions and the state
of the world. While many solutions have been proposed for addressing biases in model
predictions, in this thesis we focus on addressing fairness concerns after model predictions.
In this thesis, we study two problems: (1) first, how can we translate black-box model
predictions into fair decisions? (2) and second, how do fair decisions impact the underlying
distributions when there is a feedback loop?

For the first problem, we propose a meta-algorithm that combines black-box predictions
in a way that balances different error metrics (FPR and FNR) between groups. For the
second problem, we showed that there are often trade-offs between fairness and accuracy
in the short term and that fair decisions could also lead to backfire effects in the long term.
We argue that applying algorithmic decisions to people requires careful evaluation of the
different components that come into play.

In this section, we also discuss how the problems discussed in this thesis can be re-
lated to a broad research area beyond fairness. We think that many of these problems and

approaches are closely related, opening the door for exciting future work.

107

Definition 3.1.3 (EqOdds). Let FPR, = P(Y = 1|Z = z,Y = 0) and FNR, = P(Y =
0|Z = z,Y = 1) be the False Positive Rate (FPR) and the False Negative rate (FNR)
for group z respectively. An algorithm is said to satisfy Equalized FPR (EqFPR) and
Equalized FNR (EqgFNR) on group A and group B respectively if FPR4 = FPRg and
FNR4 = FNRg. A randomized algorithm satisfies EqOdds if it satisfies EgFPR and
EqFNR.

In EqOdds metric, the algorithm requires the algorithm has equal false positive and

false negative rates for all groups.

3.2 Online Algorithms

3.2.1 Multiplicative weights algorithm (MW)

The Multiplicative Weights (MW), proposed by Arora et al. [2012], is a frequently used
aggregation algorithm for achieving sub-linear regret. In the MW algorithm, a decision
maker has a choice of d experts. The main idea is that the algorithm maintains weights wy

on the an expert f based on its performance up to the current round t.

 

¢ Prediction step: At prediction step, expert f is selected with probability n° = Sut

and it’s prediction is adopted for this round.

¢ Update Step: At update step, suppose an expert f incurs loss I. The weight of each

expert according to exponential rule:

wit = wi (1 — nit

The original MW algorithm (Arora et al. [2012]) provides a bound for the total expected

loss of the algorithm by the total loss of the best experts with the following theorem:

Theorem 1. MW Regret Bound ([Arora et al., 2012] Assume that the loss & is bounded in

51

List of Figures

1-1

3-1

3-2

3-3

3-4

3-5

3-6

A typical machine learning cycle contains five stages: the state of the world
describes the true underlying distribution; data is sampled from the state
of the world; a machine learning model learns patterns from the training
dataset; the model makes predictions on new instances; these predictions
are transformed into decisions about an individual, an individual could take

actions and change the state of the world. ...............004

From predictions to fair decisions. .. 2... 0... eee ee
An example on college admission with experts. .............00-
A figure depicting the online learning process... ...........004

A figure depicting online learning with constant delay with r4 = 2 and

An example of predicting one example in G-FORCE.............

This figure shows how G-FORCE process an input pair (x, z), where z
assumed to be B. In the optimization step, G-FORCE samples from PMF
[¢2,+; Yp,_| constructed from G-FORCE statistics and selects MW instance
(B,+) to use. In prediction step, instance (B,+) samples a classifier f; to
predict. In the update stage, the true label revealed to be —, indicating
that G-FORCE selected the wrong instance to use in the first stage. G-
FORCE only updates the weights for the correct instance (B,-), as well as
the G-FORCE statistics, 26.6 be eee ee es

49

(DT) and Multi-Layer Perceptron (MLP). These classifiers are trained using 70% of the

data set. The remaining 30% of the dataset is set aside to simulate the online arrival of

individuals. We compare our G-FORCE algorithm with the GroupAware in terms of regret

and fairness. We repeated the experiments 1000 times for German and COMPAS, as well

as 10 times for Adult, by randomizing the arrival sequence of individuals.

 

 

 

Individual Experts Combined Experts
LSVM RBFSVM DT MLP_ LR | Group-Aware G-FORCE

FPR | 0.022 0.046 0.043 0.047 0.047 0.052 0.035

Adult FNR| 0.026 0.199 0.200 0.214 0.214 0.163 0.083
EER | 0.058 0.062 0.062 0.061 0.061 0.074 0.069

FPR | 0.00 0.371 0.471 0.421 0.050 0.373 0.329

German FNR | 0.000 0.320 0.770 0.680 0.650 0.207 0.181
EER | 0.090 0.090 0.208 0.210 0.280 0.093 0.098

FPR | 0.190 0.150 0.160 0.158 0.240 0.191 0.184

COMPAS FNR | 0.256 0.240 0.260 0.240 0.340 0.264 0.249
EER | 0.019 0.010 0.010 0.010 0.010 0.016 0.019

 

 

 

Table 3.5: €-Fairness of base experts, GroupAware and G-FORCE .

Results in Table-3.6 show a general improvement in fairness over the GroupAware al-

gorithm, both in terms of equalized FPR and FNR, along with a small increase in regret.

For Adult data set, we plot the performance of the algorithm over time (Figure 3-9).

Although German and COMPAS have fewer examples, and thus the standard deviation is

higher to make a conclusion, there is still a slight improvement over fairness with slight

increase in regret.

 

 

 

 

 

 

 

 

 

 

Adult Compas German
FPR FNR Regret FPR FNR Regret FPR FNR Regret
GroupAware | 0.05+ 0.01 | 0.174 0.02 | 0.00+ 0.00 | 0.204 0.04 | 0.274 0.04 | 0.01+ 0.00 | 0.40+ 0.13 | 0.21+ 0.08 | 0.01+ 0.01
G-FORCE | 0.04+ 0.01 | 0.08 0.01 | 0.01+ 0.00 | 0.18+ 0.03 | 0.25+ 0.04 | 0.01+ 0.01 | 0.324 0.15 | 0.18+ 0.01 | 0.01+ 0.01

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Table 3.6: Equalized FPR, equalized FNR and regret on real datasets. Lower numbers are better.

69

4-8

4-10

4-12

4-13

4-14

4-15

An illustration of the backfire effects of a policy. 6,4 and 6,3 measures
the impact of decisions on the orange group (group A) and blue group
(group B) respectively. Here group B is the disadvantaged group since
its target variable distribution lies on the lower spectrum. Compared to
the initial distribution at time ¢ = 0, decisions lead to backfire effects in
terms of WGI for group B (the center is decreased and spread is increased.
Decisions also lead to backfire effects in terms of BGI where group A and
group B’s distributions are further apart... 2... .......2..2004
The initial distribution for the FICO score dataset. Left: The initial distri-
bution for the group ratio. Middle: The initial distribution for the features
(credit score). Right: The initial distribution for the target variable (repay
probability). 2 ick ewe Ree ERM REE RHE RH EEE EES
One-step simulation on mean-WGI under different fixed threshold values
for the forgiving (left), neutral (middle), and harsh settings (right) respec-
tively. The red dashed line indicates optimal threshold for MaxUtil.
Multi-step simulation on mean-WGI under different fixed threshold values
for the forgiving (left), neutral (middle), and harsh settings (right) respec-
tively. Here the utility is the average utility over the simulation steps.
One-step simulation on var-WGI under different fixed threshold values for
the forgiving (left), neutral (middle), and harsh settings (right) respectively.
The red dashed line indicates Tyyaxu1i for each setting.Here the utility is the
average utility over the simulation steps... ...........-.200048
Multi-step simulation on var-WGI under different fixed threshold values for
the forgiving (left), neutral (middle), and harsh settings (right) respectively.
Mean-WGI and mean-BGI for different fairness policies. The dashed line
indicates the advantaged group, and the solid line indicates the disadvan-
taged QTOUPs ws se i oe How wy Bee Ta we EEE wwe Ee eo 8
Mean-WGI and mean-BGI for different fairness policies. The dashed line
indicates the advantaged group, and the solid line indicates the disadvan-

faGed SOUPs ce csv ss mw eee OO 8 ee we an eee ew

88

97

97

98

98

Notation

Meaning

 

Z

Protected group attribute such as gender or race. In binary case, we will refer
to the groups as group A and group B, where A represents the advantaged
group and B represents the disadvantaged group.

 

 

 

 

 

Dz P(Z = z). Probability that a sampled example belongs to group z.

xX Feature variables other than group attribute.

fx Ground-truth function that maps from group attribute 7 to features X.

Y Target variable. In the first part of the thesis the target variable is binary,
where Y € {0,1}. In the second part of the thesis the target variable is a
probability where Y € [0, 1].

O Outcome variable. If the target variable a probability, O is a realized binary
outcome sampled from the probability.

fy Ground-truth function that maps from feature X to target variable Y.

D P(X, Y, Z). Ground-truth distribution where the dataset is sampled from.

(x,y,z) (a#,y,z) ~ D. An individual sampled from the distribution is a tuple of the
group attribute, feature variables, and target variable.

Y Model prediction.

l Loss function for measuring the loss between prediction Y and ground truth
target Y.

FPR FPR=P(Y =1|Y =0). False positive rate.

FNR FNR=P(Y =0|Y = 1). False negative rate.

EqFPR P(Y = 1|Y =0,Z = A) = P(Y = 1|Y = 0,2 = B). Equalized false
positive rate.

EqFNR P(Y = 1|Y =0,Z = A) = P(Y = O|Y = 1,Z = B). Equalized false
negative rate.

DemoPar P(Y = 1|Z = A) = P(Y = 1|Z = B). Demographic parity.

EqOdds Equalized odds requires both EqFPR and EqFNR.

D Action or decision made on an individual. This is used inreplace of Y in se-
quential decision making.

T D = 1(Y > 7). Threshold on target variable at which a positive decision is
issued.

T Total number of time steps in sequential decision making.

t A single time step t.

Table 1: Notation table of the terms used throughout the thesis.

19

will increase by 1. If an applicant defaults, the new credit score will be decreased
by 1 and the bank’s utility will be decreased by 1. When an applicant’s credit score

decreases, so does the repaying probability.

We now illustrate what will happen in a one-step feedback loop for banks using the
following policies: max profit, demographic parity, and equalized odds. Assuming there
are 10 applicants, 5 from group A, and 5 from the group B, and Cmax = 10. The initial
credit scores X of the applicants are shown in Figure 4-2, which depends on the group

membership.

 

cut G@@ @@®@

 

Group B eu) & ) Gs ) (bs ) . (bs )
Credit Score . = ™
3 4 5 6 7 8 9
bp =4.2 Ha = 6.4

Figure 4-2: Initial credit scores distribution of group A (advantaged group) and group B (disadvan-
taged group).

The mean score of group A is j14 = 6.4, and the mean score of group B is zg = 4.2,
with their difference A = 2.2. We refer to the group with higher initial mean as the
advantaged group (group A).

We keep track of two metrics: the bank’s utility and the group welfare disparity.

¢ Bank’s utility: The bank’s profit will be increased by 1 if an applicant repays, and

will be decreased by 1 if an applicant defaults.

¢ Group disparity: We measure disparity as the absolute difference of group means of

credit scores, ie., A = |Ju4 — pup.

Max Profit The first bank issues loans based on a fixed threshold on credit score regard-
less of group. Specially, an applicant will be approved if the credit score c > 5 since this is
a break-even point for the bank. When an applicant has a credit score of 5, there are 50/50

chance that the applicant will default and the expected profit of the bank is 0.

78

4.6

4.7
4.8
4.9

6.1
6.2

A mapping between notations in the literature and notations used in our

framework. .. 2... ee 85
Set of structural equations and their purpose... .............004 85
Evaluation Metrics. . 2... 94

Between-group impact (g-BGI) when measured using different g function

(forgiving setting). The bold number indicates the policy that results in the

biggest g-BGI. Using different metrics g leads to different conclusions... . . 102
Equalized FPR by fixing py = 0.9,pp =O0.1,fa4=0.7 .......-.. 113
Equalized FNR by fixing pa = 0.9,pp =O.l,4ay =O0.7......00020. 113

18

1.2.1 Dataset Collection Bias

Machine learning models often need to deal with large messy datasets that are not collected
under clear guidelines. As mentioned in the White House “Big Data” report, [White-House,
2016], selection bias — where data input to the model does not represent the actual popula-

tion — is a main source of discrimination.

Unbalanced Dataset First, a large dataset is not always a diverse one. In fact, widely
used machine learning datasets often suffer from a lack of diversity. For example, many fa-
cial recognition datasets have been collected through Flickr, and mostly consist of faces of
white people [Karkkainen and Joo, 2021]. Such datasets have been widely used in different
applications including image up-sampling, where the goal is to construct high-resolution
images from corresponding low-resolution inputs. Recently, it was discovered that an im-
age up-sampling model [Menon et al., 2020] trained on this dataset outputs a white face
when given Barack Obama’s low-resolution picture. This shows that using unbalanced data

can make the model output collapse into the majority class.

Historical and systematic biases In addition to the unbalanced dataset problem, his-
torical and systematic biases are also prevalent in collected datasets. Learning from this
data puts the model in danger of repeating those systematic biases — discrimination against
certain social groups and reinforcement of prevailing cultural stereotypes and existing de-
mographic inequalities. For example, word embedding, a popular framework to represent
text data as vectors, is often the first step in training a large language model. A recent study
shows that word embeddings trained on Google News article [Bolukbasi et al., 2016] ex-
hibit female/male gender stereotypes, such that males are more likely to be associated with
computer programmers and females are more likely to be associated with babysitters. This
shows that machine learning models can amplify historical biases that exist in data.

Much work has been done to try to address biases at this stage, and these approaches
are referred to as pre-processing techniques [Zemel et al., 2013, Louizos et al., 2016,
du Pin Calmon et al., 2017]. One possible solution is to collect more data from under-

represented groups, but this can be difficult to achieve due to self-selection biases. For in-

25

. In(1 —
where y(n) is defined as y(n) = ee

3.4.2.2 Fairness bound

For the bound on fairness, we assume each expert f € F satisfies e-EqOdds with respect

to data distribution P,,.,,. for some unknown ¢, i.e., for y € {+,—};

: Lav) _»p LyBy
ss eer Cay EY 2 Cry

 

 

 

 

<e, (3.5)

 

 

where C’,,, is the cardinality of group z and label y. Here € represents the maximum abso-
lute difference of FPR and FNR between two groups, and we don’t put restriction on the

value of e.

Theorem 6 (Fairness Bound). Let z* be the group that with the lowest FPR (FNR), and let
f*(z*) be the expert with lowest FPR (FNR) for group 2*, where group z* is the group with
lower FPR (FNR). For G-FORCE , and for q = (qa,-, 98,-, V4.4, 98,4] we have:

AWA, B,-7B,— |

FPR, — FPRg| <|(1+7— (ny) FPRpcesy) te(1 +) +
| ‘A Bl <|+n- y(n) fei) + (1 +7) oun, pee
SS —___

CFPR

hepr(@q)

(3.6)

ee ra
|FNRa — FNRp| < | (1+7—7(n)) FNRpesy + (1 +9) + tat 4 SB eB
SCs Pata T PBlBsT
CPNR =—_eO
hpnr(Q)

(3.7)
Implication of fairness bound The absolute difference in FPR (|F PR4 — FPRal|) can
be upper bounded by the summation of three term: (1) The FPR of the best expert for the
best group; (2) constant term crpr; (3) and a function hyppr(q) of meta selection probabil-

ity g. The same analogy applies to the FNR bound (|F NR4 — F'N Ra|). Here we give an

explanation for the individual terms in the bound:

* The first term in FPR bound depends on the best expert f*, where best is defined as

the expert that achieves the lowest FPR over all groups. This is similar to the regret

61

Optimization Step Prediction Step Update Stage

GB, + t
= T of MW True Label Revealed
(0; = B) > > ® instance (B,+) =a Update only
er)
s +,
ee

 

Update
G-FORCE
statistics

 

 

;
[A-q =b)

I

 

 

v
Weights of each expert

 

G-FORCE Statistics

   
 

B

WhB+

 

 

 

 

  

 

 

 

 

 

Mae.

 

Figure 3-6: This figure shows how G-FORCE process an input pair (x, z), where z assumed to
be B. In the optimization step, G-FORCE samples from PMF [qz,+,9¢2,—] constructed from G-
FORCE statistics and selects MW instance (B,+) to use. In prediction step, instance (B,+) samples
a classifier f; to predict. In the update stage, the true label revealed to be —, indicating that G-
FORCE selected the wrong instance to use in the first stage. G-FORCE only updates the weights
for the correct instance (B,-), as well as the G-FORCE statistics.

 

eS

Multiplicative Weights | S7/_ me +n = ile + a

)

t 0) dopny Coe + 2nd
+n)
tn)

GroupAware | J? wai WE
G-FORCE | 77 tet
G-FORCE (delayed) | S7/_, net

Roe

th. +44 + heea(a)

rb. + 4nd + D™*hraa(q)

dim FE GP iP
ee

 

 

In In IA IA

 

Table 3.3: Comparison on regret bound for the three algorithms.

where 7 can be interpreted as the learning rate. When 7 is large, the weight decay is
faster. In addition, we also update the G-FORCE statistics used to compute q*. Note that
although we recalculate q* at early rounds since the estimation of G-FORCE statistics has
not converged, As time goes on, the estimation of G-FORCE statistics converge to the true

value, and q* would also converge.

3.4.2 Theoretical Analysis of G-FORCE

One key contribution of this thesis is to show that: (1) the fairness loss in G-FORCE can be
asymptotically upper bounded as a function of q,, and q.,_,and (2) the function values can

be reduced to zeros by solving for qz,4 and q,,-, which further minimizes the upper bound.

57

learning. In the case the samples come from population groups, could we adapt the MW
algorithm to guarantee fairness as well? Blum et al. [2018] first proposed to adapt MW
algorithm for online learning with fairness guarantees. The idea is to run separate instances
of the MW for each group in order to equalize the error rates among groups.

However, equalized error rates is a very simplified notion of fairness. In many real
world applications, the impact or cost of false positives and false negatives could be very
different. For example, as discussed in the first chapter, in the COMPAS example, the
model satisfies approximate equalized error rates on white and black defendants, but the
model has a much higher false positive rates for black defendants. In this paper, we in-
troduce G-FORCE , a randomized algorithm achieving approximate EqOdds, which guar-
antees that both false positive rate and false negative rate are equalized. We achieve this
by keeping separate instances of MW instance for each sensitive group-label combination
(z,y). This allows us to provide an upper bound for the number of false positives and
negatives for each group. We show that, given a set of black box experts, it is possible to
obtain an optimal meta selection probability for choosing between different MW instances
that, in turn, will balance regret and fairness. We also show that the algorithm can work in
the delayed feedback setting, where the true label is not revealed instantly after a decision
is made.

G-FORCE can be applied to a wide range of applications as it could work alongside
with human decision makers and correct potential biases. A user could choose the hyper-
parameter \ to set a desirable trade-off between fairness and accuracy. We are also deploy-

ing the algorithm to a real world application.

71

Let q = [¢4,_, 98,-, 94,4, 48,4" be the vector of meta selection probability . Specifically,

|FPR4 — FPR3| <

 

crpr + hrpr(q)|

|FNRa— FNRp| <

 

Crnr + henr(q)|

where crpr, Crnpr are constants that depend on the factors intrinsic to the problem (data
distribution and the underlying metrics of the experts), and hypr,hryr are functions of

meta selection probability q. A formal version of the theorem is stated in Theorem- 6.

In this section, we aim to develop an upper bound on EqOdds for G-FORCE . We start
by first providing an upper bound on regret for the worst cases scenarios, as well as a lower

bound on regret for the best case scenarios (we leave the proof to the appendix).

Since there is randomness involved in the selection of MW instances, we define the

costs of using a sub-optimal instance as cross-instances cost.

Definition 3.4.1 (Cross-instances cost). Let Ty = Sut denote the probability of
choosing expert f when using instance (z,y). We define the cross-instances cost at round t

as the difference in expected loss between selecting right instance (z,y) and wrong instance

(zy):

t _ t t t t
Oy = » They!” Ob ay 7 » They” Oo ay
SEF SEF

——
expected losses with wrong instance (z,y’) expected losses with instances (z,y)

 

For example, «,,_ is the cross-instances cost of selecting the wrong MW instance (z, —)
when the actual example has y = +. The cross-instances cost is non-negative since the
expected losses using the wrong instance would be larger than the expected losses using
the correct instance. The cross-instances cost is larger when the weight vector learned by
the wrong MW instance and the weight vector learned by the right MW instance are more

disparate.

58

42

Chapter 2

Background

In this chapter, we will first introduce some preliminaries and backgrounds of fairness in
machine learning. We will also introduce some of the most well-used definitions fairness,
which will be referred throughout the thesis. We will then introduce related literature in the

context of algorithmic fairness in a sequential setting.

2.1 Preliminaries

Throughout this thesis, we assume the underlying state of the world is represented by the
joint distribution (X,Y, Z) ~ D, where Z € {A, B} corresponds to the partition on sensi-
tive/protected attributes such as gender or race, X € 4 corresponds to the feature vectors,
and Y € ¥ denotes the ground-truth labels.

In supervised learning, the goal of the model is to learn a parametrized function fg that

minimizes the expected risk with respect to the loss function ¢:
min Ecxy,z)-vlé(fo(X), Y)] (2.1)

where @ is the parameter. We use Y= fo(X) to denote the predicted label from the
model. The original goal of the learning problem doesn’t contain the sensitive attribute in
the objective function.

When this classical risk minimization framework is applied to a dataset involving peo-

33


selected is bounded as:

 

we
a = wh +
ma tt ie ee (6.7)
fF ® {EF m4

where af _ < 1 is the difference of loss in expectation made when using the incorrect

weights of the MW algorithm on group z, + (Cross-Instance Cost). Then

E|
E[L

 

 

wy, Whe
(i: ye L . Es ate oe ae] (6.8)

SEF + fEF

 

 

 

T
=>
£
Le] Ise Can + Dida at (6.9)
t=1 |

CF Or:

where using 6.27, we finally obtain:

Ind
EL] < + mLye4 + + oat sat (6.10)

t

Similarly,

 

Ind
[Lz] < (4 Me ++ Dd a, (6.11)

 

 

 

The expected total errors on group z is, adding the two equations above:

 

 

 

 

Ind
[L.] < (1+ m)Lpe+ 27 + Oo dat + ody ot.)
t

t

In the same way, the expected total errors (considering z = A, B) is:

 

 

 

 

Ind
IL] < (Lt mL +4—~ +0 (6.12)

where all the Cross-Instance Costs are condensed in:

a= S: dey > Orgs
t

2€{A,B}.ye{—+}

116

 

Figure 3-5: An example of predicting one example in G-FORCE .

might still look decent and this concerning problem gets swept under the rug. It’s important

for the aggregation algorithm to perform well on both label classes.

3.3.3. Need to consider delayed feedback

In many real world applications, true labels or outcomes are not instantly revealed and
an algorithm often needs to work with delayed feedback. One example of constant delay
is college admission process we described. During the rolling admissions process, the
performance of a student is generally evaluated at the end of the semester, while colleges
typically need to offer admission in a rolling basis. There is a constant gap between decision
time (college offers admission) and feedback time (the admitted students’ performances are

evaluated).

3.4 G-FORCE algorithm

We propose a novel randomized MW algorithm that achieves EqOdds in an online stochas-
tic setting. In order to satisfy EqOdds, we also need a provable bound on the number of
false positives and false negatives made by the algorithm on each group. The idea is to
run separate MW instances not only for groups but also for label classes, where each MW
instance has a separate set of weights for the experts. Throughout the chapter, we uses tuple

(z, y) to refer to a MW instance trained for subset of data with group z and label y. Each

54

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

5 5 5 tT
Qo M0 Yo
-5 -5 -5
- 0 5 -5 60 5 -5 O 5
x1 x1 x1
5 5 5
X 0 0 Yo
-5 -5 -5
5 0 5 5 oO 5 5 0 5
x1 x1 x1
5 5 5
10 @O.. GAs. OF
-5 }}—}— 4-8 RH OSE 4
-5 0 5 -5 600 5 -5 5
x1 x1 x1

Figure 4-19: Synthetic Gaussian Example. Top: MZ, = [1,1]. Middle: Mj = [1,—1]. Bottom:
M3 = [-1, 1).

106

where D is the underlying distribution and D¢ is the distribution constrained by some
fairness metric C for group z, and 74, 7g are group-specific policies for group A and group
B respectively.

In the rest of this chapter, we restrict our attention to threshold policy, where the policy
function is a threshold function on the target variable Y. In real applications, the policy
function should put a threshold on features based on implicitly learned mapping from the
features to the target variable. The reason that we directly put a threshold on the target

variable is to eliminate the effect of complications on learning the policy function.

Definition 4.3.2. A threshold policy assigns positive action when Y' > r for some thresh-

old r, i.e., .(D' = 1|Y") = P(Y' > 7|Z = 2).

We list a few threshold policies that are based on commonly used fairness constraints

and show that they can be reduced in this form:

¢ A Maximum Utility (MaxUtil) policy maximizes the expected utility without con-

straint.

A Demographic Parity (DemoPar) policy maximizes the expected utility subject to
the demographic parity constraints, which requires that both groups have equalized

positive rates on decisions, i.e., E[D' = 1|Z = A] = E[D’ = 1|Z = B). This is

 

 

 

 

 

 

 

 

equivalent to Ez—4|74(Y*)| = Ez-plre(Y")].

An Equalized Opportunity (EqOpp) policy maximizes the expected utility subject to
the equalized opportunity constraints, which requires that both groups have equalized

false positive rates,i.e., E[D' = 1|Y' = 0,7 = A] = E[D' = 1|Y' =0,Z = Bl.

 

 

 

 

 

 

This is equivalent to Ey+,z—4[7a(Y*)] = Eyto,z=n[73(Y")]-

As specified by the order in the list, each policy requires finding optimal thresholds
within a smaller search space specified by more restricted constraints. In generally

we would expect Uyaxurir 2 Udemorar = Uzqopp-

86

 

y
Group Attribute / Zz

 

 

 

 

New Credit
Score
State of
the Credit Score x x
World

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

Repay Feedback
Probability \ ¥ ¥ ]
| , y
mons [reacts
¢ | __ ul =
Action Ae
(Loan Approval)

Figure 4-1: An overview of the feedback loop in the loan application example.

We first use figure-4-1 to illustrate the different components in the loan lending process

as a causal graph.
* State of the World The state of world consists of a tuple (X, Y, Z).
— Sensitive Attribute (7): Each individual comes with a sensitive attribute Z ©
{A, B}, such as race, gender etc. The sensitive attribute is time invariant.

— Credit Score (X): Initially, each individual starts with a credit score X that

depends on group attribute Z.

— Repaying Probability (Y): The repaying probability Y is a function of the credit
score Y = X/10.

¢ Model: A model takes the state of the world and generates a prediction for the re-

paying probability. The process is indicated by the purple link.

¢ Loan Approval Decision : Based on the prediction, a binary loan approval deci-
sion A is issued, which could potentially depend both on an applicant’s credit score

X and the sensitive attribute Z.

¢ Feedback: A decision will have a feedback effect (indicated by the red link) on the
credit scores. In particular, if an applicant successfully repays a loan, the new credit

score X’ (or X‘+! in a multi-step process) will increase by 1 and the bank’s utility

77

the shift from distribution Y° to distribution Y* to capture the distribution in a more fine-
grained way. Here we categorize the possible functions into three categories: (1) functions
that measure the shift of the center of a distribution; (2) functions that measures the shift
of the spread of a distribution; (3) functions that measure the shift of the shape of a distri-

bution.

¢ Center shift: Center shift measures the change of the target variable for a typical

individual in the distribution.

— Difference in mean of target variable distribution (Mean-WGI):
9(Pyz, Pye) = E[¥2] — E[Y?]
— Difference in quantiles (Quantile-WGI):
g(Pys, Pys) = Qe(¥z) — Qx(¥?)

where Q,;, is the k-quantile function. When k = 2, this is equivalent to the
median of the distribution, which quantifies the change for a median individual

in the distribution.
¢ Spread shift: Spread shift measures the change of variability of a distribution.

— Difference in variance of target variable distribution (var- WGI):
o(Pys, Pro) = var{¥] — var(¥2]

— Application inspired metric: One interesting choice with real-world implica-
tions is the Gini coefficient, which measures income inequality within a popu-

lation group.

¢ Shape shift: Shape shift measures the distributional change of cumulative density

functions.

89

0.10
0.08
80.06
o
20.04
0.02
0.00

 

 

 

 

 

 

 

 

 

 

 

 

  

  

 

 

 

 

 

Regret
°
an
°

0.00

 

 

 

 

 

0.10 0.4
—— GroupAware —— GroupAware | __ —— GroupAware
—— G-FORCE wos —— G-FORCE 203 —— GFORCE
& 0.06 =
| I
© 0.04 ©
= 0.02 =
0 10000 20000 9:00 0 10000 20000 0:0 10000 20000
t t
(a) Regret (b) Equalized FPR (c) Equalized FNR
0.8 0.6
GroupAware —— GroupAware 0s —— GroupAware
—— G-FORCE 30.6 —— G-FORCE = —— G-FORCE
x £04
& =
10.4 10.3
nd e
2 = 0.2 Ra
+02 xo
0.1
0 100 200 300 °° 6 100 200 300° 100 200 300
t t t
(d) Regret (e) Equalized FPR (f) Equalized FNR
0.4 0.6
—  GroupAware —— GroupAware és —— GroupAware
—— G-FORCE 30.3 —— G-FORCE = — G-FORCE
&
w
10.2
x
&
+01 x
\ 0.1
- : ; ; 0.0. ; : ;
0 500 1000 1500 0 500 1000 1500 500 1000 1500
t t t
(g) Regret (h) Equalized FPR (i) Equalized FNR

Figure 3-9: G-FORCE shows a clear improvement over GroupAware on both equalized FPR (bot-

tom left) and equalized FNR (bottom right) on adult dataset.

We also report the error rates and associated ¢-fairness of each classifier in the ap-

pendix. The base classifiers expose similar and more mild behaviors (compared with in

real datasets) which makes the task of the algorithm easier, and thus the results are less

significant compared to the real dataset.

3.7

Conclusion

Many real world applications require decision makers to make decisions in a sequential set-

ting. Multiplicative weights algorithm is a classical no-regret algorithm used in sequential

70

ot = y wey

SEF

- > whe (1 _ erent & Ay=t}
LoF

=o =n ah So Gat)

feF ®.4 TED

 

Thus by the recursive function, we have

 

esa Tao Whe >So G24)
fF + TeEDt
T

= aJJo 00 Yo Gea) (6.23)

t=1 feF P+ TeEDt

 

Although the weight update uses a different schedule, there will still be T updates after

T rounds. Thus the same MW update rule applies:

T pt, =
WEA, = wh, (1) Eeateton

= (1 _ nee Cpe (6.24)

where w},. = 1, as all the weights are initialized.

Using 6.23 and 6.24,

 

wht = (1- ME aSa+ < OPH < <I 1-1 What SOG.) 25)

feF 24 TED:

123

Bibliography

Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778,
2016.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Z. Chen, Nikhil Tho-
rat, Fernanda B. Viégas, Martin Wattenberg, Gregory S. Corrado, Macduff Hughes, and Jeffrey
Dean. Google’s multilingual neural machine translation system: Enabling zero-shot translation.
Transactions of the Association for Computational Linguistics, 5:339-351, 2017.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of go with deep neural networks and tree search. Nature, 529:484—489, 2016.

Ziad Obermeyer, Brian W. Powers, Christine Vogeli, and S. Mullainathan. Dissecting racial bias in
an algorithm used to manage the health of populations. Science, 366:447 — 453, 2019.

Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science
Advances, 4, 2018.

Matthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereo-
types in image search results for occupations. Proceedings of the 33rd Annual ACM Conference

on Human Factors in Computing Systems, 2015.

Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. Semantics derived automatically from
language corpora contain human-like biases. Science, 356:183 — 186, 2017.

Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. ProPublica, May
23rd, 2016.

S. Verma and J. Rubin. Fairness definitions explained. 20/8 IEEE/ACM International Workshop on
Software Fairness (FairWare), pages 1-7, 2018.

White-House. Big data: A report on algorithmic systems, opportunity, and civil rights. 2016.
Kimmo Karkkdinen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender,

and age for bias measurement and mitigation. 202] IEEE Winter Conference on Applications of
Computer Vision (WACV), pages 1547-1557, 2021.

131

True Class

Positive Negative

 

 

 

 

 

oO
2| P FP
n

3 C | Ftp Utp || Pip Up

Boo

oa &

® Oo

_ oO |

oa >
=| FN || TN
> L fn, Ufn Ltn, Utn
a

 

 

 

Figure 4-7: Parameters defined in terms of the confusion matrix.

arrow indicates a policy function that generates the decision, and the red arrow indicates
the feedback effect of the decision.

We restrict our attention to linear utility functions and feature updates. Based on the
realized outcome Of ~ Bernoulli(Y ) and decision D‘, we can construct a confusion ma-
trix containing true positive (TP), false positive (FP), true negative (TN), and false negative
(FN).

Specifically, if a qualified (O = 1) candidate is accepted (D = 1), the decision-maker
gains utility w,, > 0 and the individual’s feature is increased by X;,; if an unqualified
(O = 0) candidate is accepted (D = 1) , the decision-maker’s utility is decreased by
Usp > 0 and the individual’s feature is decreased by X;, > 0. If an unqualified (O = 0)
candidate is rejected (D = 0), the decision-maker gains utility w;,, > 0 and the individual’s
feature is increased by X;,; if a qualified (O = 1) candidate is rejected (D = 0), the
decision-maker’s utility is decreased by uy, > 0 and the individual’s feature is decreased
by Xn > 0. In many cases, the utility and features won’t change upon a negative action,

and tin = Ufn = Xin = X fn = 0 could be set to 0.

4.3.3 Threshold policies

One of the most common solutions in fair machine learning is constrained optimization,

where the goal is to learn a model that minimizes the expected loss with respect to loss

84

Algorithmic Fairness in Sequential Decision Making
by
Yi Sun

Submitted to the Institute for Data, Systems and Society
in partial fulfillment of the requirements for the degree of

Doctor of Philosophy in Social and Engineering Systems
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
September 2022

© Massachusetts Institute of Technology 2022. All rights reserved.

AUUHOP 5 5 irs sa sys eee were ox wes ¥ 5 Pm 8 Es wm we Be Bo iar ys We ES BORE Go 8 oe a eB
Institute for Data, Systems and Society
October, 2022

Certified by 0.0... ccc cence tne eee
Kalyan Veeramachaneni
Principal Research Scientist
Thesis Supervisor

Accepted DY... 0... ieee cece eee eee n nee n een eee n eet nee ennees

Fotini Christia
Program Chair, Social and Engineering Systems Doctoral Program

three statistics above are estimated with Bayesian and Dirichlet Prior. We use 7 = 0.35 in

experiments.

3.6.1 Case study: Synthetic Datasets

It is important to test what can be achieved for both algorithms under extreme scenarios.

Datasets We create a synthetic data framework that allows us to control the distributions
and experts with certain properties. The balance between group attribute and labels is
controlled by setting parameters p4, {14,+, }/B,+. For this purpose, we create one synthetic
dataset with imbalanced setting one with balanced setting. The first one is imbalanced
setting where group A is the majority group with higher percentage positive labels, and
group B is the minority group also with lower percentage positive labels. In particular, we
have pa = 0.9, 44,4 = 0.7, 42,4 = 0.3. The second one is the balanced setting where
each group-label combination has the equal number of examples. We visualize these two

settings in figure-3-7.

Creating Black-box Experts It is also important to test the efficacy of our approach
when experts have disparate performances or are extremely biased towards different groups.
For binary classification with two groups, we create four extreme expert, where each is per-
fect (100% accurate) for one of the group-label subsets ({A, +}, {A, —},{B,+},{B,—-}),
and random (50% accurate) for the other three. Thus for each group-label subset, there is

at least one perfect expert/classifier.

  

(a) Imbalanced Setting. (b) Balanced Setting.

Figure 3-7: The size of each color block is proportional to the number of examples in that group-
label subset. Imbalanced setting is created with p4 = 0.9, 44,4 = 0.7, 4B, = 0.3 and balanced
setting is created with p4 = 0.5, wa. = 0.5, wp,4 = 0.5.

66

Name | Target Variable Framework

 

Liu et al. [2018] Continuous MDP with linear transition function
D’ Amour et al. [2020a] Discrete MDP with linear transition function
Zhang et al. [2020] Binary POMDP

Ours Continuous MDP with general transition functions

 

Table 4.1: Setting of the four frameworks.

¢ General transition functions: We provide a framework to model dependency between
random variables using structural equations. This allows the dependency between

features and variables to be a general function.

Distributional change: In our work, we model the target variable as a continuous
variable that measures the qualification probability of each individual. In this case,

we can characterize the distributional change of the target variable beyond the mean.

Metrics for disparate impact across groups: All three previous works provide an
analysis of the group-wise outcome change separately. There is a lack of a clear
metric to measure the disparity of outcome change between groups. We provide a

new metric

4.2 Motivating Example

We next use a loan lending example to show how algorithmic decisions could further seg-
regate distributions of different population groups. Loan lending is a classical example
that has been widely used to study fairness Liu et al. [2018]. The Equal Credit Opportunity
Act, a United States law enacted in 1974, makes it unlawful for any creditor to discriminate
against any applicant on the basis of race, color, religion, national origin, sex, marital sta-
tus, or age. Suppose a bank predicts whether approving or rejecting loan applications from
a stream of applicants. To simplify the process, the only observed features are sensitive
attribute and credit score. Each applicant has a group attribute Z € {A, B} and a discrete

credit score c € [1, 10].

76

Chapter 4

Study of Fairness with Feedback Loop

73

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

State of the World P(X,Y,Z)
Sampled (x1) (x2) (x8 ( x® )
Example SS 4
pa ¥ ¥3 ¥ fu
Experts Predictions be is 288
Ye) jv2| |ee} jee} fxs
J I J y 1
Algorithm
v v y ¥ v
Action y! Y? ¥ y+ yr
Feedback y} a ys yt Y®
Figure 3-3: A figure depicting the online learning process.

z' © Z is the group attribute .

* Each expert i makes a prediction jf = f(x‘, z'). According to the aggregation
algorithm , a final decision g' = f*(zx’, z') is assigned to the individual .

* At time ¢, a set of labels D!, = {y" : t! +72 = t} is revealed, where 7,,, > 0 is the
delay duration for an individual from group z with label y. Here the examples arrive

at time ¢’ will be revealed at time ¢, where t = t' + Tz.

3.1.3 Metric for evaluating accuracy
A frequent performance metric in online learning is Regret, which compares the perfor-
mance of the algorithm with respect to the best fixed expert in hindsight.
Definition 3.1.1 (Regret). After T rounds, regret is formally expressed as
7 i
Regret(T) = S> e(f*(x', 2'),y") — int 3° e(f(2', 24), y') 3.1)

t=1 t=1

The typical goal of online learning is to design a training algorithm that achieves sub-

linear regret compared with the best fixed experts in hindsight over the 7’ rounds; i.e.

49

 

Group A Group B
Applicant | a, a2 a3 a4 ads Mean} by bo bg by bs Mean
CreditScoreX | 4 5 6 8 9 6.4 2 3 4 5 +7 4.2
Decision] 0 1 1 1 1 0.8 0 0 0 1 1 0.2
New Expected X’| 4 5 62 86 98 672 )/2 3 4 5 74 4.28

 

Table 4.2: Outcome when using a policy that has the same threshold regardless of group.

threshold
1

+=

 

 

 

 

~_ om ea a ea

G A ( . (
®@® @: ©
nn AO A A. A, ‘aa

Group B ) 9) i (65 y
U J ;
Credit Score : : >

2 3 4 5 6 7 8 9
bp = 4.28 Ma = 6.72

Figure 4-3: Outcome when using a policy that has the same threshold regardless of group.

In table-4.2, we computed the new average credit scores for the two groups. Both
groups ameliorate with higher average credit scores, though the new score differences be-

tween the two groups A’ = 2.44 is slightly higher than the initial A = 2.2.

Demographic Parity The second bank uses demographic parity as a fairness metric,
which requires the bank to issue loans to the same percentage of people in both groups.
Thus, if 4 out of 5 applicants are qualified for the loan in group A, the bank will also give

out loans to 4 out 5 applicants in group B.

threshold

1

=4 ‘

oD 5 -
..G:8 @ @@

2

 

ni =_
| Group B ) (bs )

2 3 4. 5 6 7 8 9
bp = 4.16 Ha = 6.72
Figure 4-4: Outcome when using a demographic parity policy that issue loans to the same percent-

age of people in both groups.

79

Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. Big data, 5 2:153-163, 2017.

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair deter-
mination of risk scores. Innovations in Theoretical Computer Science, 2017.

Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In
NIPS, 2017.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fair-
ness beyond disparate treatment and disparate impact: Learning classification without disparate
mistreatment. In Proc. of the 26th Int. Conf. on World Wide Web, pages 1171-1180, 2017.

H. Heidari, Vedant Nanda, and K. Gummadi. On the long-term impact of algorithmic decision
policies: Effort unfairness and feature segregation through social learning. In JCML, 2019.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proc. of the 3rd Innovations in Theoretical Computer Science Confe.,
ITCS ’12, pages 214-226, 2012b. doi: 10.1145/2090236.2090255.

Ben Green. The false promise of risk assessments: epistemic reform and the limits of fairness.
Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 2020.

Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymander-
ing: Auditing and learning for subgroup fairness. In JCML, 2018.

Silvia Chiappa. Path-specific counterfactual fairness. In AAAJ, 2019.

Elliot Creager, David Madras, T. Pitassi, and R. Zemel. Causal modeling for fairness in dynamical
systems. ArXiv, abs/1909.09141, 2020.

Amir-Hossein Karimi, Bernhard Schélkopf, and Isabel Valera. Algorithmic recourse: from coun-
terfactual explanations to interventions. Proceedings of the 2021 ACM Conference on Fairness,
Accountability, and Transparency, 2021.

Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online learning with an un-
known fairness metric. In NeurIPS, 31, pages 2600-2609. 2018.

Matthew Joseph, Michael Kearns, Jamie H. Morgenstern, and Aaron Roth. Fairness in learning:
Classic and contextual bandits. In NeurIPS, pages 325-333, 2016.

Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C. Parkes. Cal-
ibrated fairness in bandits. In Proceedings of the 4th Workshop on Fairness, Accountability, and
Transparency in Machine Learning (Fat/ML 2017), 2017.

Yahav Bechavod, Katrina Ligett, Aaron Roth, Bo Waggoner, and Steven Z. Wu. Equal opportunity
in online classification with partial feedback. In NeurIPS, 32, pages 8972-8982. 2019.

Avrim Blum, Suriya Gunasekar, Thodoris Lykouris, and Nati Srebro. On preserving non-

discrimination when combining expert advice. In NeurIPS, 31, pages 8376-8387. NeurIPS 2018,
2018.

133

List of Tables

1.1
1.2

3.1
3.2
3.3
3.4

3.5
3.6

4.1
4.2

4.3

4.4

4.5

Notation table of the terms used throughout the thesis. ...........

A list of protected attributes... 2.2... ee
A glossary of the terms used in the machine learning cycle. The bold terms

are the nodes in the cycle, and the italic terms are the edges in the cycle. . .

Notation table for the terms used in this chapter. ..............
Unique properties of the setting. ©... ........... 0... 000.0.
Comparison on regret bound for the three algorithms. ............
Summary statistics of datasets. Here p4 is the percentage of group A, /i.4+
is the percentage of positive labels in group A, and j1g,+ is the percentage
of positive labels in groupB. .... 2... ..... 0000 ee eee
e-Fairness of base experts, GroupAware and G-FORCE. ..........
Equalized FPR, equalized FNR and regret on real datasets. Lower numbers

fre Deter. aij 2 2k 6 Rie BOE 6b RHEE Ee GBH S EE oe

Setting of the four frameworks... ........2..0......0000.

Outcome when using a policy that has the same threshold regardless of

Outcome when using a demographic parity policy. The bank issues loans
to the same fraction of people (80%) in both group. .............
Outcome when using an equalized opportunity policy. The bank issues
loans to the same fraction of qualified applicants (50%) in both groups... .

Notation table for the terms used in this chapter. ..............

17

68

80

[0,1] and < $ Then after T rounds, for any expert f among the d experts we have:

= Ind
Sot’ < (1+n) oye
t=1

Regret(T) < O(VT Ind) if n= ne

where n° is the selection distribution over the set of experts at time t.

This first equation shows that the expected cumulative loss achieved by the MW algo-
rithm is upper bounded by the cumulative loss of the best fixed expert in hindsight plus a
constant term ae The constant term scales with the number of experts d. If we set 7 to be

nd the first equation can be rearranged into Regret(T) = Svy_, wl'—(1+n) Si, “Os
O(VT Ind). In other words, this powerful theorem shows that MW algorithm achieves

sub-linear regret.

3.2.2 Group-aware MW algorithm

Blum et al. [2018] first proposed a group-aware version of the MW algorithm for achieving
fairness in online adversarial setting, where the examples are not i.i.d sampled from the
distribution. The fairness metric they use is equalized error rates. The idea is to maintain
separate set of weights for each group attribute z. They demonstrated that this is necessary
to achieve equalized error rates across groups.

They presented the regret and equalized error rate achieved by the algorithm.

Theorem 2. GroupAware Regret Bound ([Blum et al., 2018])

Assume that the loss & is bounded in [0,1] and < 3

z Ind
vores (1+7) Soe + 2nd
t=.

where 1 is the selection distribution over the set of experts at time t.

The regret of the group-aware version of the MW is almost the same as the original

regret bound, except for a multiplier of 2 on the constant term =* tad

52

— Wasserstein-1 distance (W1-WGID):

1
(Pv. Pro) = [ |FY.(y) — Fy. (y)|dy
0

where F} Y is the CDF function of distribution Py+.

Next, we define between-group impact, which captures how decisions shift distributions

of two population groups differently.

Definition 4.4.2 (Between-group impact (BGI)). We define the between-group impact at
time step t as

Aap = |i 4 ~ aed (4.3)

The backfire effect appears when algorithmic decisions shape the group-wise distribu-

tions in different ways that further increase the disparity between them.
Definition 4.4.3 (Backfire effect). We say that a policy has a backfire effect if:

* g-WGI<0 if g measures center or shape of distribution; or g-WGI<0 if g measures

spread of a distribution.
¢ BGI is increased compared to the initial distribution, i.e., AT > A®.

We use Figure 4-8 to illustrate the backfire effect in terms of within-group impact and

between-group impact.

4.4.3 Disparity in a broader context

While disparity could be defined statistically, it is important to understand the implications
of disparity and segregation in a broader context. We draw insights from closely related
concepts in sociology and economics regarding inequality and discuss how these concepts

can be adapted to quantify the inequality introduced by algorithmic decisions.

Social Segregation Racial segregation is a well-studied phenomenon in sociology, where

population groups are separated geographically. In the context of machine learning, we can

90

Notations used in MDP literature Notations used in this framework

 

State S' A state S contains (Z, X,Y)

Set of actions A Binary decisions D = {0, 1}

Transition function P Structure equation for feature update fx
Reward function R Utility function U

 

Table 4.6: A mapping between notations in the literature and notations used in our framework.

 

 

Notation Meaning
Yt = fy(X‘) Function links features X to target variable Y.
Dt = 7(S*) Function links state S to decision D.

X'+1 = fx(X', D', O") Feature update function that links features new feature X'+! to
old feature X‘, decision D’, and outcome O'
U = fu(O', D') Function links utility to outcome and decision.

 

Table 4.7: Set of structural equations and their purpose.

function £ and subject to some fairness constraints [Donini et al., 2018]. For example, the

following constraint ensures demographic parity:

 

 

min Exy.z)v[L(fo(X, Z), Y)]

 

 

 

s.t. Evxy,z)~vlfo(X, Z)] = Evxy,z)~v|fo(X, Z)]

 

In sequential decision making, the decision maker uses a policy function 7 as guidance for

sequential decisions, where decisions are repeatedly sampled from this function.

Definition 4.3.1. A policy 7 : S — [0,1] is a function that maps from states S € S to the

probability distribution over decision d, i.e., 7(d|s) = P(D* = d|S* = s).

In sequential decision making, a decision-maker repeatedly maximizes the utility sub-
ject to the fairness constraints as in the one-step optimization. If the probability of an
individual coming from group z is p., we can decompose the utility with respect to the

group distribution:

 

 

max — paEpinn,(st)(U(D', Y)] + ppEpinep(sy(U(D", Y")]
m=(T4,7B) (4.1)

st. Egtps [wa(S")] = Egeups, [7a(S")]

 

 

85
