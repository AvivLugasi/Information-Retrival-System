Identifying and Reducing Gender Bias in Word-Level Language Models
Samuel R. Bowman1,2,3
bowman@nyu.edu

Shikha Bordia1
sb6416@nyu.edu
1

Dept. of Computer Science
New York University
251 Mercer St
New York, NY 10012

2

Center for Data Science
New York University
60 Fifth Avenue
New York, NY 10011

arXiv:1904.03035v1 [cs.CL] 5 Apr 2019

Abstract
Many text corpora exhibit socially problematic
biases, which can be propagated or amplified
in the models trained on such data. For example, doctor cooccurs more frequently with
male pronouns than female pronouns. In this
study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and
the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for
the language model that minimizes the projection of encoder-trained embeddings onto
an embedding subspace that encodes gender;
(iv) finally, evaluate efficacy of our proposed
method on reducing gender bias. We find this
regularization method to be effective in reducing gender bias up to an optimal weight
assigned to the loss term, beyond which the
model becomes unstable as the perplexity increases. We replicate this study on three training corpora—Penn Treebank, WikiText-2, and
CNN/Daily Mail—resulting in similar conclusions.

1

Introduction

Dealing with discriminatory bias in training data
is a major issue concerning the mainstream implementation of machine learning. Existing biases in data can be amplified by models and the
resulting output consumed by the public can influence them, encourage and reinforce harmful
stereotypes, or distort the truth. Automated systems that depend on these models can take problematic actions based on biased profiling of individuals. The National Institute for Standards and
Technology (NIST) evaluated several facial recognition algorithms and found that they are systematically biased based on gender (Ngan and Grother,
2015). Algorithms performed worse on faces labeled as female than those labeled as male.

3

Dept. of Linguistics
New York University
10 Washington Place
New York, NY 10003

Models automating resume screening have also
proved to have a heavy gender bias favoring male
candidates (Lambrecht and Tucker, 2018). Such
data and algorithmic biases have become a growing concern. Evaluation and mitigation of biases
in data and models that use the data has been a
growing field of research in recent years.
One natural language understanding task vulnerable to gender bias is language modeling. The
task of language modeling has a number of practical applications, such as word prediction used in
onscreen keyboards. If possible, we would like
to identify the bias in the data used to train these
models and reduce its effect on model behavior.
Towards this pursuit, we aim to evaluate the effect of gender bias on word-level language models
that are trained on a text corpus. Our contributions
in this work include: (i) an analysis of the gender bias exhibited by publicly available datasets
used in building state-of-the-art language models;
(ii) an analysis of the effect of this bias on recurrent neural networks (RNNs) based word-level
language models; (iii) a method for reducing bias
learned in these models; and (iv) an analysis of the
results of our method.

2

Related Work

A number of methods have been proposed for
evaluating and addressing biases that exist in
datasets and the models that use them. Recasens
et al. (2013) studies the neutral point of view
(NPOV) edit tags in the Wikipedia edit histories
to understand linguistic realization of bias. According to their study, bias can be broadly categorized into two classes: framing and epistemological. While the framing bias is more explicit, the epistemological bias is implicit and
subtle. Framing bias occurs when subjective or
one-sided words are used. For example, in the

Cross Entropy Loss
Training Word Level Language Model

Training Corpus

Generated Text
Bias in Generated Text

Cross Entropy Loss
+ λ(N.B)

Generated Text
with
Regularization

Bias after Regularization

Bias in Training Corpus

Figure 1: Word level language model is a three layer LSTM model. λ controls the importance of minimizing bias
in the embedding matrix.

sentence—“Usually, smaller cottage-style houses
have been demolished to make way for these McMansions.”, the word McMansions has a negative connotation towards large and pretentious
houses. Epistemological biases are entailed, asserted or hedged in the text. For example, in
the sentence—“Kuypers claimed that the mainstream press in America tends to favor liberal
viewpoints,” the word claimed has a doubtful effect on Kuypers statement as opposed to stated
in the sentence—“Kuypers stated that the mainstream press in America tends to favor liberal
viewpoints.” It may be possible to capture both of
these kinds of biases through the distributions of
co-occurrences. In this paper, we deal with identifying and reducing gender bias based on words
co-occurring in a context window.
Bolukbasi et al. (2016) propose an approach to
investigate gender bias present in popular word
embeddings, such as word2vec (Mikolov et al.,
2013). They construct a gender subspace using a
set of binary gender pairs. For words that are not
explicitly gendered, the component of the word
embeddings that project onto this subspace can be
removed to debias the embeddings in the gender
direction. They also propose a softer variation
that balances reconstruction of the original embeddings while minimizing the part of the embeddings that project onto the gender subspace. We
use the softer variation to debias the embeddings
while training our language model.
Zhao et al. (2017) look at gender bias in the context of using structured prediction for visual object
classification and semantic role labeling. They ob-

serve gender bias in the training examples and that
their model amplifies the bias in its predictions.
They impose constraints on the optimization to reduce bias amplification while incurring minimal
degradation in their model’s performance.
Word embeddings can capture the stereotypical bias in human generated text leading to biases
in NLP Applications. Caliskan et al. (2017) conduct Word Embedding Association Test (WEAT).
It is based on the hypothesis that word embeddings
closer together in high dimensional space are semantically closer. They find strong evidence of
social biases in pretrained word embeddings.
Rudinger et al. (2018) introduce Winogender
schemas1 and evaluate three coreference resolution systems—rule-based, statistical and neural
systems. They find that these systems’ predictions
strongly prefer one gender over the other for occupations.
Font and Costa-Jussà (2019) study the impact
of gender debiasing techniques by Bolukbasi et al.
(2016) and Zhao et al. (2018) in machine translation. They find these methods to be effective,
and even a noted BLEU score improvement for the
debiased model. Our work is closely related but
while they use debiased pretrained embeddings,
we train the word embeddings from scratch and
debias them while the language model is trained.
May et al. (2019) extend WEAT to state-ofthe-art sentence encoders: the Sentence Encoder
Association Test (SEAT). They show that these
tests can provide an evidence for presence of bias.
1
It is Winograd Schema-style coreference dataset consisting of pair of sentences that differ only by a gender pronoun

However, the cosine similarity between sentences
can be an inadequate measure of text similarity
in sentences. In this paper, we attempt to minimize the cosine similarity between word embeddings and gender direction.
Gonen and Goldberg (2019) conduct experiments using the debiasing techniques proposed by
Bolukbasi et al. (2016) and Zhao et al. (2018).
They show that bias removal techniques based on
gender direction are inefficient in removing all aspects of bias. In a high dimensional space, spatial distribution of the gender neutral word embeddings remain almost same after debiasing. This
enables a gender-neutral classifier to still pick up
the cues that encode other semantic aspects of
bias. We use softer variation of the debiasing
method proposed by Bolukbasi et al. (2016) and
attempt to measure the debiasing effect from the
minimal changes in the embedding space.

3

Methods

We first examine the bias existing in the datasets
through qualitative and quantitative analysis of
trained embeddings and cooccurrence patterns.
We then train an LSTM word-level language
model on a dataset and measure the bias of the
generated outputs. As shown in Figure 1, we then
apply a regularization procedure that encourages
the embeddings learned by the model to depend
minimally on gender. We debias the input and the
output embeddings individually as well as simultaneously. Finally, we assess the efficacy of the
proposed method in reducing bias.
We observe that when both input and output embeddings are debiased together, the perplexity of
the model shoots up by a much larger number than
the input or the output embeddings debiased individually. We report our results when only input
embeddings are debiased. This method, however,
does not limit the model to capture other forms of
bias being learned in other model parameters or
output embeddings.
The code implementing our methods can be
found in our GitHub repository.2
3.1

Datasets and Text Preprocessing

We compare the model on three datasets–Penn
Treebank (PTB), WikiText-2 and CNN/Daily
Mail. The first two have been used in language
modeling for a long time. We include CNN/Daily
2

https://github.com/BordiaS/language-model-bias

Mail dataset in our experiments as it contains a
more diverse range of topics.
PTB Penn Treebank comprises of articles ranging from scientific abstracts, computer manuals,
etc. to news articles. In our experiments, we
observe that PTB has a higher count of male
words than female words. Following prior language modeling work, we use the Penn Treebank
dataset (PTB; Marcus et al., 1993) preprocessed
by Mikolov et al. (2010).
WikiText-2 WikiText-2 is twice the size of the
PTB and is sourced from curated Wikipedia articles. It is more diverse and therefore has a
more balanced ratio of female to male gender
words than PTB. We use preprocessed WikiText-2
(Wikitext-2; Merity et al., 2016).
CNN/Daily Mail This dataset is curated from a
diverse range of news articles on topics like sports,
health, business, lifestyle, travel etc. This dataset
has an even more balanced ratio of female to male
gender words and thus, relatively less biased than
the above two. However, this does not mean that
the use of pronouns is not biased. This dataset
was released as part of a summarization dataset by
Hermann et al. (2015), and contains 219,506 articles from the newspaper the Daily Mail. We subsample the sentences by a factor of 100 in order
to make the dataset more manageable for experiments.
3.2

Word-Level Language Model

We use a three-layer LSTM word-level language
model (AWD-LSTM; Merity et al., 2018) with
1150 hidden units implemented in PyTorch.3
These models have an embedding size of 400 and
a learning rate of 30.
We use a batch size of 80 for Wikitext-2 and
40 for PTB. Both are trained for 750 epochs.
The PTB baseline model achieves a perplexity
of 62.56. For WikiText-2, the baseline model
achieves a perplexity of 67.67.
For CNN/Daily Mail, we use a batch size of 80
and train it for 500 epochs. We do early stopping
for this model. The hyperparameters are chosen
through a systematic trial and error approach. The
baseline model achieves a perplexity of 118.01.
All three baseline models achieve reasonable
perplexities indicating them to be good proxies for
standard language models.
3

https://github.com/salesforce/awd-lstm-lm

λ
train
0.0
0.001
0.01
0.1
0.5
0.8
1.0

Fixed Context
µ
σ
β
0.83
0.74
0.69
0.63
0.64
0.70
0.76
0.84

1.00
0.91
0.88
0.81
0.82
0.91
0.96
0.94

0.40
0.34
0.31
0.33
0.39
0.45
0.38

Infinite Context
µ
σ
β
3.81
2.23
2.43
2.56
2.30
2.91
3.43
2.42

4.65
2.90
2.98
3.40
3.09
3.76
4.06
3.02

0.38
0.35
0.36
0.24
0.38
0.26
-0.30

P pl.
62.56
62.69
62.83
62.48
62.5
63.36
62.63

Table 1: Experimental results for Penn Treebank and generated text for different λ values

λ
train
0.0
0.001
0.01
0.1
0.5
0.8
1.0

Fixed Context
µ
σ
β
0.80
0.70
0.69
0.61
0.65
0.70
0.65
0.74

1.00
0.84
0.84
0.79
0.82
0.88
0.84
0.92

0.29
0.27
0.20
0.24
0.31
0.28
0.27

Infinite Context
µ
σ
β
3.70
3.48
2.32
1.88
2.26
2.25
2.07
2.32

4.60
4.29
3.12
2.69
3.11
3.17
2.98
3.21

0.15
0.16
0.14
0.06
0.20
0.18
-0.08

P pl.
67.67
67.84
67.78
67.89
69.07
69.36
69.56

Table 2: Experimental results for WikiText-2 and generated text for different λ values

λ
train
0.0
0.1
0.5
0.8
1.0

Fixed Context
µ
σ
β
0.72
0.51
0.38
0.34
0.40
0.62

0.94
0.68
0.52
0.48
0.56
0.83

0.22
0.19
0.14
0.19
0.21

Infinite Context
µ
σ
β
0.77
0.43
0.85
0.79
0.96
1.71

1.05
0.59
1.38
1.31
1.57
2.65

0.29
0.22
0.20
0.23
0.31

P pl.
118.01
116.49
116.19
121.00
120.55

Table 3: Experimental results for CNN/Daily Mail and generated text for different λ values

3.3

Quantifying Biases

For numeric data, bias can be caused simply by
class imbalance, which is relatively easy to quantify and fix. For text and image data, the complexity in the nature of the data increases and it
becomes difficult to quantify. Nonetheless, defining relevant metrics is crucial in assessing the bias
exhibited in a dataset or in a model’s behavior.
3.3.1

Bias Score Definition

In a text corpus, we can express the probability of
a word occurring in context with gendered words
as follows:
P (w|g) =

c(w, g)/Σi c(wi , g)
c(g)/Σi c(wi )

where c(w, g) is a context window and g is a set of
gendered words that belongs to either of the two
categories: male or female. For example, when
g = f , such words would include she, her, woman

etc. w is any word in the corpus, excluding stop
words and gendered words. The bias score of a
specific word w is then defined as:

biastrain (w) = log

P (w|f )
P (w|m)



This bias score is measured for each word in the
text sampled from the training corpus and the text
corpus generated by the language model. A positive bias score implies that a word cooccurs more
often with female words than male words. For an
infinite context, the words doctor and nurse would
cooccur as many times with a female gender as
with male gender words and the bias scores for
these words will be equal to zero.
We conduct two sets of experiments where we
define context window c(w, g) as follows:
Fixed Context In this scenario, we take a fixed
context window size and measure the bias scores.

We generated bias scores for several context window sizes in the range (5, 15). For a context size k,
there are k words before and k words after the target word w for which the bias score is being measured. Qualitatively, a smaller context window
size has more focused information about the target word. On the other hand, a larger window size
captures topicality (Levy and Goldberg, 2014). By
choosing an optimal window of k = 10, we give
equal weight of 5% to the ten words before and the
ten words after the target word.
Infinite Context In this scenario, we take an infinite window of context with weights diminishing exponentially based on the distance between
the target word w and the gendered word g. This
method emphasizes on the fact that the nearest
word has more information about the target word.
The farther the context gets away from a word,
the less information it has about the word. We
give 5% weight to the words adjacent to the target
word as in Fixed Context but reduce the weights
of the words following by 5% and 95% to the rest;
this applied recursively gives a base of 0.95. This
method of exponential weighting instead of equal
weighting adds to the stability of the measure.
3.3.2

Bias Reduction Measures

To evaluate debiasing of each model, we measure
the bias for the generated corpus.
biasλ (w) = log(

P (w|f )
)
P (w|m)

To estimate the amplification or reduction of the
bias, we fit a univariate linear regression model
over bias scores of context words w as follows:
biasλ (w) = β ∗ biastrain (w) + c
where β is the scaled amplification measure relative to the training data. Reducing β implies debiasing the model.
We also look at the distribution of the bias by
evaluating mean absolute bias and deviation in
bias scores for each context word in each of the
generated corpora.
µλ = mean(abs(biasλ )); σλ = stdev(biasλ )
We take the mean of absolute bias score as the
word can be biased in either of the two directions.

3.4

Model Debiasing

Machine learning techniques that capture patterns
in data to make coherent predictions can unintentionally capture or even amplify the bias in data
(Zhao et al., 2017). We consider a gender subspace present in the learned embedding matrix in
our model as introduced in the Bolukbasi et al.
(2016) paper. We train these embeddings on the
word level language model instead of using the
debiased pretrained embeddings (Font and CostaJussà, 2019). We conduct experiments for the
three cases where we debias—input embeddings,
output embeddings, and both the embeddings simultaneously.
Let w ∈ SW be a word embedding corresponding to a word in the word embedding matrix W .
Let
Di , . . . , D n ⊂ S W
be the defining sets4 that contain gender-opposing
words, e.g. man and woman. The defining sets
are designed separately for each corpus since certain words may not appear in another corpus. We
consider it a defining set if both gender-opposing
words occur in the training corpus.
If ui , vi are the embeddings corresponding to
the words man and woman, then {ui , vi } = Di .
We consider the matrix C which is defined as a
stack of difference vectors between the pairs in the
defining sets. We have:
 u −v 
( 12 1)


..
C=
 = U ΣV
.
un −vn
( 2 )
The difference between the pairs encodes the
gender information corresponding to the gender
pair. We then perform singular value decomposition on C, obtaining U ΣV . The gender subspace
B is then defined as the first k columns (where k
is chosen to capture 50% of the variation) of the
right singular matrix V :
B = V1:k
Let N be the matrix consisting of the embeddings for which we would like the corresponding
words to exhibit unbiased behavior. If we want
the embeddings in N to have minimal bias, then
its projection onto the gender subspace B should
be small in terms its the squared Frobenius norm.
4

See the supplement for corpus-wise defining sets

Target Word

λ
0.0

crying

0.5
1.0
0.0

fragile

0.5
1.0

0.0
leadership

0.5
1.0
0.0

prisoner

0.5
1.0

Sample From Generated Text
“she was put on her own machine to raise money for her own wedding <unk> route which saw her
crying and <unk> down a programme today . effects began by bottom of her marrow the <unk>”
“he <unk> in the americas with the <unk> which can spread a <unk> circumcision ceremony
made last month . as he <unk> his mother s <unk> crying to those that”
“he discovered peaceful facebook remains when he was caught crying officers but was arrested after
they found the crash hire a man <unk> brown shocked his brother <unk> over”
“camilla said she talked to anyone and had previously left her love of two young children . it all
comes with her family in conviction of her son s death . it s been fragile . the <unk> and retail boy
that was rik s same maker identified genuinely <unk> attacked all”
“his children at nearby children s hospital in <unk> and went <unk> years after he was arrested on
<unk> bail . she spent relaxed weeks in prison after being sharply in fragile <unk> while she was
jailed and strangled when she was born in <unk> virginia”
“could they possibly have a big barrier to jeff <unk> and <unk> my son all intelligence period that
will contain the east country s world from all in the world the truth is when we moved clear before
the split twenty days earlier that day . none of the distributed packs on the website can never <unk>
re able to <unk> it the second time so that fitting fragile <unk> are and less the country is <unk>
. it came as it was once <unk> million lead jobs mail yorkshire . adoption of these first product is
ohio but it is currently almost impossible for the moon to address and fully offshore hotly ”
“mr <unk> worked traditions at the squadron base in <unk> rbs to marry the us government .he
referring to the mainland them in february <unk> he kept communist leadership from undergoing”
“obama s first wife janet had a chance to run the opposition for a superbowl event for charity the
majority of the south african people s travel stage <unk> leadership while it was married off christmas”
“the woman s lungs and drinking the ryder of his daughters s leadership morris said businesses .
however being of his mouth around wiltshire and burn talks from the hickey s <unk> employees”
“his legs and allegedly killed himself by suspicious points . in the latest case after an online page he
left prisoner in his home in <unk> near <unk> manhattan on saturday when he was struck in his
car operating in <unk> bay smoking <unk> and <unk> <unk> when he had”
“it is something that the medicines can target prisoner and destroy <unk> firms in the uk but i hope
that there are something into the on top getting older people who have more branded them as poor .”
“the ankle follows a worker <unk> her <unk> prisoner she died this year before now an profile
which clear her eye borrowed for her organ own role . it was a huge accident after the drugs she had”

Table 4: Generated text comparison for CNN/Daily Mail for different λ values

Therefore, to reduce the bias learned by the embedding layer in the model, we can add the following bias regularization term to the training loss:

LB = λkN Bk2F

4
4.1

We input 2000 random seeds in the language
model as starting points to start word generation.
We use the previous words as an input to the language model and perform multinomial selection to
generate up the next word. We repeat this up to
500 times. In total, we generate 106 tokens for all
three datasets for each λ and measure the bias.

Model

After achieving the baseline results, we run experiments to tune λ as hyperparameter. We report an
in-depth analysis of bias measure on the models
with debiased input embeddings.
4.2

where λ controls the importance of minimizing
bias in the embedding matrix W (from which N
and B are derived) relative to the other components of the model loss. The matrices N and C
are updated each iteration during the model training.

Experiments

Results and Text Examples

We calculate the measures stated in Section 3.3 for
the three datasets and the generated corpora using
the corresponding RNN models. The results are
shown in Tables 1, 2 and 3. We see that the µ consistently decline as we increase λ until a point, beyond which the model becomes unstable. So there
is a scope of optimizing the λ values. The detailed
analysis is presented in Section 4.3
Table 4 shows excerpts around selected target
words from the generated corpora to demonstrate
the effect of debiasing for different values of λ.
We highlight the words crying and fragile that are
typically associated with feminine qualities, along

with the words leadership and prisoners that are
stereotyped with male identity. These biases are
reflected in the generated text for λ = 0. We notice increased mention of the less probable gender in the subsequent generated text with debiasing (λ = 0.5, 1.0). For fragile, the generated text
at λ = 1.0 has reduced the mention of stereotyped
female words but had no mentions of male words;
resulting in a large chunk of neutral text. Similarly, in prisoners, the generated text for λ = 0.5
has no gender words.
However, these are small snippets and the bias
scores presented in the supplementary table quantifies the distribution of gender words around the
target word in the entire corpus. These target
words are chosen as they are commonly perceived
gender biases and in our study, they show prominent debiasing effect.5
4.3

Analysis and Discussion

We consider a text corpus to be biased when it has
a skewed distribution of words cooccuring with
one gender vs another. Any dataset that has such
demographic bias can lead to (potentially unintended) social exclusion (Hovy, 2015). PTB and
WikiText-2 consist of news articles related to business, science, politics, and sports. These are all
male dominated fields. However, CNN/Daily Mail
consists of articles across diverse set of categories
like entertainment, health, travel etc. Among the
three corpora, Penn Treebank has more frequent
mentions of male words with respect to female
words and CNN/Daily Mail has the least.
As defined, bias score of zero implies perfectly
neutral word, any value higher/lower implies female/male bias. Therefore, the absolute value of
bias score signifies presence of bias. Overall bias
in a dataset can be estimated as the average of absolute bias score (µ). The aggregated absolute bias
scores µ of the three datasets—Penn Treebank,
WikiText-2, and CNN/Daily Mail—are 0.83, 0.80,
and 0.72 respectively. Higher µ value in this measure means on-an-average the words in the entire
corpus are more gender biased. As per the Tables
1, 2, and 3, we see that the µ consistently decline
as we increase λ until a point, beyond which the
model becomes unstable. So there is a scope of
optimizing the λ values.
The second measure we evaluated is the standard deviation (σ) of the bias score distribution.
5

For more examples, refer to the supplement

Less biased dataset should have the bias score concentrating closer to zero and hence lower σ value.
We consistently see that, with the initial increase
of λ, there is a decrease in σ of the bias score distribution.
The final measure to evaluate debiasing is comparison of bias scores at individual word level. We
regress the bias scores of the words in generated
text against their bias scores in the training corpus
after removing the outliers. The slope of regression β signifies the amplification or dampening effect of the model relative to the training corpus.
Unlike the previous measures, this measure gives
clarity at word level bias changes. A drop in β signifies reduction in bias and vice versa. A negative
β signifies inversion in bias assuming there are no
other effects of the loss term. In our experiments,
we observe β to increase with higher values of λ
possibly due to instability in model and none of
those values go beyond 1.
We observe that corpus level bias scores like µ,
σ are less effective measures to study efficacy of
debiasing techniques because they fail to track the
improvements at word level. Instead, we recommend a word level score comparison like β to evaluate robustness of debiasing at corpus level.
To choose the context window in a more robust manner, we take exponential weightings to
the cooccurrences. The results for aggregated average of absolute bias and standard deviation show
the same pattern as in fixed context window.
As shown in the results above, we see that
the standard deviation (σ), absolute mean (µ) and
slope of regression (β) reduce for smaller λ relative to those in training data and then increase
with λ to match the variance in the original corpus. This holds for the experiments conducted
with fixed context window as well as with exponential weightings.

5

Conclusion

In this paper, we quantify and reduce gender bias
in word level language models by defining a gender subspace and penalizing the projection of the
word embeddings onto that gender subspace. We
device a metric to measure gender bias in the training and the generated corpus.
In this study, we quantify corpus level bias
in two different metrics—absolute mean (µ) and
standard deviation (σ). However, for evaluating
debiasing effects, we propose a relative metric (β)

to study the change in bias scores at word level in
generated text vs. training corpus. To calculate
β, we conduct an in-depth regression analysis of
the word level bias measures in the generated text
corpus over the same for the training corpus.
Although we found mixed results on amplification of bias as stated by Zhao et al. (2017), the debiasing method shown by Bolukbasi et al. (2016)
was validated with the use of novel and robust
bias measure designed in this paper. Our proposed
methodology can deal with distribution of words
in a vocabulary in word level language model and
it targets one way to measure bias, but it’s highly
likely that there is significant bias in the debiased
models and data, just not bias that we can detect
on this measure. It can be concluded different bias
metrics show different kinds of bias (Gonen and
Goldberg, 2019).
We additionally observe a perplexity bias tradeoff as a result of the additional bias regularization
term. In order to reduce bias, there is a compromise on perplexity. Intuitively, as we reduce bias
the perplexity is bound to increase due to the fact
that, in an unbiased model, male and female words
will be predicted with an equal probability.

6

Acknowledgements

We are grateful to Yu Wang and Jason Cramer for
helping to initiate this project, to Nishant Subramani for helpful discussion, and to our reviewers
for their thoughtful feedback. Bowman acknowledges support from Samsung Research.

References
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? Debiasing word embeddings. In D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 29, pages 4349–4357. Curran
Associates, Inc.
Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.
Joel Escudé Font and Marta R. Costa-Jussà. 2019.
Equalizing gender biases in neural machine translation with word embeddings techniques. CoRR,
abs/1901.03116.
Hila Gonen and Yoav Goldberg. 2019. Lipstick on a
pig: Debiasing methods cover up systematic gender

biases in word embeddings but do not remove them.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Association for Computational Linguistics.
Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 1693–1701. Curran Associates,
Inc.
Dirk Hovy. 2015. Demographic factors improve classification performance. In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 752–762.
Anja Lambrecht and Catherine E Tucker. 2018. Algorithmic bias? an empirical study into apparent gender-based discrimination in the display of
stem career ads. Social Science Research Network
(SSRN).
Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 302–308.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.
Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On measuring social bias in sentence encoders. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.
Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and optimizing LSTM
language models. In International Conference on
Learning Representations.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843.
Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048. ISCA.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing
systems, pages 3111–3119.

Mei Ngan and Patrick Grother. 2015. Face recognition vendor test (FRVT) performance of automated
gender classification algorithms. US Department
of Commerce, National Institute of Standards and
Technology.
Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic models for analyzing and detecting biased language. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1650–1659.
Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, New Orleans, Louisiana.
Association for Computational Linguistics.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification using
corpus-level constraints. In EMNLP, pages 2979–
2989. Association for Computational Linguistics.
Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and KaiWei Chang. 2018. Learning gender-neutral word
embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pages 4847–4853. Association for Computational Linguistics.

A

Defining sets

The gender pair list for each corpus is designed
separately. We consider only those gender pairs
that occur in the training corpus. Below are the
gender lists corresponding to each corpus:
A.1

Penn Treebank

Male Words: “actor” “boy” “father” “he” “him
” “his” “male” “man” “men” “son” “sons”
“spokesman” “wife” “king” “brother”
Female Words: “actress” “girl ” “mother”
“she” “her ” “her” “female” “woman” “women”
“daughter” “daughters” “spokeswoman” “husband” “queen” “sister”
A.2

WikiText-2

Male Words: “actor” “Actor” “boy” “Boy”
“boyfriend” “Boys” “boys” “father” “Father” “Fathers” “fathers” “Gentleman” “gentleman” “gentlemen” “Gentlemen” “grandson” “he” “He”
“hero” “him” “Him” “his” “His” “Husband”
“husbands” “King” “kings” “Kings” “male”
“Male” “males” “Males” “man” “Man” “men”
“Men” “Mr.” “Prince” “prince” “son” “sons”
“spokesman” “stepfather” “uncle” “wife” “king”

Female Words: “actress” “Actress” “girl” “Girl”
“girlfriend” “Girls” “girls” “mother” “Mother”
“Mothers” “mothers” “Lady” “lady” “ladies”
“Ladies” “granddaughter” “she” “She” “heroine”
“her” “Her” “her” “Her” “Wife” “wives” “Queen”
“queens” “Queens” “female” “Female” “females” “Females” “woman” “Woman” “women”
“Women” “Mrs.” “Princess” “princess” “daughter” “daughters” “spokeswoman” “stepmother”
“aunt” “husband” “queen”
A.3

CNN/Daily Mail

Male Words: “actor” “boy” “boyfriend” “boys”
“father” “fathers” “gentleman” “gentlemen”
“grandson” “he” “him” “his” “husbands” “kings”
“male” “males” “man” “men” “prince” “son”
“sons” “spokesman” “stepfather” “uncle” “wife”
“king” “brother” “brothers”
Female Words: “actress” “girl” “girlfriend”
“girls” “mother” “mothers” “lady” “ladies”
“granddaughter” “she” “her” “her” “wives”
“queens”
“female”
“females”
“woman”
“women” “princess” “daughter” “daughters”
“spokeswoman” “stepmother” “aunt” “husband”
“queen” “sister” “sisters”

B

Word Level Bias Examples

Tables 5 and 6 show the bias scores at individual
word level for selected words for Wikitext-2. The
tables show how the scores vary for the training
text and the generated text for different values of
λ
Tables 7 and 8 show the bias scores at individual word level for selected words for CNN/Daily
Mail. The tables show how the scores vary for the
training text and the generated text for different
values of λ

Target Words

training

λ=0.0

λ=0.01

λ=0.1

λ=0.5

λ=0.8

λ=1.0

-0.76
-0.95
-0.68
-0.52
-0.59
-0.69
-0.01
-0.01
-0.35
-0.84
-0.76
-0.46
-0.22
-0.20
-1.58
-0.60
-0.12
-0.92
-0.24
-0.39

-1.20
-1.06
-1.06
-0.91
-1.06
-2.06
-1.89
-1.76
-1.20
-0.91
-1.20
-2.06
-0.91
-1.06
-1.60
-0.79
-1.06
-1.29
-0.88
-1.20

-0.87
-0.23
0.09
-0.26
0.11
-0.09
-0.39
-0.99
-0.18
0.57
-0.23
0.03
-0.12
-0.36
-0.04
-0.31
0.17
-0.81
-0.48
0.54

-0.32
-1.06
-0.56
-0.22
-0.34
-0.14
-0.61
-0.86
-1.01
0.00
-0.63
-0.36
-0.41
-0.16
-0.30
-0.29
-1.01
-1.02
-0.23
-0.16

-0.17
-0.13
-0.14
-0.48
-0.84
-0.32
-0.64
-0.64
-0.84
-0.01
-0.74
-1.00
-0.40
-0.27
-1.08
-0.32
-0.79
-0.57
-0.49
-0.68

0.13
-0.37
-0.44
-0.26
-0.92
0.08
-1.14
-0.16
-0.11
-0.39
0.43
0.17
-0.57
-0.06
-0.30
-0.51
-0.95
-0.67
-0.52
0.58

-1.48
-0.94
-0.23
0.01
-0.61
0.53
-0.56
0.07
0.36
-0.83
-0.81

Arts
Boston
Edward
George
Henry
Peter
Royal
Sir
Stephen
Taylor
ambassador
failed
focused
idea
manager
students
university
wife
work
youth

-0.53
-0.42
-1.06
-0.50
-0.70
-1.03
-0.13

Table 5: WikiText-2 bias scores for the words biased towards male gender for different λ values

Target Words

training

Katherine
Zenobia
childhood
cousin
humor
invitation
parents
partners
performances
producers
readers
stars
talent
wore

1.78
0.05
0.48
0.13
0.34
0.19
0.51
0.85
0.79
1.04
0.22
0.85
0.02
0.09

λ=0.0

λ=0.01

λ=0.1

λ=0.5

λ=0.8

λ=1.0

2.27
0.88
1.80
0.88
1.29
1.80
0.76
2.27
1.02
1.58
0.88
1.58
0.88
0.88

1.38
1.84
0.12
0.67

0.69
0.47
1.10
0.13
0.69
0.69
0.08
0.98
0.16
0.78
0.29
0.90
0.10
0.29

0.95
0.65
0.37
0.09
0.61
0.57
0.45
0.87
0.03
1.35
0.36
0.46
0.31
0.65

0.75
1.24
0.38
0.67
0.34
-0.44
0.57
-0.17
0.10
-1.45
-0.32
-0.28
-0.86
0.16

0.70

-0.87
0.77
-0.28
-0.20
0.33
0.28
0.16
-0.75
0.48

0.34
0.71
-0.25
1.11
3.22
-1.80
0.18
-1.29
-0.08
-0.69

Table 6: WikiText-2 bias scores for the words biased towards female gender for different λ values

Target Words

training

λ=0.0

λ=0.1

λ=0.5

λ=0.8

λ=1.0

abusers
acting
actions
barrister
battle
beneficiary
bills
businessman
cars
citizen
cocaine
conspiracy
controversial
cooking
cop
drug
executive
fighter
fraud
friendly
heroin
journalists
lawyer
lead
leadership
notorious
offensive
officer
outstanding
parole
pensioners
prisoners
religion
reporters
representatives
research
resignation
sacrifice
supervisor
violent

-0.66
-0.23
-0.27
-1.35
-0.27
-1.64
-0.32
-0.19
-0.43
-0.03
-0.59
-0.57
-0.21
-0.48
-1.30
-0.76
-0.04
-0.59
-0.17
-0.48
-0.57
-0.25
-0.39
-0.47
-0.25
-0.18
-0.17
-0.25
-0.25
-0.54
-0.48
-0.52
-0.41
-0.60
-0.07
-0.34
-0.95
-0.03
-0.66
-0.17

-1.17
-0.81
-0.51
-2.00
-0.53
-1.87
-0.53
-1.81
-0.55
-0.30
-1.00
-0.73
-0.39
-0.53
-1.42
-0.82
-0.34
-0.90
-0.30
-0.53
-0.67
-1.08
-0.47
-0.50
-0.74
-0.64
-0.39
-0.29
-1.55
-0.86
-0.86
-0.99
-0.97
-0.93
-0.48
-0.46
-1.67
-1.08
-0.92
-0.54

-0.56
-0.59
-0.06
-0.64
-0.10
-1.06
-0.18
-0.71
-0.32
-0.03
-0.84
-0.66
-0.39
-0.24
-0.77
-0.53
-0.22
-0.48
-0.16
-0.30
-0.28
-0.55
-0.14
-0.40
-0.28
-0.36
-0.28
-0.21
-0.98
0.00
-0.77
-0.18
-0.15
-0.26
-0.40
-0.05
-0.61
-0.38
-0.44
-0.07

-0.77
-0.35
-0.07
-0.76
-0.32
-0.22
-0.50
-0.45
-0.11
-0.22
-0.44
-0.39
-0.02
-0.22
-0.72
-0.42
-0.04
-0.36
-0.19
-0.23
-0.26
-0.76
-0.10
-0.09
-0.68
-0.22
-0.17
-0.13
-0.50
-0.08
-0.07
-0.29
-0.48
-0.05
-0.18
-0.33
-0.58
-0.17
-0.25
-0.22

-0.16
-0.54
-0.53
-0.08
-0.16
0.63
0.23
-0.53
-0.24
-0.01
-0.42
-0.83
-0.17
0.07
0.00
-0.54
-0.48
-0.89
0.10
0.36
-0.66
-0.44
-0.20
-0.07
-0.57
-0.12
-0.52
-0.17
0.03
0.07
0.64
-0.17
0.18
-0.52
-0.46
0.03
-0.40
-1.29
-0.17
-0.19

-1.93
0.60
-0.45
-0.69
0.21
0.69
-1.93
-0.27
0.04
-0.32
-0.43
-0.43
-0.52
0.26
-0.63
-0.36
-0.11
-0.63
-0.21
0.49
-0.50
-0.32
-0.99
-1.49
-0.11
-0.19
0.04
-1.30
0.40
-1.59
-1.68
-0.97
-0.83
-0.58
-1.12
0.48
-0.19

Table 7: CNN/Daily Mail bias scores for the words biased towards male gender for different λ values

Target Words

training

λ=0.0

λ=0.1

λ=0.5

λ=0.8

λ=1.0

abusive
appealing
bags
beloved
carol
chatted
children
comments
crying
designer
designers
distressed
divorced
dollar
donated
donating
embracing
encouragement
endure
expecting
feeling
festive
fragile
happy
healthy
hooked
hurting
indian
kissed
kissing
loving
luxurious
makeup
mannequin
married
models
pictures
pray
relationship
scholarship
sharing
sleeping
stealing
tears
thanksgiving
waist

0.00
0.44
0.34
0.17
0.76
0.03
0.29
0.17
0.28
0.73
0.44
0.15
0.68
0.44
0.52
1.29
1.13
0.85
0.85
1.01
0.21
0.15
0.44
0.32
0.52
0.78
0.75
0.18
0.31
0.26
0.41
0.59
1.60
0.95
0.29
0.35
0.08
0.62
0.53
0.80
0.58
0.18
0.10
0.50
0.85
1.33

0.40
1.22
1.42
0.35
1.41
1.83
0.46
0.46
0.70
0.80
2.14
0.53
0.70
1.63
0.57
1.38
1.78
0.94
0.94
1.07
0.84
0.53
0.94
0.66
0.64
1.38
1.13
0.28
1.03
1.14
0.73
0.82
1.63
1.92
0.37
1.22
0.50
1.58
0.62
1.16
0.73
0.71
0.48
0.58
2.14
1.45

0.06
0.23
0.48
0.27
0.20
0.20
0.36
0.04
0.19
0.57
1.29
0.23
0.18
0.65
0.06
0.27
0.74
0.22
0.26
0.26
0.16
0.52
0.20
0.10
0.26
0.12
0.33
0.15
0.17
0.54
0.43
0.17
0.07
0.70
0.34
0.28
0.10
0.25
0.39
0.80
0.33
0.27
0.32
0.44
1.14
0.68

0.39
0.30
0.05
0.15
0.39
0.19
0.26
0.02
0.57
0.69
0.76
0.26
0.10
0.59
0.15
0.80
0.55
0.50
0.29
0.12
0.25
0.14
0.45
0.11
0.45
0.12
0.34
0.02
0.19
0.61
0.18
0.44
0.22
0.04
0.09
0.38
0.04
0.35
0.32
0.70
0.67
0.35
0.18
0.12
1.08
0.02

0.48
-0.68
0.16
0.52
0.27
-0.14
0.41
-0.35
0.17
0.53
-0.11
-0.56
0.31
-0.24
0.68
-0.03
1.48
0.37
1.02
0.53
0.16
0.21
-0.20
0.11
0.24
-0.11
0.44
-0.02
0.28
0.44
0.15
-0.03
1.09
1.42
0.30
0.90
-0.06
-0.25
0.58
0.53
0.42
0.56
0.06
0.45
0.90
0.31

-0.65
1.16
0.64
0.36
0.48
-0.25
0.27
-0.14
0.87
-1.53
1.11
1.36
0.88
0.26
-0.21
-0.94
0.55
0.06
0.29
0.26
0.25
0.25
-0.09
0.26
-0.26
-0.22
-0.14
-0.34
-0.83
0.42
0.08
0.59
0.96
0.43
0.45
0.17
0.58
-0.53
0.35
0.96

Table 8: CNN/Daily Mail bias scores for the words biased towards female gender for different λ values

