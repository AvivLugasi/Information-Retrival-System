2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW) | 978-1-6654-4488-0/21/$31.00 ©2021 IEEE | DOI: 10.1109/EDOCW52865.2021.00036

2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)

A Survey on Explainable Artificial Intelligence
Techniques and Challenges
Ambreen Hanif

Xuyun Zhang

Steven Wood

Department of Computing
Macquarie University
Sydney, Australia
ambreen.hanif@hdr.mq.edu.au

Department of Computing
Macquarie University
Sydney, Australia
xuyun.zheng@mq.edu.au

Prospa
Sydney, Australia
steven.wood@prospa.com

in vital and sensitive areas such as healthcare, manufacturing,
security, law enforcement, banking and finance, education, and
construction.
Recent Machine learning (ML) algorithms have piqued the
interest of investors and researchers due to occasions in which
AI-based systems surpassed human specialists in a variety of
open tasks. With this aspect, a few crucial systems to note
are, for example, Computer vision (CV) based techniques
have surpassed the conventional methods in the domain. They
have conquered the human expert in related open challenges,
e.g., image-classification as ImageNet [42], object-detection
challenge, e.g. COCO [31] and AlexNet [27]. These accomplishments are not restricted to CV; ML approaches have
also made substantial progress in natural language tasks that
includes visual question answering [3] and machine translation
of variety of languages [6]. AI has also astounded the world
with its skill in strategy games. Go is an example when
AlphaGo [45], an AI player has beaten the world champion. In
similar fashion, games based on deep reinforcement learning
(RL) have topped the leader boards in several gaming domains
since then, such as Texas hold’em poker [8] and Dota 2 [2].
While the abovementioned accomplishments have been
achieved, these intelligent solutions have one thing in common: they are incredibly complex and lack transparency in
nature. We can only approach them as a black box because
we don’t know how an algorithm reached a particular decision.
Furthermore, we are unable to interpret the actions taken by
the system to obtain the outputs. The opaque, non-scrutinized
character and biased behaviour of the systems resulting from
data and human learning systems have prompted significant
worries about AI’s broad-scale adaptability. The issue of
responsible AI revolves around explainable systems. However,
the existing limitations of AI focused on law enforcement
are significant impediments to the industry’s growth. These
issues have fueled the rise of Explainable Artificial Intelligence
(XAI) [17] and/or Interpretable Machine Learning (IML) [36].
The field of XAI/IML and its constituents approaches are not
new, but the need of the hour is to deploy these techniques with
modern ML algorithms because the goal of these approaches
is to provide insights into the decision-making steps involved
in black-box systems, which can contribute to improving the
interpretability or understanding of the system’s decisions.

Abstract—In the last decade, the world has envisioned tremendous growth in technology with improved accessibility of data,
cloud-computing resources, and the evolution of machine learning
(ML) algorithms. The intelligent system has achieved significant
performance with this growth. The state-of-the-art performance
of these algorithms in various domains has increased the popularity of artificial intelligence (AI). However, alongside these
achievements, the non-transparency, inscrutability and inability
to expound and interpret the majority of the state-of-the-art
techniques are considered an ethical issue. These flaws in AI
algorithms impede the acceptance of complex ML models in a
variety of fields such as medical, banking and finance, security,
and education. These shortcomings have prompted many concerns about the security and safety of ML system users. These
systems must be transparent, according to the current regulations
and policies, in order to meet the right to explanation. Due
to a lack of trust in existing ML-based systems, explainable
artificial intelligence (XAI)-based methods are gaining popularity.
Although neither the domain nor the methods are novel, they
are gaining popularity due to their ability to unbox the black
box. The explainable AI methods are of varying strengths, and
they are capable of providing insights to the system. These
insights can be ranging from a single feature explanation to the
interpretability of sophisticated ML architecture. In this paper,
we present a survey of known techniques in the field of XAI.
Moreover, we suggest future research routes for developing AI
systems that can be responsible. We emphasize the necessity of
human knowledge-oriented systems for adopting AI in real-world
applications with trust and high fidelity.
Index Terms—Interpretable Machine Learning, Explainable
Artificial Intelligence, Survey, Machine Learning, Knowledgeintensive, Trustworthy,

I. I NTRODUCTION
We are on the verge of the fourth industrial revolution,
in which the significance of artificial intelligence (AI) is
not deniable, and apparent growth is exponential rather than
linear. According to the global survey, the artificial intelligence
market was worth 29.8 billion dollars in 2020 and is expected
to reach USD 299.64 billion by 2026 [1], at a 35.6% annual
increase. This multi-billion-dollar sector has drawn clients due
to the huge gains in the performance of AI-based systems
over the last decade in a variety of fields. With such rapid
growth and potential, it is anticipated that AI will transform
the landscape of industry by enabling multiple applications
Thanks to Data Analytics Research Lab, Macquarie University

978-1-6654-4488-0/21/$31.00 ©2021 IEEE
DOI 10.1109/EDOCW52865.2021.00036

81

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

clusions? The medical system, for example, can provide
information that the input image is cancerous because a
specific portion of the image reveals malignant findings;
this will allow doctors to rely on the system generated
forecasts. Because they understand why the choice was
made and what went wrong, they can improve the system.

In this study, we will present an overview of a wide range
of XAI/IML techniques, as well as a discussion of various
strategies for explanation and interpretations. Furthermore,
thanks to this new XAI module for intelligent systems, we
discuss how much of these interpretations are understandable
and contribute to providing insights to the systems.
This paper is divided into the sections listed below. Section
II discuss the concept of XAI. Section III review the fundamental terminology of XAI/IML. Section IV provides an
overview of the existing techniques as well as the essential
characteristics of the approaches. Section V is on XAI in law
enforcement. Section VI is the conclusion of the comprehensive study. Section VII present challenges, our findings, as well
as future work on these strategies.

III. T ERMINOLOGY OF E XPLAINABLE A RTIFICIAL
I NTELLIGENCE
The explanatory statement has a distinct meaning for the
community, and based on the application, the explanation
changes. The information generated by the explainable AI
systems are split into primarily two categories; i) Explanations;
ii) Interpretations. Although they are used interchangeably in
the literature, the definitions of these terms differ significantly.

II. E THICAL P RINCIPLES IN E XPLAINABLE A RTIFICIAL
I NTELLIGENCE

A. Explanation
The explanations are system-generated outputs that can
provide more information and are a meaningful set of assertions for human comprehension. These remarks explain
why a particular decision was made. For example, if a health
care system explains why a specific image is labelled as
malignant with a particular image marking and similar photos
from the data set. For one particular system, these instructions
may be human intelligible. A formal definition is: An” active
characteristic of a model, denoting any action or procedure
taken by a model with the intent of classifying or detailing its
internal functions” [7] [46]

To meet the requirements of Ethics in AI, explainability in
AI is primarily focused on the following terms. Responsible
AI has three pillars including, Accountability, Responsibility
and Transparency known as A.R.T. [10]. Later own other terms
were added to Ethics as Fairness, Bias.
• Accountability: It relates to the requirement to explain
to its partners, users and others with whom the system
interacts and justify its decisions and actions.
• Responsibility: It refers to the role of individuals and
to AI’s capacity to respond to decisions and recognise
mistakes or unexpected outcomes.
• Transparency: The client has the right to understand
decision explanations in terms, formats, and languages
that are intelligible to them. [49]. For example, if the
autonomous car can explain why it is taking the longer
route due to a road closure, the client would be assured
that the decision is transparent and is being made to
minimize unnecessary delays.
• Fidelity: We have high fidelity when the system’s explanations are fully or maximum incorporated into the
decision-making process. However, if explanations are
not taken into account in decision-making, we get low
fidelity recognition.
• Bias: How can the system user ensure that the AI system
does not learn a biased perspective of the system as a
result of insufficient data gathering or model construction,
or as a result of human personality bias? If the system’s
data contains information on specific people in the community, the intelligent agent will be prejudiced in making
decisions based on the data set.
• Causality: Can the model trained from current data offer
us with proper insights as well as the unboxing of the
black box?
• Fairness: It determines if we can claim that decisions
made by AI systems are fair after explaining the system?
That is, they are unaffected by the limitations of the data.
It also tells the extent of fairness for a specific system.
• Safety: Can we depend on an AI system’s choice with
confidence if we don’t know how it came to those con-

B. Interpretation
The interpretation means that the output can reveal which
features contributed to this particular output and how the
system retrieved this decision by simulating system activity.
In general, interpretation discusses the characteristics that influenced the decisions. They are not always understandable to
humans or domain specialists; nevertheless, machine learning
or data scientists can comprehend which part of the data
contributes to specific outcomes. These interpretations can
help us understand what type of data is required for the system
to make specific decisions. The passive nature refers to the
degree to which an observer’s model makes sense [46]. A
system can be interpreted by a user who cannot observe simply
how inputs are translated to outputs [11]. This system is not
merely interpretable.
When we generate interpretation or explanation of a system,
there are other associated matrices to measure the explanations
that are also available. These are defined below.
C. Trustworthiness
The ”confidence of whether a model will act as intended
when facing a given problem” [46].
D. Interactivity
The interactivity with the user is ”one of the goals targeted by an explainable machine learning model” [7]. This
is especially important in fields where ”users are of great
importance”.

82

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

2) Logistic Regression: Logistic regression is an extension
of linear regression that provides a solution to the classification
problem. Instead of a straight-line relationship, it squeezes the
output between [0,1] to anticipate the likelihood of the class
as an output. The weight indicates the interpretation of the
inclination direction. It can be both negative and positive.
3) Decision Tree Based Models: Tree-based models operate
on the premise of repeatedly separating data based on specified
cutoff values in the system feature set. This property makes
them suitable models for predicting interactive features in
data where linear and logistic regressions fail. With trees of
manageable height and width, these models will be interpretable. Tree-based models are much more difficult to comprehend for dense trees, and even minor input variations can
significantly impact the output because these models are not
smooth. Another disadvantage is that they are not as adaptable.
Because they resemble the workings of the sophisticated one,
these models are frequently referred to as reverse engineering
models. This is also referred to as the ‘surrogate model’ after
the pioneering work [9].
4) Decision Rules: Decision rule models are also seen to
be simple to understand. Simple if-else statements serve as
the system’s decision rules, and they can be turned to trees
using the terminal, which is a clear win in terms of clarity
and explanations. To forecast the result, these criteria follow a
generic framework and thus require a non-zero positive feature
value, with no maximum limit on the criteria.
5) Generalized Linear Models (GLM): The expansion to
the linear model is known as the Generalized Linear Model
(GLM). The usage of GLM benefits are twofold; one they
preserve the feature’s weighted sum and second, they allow for
distributions other than Gaussian. The latter was not possible
with the linear model. Finally, GLM relate the predicted
mean of worked-out distribution with the weighted-sum via
potentially nonlinear function.
6) Generalized Linear Rules: GAMs further relax the requirement of simple weighted-sum correlation and instead
presume that the result can be characterized by a combination
of arbitrary functions of each attribute.
7) Generalized Additive Models (GAMs): GAMs are useful
for deciphering a part of assumptions of linear models (i.e.
if the output y and a certain feature f without any feature
interaction follows a Gaussian Distribution). However, these
linear model extensions result in a more complicated (i.e.,
more interactions) and less interpretable model. Aside from
the methods mentioned above, a Naive Bayes Classifier, which
independently assesses the probability of classes for every
feature, and K-Nearest Neighbors, which uses data point
neighbors for prediction (regression or classification), are also
considered intrinsically interpretable approaches.
Although intrinsically interpretable models can be applied
to difficult problems, they cannot aid in the understanding of
machine learning techniques. As we progress to increasingly
correlated feature sets and non-linear problems, we must
investigate another realm of explainable techniques. In this
area, we will examine strategies that merely leverage existing

E. Stability
A model is stable if tiny disturbances such as sounds
attributable to the data source itself cannot mislead it.
F. Robustness
A model is regarded to be robust if it can tolerate perturbations that human beings have deliberately produced.
G. Reproducibility
A model can be reproduced if the same results are obtained
repeatedly if it run on the same data set numerous times.
H. Confidence
Confidence is the likelihood of an event happening. The
objective is to quantify the confidence in the decision [23].
The risk measure is defined as the assurance by users that
the correct ideas have been received through the AI-based
model [48]. A very confident model should be repeatable on
its predictions, as it should have comparable forecasts when
run on the same dataset on repeated occasions.
IV. E XPLAINABLE A RTIFICIAL I NTELLIGENCE M ETHODS
We will provide, in this section, a summary of the selected
XAI techniques and a list of their strengths, drawbacks,
and obstacles. This research is mainly based on the findings of three extensive studies and surveys [36] [22] and
[12]. For classification, we are mainly using the survey by
Xiao Li. et al. [30].In their study, they identified two major
categories for XAI-based approaches: data-driven approaches
and knowledge-driven approaches. Various post-hoc strategies
are available for explaining black-box models; one element
in common is conveying the input variables to humans to
interpret the systems. Based on the findings of several investigations, explainability approaches are classified into several
categories. These studies [12], [18], [36], [22] referred to
three types of explainable systems: intrinsically interpretable
techniques, model agnostic techniques, and example-based
methods. We will use the categories listed above and incorporate them into the respective data-driven and knowledgedriven strategies. [36] [4] and [18] also discuss neural network interpretability and Deep neural network explainability
respectively.
A. Intrinsically Interpretable Methods
The first and most important step towards XAI is to employ
models that are straightforward and self-explanatory. In this
section, we will look at a few models that are intrinsically
interpretable.
1) Linear Regression: These models are thought to be
generally interpretable, particularly when their regression coefficients have a clear meaning. The predictions of this technique
are a weighted sum of the feature set, as indicated by [36] in
chapter 4.1. The weighted sum gives the system transparency.
The model benefits from the linear nature of the relationship
between variables. This characteristic aided these systems
in standing out and will be widely accepted by numerous
domains including, but not limited to, medical, sociology,
sciences, finance, and so on.

83

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

takes into account both the main feature effect and interaction is detrimental because feature interaction is included in
importance of associated features. In nutshell, the presence
of feature interaction, significance of feature does not cause a
performance fall. Furthermore, the training and test set is used
for feature importance because it reflects iterative variation
in the shuffled data set. It is also noteworthy that feature
importance is included in the global methods.

dataset information to provide system interpretation. We’ll
divide them into three sections for the most part. Visualization
based approaches, Model Agnostic methods and Examplebased approaches respectively.
B. Model Agnostic, Visualization based approaches
1) Partial Dependence Plot (PDP): PDPs (Partial Dependence Plots) are a well-known example of model agnostic
approaches. Model agnostic approaches are interpretationbased universal approaches that can create interpretation for
any machine learning model. These plots are thought to be
useful for representing the impact of one or two variables on
the predicted outcome. These are really valuable tools for visualizing the non-linearity and other intricacies underlying the
ML model. They are global methodologies that demonstrate
the significance of features from the input data. However, they
are ’partial,’ as they can only show one or two features at a
time. They also do not take into account feature interaction,
which has an impact on projected outcomes.
2) Individual Conditional Expectation (ICE) Plot: The ICE
plots give another visualization-based way for displaying each
instance at one line. It demonstrates the effect of feature
changes on instance’s prediction. For each feature, one line
per instance means we need to plot n plots for n features,
and each plot will contain z lines to represent z instances.
This has more information than a single line overall, as in
partial dependence charts. With each line per instance, ICE
curves are more intuitive to understand than PDP curves.
The shortcoming of these curves is that we cannot detect the
correlations’ feature reliance on each instance.
3) Accumulated Local Effects (ALE) Plot: The ALE gives,
on average, the features’ influence on a machine learning
model’s prediction. These are more unbiased and faster to
compute than PDPs, and their interpretation is more meaningful, but with larger intervals, these graphs might become
wobbly. ALE charts, unlike PDPs, are not accompanied by
ICE curves. These plots are more suitable for working with
associated features.

D. Model Agnostic, Global Surrogate
This model is an interpretable approximation model which
is trained with the same data set to forecast similarly to a
black-box model. We can understand the behavior of the blackbox model by reading the surrogate model. We solve the blackbox model by creating a new one. These models are also being
chastised for providing a comprehensive grasp of the system,
which cannot be classed as interpretation.
E. Model Agnostic, Local Surrogate
This class of models are interpretable models employed
in specific predictions made by black-box machine learning
models.
1) LIME: A concrete implementation of local surrogate
models, namely Local interpretable model-agnostic explanations (LIME) is given in the reference [40]. Rather than
developing a global surrogate model, the LIME aims to train
local surrogate models to interpret individual predictions.
Another possibility is to create one or more local surrogate
models. On selected subsets of the data, local surrogate models
approximate the complicated model’s predictions. In the case
of a mortgage, several explainable models would be built for
different sorts of mortgage applicants [40].
2) Shapley Values: This is a game-based method to system
prediction. A prediction can be elaborated by supposing that
the value of each feature instance represents a ”player” in a
game and the prediction is its ”reward”. The Shapley values,
a mechanism derived from coalitions game theory, provide us
a scheme to distribute the ”payout” among the qualities fairly.
The Shapley value allows for opposing arguments and takes a
long time to compute, yet it is susceptible to misinterpretation.
3) SHAP: Some representative methods share some abovementioned qualities. Another newly proposed approach, SHapley Additive exPlanations (SHAP), proposed by Lundberg et
al. [32], has taken advantage of a mixture of Shapley values
from game theory [19], the approach attempts to fairly assign
the contributions. Rodriguez et al. employed this in their work.
[41] for explanation generation for pharmaceutical industry
data set, by Hong et al. [21] used with DNN to predict health
analytic tool life.

C. Model Agnostic,Feature Interaction Based
1) Feature Interaction: In computation, features interact
with one another. The key element of feature interaction is
that the effects of individual features do not always add up
to the combined effects. According to [22], the interaction
between two features is computed by calculating the difference
between the partial dependence function for those two features
together and the sum of the partial dependence functions for
each feature separately. This method has the disadvantage of
being computationally expensive.
2) Feature Importance: The feature relevance increases
when the values of the feature are permuted in order to
disrupt the genuine relationship between the feature and the
true outcome. If mistakes grow after rearranging the value
of feature, it emphasizes the relevance of the feature. The
relevance of a feature gives a concise and global view of
the ML model’s behavior. The fact that feature importance

F. Propagation Based Methods
The methods relying on local approximation offer another
model to comprehend complex models locally, however the
behavior in the local region may require a wide variety of
features to generate comprehension, which complicates the
process and leads to an uninterpretable system. The relevant

84

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

the data set and/or the model for explanation generation. This
is a major cause of a limitation on the explanation generation
system. As to develop an explanation system understandable
by the layman we need to add knowledge to the system to
generate more descriptive explanations.

approach is based on back propagation and forwards propagation, and it quantifies the system that uses the features through
output difference after perturbing the feature.
1) Back-Propagation Based Methods:: The approach is
based on to take derivative of the outcome using input features.
One well-known effort is DEEPLift [44], which uses a backpropagation-based strategy to create interpretation of crucial
model aspects.
2) Forward Propagation Based Methods: These methods is
used to leverage perturbations on the input to find the relevant
feature for output [13].

H. Knowledge-Based Techniques
So far, the methodologies we’ve looked at have only focused
on using data set metadata to provide an explanation. The next
simple step towards high fidelity explanation is to incorporate
information (e.g., expert domain knowledge and/or corpus
knowledge to explain the system). The information can help
to extract the structure from the ML system in order to deliver
the explanations. The explanation section can be expanded to
create more useful information based on domain knowledge
or expert knowledge. This category is being used to briefly
investigate the knowledge-based methodologies in order to
utilize this information.How they might bring value to the
development of explanations? We can find studies in the
knowledge based studies, they are contributing the expert
domain knowledge or corpora to the system. One of such study
is by Kim et al. [25] about the NN human-understandable
interpretation. The proposed approach is TCAV, a post-doc,
model agnostic approach. This approach is providing the
human understandable explanations. This is a major study
towards knowledge based systems. K-IL is another proposed
approach in this study [28]. This approach use knowledge
graphs to generate the understanding of the system. This
study claims that knowledge graphs are way forward to the
explanation generations. With the discussion of knowledge
based approaches, we wrap up this section, and we have
summarized the selected approaches in the tableI. From this
table, we can see that most of the generated interpretations
are not human understandable. We need domain expert or
AI layman to understand the explanations generated. These
methods can help the experts to understand how the system
achieved certain output from the given feature set but these
are not enough to understand that why a certain decision
was derived from the data set. To make it more human
understandable we need to provide the interactive system
frameworks for interactions [34].

G. Instance-Based Explanations
The approaches indicated above interpret the systems, however, they can be computationally highly expensive for large
setups, and the explanation generation time is insufficient. In
such cases, it may be preferable to generate explanations for
specific examples of the system rather than the overall one.
This method does not necessarily explain the model, but rather
highlights its key elements. The approach generates answers
to questions like, ”What were the driving variables in the case
of individual A?”
1) Break Down: This category of models generates local
explanations and is associated with the partial dependence
algorithm using a step-by-step approach known as ”Break
Down.” This is a greedy iterative technique for identifying
influence-based characteristics on overall response. As stated
in the game theory approach, it starts with a null team and
based on contributor’s contributions gradually populates feature values, one by one. The amount of feature’s contribution,
in each iteration, from each feature, depends on the feature
values of those already in the team, which is a disadvantage
of feature in this strategy. Because of the greedy approach,
this method is faster than the Shapley value method.
2) Counterfactual: According to Islam et al. [22], a counterfactual explanation characterizes a causal scenario as ”If X
had not occurred, Y would not have occurred.” For example, ”I
would not have known the cause for the fame if I hadn’t gone
to this coffee shop.” The interpretations of this method are
quite straightforward, and these systems do not require access
to data or models to anticipate. This method is also simple
to build, but the constraint is that we can have numerous
counterfactual representations for each instance of the system.
3) Adversarial: Most approaches suggest reducing the gap
between the antagonistic example and the instance to be controlled while adjusting the prognosis to the desired outcome
of the system. This method allows for diagnosis.
4) Feature Selection: Other proposed strategies include
instance-based feature selection, which selects features depending on each instance. In this framework, a feature selector
always attempts to maximize the quid pro quo information of
the selected features and the response variables. This approach,
however, is largely limited to ad-hoc strategies.
The above-mentioned techniques are all data drive techniques as all the results and explanations are generated from
the available data. These techniques are solely dependent on

V. XAI M ETHODS E VALUATIONS
Several explanations methods are available but there are
still some questions to answer to have a proper framework
of explanation generation. One of the important questions
is how can we understand that explanation is good or not?
If it is good how can we be trust the explanations. these
question need rise to the explanation evaluation. Either the
methods is data driven or knowledge aware we need to justify
the explanations. Specialists developed various interpretability
assessment taxonomies. In terms of rationale and complexity,
Doshi-Velez and Kim [12] split explanatory assessments into
the grounds of application, human and function. Hoffman
et al [20] talk about the common measurement methods in
four aspects: goodness, satisfaction, understanding (i.e. mental

85

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

TABLE I
T HIS TABLE SUMMARIZES THE SELECTED XAI BASED APPROACHES AND PROVIDE A SUMMARY OF THE MAJOR ATTRIBUTES OF THE SYSTEM . C OLUMN I
( NAMES OF THE APPROACHES AND CITATION ), C OLUMN II STATES THE APPROACH IS A NTE - HOC OR P OST- HOC , C OLUMN III SYSTEM USED WHICH
METHODOLOGY TO GENERATE THE EXPLANATIONS , C OLUMN IV INDICATES THE TARGET AUDIENCE FOR THE GENERATED EXPLANATIONS , C OLUMN V
PROVIDES THE INFORMATION ABOUT EXPLANATION TYPE BASED ON THE METHODS , C OLUMN VI H IGHLIGHTS IS THERE ANY EXPLANATIONS
EVALUATIONS PROVIDED ?, C OLUMN VII B LACK B OX M ODEL EXPLANATIONS . AGN MEANS M ODEL AGNOSTIC , C OLUMN VIII WHICH MODELS ARE
USED TO GENERATE THE EXPLANATIONS , C OLUMN IX INDICATE THE APPROACH IS DATA D RIVEN OR K NOWLEDGE D RIVEN , C OLUMN X HIGHLIGHTS
THE BEHAVIOUR OF THE EXPLANATION SYSTEM .
Selected XAI
Approaches

Ante-hoc
/ Post-hoc

TREPAN [9]

Post-hoc

Deep Red [52]

Post-hoc

Distilling NN [14]

Post-hoc

PFI [38]

Post-hoc

DGN-AM [38]

Post-hoc

Interpretable
[50]
CNN

Ante-hoc

LIME [40]

Post-hoc

Surrogate Model

LORE [16]

Post-hoc

Decision Tree

Post-hoc

Visualization

Post-hoc

Visualization

Vanilla
Gradient

[5]

Grad-CAM [43]
DeepLift [44]

Post-hoc

SHAP [32]

Post-hoc

MMD
- critic

[24]

[47]

Post-hoc

PDL [15]

Ante-hoc

Feature
Attribution
Feature
Attribution
Prototype
& Criticism
Data
Instance

Target
Audiance
Domain
Experts
Domain
Experts
Domain
Experts
Domain
Experts
Domain
Experts
Domain
Expert
Domain
Experts
AI
Layman
AI
Layman
AI
Layman
Domain
Experts
AI
Layman
Domain
Experts
Domain
Experts

Prototype

Developer

Ontology
based
Concept
TCAV [26]
Post-hoc
Importance
Knowledge
K-IL [28]
Post-hoc
Graph
Visualization
& Concept
IBD [51]
Post-hoc
Importance
Decision
RuleRec [33]
Post-hoc
Rule
Knowledge
[29]
Post-hoc
Graph
AGN: Agnostic, CNN: Convolution Neural Networks,

Doctor XAI [39]

a

—

Explanations
Methodology
Surrogate Model
(Decision Tree)
Surrogate Model
(Decision Tree)
Surrogate Model
(Decision Tree)
Feature
Attribution
Surrogate Model
(Decision Tree)
Visualization
Mask

Post-hoc

Explanation
Type

Scope

Explanation
Evaluation

Black
Box

Data Driven/
Knowledge

Approx.

Global

No

NN

Data-Driven

Approx.

Global

No

NN

Data Driven

Approx.

Global

No

DNN

Data Driven

Approx.

Global

No

AGN

Data Driven

Approx.

Global

No

DNN

Data Driven

Approx.

Global

No

CNN

Data Driven

Approx.

Local

No

AGN

Data Driven

Approx.

Local

No

AGN

Data Driven

Approx.

Local

No

CNN

Data Driven

Approx.

Local

No

CNN

Data Driven

Approx.

Local

No

AGN

Data- Driven

Approx.

Local

Quantitative

AGN

Data Driven

Specific

Quantitative

AGN

Data Driven

Specific

Quantitative

CNN

Data Driven

Global

Qualitative

DNN

Data Driven

Approx.

Global

No

AGN

Data Driven

Approx.

Global

Quantitative

DNN

Approx.

local

No

CNN

AI
Experts

Approx.

Global

No

CNN

AI
Layman

Approx.

Domain
Experts
Domain
Experts
Domain
Experts

Instance
based
Instance
based
Case
based

Knowledge
aware
Knowledge
aware
Knowledge
aware

Knowledge
based
Knowledge
Experts
Approx.
Global
No
AGN
aware
NN: Neural networks, DNN:Deep Neural Networks, Approx. Approximate
Local

No

—

Behaviour/
Performance
Tree
Extraction
Rule
Extraction
Extract
Model
Feature
Importance
Extract
Model
Model
Design
Local
Approx.
Rule
Based.
Propogation
based
Propogation
Based.
Feature
Importance
Feature
Importance
Instance
based
Counterfactuals
Prototype
Based
Rule
based
Human
in loop
External
Knowledge
Corpus
Internal
Knowledge
External
knowledge

A. Computational Metrics

model, interest and confidence on explanations from XAI) and
measurement of efficiency. Recently the literature on assessment techniques has been concluded by Mohseni et al. [35] via
targeted audience (e.g. AI laymen, data specialists and experts
in machines) and metrics for the verification of ”efficiency” of
explications. Based on the existing literature model evaluations
can be categorized into two methods computations metrics
and cognitive metrics. The metrics can help us to evaluate
the model .

These metrics are referred to as the numerical indicator
of the quality of explanations generated for XAI system
[35].The measurements are generally carried out using suitably
developed equations using the data in hand. Computational
metrics may therefore be used without human participation as
guidelines for building interpretation techniques [37].
B. Cognitive Metrics
The explanations for human beings are measured by cognitive metrics. Since the primary aim of XAI is to make the

86

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

are of the same importance. So for policing, the explanation
pipeline begins with the data curation process.

logic and the process behind the machine understandable to
humans, the assessment of human subjects is a blunt indication
of explanations. Although compute metrics should be taken
carefully in particular tasks or applications, the assessment of
subjects might generalise into many circumstances by explicitly requesting that participants identify the best explanation
from their perspectives.
With the help of computations and cognitive metrics we can
evaluate and understand is the model fulfilling the ethics and
definitions of AI mentioned in section II and III of the paper.

VII. C ONCLUSION
According to the literature survey presented, explainability
in pre-modelling is a realistic, if under-focused, alternative for
avoiding transparency concerns. Furthermore, we can see that
the system-generated explanation requires further information.
These engendered explanations are the approximations of the
systems generated by other models, which may need to qualify
some crucial stages to justify the transparency of the proposed
approaches. Another finding from this survey is that the
knowledge infused approaches have more room to explore and
enhance the system capability to generate the explanations. But
this also raises many questions in terms of adding a domain
or expert knowledge to the system. How will we address the
data challenges to the knowledge involvement, e.g. Which
particular domain knowledge can be considered safe to add to
the system and ensure the fairness of the system even with the
added knowledge to the system? Therefore, the ultimate focus
is on the explainability of “black box” models using domain
knowledge. Simultaneously, we must concentrate on assessing
or quantifying explainability, utilizing both human and nonhuman studies. Our understanding from this study is that the
current explanations are simply approximating the models for
certain occurrences in order to provide explanations.

VI. XAI IN P OLICING AND LAW ENFORCEMENT
The increasing use of technology in cybercrime, e.g. violating material and image fake, has posed a significant threat to
law enforcement agencies and police. These professional and
personal challenges need to be addressed as these are vital for
the community safety and security and for law enforcement
agencies to fulfil their responsibilities toward society. To combat these challenges, the agencies also need to develop intelligent systems to solve the technology riddles by criminals. Artificial intelligence (AI) technology provide clear advantages for
law enforcement in combating these threats while significantly
minimising the risk of harm to investigators. However, such
technologies must be developed and implemented by proper
legal, ethical, and transparent frameworks, especially given the
importance of maintaining community trust ineffective police.
AI-related technologies have been explored and implemented
by police forces all around the world to varying degrees. Face
recognition, optimised resource allocation, crime prediction,
traffic law enforcement, and text/social media analysis are
only a few examples of applications. By their very nature,
law enforcement agencies are highly operational and nimble,
needing to respond quickly to changing circumstances in order
to combat threats to community safety. The distribution of
resources is critical. It’s vital that performance indicators
for collaborative research with universities are written so
that research findings may be used as quickly and simply
as feasible in reality. Modern policing, for example, relies
mainly on ICT infrastructure adapted to operational needs,
such as forensics and intelligence capabilities. AI systems
produced through collaborative research should be built to
integrate with current strategies and workflows in terms of
technology. AI techniques are up-and-coming by lowering the
labour of investigators, expediting work, and enhancing the
welfare of the people responsible for viewing and labelling
such data. AI approaches can be used to improve several
forms of law enforcement in addition to countering crimes.
In a way, they are evolving data analysis skills already owned
by agencies and are adapted to the ever-greater amounts of
data that characterise many contemporary crime inquiries. In
addition to research and development hurdles in enhancing the
technology itself, however, its application in law enforcement
faces various challenges. When it comes to policing and law
enforcement with ethics and trust, it’s not a mere explanation
of the results generated from the machine learning algorithms.
At the same time, the data collection and labelling pipeline

VIII. C HALLENGES AND F UTURE R ESEARCH D IRECTIONS
In this paper, we presented a summary of the evolving and
progressing ML models and highlighted preliminary works
in XAI. However, there is still a long road to humanunderstandable explanations despite all this growth and development in the XAI domain. There are few challenges and
open questions on the applicability and replication of these
models. In our analysis, one of the significant challenges is
adding more layers. We are adding one or more layers to
understand or explain the systems. Are these layers a value
addition to generate the explanation, or are they another blackbox model to explain the existing one? Can these approaches
provide the needed transparency to the system? In the long run,
these explanations not only provide understanding. We need
to develop a standard framework for deploying the XAI based
systems. This standard framework should provide flexibility
and adaptability as a common attribute for the broad range of
applications in vast domains.
R EFERENCES
[1] Global Artificial Intelligence Market Size 2021 Rise at.
[2] OpenAI Five.
[3] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell,
C Lawrence Zitnick, Dhruv Batra, and Devi Parikh. VQA: Visual
Question Answering www.visualqa.org. In Proceedings of the IEEE
international conference on computer vision, pages 2425–2433, 2015.
[4] Robert Andrews, Joachim Diederich, and Alan B. Tickle. Survey and
critique of techniques for extracting rules from trained artificial neural
networks. Knowledge-Based Systems, 8(6):373–389, 12 1995.
[5] David Baehrens, Stefan Harmeling, Motoaki Kawanabe, Katja
Hansen KHANSEN, and Carl Edward Rasmussen. How to Explain
Individual Classification Decisions Timon Schroeter * Klaus-Robert M
¨ uller. Journal of Machine Learning Research, 11:1803–1831, 2010.

87

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

[29] Maryam Labaf, Pascal Hitzler, and Anthony B Evans. Propositional
Rule Extraction from Neural Networks under Background Knowledge.
In NeSy, 2017.
[30] Xiao-Hui Li, Caleb Chen Cao, Yuhan Shi, Wei Bai, Han Gao, Luyu Qiu,
Cong Wang, Yuanyuan Gao, Shenjia Zhang, Xun Xue, and Lei Chen.
A Survey of Data-driven and Knowledge-aware eXplainable AI. IEEE
Transactions on Knowledge and Data Engineering, pages 1–1, 3 2020.
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross
Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common Objects in Context.
Lecture Notes in Computer Science (including subseries Lecture Notes
in Artificial Intelligence and Lecture Notes in Bioinformatics), 8693
LNCS(PART 5):740–755, 5 2014.
[32] Scott M Lundberg, Paul G Allen, and Su-In Lee. A Unified Approach
to Interpreting Model Predictions. In Advances in Neural Information
Processing Systems, volume 30, Long Beach, CA, 2017.
[33] Weizhi Ma, Woojeong Jin, Min Zhang, Chenyang Wang, Yue Cao, Yiqun
Liu, Shaoping Ma, and Xiang Ren. Jointly learning explainable rules
for recommendation with knowledge graph. The Web Conference 2019
- Proceedings of the World Wide Web Conference, WWW 2019, pages
1210–1221, 5 2019.
[34] Tim Miller. Explanation in artificial intelligence: Insights from the social
sciences. Artificial Intelligence, 267:1–38, 2 2019.
[35] MohseniSina, ZareiNiloofar, and RaganEric D. A Multidisciplinary
Survey and Framework for Design and Evaluation of Explainable AI
Systems. ACM Transactions on Interactive Intelligent Systems (TiiS), 8
2021.
[36] Christopher Molnar. Interpretable Machine Learning. Lulu.com, 2021.
[37] Shane T. Mueller, Robert R. Hoffman, William Clancey, Abigail Emrey,
and Gary Klein. Explanation in Human-AI Systems: A Literature MetaReview, Synopsis of Key Ideas and Publications, and Bibliography for
Explainable AI. arXiv preprint arXiv:1902.01876, 2 2019.
[38] Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and
Jeff Clune. Synthesizing the preferred inputs for neurons in neural
networks via deep generator networks. Advances in neural information
processing systems, 29:3387–3395, 2016.
[39] Cecilia Panigutti, Alan Perotti, and Dino Pedreschi. Doctor xai: an
ontology-based approach to black-box sequential data classification
explanations. In Proceedings of the 2020 conference on fairness,
accountability, and transparency, pages 629–639, 2020.
[40] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”Why should
i trust you?” Explaining the predictions of any classifier. In Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, volume 13-17-August-2016, pages 1135–1144.
Association for Computing Machinery, 8 2016.
[41] Raquel Rodrı́guez-Pérez and Jürgen Bajorath. Interpretation of machine
learning models using shapley values: application to compound potency
and multi-target activity predictions. Journal of Computer-Aided Molecular Design 2020 34:10, 34(10):1013–1026, 5 2020.
[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer
Vision 2015 115:3, 115(3):211–252, 4 2015.
[43] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-CAM: Visual
Explanations From Deep Networks via Gradient-Based Localization,
2017.
[44] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning
important features through propagating activation differences. In International Conference on Machine Learning, pages 3145–3153. PMLR,
2017.
[45] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis
Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy
Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and
Demis Hassabis. Mastering the game of Go with deep neural networks
and tree search. Nature 2016 529:7587, 529(7587):484–489, 1 2016.
[46] Erico Tjoa and Cuntai Guan. A Survey on Explainable Artificial
Intelligence (XAI): Towards Medical XAI. IEEE Transactions on Neural
Networks and Learning Systems, 14(8):1–21, 7 2019.
[47] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual
Explanations without Opening the Black Box: Automated Decisions and

[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural
Machine Translation by Jointly Learning to Align and Translate. 3rd
International Conference on Learning Representations, ICLR 2015 Conference Track Proceedings, 9 2014.
[7] Alejandro Barredo Arrieta, Natalia Dı́az-Rodrı́guez, Javier Del Ser,
Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia,
Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and
Francisco Herrera. Explainable Artificial Intelligence (XAI): Concepts,
Taxonomies, Opportunities and Challenges toward Responsible AI.
Information Fusion, 58:82–115, 2020.
[8] Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer
poker. Science, 365(6456):885–890, 8 2019.
[9] Mark W Craven and Jude W Shavlik. Extracting Thee-Structured
Representations of Thained Networks. Neural Information Processing
Systems, 8:24–30, 1995.
[10] Virginia Dignum. Responsible Artificial Intelligence: Designing Ai for
Human Values. Daffodil International University, 9 2017.
[11] Derek Doran, Sarah Schulz, and Tarek R. Besold. What Does Explainable AI Really Mean? A New Conceptualization of Perspectives. CEUR
Workshop Proceedings, 2071, 10 2017.
[12] Finale Doshi-Velez and Been Kim. Towards A Rigorous Science of
Interpretable Machine Learning. arXiv preprint arXiv:1702.08608, 2
2017.
[13] Ruth C. Fong and Andrea Vedaldi. Interpretable Explanations of
Black Boxes by Meaningful Perturbation. In Proceedings of the IEEE
international conference on computer vision, pages 3429–3437, 2017.
[14] Nicholas Frosst and Geoffrey Hinton. Distilling a Neural Network Into
a Soft Decision Tree. arXiv preprint arXiv:1711.09784, 2017.
[15] Alan H. Gee, Diego Garcia-Olano, Joydeep Ghosh, and David Paydarfar. Explaining Deep Classification of Time-Series Data with Learned
Prototypes. CEUR workshop proceedings, 2429:15, 8 2019.
[16] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi,
Franco Turini, and Fosca Giannotti. Local Rule-Based Explanations of
Black Box Decision Systems. arXiv preprint arXiv:1805.10820, 5 2018.
[17] D Gunning, M Stefik, J Choi, T Miller, S Stumpf, and G-Z Orcid ; Yang.
XAI-Explainable artificial intelligence. Science Robotics, 4(37):7120,
2019.
[18] Patrick Hall and Navdeep Gill. Explain Your AI H2O Driverless AI,
volume II. O’Reilly Media, second edition, 2019.
[19] Sergiu Hart. Shapley Value. Game Theory, pages 210–216, 1989.
[20] Robert R. Hoffman, Shane T. Mueller, Gary Klein, and Jordan Litman.
Metrics for Explainable AI: Challenges and Prospects. arXiv preprint
arXiv:1812.04608, 12 2018.
[21] Chang Woo Hong, Changmin Lee, Kwangsuk Lee, Min-Seung Ko,
Dae Eun Kim, and Kyeon Hur. Remaining useful life prognosis for
turbofan engine using explainable deep neural networks with dimensionality reduction. Sensors, 20(22):6626, 2020.
[22] Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor, and
Mohiuddin Ahmed. Explainable Artificial Intelligence Approaches: A
Survey. arXiv preprint arXiv:2101.09429, 1 2021.
[23] Bhavya Kailkhura, Brian Gallagher, Sookyung Kim, Anna Hiszpanski,
and T. Yong-Jin Han. Reliable and explainable machine-learning
methods for accelerated material discovery. npj Computational Materials
2019 5:1, 5(1):1–9, 11 2019.
[24] Been Kim, Rajiv Khanna, and Oluwasanmi Koyejo. Examples are not
Enough, Learn to Criticize! Criticism for Interpretability. Advances in
neural information processing systems, 29, 2016.
[25] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler,
Fernanda Viegas, and Rory Sayres. Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors
(TCAV). 35th International Conference on Machine Learning, ICML
2018, 6:4186–4195, 11 2017.
[26] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler,
Fernanda Viegas, and Rory sayres. Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV).
In International conference on machine learning, pages 2668–2677.
PMLR, 7 2018.
[27] KrizhevskyAlex, SutskeverIlya, and HintonGeoffrey E. ImageNet classification with deep convolutional neural networks. Communications of
the ACM, 60(6):84–90, 5 2017.
[28] Ugur Kursuncu, Manas Gaur, and Amit Sheth. Knowledge Infused
Learning (K-IL): Towards Deep Incorporation of Knowledge in Deep
Learning. CEUR Workshop Proceedings, 2600, 12 2019.

88

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

[48]

[49]

[50]

[51]

[52]

the GDPR. Harvard Journal of Law & Technology (Harvard JOLT),
31:841, 2017.
Jonas Wanner, Lukas-Valentin Herm, and Kai Heinrich. White, Grey,
Black: Effects of XAI Augmentation on the Confidence in AI-based Decision Support Systems Managing the Long Tail of Business Processes
View project Design of Deep Learning-based Information Systems View
project. In ICIS, 2020.
Adrian Weller. Transparency: Motivations and Challenges. Lecture Notes
in Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics), 11700 LNCS:23–40,
7 2017.
Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable
Convolutional Neural Networks. Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, pages
8827–8836, 10 2017.
Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable
Basis Decomposition for Visual Explanation. In Proceedings of the
European Conference on Computer Vision (ECCV), pages 119–134,
2018.
Jan Ruben Zilke, Eneldo Loza Mencı́a, and Frederik Janssen. DeepRED
– Rule extraction from deep neural networks. Lecture Notes in Computer
Science (including subseries Lecture Notes in Artificial Intelligence and
Lecture Notes in Bioinformatics), 9956 LNAI:457–473, 2016.

89

Authorized licensed use limited to: University Haifa. Downloaded on December 19,2022 at 19:58:47 UTC from IEEE Xplore. Restrictions apply.

