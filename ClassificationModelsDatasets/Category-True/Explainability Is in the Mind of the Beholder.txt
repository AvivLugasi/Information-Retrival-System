Explainability Is in the Mind of the Beholder:
Establishing the Foundations of Explainable Artificial Intelligence
KACPER SOKOL∗† , Intelligent Systems Laboratory, University of Bristol, United Kingdom and ARC Centre of
Excellence for Automated Decision-Making and Society, RMIT University, Australia

PETER FLACH, Intelligent Systems Laboratory, University of Bristol, United Kingdom
Explainable artificial intelligence and interpretable machine learning are research domains growing in importance. Yet, the underlying

arXiv:2112.14466v2 [cs.AI] 9 Sep 2022

concepts remain somewhat elusive and lack generally agreed definitions. While recent inspiration from social sciences has refocused
the work on needs and expectations of human recipients, the field still misses a concrete conceptualisation. We take steps towards
addressing this challenge by reviewing the philosophical and social foundations of human explainability, which we then translate
into the technological realm. In particular, we scrutinise the notion of algorithmic black boxes and the spectrum of understanding
determined by explanatory processes and explainees’ background knowledge. This approach allows us to define explainability as
(logical) reasoning applied to transparent insights (into, possibly black-box, predictive systems) interpreted under background knowledge
and placed within a specific context – a process that engenders understanding in a selected group of explainees. We then employ this
conceptualisation to revisit strategies for evaluating explainability as well as the much disputed trade-off between transparency and
predictive power, including its implications for ante-hoc and post-hoc techniques along with fairness and accountability established
by explainability. We furthermore discuss components of the machine learning workflow that may be in need of interpretability,
building on a range of ideas from human-centred explainability, with a particular focus on explainees, contrastive statements and
explanatory processes. Our discussion reconciles and complements current research to help better navigate open questions – rather
than attempting to address any individual issue – thus laying a solid foundation for a grounded discussion and future progress of
explainable artificial intelligence and interpretable machine learning. We conclude with a summary of our findings, revisiting the
human-centred explanatory process needed to achieve the desired level of algorithmic transparency and understanding in explainees.
Additional Key Words and Phrases: Defining Explainability; Interpreting and Understanding Explanations; Human-Centred Perspective;
Machine Learning Interpretability; Artificial Intelligence Explainability.

Highlights
Explainability is an interactive process that engenders understanding, not knowledge.
Understanding is derived via reasoning applied to transparent insights into models.
Transparency of insights and understanding are gauged on a continuous spectrum.
Insights are interpreted under background knowledge mediated via operational context.
Context & Significance Automated decision-making systems based on artificial intelligence and machine learning algorithms are becoming ubiquitous across many domains of everyday life. Increasingly often, they are in a
position to impact human affairs but their operation remains mostly unchecked, raising fairness, accountability
and transparency concerns. Focusing on explainability, the past decade has offered rapid advances to the benefit of

∗ Corresponding
† Lead

contact.

author.

Authors’ addresses: Kacper Sokol, K.Sokol@bristol.ac.uk, Kacper.Sokol@rmit.edu.au, Intelligent Systems Laboratory, University of Bristol, United
Kingdom and ARC Centre of Excellence for Automated Decision-Making and Society, RMIT University, Australia; Peter Flach, Peter.Flach@bristol.ac.uk,
Intelligent Systems Laboratory, University of Bristol, United Kingdom.

1

2

Kacper Sokol and Peter Flach
society, yet this research space still lacks a universal foundation. This work synthesises interdisciplinary findings to
date and offers a clear path forwards.
Bigger Picture

This work organises contributions to explainable artificial intelligence and interpretable machine

learning from across sociology, philosophy and computer science. Such a comprehensive and interdisciplinary
view enables us to identify common themes and establish a unified, scientific foundation for future research by
defining explainability of predictive systems through its ingredients and roles, with a strong focus on humans and
operational contexts. Our meta-analysis provides a high-level introduction to key concepts and a methodological
framework to navigate them, thus complementing topic-oriented surveys. The contributions offer a focal point to
guide development and evaluation of such techniques centred on human understanding and not just transfer of
knowledge. This perspective should help to advance the field beyond an academic exercise and enable creation of
truly explainable systems, accelerating proliferation of comprehensible models in scientific and social applications.

1

GREAT EXPECTATIONS – EXPLAINABLE MACHINES

Transparency, interpretability and explainability promote understanding and confidence. As a society, we strive for
transparent governance and justified actions that can be scrutinised and contested. Such a strong foundation provides a
principled mechanism for reasoning about fairness and accountability, which we have come to expect in many areas.
Artificial Intelligence (AI) systems, however, are not always held to the same standards. This becomes problematic
when data-driven algorithms power applications that either implicitly or explicitly affect people’s lives, for example
in banking, justice, job screenings or school admissions [97]. In such cases, creating explainable predictive models or
retrofitting transparency into pre-existing systems is usually expected by the affected individuals, or simply enforced
by law. A number of techniques and algorithms are being proposed to this end; however, as a relatively young research
area, there is no consensus within the AI discipline on a suite of technology to address these challenges.
Building intelligible AI systems is oftentimes problematic, especially given their varied, and sometimes ambiguous,
audience [58, 106], purpose [46] and application domain [13]. While intelligent systems are frequently deemed (unconditionally) opaque, it is not a definitive property and it largely depends on all of the aforementioned factors, some
of which fall beyond the consideration of a standard AI development lifecycle. Without clearly defined explainability
desiderata [123] addressing such diverse aspects can be challenging, in contrast to designing AI systems purely based
on their predictive performance, which is often treated as a quality proxy that can be universally measured, reported
and compared. In view of this disparity, many engineers (incorrectly) consider these two objectives as inherently
incompatible [112], thus choosing to pursue high predictive performance at the expense of opaqueness, which may be
incentivised by business opportunities.
While high predictive power of an AI system might make it useful, its explainability determines its acceptability. The
pervasiveness of automated decision-making in our everyday life, some of it bearing social consequences, requires
striking a balance between the two that is appropriate for what is at stake; for example, approaching differently a
car autopilot and an automated food recommendation algorithm. Another domain that could benefit from powerful
and explainable AI is (scientific) discovery [109] – intelligent systems may achieve super-human performance, e.g.,
AlphaGo [117], however a lack of transparency renders their mastery and ingenuity largely unattainable. Such observations have prompted the Defense Advanced Research Projects Agency (DARPA) to announce the eXplainable AI
(XAI) programme [44, 45] that promotes building a suite of techniques to (i) create more explainable models while

Explainability Is in the Mind of the Beholder

3

preserving their high predictive performance and (ii) enable humans to understand, trust and effectively manage
intelligent systems.
To address these challenges, AI explainability and Machine Learning (ML) interpretability solutions are developed
at breakneck speed, giving a perception of a chaotic field that may seem difficult to navigate at times. Despite these
considerable efforts, a universally agreed terminology and evaluation criteria are still elusive, with many methods
introduced to solve a commonly acknowledged but under-specified problem, and their success judged based on ad-hoc
measures. In this paper we take a step back and re-evaluate the foundations of this research to organise and reinforce its
most prominent findings that are essential for advancing this field, with the aim of providing a well-grounded platform
for tackling DARPA’s XAI mission. Our work thus reconciles and complements the already vast body of technical
contributions, philosophical and social treatises, as well as literature surveys by bringing to light the interdependence of
interdisciplinary and multifaceted concepts upon which explainable AI and interpretable ML are built. Our discussion
manoeuvres through this incoherent landscape by connecting numerous open questions and challenges – rather than
attempting to solve any individual issue – which is achieved through a comprehensive review of published work that
acknowledges any difficulties or disagreements pertaining to these research topics.
In particular, we review the notions of a black box and opaqueness in the context of artificial intelligence, and
formalise explainability – the preferred term in our treatment of the subject (Section 2). We discuss the meaning
and purpose of explanations as well as identify theoretical and practical prerequisites for lifting unintelligibility of
predictive systems, based on which we define explainability as a socially-grounded technology providing insights that
lead to understanding, which both conceptualises such techniques and fixes their evaluation criterion. Furthermore, we
show how transparency, and other terms often used to denote similar ideas, can be differentiated from explainability
– both overcome opaqueness, but only the latter leads to understanding – which we illustrate with decision trees
of different sizes. While the premise of our definition is clear, understanding largely depends upon the explanation
recipients, who come with a diverse range of background knowledge, experience, mental models and expectations [41].
Therefore, in addition to technical requirements, explainability tools should also embody various social traits as their
output is predominantly aimed at humans. We discuss these aspects of AI and ML explainers in Section 3, which
considers the role of an explanation audience and people’s preference for contrastive statements – XAI insights inspired
by explainability research in the social sciences [88]. We also examine the social and bi-directional explanatory process
underlying conversational explanations between humans, highlighting the desire of explainees (i.e., the audience
of explanations) to customise, personalise and contest various aspects of explanatory artefacts provided for opaque
systems within a congruent interaction [125, 129]. We then connect these desiderata to validation and evaluation
approaches for explainable AI and Interpretable ML (IML) techniques, arguing for studies that concentrate on assessing
the understanding gained by the target audience in favour of other metrics.
Next, in Section 4, we take a closer look at explainability by design (ante-hoc) and techniques devised to remove
opaqueness from pre-existing black boxes (post-hoc and, often, model-agnostic), focusing on the latter type given that
such methods are universally applicable to a wide variety models, which increases their potential reach and impact.
While these explainers are appealing, their modus operandi can be an unintended cause of low-fidelity explanations
that lack truthfulness with respect to the underlying black box [112]. Furthermore, their flexibility means that, from
a technical perspective, they can be applied to any predictive model, however they may not necessarily be equally
well suited to the intricacies of each and every one of them. Creating a faithful post-hoc explainer requires navigating
multiple trade-offs reflected in choosing specific components of otherwise highly-modular explainability framework
and parameterising these building blocks based on the specific use case [124, 126, 130, 131]. These observations prompt

4

Kacper Sokol and Peter Flach

us to revisit the disputed transparency–predictive power trade-off and assess benefits of interpretability that go beyond
understanding of predictive algorithms and their decisions, such as their fairness and accountability.
We continue our investigation in Section 5 by assessing explainability needs for various parts of predictive systems
– data, models and predictions – as well as multiplicity and diversity of these, sometimes incompatible, insights. To
this end, we use an explainability taxonomy derived from the Explainability Fact Sheets [123] to reason about such
systems within a well-defined framework that considers both their social and technical requirements. Notably, it covers
human aspects of explanations, thus giving us a platform to examine the audience (explainees), explanation complexity
and fidelity, as well as the interaction mode, among many others. This discussion leads us to a high-level overview of
landmark XAI and IML literature that highlights the interplay between various (often interdisciplinary and multifaceted)
concepts popular in the field, thus painting a coherent perspective. Section 6 then summarises our main observations
and contributions:
• we formally define explainability and catalogue other relevant nomenclature;
• we establish a spectrum of opaqueness determined by the desired level of transparency and interpretability;
• we identify important gaps in human-centred explainability from the perspective of current technology;
• we dispute universality of post-hoc explainers given their complexity and high degree of modularity; and
• we address explanation multiplicity through explanatory protocols for data, models and predictions.
These insights pave the way for the development of more intelligible and robust machine learning and artificial
intelligence explainers.
2

DEFINING BLACK-BOX AND EXPLAINABLE ARTIFICIAL INTELLIGENCE

To avoid a common criticism of explainability research, we begin by discussing the concept of interpretability. To
this end, we identify causes of opaqueness when dealing with intelligent systems and assess prerequisites for their
understanding. In this setting we observe shades of black-boxiness: an interpretability spectrum determined by the
extent of understanding exhibited by explainees, which, among others, is conditioned upon their mental capacity and
background knowledge. We link this finding with various notions used in XAI and IML literature, a connection that
helps us to fix the nomenclature and define explainability (our preferred term).
The term black box can be used to describe a system whose internal workings are opaque to the observer – its
operation may only be traced by analysing its inputs and outputs [10, 21]. Similarly, in computer science (including AI
and ML) a black box is a (data-driven) algorithm that can be understood as an automated process that we cannot reason
about beyond observing its behaviour. For AI in particular, Rudin [112] points out two main sources of opaqueness:
(i) a proprietary system, which may be transparent to its creators, but operates as a black box; and (ii) a system that
is too complex to be comprehend by any human. While the latter case concerns entities that are universally opaque
for the entire population, we argue that – in contrast to a binary classification [27] – this definition of black boxes
essentially establishes a (continuous) spectrum of understanding. Notably, different levels and degrees of transparency
and understandability have previously been pointed out and discussed in relation to individual elements of predictive
systems, explainees’ background knowledge and complexity of information conveyed by explanations, however these
observations are often limited to multiple, hand-crafted discrete categories [8, 57, 74, 83, 109].
For example, a seminal inquiry into opaqueness of visual perception systems by Marr [83] suggested three different
levels at which information processing devices can be understood. The top tier is computational theory, which concerns
abstract specification of the problem at hand and the overall goal. It is followed by representation and algorithm,

Explainability Is in the Mind of the Beholder

5

which deals with implementation details and selection of an appropriate representation. The final level is hardware
implementation, which simply establishes physical realisation of the explained problem. To illustrate his framework,
Marr [83] argued that understanding why birds fly cannot be achieved by only studying their feathers: “In order to
understand bird flight, we have to understand aerodynamics; only then do the structure of feathers and the different
shapes of birds’ wings make sense.” Nonetheless, he points out that these three tiers are only loosely related and some
phenomena may be explained at only one or two of them, therefore it is important to identify which of these levels
need to be covered in each individual case to arrive at understanding.
Lipton’s categorisation of transparency [74] – which he defines as the ability of a human to comprehend the (antehoc) mechanism employed by a predictive model – may be roughly seen as a modern interpretation of Marr’s levels
of understanding [83]. The first of Lipton’s dimensions is decomposability, which entails appreciation of individual
components that constitute a predictive system, namely: input, parameterisation and computation; it can be compared
to Marr’s computational theory. Next, algorithmic transparency involves understanding the modelling process embodied
by a predictive algorithm, which relates to Marr’s representation and algorithm. Finally, simulatability enables humans
to simulate a decisive process in vivo at the level of the entire model, capturing a concept similar to Marr’s hardware
implementation. These three levels of Lipton’s notion of transparency span diverse processes fundamental to predictive
modelling, and their understanding can offer a comprehensive, albeit predominantly technical, view of such systems.
While not universally recognised, knowledge, perception and comprehension of a phenomenon undeniably depend
upon the observer’s cognitive capabilities and mental model, the latter of which is an internal representation of this
phenomenon built on real-world experiences [65]. For example, Kulesza et al. [65] outline a fidelity-based understanding
spectrum spanning two dimensions:
completeness captures how truthful the understanding is overall (generality); and
soundness determines how accurate the understanding is for a particular phenomenon (specificity).
Therefore, a complete understanding of an event from a certain domain is equivalently applicable to other, possibly
unrelated, events from the same domain; for example, gravity in relation to a pencil falling from a desk. A sound
understanding, on the other hand, accurately describes an event without (over-)simplifications, which may result
in misconceptions; for example, leaving a pencil on a slanted surface results in it falling to the ground. Striking the
right balance between the two depends upon the observer and may be challenging to achieve: completeness without
soundness is likely to be too broad, hence uninformative; and the opposite can be too specific to the same effect.1
Within this space, Kulesza et al. [65] identify two particularly appealing types of a mental model:
functional which is enough to operationalise a concept but does not necessarily entail the understanding of its
underlying mechanism (akin to The Chinese Room Argument [103, 116]); and
structural which warrants a detailed understanding of how and why a concept operates.
For example, a functional understanding of a switch and a light bulb circuit can be captured by the dependency between
flipping the switch and the bulb lighting up. A structural understanding of the same phenomenon, on the other hand,
may focus on the underlying physical processes, e.g., closing an electrical circuit allows electrons to “move”, which heats
up the bulb’s filament, thus emitting light (note simplifications employed by this explanation). The former understanding
is confined to operating a light switch, while the latter can be generalised to many other electrical circuits. Each one
1 Note

that comparable distinctions can be found across literature. For example, a differentiation between functional and mechanistic understanding, where
the former concerns “functions, goals, and purpose[s]” and the latter relies on “parts, processes, and proximate causal mechanisms” [76, 77]. A similar
categorisation is also pertinent to knowledge, which can either be declarative or procedural [41] – the former allows to recall facts (i.e., “knowing that”)
and the latter translates to skills that enable performing a cognitive task (i.e., “knowing how”).

6

Kacper Sokol and Peter Flach

is aimed at a different audience and their complexity should be fine-tuned for the intended purpose as explanations
misdirected towards an inappropriate audience may be incomprehensible. These observations lead us to argue that such
a spectrum of understanding in human explainability can constitute a yardstick for determining explanatory qualities
of predictive algorithms – a link that has mostly been neglected in the literature, but which can help us to explicitly
define popular XAI and IML terminology.
A considerable amount of research into explainable AI and interpretable ML published in recent years appears to
suggest that it is a freshly established field; however, in reality it is more of a renaissance [41]. While work in this area
indeed picked up the pace in the past decade, interest in creating transparent and explainable, data-driven algorithms
dates back at least to the 1990s [112], and further back to the 1970s if expert systems are taken into account [41, 72].
With such a rich history and the increased publication velocity attributed to the more recent re-establishment of the
field, one may think that this research area has clearly defined objectives and a widely shared and adopted terminology.
However, with an abundance of keywords that are often used interchangeably in the literature – without precisely
defining their meaning – this is not yet the case. The most common terms include, but are not limited to:
• explainability,

• observability,

• transparency,

• explicability,

• intelligibility,

• comprehensibility,

• understandability,

• interpretability,

• simulatability,

• explicitness,

• justification,

• rationalisation,

• sensemaking,

• insight,

• evidence,

• reason, and

• cause.
Other keywords – such as function (of), purpose (of), excuse, consequence, effect, implication and meaning – can also
be found in non-technical explainability research [1, page 32]. Additionally, explanandum or explicandum often appear
in XAI and IML literature, however these terms, which are borrowed from philosophy of science, denote the concept to
be explained.
While early XAI and IML research might have missed out on an opportunity to clearly define its goals and nomenclature, recent work has attempted to rectify this problem [3, 5, 8, 14–16, 24, 36, 38, 43, 52, 57, 70, 74, 81, 82, 91–
93, 95, 99, 100, 109, 110, 112, 133, 135, 144]. Within this spaces, some authors simply list the relevant terms without
assigning any meaning to them [81] while others cite dictionary definitions [8, 99, 109] – e.g., to interpret is “to explain
[. . . ] the meaning of” or “present in understandable terms” according to Merriam-Webster – or suggest that many
such keywords are synonymous [14]. It is also common to find circular or tautological definitions, which use one
term from the list shown above to specify another [3]; for example, “something is explainable when we can interpret
it”, “interpretability is making sense of ML models”, “interpretable systems are explainable if their operations can be
understood by humans” or “intelligibility is the possibility to comprehended something”. Hierarchical and ontological
definitions of these terms also appear in the literature [14, 110, 133], often creating a web of connections that is difficult
to parse, follow and apply. Another counterproductive approach to defining these concepts assumes that their meaning
is inherently intuitive or can only be captured by tacit knowledge – viewpoints that can be summarised by “I know it
when I see it” phrase [95].
A different route to specifying these terms is binding keywords to particular components of a predictive pipeline or
associating them with the technical and social properties of such a system [74, 82, 109]; however, the former is just a
labelling strategy and the latter assumes that we can achieve explainability by simply fulfilling a set of requirements.
A fictitious scenario under this purview could determine that data are understandable, models are transparent and
predictions are explainable; the overall interpretability of predictive pipelines, on the other hand, is determined by

Explainability Is in the Mind of the Beholder

7

the fidelity, brevity and relevance of the insights produced by the designated method. More specifically, transparency
is oftentimes associated with ante-hoc methods (the internal mechanisms of a predictive model) and interpretability
with post-hoc approaches (the observable behaviour a system) [74]; alternatively, interpretability is designated for
models and explainability for their outputs (more precisely, reasons behind predictions) [57]. Similarly, simulatability
may be linked to an entire model, decomposability to its individual components and algorithmic transparency to the
underlying training algorithm [74]. Interpretability has also been defined as a domain-specific notion that imposes “a
set of application-specific constraints on the model”, thus making this concept only applicable to predictive models that
can provide their own explanations (i.e., ante-hoc interpretability) and should not, along with the term explainability, be
used to refer to “approximations to black box model predictions” (i.e., post-hoc explainability) [112]. In this particular
view, therefore, a predictive model is interpretable if it “obeys structural knowledge of the domain, such as monotonicity,
causality, structural (generative) constraints, additivity or physical constraints that come from domain knowledge”.
Alternatively, interpretability could be used as an umbrella term and refer to data, models and post-hoc approaches
as long as it facilitates extracting helpful information from these components or produces insights into them [93]. This
designation, however, may not be universal and, to complicate matters even more, certain terms can be used with respect
to multiple elements of a predictive system, causing confusion. For example, transparency may relate to predictive
models, interpretability to input data and models, and explainability to data, models and human recipients [109]. While
transparency may be uniquely linked to predictive models, it can still carry multiple meanings and apply to operations
of a model as well as its design, components and the underlying algorithmic process [74, 109]. Desiderata-based
definitions appearing in the literature, on the other hand, specify explainability through a mixture of properties such as
interpretability (determined by clarity & parsimony) and fidelity (consisting of completeness & soundness) [82, 110];
nonetheless, note that achieving both does not necessarily guarantee better comprehension of the explained system.
In like manner, Alvarez-Melis et al. [5] used the weight of evidence idea from information theory to mathematically
define AI and ML explainability and outline a precise list of its desiderata. While appealing, the complexities of the
real world make their conceptualisation difficult to apply at large. Murdoch et al. [93] followed a similar route and
captured interpretation, understanding and evaluation of data-driven systems in the predictive–descriptive–relevant
(PDR) framework, which spans: predictive performance (accuracy) of a model; descriptive capabilities of its (post-hoc)
explainer quantified via fidelity; and relevancy of the resulting explanatory insights determined by their ability to
provide “insight[s] for a particular audience into a chosen problem”.
Within this chaotic landscape some researchers propose flexible definitions that are inspired by interdisciplinary
work and can accommodate a variety of contexts while maintaining a precise and coherent meaning. Gilpin et al. [38]
offered definitions of “explanation”, “interpretability” and “explainability” drawing from a broad body of literature in an
effort to standardise XAI and IML findings. While their notions appear somewhat vague – explanations should answer
“Why?” and “Why-should?” questions until such questions can no longer be asked – they argue for making explanations
interpretable and complete, striving towards human understanding that depends on the explainee’s cognition, knowledge
and biases. Similarly, Biran and McKeown [16] were concerned with explanations, which they characterised as “giving
a reason for a prediction” and answering “how a system arrives at its prediction”. They also defined justifications as
“putting an explanation in a context” and conveying “why we should believe that the prediction is correct”, which,
they note, do not necessarily have to correspond to how the predictive system actually works. Notably, many of these
observations reappear across diverse publications, with the shared theme indicating that explanations should always
answer an implicit or explicit “Why?” question [61], in addition to addressing “What?” and “How?” [88, 99]. In a later
piece of work, Biran and Cotton [15] defined explainability as “the degree to which an observer can understand the

8

Kacper Sokol and Peter Flach

cause of a decision” (also adopted by Miller [88]), thus making it much more explainee-centred. While many authors
use the term cause rather loosely in XAI and IML research, we argue against such practice – it is important to reserve it
exclusively for insights extracted from causal models [102].
More recently, based on an extensive review of literature in computer science and related disciplines, Mohseni
et al. [91] provided a collection of definitions for the most common terms in explainable AI and interpretable ML,
nonetheless the underlying rationale is predominantly qualitative making them difficult to operationalise. Similarly,
Arrieta et al. [8] differentiated between the following terms: understandability/intelligibility, comprehensibility, interpretability/transparency and explainability. Specifically, they defined interpretability or transparency as a passive
characteristic of a model that allows humans to make sense of it on different levels – e.g., its internal mechanisms
and derivation of predictions – therefore relating it to the cognitive skills, capacities and limitations of individual
explainees. Explainability, on the other hand, was described as an active characteristic of a model that is achieved
through actions and procedures employed (by the model) to clarify its functioning for a certain audience. Montavon
et al. [92] also offered definitions of these two terms – interpretability and explainability – from a perspective of
functional understanding [76, 77]. They characterised interpretability as a mapping of an abstract concept, e.g., a
predicted class, onto a domain that can be comprehended by a human; explainability, in their view, is responsible for
providing a collection of factors – expressed in an interpretable domain – that contribute to an automated decision of a
particular instance. In summary, their goal is to study and understand how inputs are mapped to outputs, possibly via a
human-comprehensible representation of relevant concepts.
Each definition conveys a more or less precise meaning that can be used to label relevant techniques, however they
do not necessarily clarify and help to navigate the complex landscape of IML and XAI research. To organise this space,
we categorise the underlying terminology based on three criteria:
• properties of systems;
• functions and roles that they serve; and
• actions required to process, assimilate and internalise information elicited by them.
The core concept around which we build our nomenclature is explainability; we define it as insights that lead to
understanding (the role of an explanation) – a popular and widely accepted rationale in the social sciences [1, 9, 59, 75,
98, 143]. While it may seem abstract, understanding can be assessed with questioning dialogues [7, 80, 137–139] – e.g.,
a machine interrogating the explainees to verify their understanding of the phenomenon being explained at the desired
level of detail – which are the opposite of explanatory dialogues. Such a process reflects how understanding is tested in
education, where the quality of tuition as well as knowledge and skills of pupils are evaluated through standardised
tests and exams (albeit not without criticism [86]). Furthermore, encouraging people to explain a phenomenon helps
them to realise the extent of their ignorance and confront the complexity of the problem, which are important factors
in exposing The Illusion of Explanatory Depth [111] – a belief that one understands more than one actually does.
This notion of explainability and the three building blocks of XAI and IML terminology allow us to precisely define
the other popular terms. Therefore,
• observability,

• transparency,

• explicability,

• intelligibility,

• comprehensibility,

• understandability,

• interpretability,

• simulatability, and

• explicitness

are properties of an AI or ML system that enable it to directly (ante-hoc) or indirectly (post-hoc) convey information
of varied complexity, the understanding of which depends upon the cognitive capabilities and (domain) expertise of the

Explainability Is in the Mind of the Beholder

9

explainee. For example, observing an object falling from a table is a transparent phenomenon per se, but the level of its
understanding, if any, is based upon the depth of the observer’s physical knowledge. Such characteristics provide
• justification,

• rationalisation,

• evidence, and

• reason

• insight,

(roles) that can be used to
• reason about,

• make sense of,

• rationalise,

• justify,

• interpret, or

• comprehend

(note that here these are used as verbs) behaviour of a – black-box or glass-box – predictive system, all of which are
actions that under the right circumstances lead to understanding. While simulatability is also based upon observing
a transparent process and replicating it, such an action does not necessarily imply understanding of the underlying
phenomenon – recall the difference between declarative and procedural knowledge [41], structural and functional
mental models [65], functional and mechanistic understanding [76, 77] and The Chinese Room Argument [116] discussed
earlier. Lastly, a cause has a similar meaning to a reason, but the first one is derived from a causal model, whereas the
latter is based purely on observations of the behaviour of a (black-box) model.
Such a setting makes a welcome connection between the XAI and IML terminology synthesised by the equation
Explainability = Reasoning (Transparency | Background Knowledge) ,
|
{z
}
understanding

which defines Explainability as the process of deriving understanding – i.e., extracting meaning – through Reasoning applied to Transparent insights distilled from a data-driven predictive system that are adjusted to the explainee’s
Background Knowledge. In this process, the Reasoning can either be done by the explainer or the explainee, and there
is an implicit assumption that the explainee’s Background Knowledge aligns with the Transparent representation of
the predictive model. If the latter does not hold, mitigation techniques such as employing an interpretable representation
can be used to communicate concepts that are otherwise incomprehensible [107, 126, 130]. Reasoning also comes in
many different shapes and sizes depending on the underlying system (Transparency) as well as the explainer and
the explainee (Background Knowledge); for example, logical reasoning with facts, causal reasoning over a causal
graph, case-based reasoning with a fixed similarity metric, and artificial neuron activation analysis for a shallow neural
network.
Therefore, linear models are transparent given a reasonable number of features; additionally, with the right ML and
domain background knowledge – requirement of normalised features, effect of feature correlation and the meaning
of coefficients – the explainee can reason about their properties, leading to an explanation based on understanding.
Similarly, a visualisation of a shallow decision tree can be considered both transparent and explainable assuming that
the explainee understands how to navigate its structure (ML background knowledge) and the features are meaningful
(domain background knowledge); again, it is up to the explainee to reason about these insights. When the size of a tree
increases, however, its visualisation loses the explanatory power because many explainees become unable to process
and reason about its structure. Restoring the explainability of a deep tree requires delegating the reasoning process to an
algorithm that can digest its structure and output sought after insights in a concise representation. For example, when
explaining a prediction, the tree structure can be traversed to identify a similar instance with a different prediction, e.g.,
as encoded by two neighbouring leaves with a shared parent, thus demystifying the automated decision [121, 122].

10

Kacper Sokol and Peter Flach
While understanding and applying the newly acquired knowledge to unseen tasks are recurring themes in XAI and

IML literature [14, 15, 24, 28, 36, 57, 110] – with a few notable exceptions [70, 99, 144] – they rarely ever play the central
role. For example, Palacio et al. [99] define an explanation as “the process of describing one or more facts, such that it
facilitates the understanding of aspects related to said facts (by a human consumer)”. Similarly, Yao [144] “highlight[s]
one important feature of explanations: they elicit understanding (in humans)”; Yao proceeds to suggest that the three
levels of analysis proposed by Marr [83], and discussed earlier, should be extended with a social level to reflect that AI
models, especially the ones of concern to XAI and IML researchers, do not operate in isolation from humans.
Furthermore, some researchers appear to converge towards a similar definition to ours. Papenmeier et al. [100] characterise transparency as “the extent to which information is exposed to a system’s inner workings” and interpretability as
“the extent to which transparency is meaningful to a human”, both of which lead them to formalise explanations as “the
mechanisms by which a system communicates information about its inner workings (transparency) to the user”. In like
manner, Roscher et al. [109] posit that “the scientist is using the data, the transparency of the method, and its interpretation to explain the output results (or the data) using domain knowledge and thereby [. . . ] obtain[s] a scientific outcome”,
which process should lead to understanding by presenting properties of ML models in humans-comprehensible terms;
they further suggest that “explainability usually cannot be achieved purely algorithmically”, which resonates with
the role of human reasoning in our definition. Similarly, Langer et al. [70] identify a “(given) context” as the element
moderating “explanatory information” to facilitate “stakeholders’ understanding” (of a subset of components present in
a complex system). Additionally, while not explicitly stated, one interpretation of Rosenfeld and Richardson’s notion of
explanations [110] is a collection of human-centred processes that allow explainees to understand a predictive model
by presenting them with a suitable representation of its logic (a concept that they call explicitness), however based on a
graphical representation of their framework explainability is achieved through interpretability, only one component of
which is transparency. Adjacent to XAI and IML, Bohlender and Köhl [17] discussed explanations in software systems,
which are meant to “resolve [a] lack of understanding” pertaining to a particular aspect (explanandum) of a system
for a specific audience and “the processing of [an] explanation [. . . ] is what makes [an] agent [. . . ] understand the
explanandum” – an action that may require the use of cognitive or computational resources and that may only be
operationalised in specific contexts determined, for example, by the background knowledge of the explainees.
Given the importance of understanding in our definition, as well as explainability research outside of XAI and IML [1,
pages 23, 42, 57], it is crucial to review its acquisition and operationalisation together with how it is distinct from
and more desirable than not only declarative but also procedural knowledge. For example, consider justifications that
can be seen to communicate why a decision is correct without necessarily providing the exact logic behind it [15],
therefore preventing the explainee from internalising and reapplying these insights to other scenarios. Similarly,
knowledge can be acquired and recalled thus giving the impression of understanding but recitation in itself does not
imply comprehension or operationalisation (i.e., an ability to apply it) – an observation that follows from The Chinese
Room Argument [116]. For example, memorising a textbook or answers to a set of questions may suffice to pass an
exam but falls short of effectively resolving related yet distinct problems [41]. This is an important insight for evaluating
explainability – as defined in this paper – since we first have to specify what it means to attain understanding before we
can assess effectiveness of the explanatory process [98]. Furthermore, internalising knowledge into understanding and
correctness thereof are conditioned on the reasoning capabilities and background knowledge of the explainee [1, 41].
The detrimental effects of misalignments in this space were shown by Bell et al. [11], who reported that explainability
mechanisms pertinent to inherently interpretable models may be confusing, especially so for a lay audience; for example,

Explainability Is in the Mind of the Beholder

11

ante-hoc transparency of decision trees achieved though a visualisation of their structure misleads people (lacking
technical expertise) into believing that the feature used by the root-node split is the most important attribute.
3

HUMANS AND EXPLANATIONS – TWO SIDES OF THE SAME COIN

Defining explainability as leading to understanding and our categorisation into properties, functions and actions highlight
an important aspect of this research topic: explanations do not operate in a vacuum, they are highly contextual and
directed at some autonomous agent, either a human or machine, who is as important as the explainability algorithm
itself. Notably, up until recently XAI and IML research has been undertaken mostly within the computer science
realm [89], thus bringing in various biases and implicit assumptions from this predominantly technical field. While
some explainability research has found its way into other scientific disciplines, e.g., law [136], the majority gravitated
around technical properties. This research agenda was disrupted by Miller et al. [89], who observed that the function
of an explanation and its recipients are largely neglected – a phenomenon which they dubbed “inmates running the
asylum” – leading to a substantial paradigm shift. Miller’s follow-on work [88] grounded this observation in (human)
explainability research from the social sciences, where this topic has been studied for decades, thus providing invaluable
insights that can benefit XAI and IML.
Miller’s findings [88] have arguably reshaped the field, with a substantial proportion of the ensuing research
acknowledging the explainees – their autonomy, goals, expectations, intentions and interactions. While explainability
of data-driven systems has various benefits, it is usually in focus when an AI agent fails, behaves anomalously or operates
inconsistently with the explainee’s beliefs, expectations or mental model, e.g., an unexpected ML prediction causing a
disagreement; alternatively, an explanation may be requested to support learning or provide information needed to
solve a problem or complete a task [41]. In such cases, explainees’ preferences, needs and goals should be addressed to
maximise the effectiveness of an explanation, for example by appropriately adjusting its complexity [41, 88]. This step
can be further improved by treating explainability as a process instead of one-off information offloading [41, 88]; by
satisfying the explainees’ natural desire to interact and communicate with the explainer within a predictable protocol,
they are provided with an opportunity to seamlessly customise and personalise the explanation [125]. Perhaps the most
influential of Miller’s observations is the humans’ preference for contrastive explanations given their prominence in
everyday life. We discuss these three fundamental aspects of human-centred explainability in more detail below.
Understanding can be an elusive objective when it comes to explaining intelligent systems since each unique
explanation audience may expect to receive different insights, e.g., a medical diagnosis can be communicated in terms
of test results or observable symptoms depending on whether it is directed towards medical staff or patients. While in
our considerations we implicitly assume that the explanation recipient is a human, it may as well be another algorithm
that further processes such insights, in which case other, more suitable, properties would be of interest. When taken
into account, the purpose of explainability and the explainee’s goal also influence the explanation composition [41].
For example, an explanation will look different when it helps to debug an ML model and when it justifies a negative
outcome of a loan application; note that the target audience also differs, with the former aimed at ML engineers
and the latter at lay people. A complementary view, based on a means-end account of XAI [20], argues to examine
“what should be explained (topic), to whom something should be explained (stakeholder), why something should be
explained (goal), and how something should be explained (instrument)”. Addressing such desiderata by accounting
for the explainee’s cognitive capabilities and skill level, however, is challenging as it requires access to the explainee’s
background knowledge and mental model, which are vague and often undefined concepts that cannot be easily extracted
and modelled.

12

Kacper Sokol and Peter Flach
Nonetheless, just by considering the audience and purpose of an explanation, we can identify (and deliver) a collection

of relevant properties. In certain cases, such as the aforementioned loan application, the actionability of explanatory
insights is crucial, e.g., suggesting that an individual would receive a loan had he or she been 10 years younger is futile.
Multiplicity of apparently indistinguishable arguments can also decrease the perceived quality of an explanation when
one is chosen at random without a user-centred heuristic in place, which, again, depends on the application domain
and audience. For example, research suggests [88] that if one of multiple, otherwise equivalent, time-ordered events has
to be chosen as an explanation, the most recent one will best resonate with the explainee; additionally, prioritising
explanations by their novelty will keep the explainee engaged and attentive, and distinguishing between sufficient and
necessary conditions for a given outcome can help to reduce cognitive load. While desirable, brevity of an explanation
can sometimes be at odds with its comprehensiveness and completeness – sacrificing the big picture (which in itself
may be too convoluted to understand) for concise communication [64]. Explanatory minimalism, nonetheless, bears
the danger of oversimplification; however, when it is a strict requirement, explanation soundness can be favoured to
focus on factors pertinent to the explained instance and discard more general reasons that are largely irrelevant. Such
an approach can introduce inaccuracies with respect to the overall data-driven system, but the explanations remain
truthful for the individual instance. Striking the right balance between generality and specificity of an explanation – as
well as achieving all the other aforementioned desiderata – is notoriously challenging and often requires tuning its
soundness and completeness for the intended audience and application, which itself may be impractical when done
manually, and prohibitively difficult through capturing the explainee’s mental model.
While posing problems for AI explainers, satisfying this wide range of diverse assumptions and expectations comes
naturally to humans when they engage in an explanatory process among themselves. This is partly due to shared
background knowledge, and is further amplified by interactive communication that allows to rapidly iterate over
questions, exchange informations and refine answers to arrive at understanding. One explanation does not fit all and
treating explainability as a bi-directional process provides a platform to appreciate uniqueness of each explainee through
personalised explanations [125] that enable transfer of knowledge and help to develop understanding [41]. While
these topics have received relatively little attention in the XAI and IML literature, we can draw design insights and
inspirations from research on explanatory debugging of predictive models [64]. Therefore, an interactive explanatory
process should be iterative, enabling the explainee to learn, provide feedback and receive updated insights until reaching
a satisfactory conclusion; the explainer ought to always honour user-provided feedback by incorporating it into the
explanation generation process, or clearly communicate a precise reason if that is impossible; the communication should
be reversible to allow the explainee to retract a requested change or annul a piece of feedback when it was provided by
mistake, or to explore an interesting part of the predictive model through a speculative enquiry; and, finally, the whole
process should be incremental to easily attribute each piece of feedback to an explanation change, thereby showing
up-to-date results regardless of how small the tweaks are.
Even though dialogue is fundamental to human explainability, it is largely absent in XAI and IML techniques [125],
which are often based on one-way communication, where the user receives a description of a data-driven system
without an opportunity to request more details or contest it. A similar interaction in a form of the aforementioned
questioning dialogues can also be used to judge the explainee’s understanding of the explained concept, thus be a proxy
for assessing effectiveness of the explainer. Notably, human dialogue tends to be verbal or written, both of which are
based on the natural language. While ubiquitous, this form of communication is not equally effective in conveying all
types of information, requiring humans to augment it with visual aids, which are especially helpful when the interaction
serves explanatory purposes. The same strategy can be adopted in explainable AI and interpretable ML, where the

Explainability Is in the Mind of the Beholder

13

explainer would switch between various explanatory artefacts – such as (written and spoken) text, images, plots,
mathematical formulation, numbers and tables – depending on which one is best suited for the type of information
being communicated in a given context [41]. Mixing and matching them is also possible, e.g., a numerical table or a plot
complemented with a caption, and may be beneficial as the whole can be greater than the sum of its parts, especially
that certain explanation types may require a specific communication medium or explanatory artefact to be effective.
Using visualisation, textualisation and (statistical) summarisation, however, does not guarantee a coherent relation,
structure or story conveyed by these communication media alone, which could possibly be achieved by grounding
them in a shared context through logical reasoning or formal argumentation [29]; additional inspiration can be found in
sensemaking theory, which was recently adapted to XAI and IML applications [52].
Contrastive explanations – more specifically, counterfactuals – dominate the human explanatory process and are
considered the gold standard in explainability of data-driven systems [88]. They juxtapose a hypothetical situation
(foil) next to the factual account with the aim to emphasise the consequences of or “would be” change in the outcome.
Counterfactuals statements can be characterised by their lineage: model-driven explanations are represented by artificial
data points (akin to centroids), whereas data-driven explanations are instances recovered from a (training) data set
(similar to medoids). Furthermore, the contrast can either be implicit – i.e., “Why class 𝑋 ?” (hence not any other class)
– or explicit – i.e., “Why class 𝑋 and not 𝑌 ?” Counterfactuals are appropriate for lay audiences and domain experts
alike, can use concepts of varying difficulty and be expressed in different media such as text and images. They are
parsimonious as the foil tends to be based on a single factor, but, if desired, can account for an arbitrary degree of
feature covariance. They support interaction, customisation and personalisation, e.g., a foil built around a user-selected
feature provided in an explanatory dialogue, which can be used to restrict their search space, possibly making them
easier to retrieve. When deployed in a user-centred application, they can provide the explainees with appealing insights
by building the foil only around actionable features. However, their effectiveness may be problematic when explaining a
proprietary predictive system, e.g., built as a black box with the intention to protect a trade secret, since counterfactual
explanations can leak sensitive information, thereby allowing the explainee to steal or game the underlying model. In
an open world, they also suffer from vaguely defined or imprecise notions known as non-concepts [95], e.g., “What is
not-a-dog?”
These idealised properties make counterfactual statements appealing, but some may get lost in practice, e.g., an
imperfect implementation, resulting in subpar explanations. On the face of it, these explanatory artefacts resemble
causal insights, but unless they are generated with a causal model [101], they should not be treated as such and instead
be interpreted as descriptors of a decision boundary used by a predictive system. If they are model-driven, as opposed
to data-driven, they may not necessarily come from the data manifold, yielding (out-of-distribution) explanations that
are neither feasible nor actionable in the real life, e.g., “Had you been 200 years old, . . . ” Even if they are consistent with
the data distribution, the foil may still come from a sparse region, thus prescribing possible but improbable feature
values [105]. Counterfactual explanations are often specific to a single data point, although humans are known to
generalise such insights to unseen and possibly unrelated cases – recall The Illusion of Explanatory Depth effect [111] –
which may result in overconfidence.
Fulfilling all of these desiderata can help in developing an explainability system that enables the explainees to (better)
understand an automated decision-making process, which, as we noted earlier, offers a yardstick by which success
of such techniques can be quantified [17, 57, 70, 98]. To be effective, however, the approach to assess, evaluate and
measure understanding has to account for the aforementioned contextual parameters such as the application domain
(and its sensitivity), function of the explanation, intended audience (and its background knowledge) as well as (technical)

14

Kacper Sokol and Peter Flach

caveats of the XAI or IML algorithm [8, 93]. Just like beauty, which is in the eye of the beholder, the (perceived) quality,
helpfulness and success of an explanation are judged by the recipient [57]. Therefore, producing explanations is a
necessary but insufficient condition of engendering understanding, which additionally requires them to be relevant to
the stakeholders, comprehensible by them and compatible with their desiderata [41, 70], for example, by aligning the
type of insights and their level of transparency towards the chosen use case and audience (in view of the anticipated
background knowledge and skills). Specifically, consider the truthfulness of explanatory information, which may be
reduced for a particular purpose without harming the audience’s ability to derive understanding; the simplification
used in depicting underground lines and stops is a case in point as it conveys the cues necessary to navigate such a
transportation system despite foregoing an accurate representation of scale and distance [98]. Notably, a transparent
representation that is universal and satisfies all the different needs and expectations of diverse explainee groups may
not exist, just like an agreement between these individuals on the objective nature of understanding. Given this strong
dependence on human perception, the effectiveness of explanations should be evaluated empirically [17] to combat
The Illusion of Explanatory Depth [111], and, in view of The Chinese Room Argument [116], the studies should go
beyond assessing simple task completion to capture the difference between knowledge and understanding within a
well-defined, real-life context expected in the deployment.

4

THE DISCORD OVER SACRIFICING EXPLAINABILITY FOR PREDICTIVE POWER

Theoretical desiderata do not always align with the operationalisation and practicalities of XAI and IML algorithms, and
the latter are what ends up affecting our lives. For example, explainability is an inherently social process that usually
involves bi-directional communication, but most implementations – even the ones using contrastive statements [134, 136]
– output a single explanation that is optimised according to some predefined metric, not necessarily addressing concerns
of an individual explainee [125]. Similarly, while inherently transparent predictive models and ante-hoc explainers
may be preferred [112], such solutions are often model-dependent, labour-intensive and tend to be application-specific,
which limits their scope as well as wider applicability and adoption. Instead, post-hoc and model-agnostic explainers
dominate the field [79, 107, 108, 130] since they are considered one-stop solutions – a unified explainability experience
without a cross-domain adaptation overhead. This silver bullet framework, however, comes at a cost: subpar fidelity
that can result in misleading or outright incorrect explanations. While increasingly such considerations find their way
into publications, they are often limited to acknowledging the method’s shortcomings, stopping short of offering a
viable solution.
literature is the perceived dichotomy between transparency and predictive power of
AI and ML systems. A popular example supporting this theory is the unprecedented
effectiveness of deep neural networks on certain tasks, whose ever increasing
complexity, e.g., the number of layers and hidden units, improves their performance
at the expense of transparency. This trade-off has been reiterated in the DARPA XAI
program’s Broad Agency Announcement [44] and supported by an appealing graph
reproduced in Figure 1. However, at the time of publication it has been a theory
based mostly on anecdotal evidence, with Rudin [112] criticising plots like this given
their lack of scale, transparency or precise performance metrics, and supporting
data. Notably, Rudin argues that investing more effort into feature engineering and

Predictive Performance

A common belief motivating many methods published in the explainability

Deep Neural
Networks

?

Support Vector
Machines
Decision
Trees

Transparency

Fig. 1. Fictitious depiction of the
anecdotal trade-off between transparency and predictive power of AI
systems [44].

Explainability Is in the Mind of the Beholder

15

data modelling can help to build inherently explainable AI and ML systems that perform on a par with their black-box
alternatives [25].
This anecdotal trade-off and a tendency to prioritise predictive power mean that explainability is often only an
afterthought. Such a mindset contributes to a landscape with an abundance of well-performing but inherently opaque
algorithms that are in need of explainability, thus creating a demand for universal explainers that are post-hoc and
model-agnostic, such as surrogates [26, 107, 130]. This seemingly uncompromising development approach – where stateof-the-art performance remains the main objective, later complemented with a post-hoc explainer – offers an attractive
alternative (and rebuttal) to designing inherently explainable AI and ML systems, whose creation arguably requires more
effort. While such explainers are compatible with any black-box model, they are not necessarily equally well suited for
every one of them [131] – after all the computer science folklore of “no free lunch” (a single, universal algorithm cannot
outperform all the others across the board) applies here as well – which is reflected in the continuous stream of novel
XAI and IML techniques being proposed in the literature, many of whom report competing or contradictory findings
(likely because of diverging operational contexts). Some post-hoc and model-agnostic explainers boast appealing
properties and guarantees, however upon closer inspection one often encounters caveats and assumptions required
for these to hold, such as the underlying “black box” being a linear model [79]. Additionally, making an explainer
model-agnostic introduces an extra layer of complexity that usually entails a degree of randomness and decreased
fidelity [130, 145], so that using them may become a stopgap to claim explainability of an inherently opaque predictive
system instead of addressing genuine explainability needs. Correctly interpreting the insights produced by XAI and
IML methods may therefore be challenging as it requires a sufficient level of (technical) expertise and alignment with
the explainees’ background knowledge for the recipients to understand the capabilities and limitations of the explainer,
thus avoid drawing incorrect conclusions – this is especially relevant to post-hoc and model-agnostic approaches given
their higher complexity [90, 124, 126, 131].
In Rudin’s view [112], many high-stakes AI and ML systems can be made explainable by design with enough effort
put towards data pre-processing, feature engineering and modelling (which otherwise, e.g., for neural networks, may
go into architecture search and parameter tuning). Such ante-hoc explainers are usually domain-specific and after
the initial engineering endeavour they are easy to manage and maintain. While this approach should be championed
for structured (tabular) data where it has been shown to perform on a par with state-of-the-art black boxes [25], the
same may be unachievable for sensory data such as images and sounds, for which opaque models, e.g., deep neural
networks, have the upper hand. In addition to black boxes modelling sensory data, pre-existing, inaccessible or legacy
predictive systems may require interpretability, in which case they can only be retrofitted with post-hoc explainers. Such
techniques are also helpful to engineers and developers working with predictive models since they enable inspection
and debugging of data-driven systems. However, falling back on off-the-shelf solutions may not guarantee acceptable
fidelity [124, 130, 131] (specifically, soundness and completeness), which is of particular importance and may require
tailor-made explainers and transparent communication of their limitations.
While composing a predictive pipeline, we have an abundance of pre-processing as well as modelling tools and
techniques at our disposal, a selection of which will end up in the final system. The XAI and IML landscape, on the other
hand, is quite different, especially for post-hoc and model-agnostic approaches: explainers tend to be end-to-end tools
with only a handful of parameters exposed to the user. In view of “no free lunch”, this is undesirable as despite being
model-agnostic, i.e., compatible with any model type, these monolithic algorithms cannot perform equally well for
every one of them [130, 131]. This variability in their behaviour can often be attributed to a misalignment between the
assumptions baked into an explainer and the properties of the explained system, which manifests itself in low fidelity.

16

Kacper Sokol and Peter Flach
Model-specific or ante-hoc explainers as advocated by Rudin [112] can be used to address this issue; however, as

discussed earlier, such a solution may have limited applicability and cannot be retrofitted to pre-existing predictive
systems. Resolving a similar challenge in machine learning and data mining often comes down to a series of investigative
steps to guide algorithmic choices down the line, which can be operationalised within a standardised process for
knowledge discovery such as KDD [30], CRISP-DM [23, 84] or BigData [4]. For example, by analysing feature correlation,
data uniformity and class imbalance, we can account for these phenomena when engineering features and training
models, thereby making the resulting systems more accountable and robust. Nonetheless, while we may have a set
of universal properties expected of XAI and IML systems [123], we lack a dedicated process that could guide the
development and assessment of explainers – their practical requirements and needs – which likely hinders adherence
to best practice. Although one can imagine a generic workflow for designing inherently interpretable (ante-hoc)
systems [112], a similar endeavour should not be neglected for model-agnostic and post-hoc explainers that could be
adapted to individual predictive black boxes by capitalising on their flexibility and modularity [130], possibly overcoming
low fidelity [124].
More recently, in response to Rudin’s call to action [112], the disputed trade-off between transparency and predictive
power has been revisited with a greater scientific rigour [11, 47]. Herm et al. [47] used explainability as a tool to aid people
in problem-solving, investigating it under the aforementioned two dimensions complemented with comprehensibility,
which should be a direct consequence of a model being recognised as explainable. Their preliminary findings are
somewhat at odds with Rudin’s postulate, especially so for high-stakes scenarios for which the user studies suggest
that an artificial neural network enhanced with SHAP explanations boasts high predictive performance and is also
perceived as explainable. Similarly, Bell et al. [11] performed empirical quantification of this trade-off in two public
policy domains and found that (inherently) interpretable models may not necessarily be more explainable than black
boxes. The results of their experiments show that even opaque models may be recognised as explainable – nonetheless,
the authors emphasise the importance of ante-hoc explainability in mission-critical domains – hinting that the trade-off
may be more nuanced than acknowledged in the literature. Notably, while the explainee’s perception may suggest
that black boxes accompanied by post-hoc explainers are up to the task, the fidelity, correctness and veracity of such
insights remain contestable, especially in view of the recipient’s susceptibility to be convinced by sheer presence of
“explanations” that themselves may not necessarily be truthful [48] or meaningful (as famously shown by The Copy
Machine study [69]). With just a few such inquiries available, the topic requires further investigation to offer a clear
view on the possible trade-off between transparency and predictive power in XAI and IML. We postulate that – in
accordance with our definition – particular focus should be put on the reasoning employed by explainees to extract
meaning and create understanding based on transparent insights into black boxes given that a misconception of how to
interpret them may result in a false sense of explainability [90].
A distinct viewpoint on this matter manifests itself in claims that we should not expect machine learning algorithms, such as deep neural networks, to be explainable and instead regulate them purely based on their real-life
performance [118] and behaviour [94, minute 29], however it is not a widely shared belief [51]. This insight comes
from the alleged inability of humans to explain their actions since such justifications are post-factum stories that are
concocted and retrofitted for the benefit of the audience. Certifying autonomous agents based on their output, on
the other hand, is consistent with human values as one can hypothesise about committing a crime, but one cannot
be punished unless such a thought is acted upon. While the origin and nature of human thought processes may be
shrouded in mystery, its formulation is expected to follow the reason of logic to be (socially) acceptable. In particular,
Miller [87] refutes performance-based validation by arguing that explainability stemming from regulatory requirements

Explainability Is in the Mind of the Beholder

17

It's like
a fan!

It's like a fan!

It's like
a spear!

It's like a spear!

It's like
a snake!

It's like a snake!

It's like
a wall!

It's like
a rope!

It's like a wall!

It's like a rope!

It'sIt's
likelike
a treetrunk!
a tree-trunk!

Fig. 2. Depiction of The Blind Men and the Elephant parable [113] illustrating that any complex subject can be studied in many ways.
It also symbolises that individual pieces of evidence may often be contradictory and insufficient to understand the bigger picture
without first being aggregated and grounded within a shared context.

is secondary to concerns arising from societal values such as ethics and trust. Importantly, making data-driven systems
understandable can instil confidence into the public as it allows the creators of such technologies to justify, control and
improve them as well as lead to new discoveries [3]. An appropriate and comprehensive explainability solution can
also become a technological springboard to reducing or eliminating bias [114, 115], unfairness [22, 66, 96] and lack of
accountability (to the benefit of robustness [2, 40], safety [6, 42] and security) from data-driven predictive models, thus
improving their trustworthiness [120].
5

EXPLANATION DIVERSITY AND MULTIPLICITY – WHAT TO EXPLAIN AND HOW TO EXPLAIN IT

So far we have primarily focused on explaining predictions and actions of intelligent systems since they are observable
and can be related to by a wide range of explainees regardless of their background. However, automated predictions
are just artefacts of a more elaborate artificial intelligence or machine learning predictive process, which manipulates
data to infer models that generalise well, thus are capable of predicting (previously unseen) instances [33]. Since
any element of this workflow can be opaque [127, 128], comprehensive explanations may need to consist of insights
pertaining to the entire predictive pipeline, discussing diverse topics such as data collection and (pre-)processing,
modelling caveats and assumptions, and the meaning and interpretation of predictions, all of which can be bundled
together in a shared user interface to provide a multi-faceted view of the investigated system [62, 63, 141].
Additionally, as each explanation may provide just a small, and quite possibly distorted, reflection of the true
behaviour of a data-driven model, achieving the desired level of transparency (and understanding) might require
communicating multiple, complementary insights for each unintelligible step or observation, which in turn bears the
danger of overwhelming and confusing the explainee. This multitude of explanatory information has to be navigated
carefully and can be understood as unique probing and inspection techniques that without a shared context may yield
competing or even contradictory evidence akin to the parable of The Blind Men and the Elephant [113] illustrated in
Figure 2. Note that this phenomenon is not unique to explainability; multiplicity of data-driven models all of whom
exhibit comparable predictive performance despite intrinsic differences, sometimes called the Rashomon effect of
statistics, is well documented [19, 31, 85, 132]. Furthermore, as AI and ML processes are directional – from data, through
models, to predictions – the latter components depend on the former, which also applies to their respective explanations.

18

Kacper Sokol and Peter Flach

For example, if data attributes are incomprehensible, explanations of models and predictions expressed in terms of
these features will also be opaque.
Explaining data may be challenging without any modelling assumptions, hence there may not necessarily exist a
pure data explanation method beyond simple summary statistics (e.g., class ratio or per-class feature distribution) and
descriptors (e.g., “the classes are balanced”, “the data are bimodal” or “these features are highly correlated”). Note that
the former simply state well-defined properties and may not be considered explanations, whereas the latter can be
contrastive and lead to understanding. Importantly, data are already a model – they express a (subjective and partial)
view of a phenomenon and come with certain assumptions, measurement errors or even embedded cultural biases (e.g.,
“How much is a lot?”). Data statements [12], data sheets [37] and nutrition labels [49] attempt to address such concerns
by capturing these (often implicit) assumptions. As a form of data explanations, they characterise important aspects of
data and their collection process in a coherent format, e.g., experimental setup, collection methodology (by whom and
for what purpose), pre-processing (cleaning and aggregation), privacy aspects, data ownership, and so on.
Explaining models in whole or in parts (e.g., specific sub-spaces or cohorts) should engender a general, truthful
and accurate understanding of their functioning. While some predictive systems may be inherently transparent, e.g.,
shallow decision trees, their simulatability [74] – the explainee’s ability to simulate their decision process mentally in
vivo – may not produce understanding (see Section 2). Popular model explanations include feature importance [18, 31],
feature influence on predictions [34], presenting the model in cognitively-digestible portions [62, 119] and model
simplification [26] (e.g., mimicking its behaviour or a global surrogate). Since not all models operate directly on the
input features, an interpretable representation may be necessary to convey an explanation, e.g., a super-pixel segmentation
of an image [107]; alternatively, if the data are comprehensible, landmark exemplars can be used to explain the behaviour
of a model or its parts [54, 55].
Predictions are explained to communicate a rationale behind a

1.0

a range of diverse aspects concerning the model’s decisive process

0.8

can be provided to the explainee. For example, the user may be
interested in feature importance [107], feature influence [79], relevant data examples [53] and training instances [60], or contrastive
statements [105, 136], to name a few. Note that while some of
these explanation types are similar to model explanations, here
they are explicitly generated with respect to a single data point

versicolor class probability

particular decision of a model. Depending on the explanation type,

PD
ICEs
ICE

0.6
0.4
0.2
0.0
1

2

3

4

5

6

petal length (cm)

and may not necessarily generalise beyond this particular case,
whereas for model explanations they convey similar information
for all data (i.e., the entire modelled space). A good example of this
duality is information communicated by Individual Conditional
Expectation (ICE [39]) and Partial Dependence (PD [34]), both of
which are feature influence explanations – the first with respect

Fig. 3. Explanation of a model predicting the probability
of the versicolor class when varying the petal length attribute for the Iris data set [32]. Individual Conditional
Expectation of a selected instance is plotted in red; the
orange curve is the Partial Dependence of the model computed by averaging all individual ICEs (displayed in grey).

to a single data point and the latter concerning a model – as shown in Figure 3. Akin to model explanations, the
information can be conveyed in the raw feature space or using an interpretable representation.
With such a diverse range (and possibly large quantity) of explanations, their presentation requirements – content,
delivery format, communication medium and provision protocol or mechanism [41, 123] – will naturally
vary [127, 128]. A simple approach to characterise an AI component is (statistical) summarisation – it is commonly used

Explainability Is in the Mind of the Beholder

19

for describing properties of data with numerical tables and vectors, which can be difficult to digest for non-experts.
Visualisation – a graphical representation of a phenomenon – is a more advanced, insightful and flexible analytical
tool. Static figures communicate information in one direction, akin to summarisation; however, creating interactive
plots can facilitate a “dialogue” with an explainee, thereby catering to a more diverse audience. Visualisations are
often supported by short narratives in the form of captions, which increase their informativeness. Textualisation – a
natural language description of a phenomenon – can express concepts of higher complexity and dimensionality than
plots, which can help to overcome the curse of dimensionality and the inherent limitations of the human visual system.
Communicating with text enables a true dialogue and has been shown to be more insightful and effective than presenting
raw, numerical and visual data [104], which can accompany the narrative to improve its expressiveness. A further
refinement of textualisation is formal argumentation [29] – a structured and logically-coherent dialogue accounting for
every disputable statement and giving the explainee an opportunity to contest the narrative, thus providing explanations
leading to understanding rather than informative descriptions. Finally, such explanatory processes can either be triggered
automatically, invoked (and driven) by the users or offered contextually whenever a need for a clarification is detected
by the explainer [41].
Thus far we have been mainly concerned with AI and ML explainability on a relatively abstract level, all of which
constitute just a small portion of XAI and IML research. In an ideal world, relevant publications would consider many
of the aforementioned factors and build their mechanisms around them, however it has only recently become a trend
and numerous early pieces of work lack such a reflection. To complement the viewpoint presented in the preceding
sections and bridge the theoretical (foundational and social) and technical (algorithmic and engineering) aspects of
explainers we briefly traverse through practical explainability research. Without aiming to be exhaustive – given
the availability of several comprehensive surveys [43, 73] – we finish this section by identifying a number of landmark
contributions that have influenced the entire research field. We also omit topics adjacent to explainability, such as
interactive exploratory user interfaces [50, 142], creative visualisations of explainability approaches [63] and systems
combining multiple explainability techniques within a single tool [141].
The most popular explainers are model-agnostic and post-hoc as they can be retrofitted into any predictive system (at
the expense of adding a modelling layer that may negatively impact explanation fidelity). These include RuleFit [35], Local
Interpretable Model-agnostic Explanations (LIME [107]), anchors [108], SHapley Additive exPlanations (SHAP [79]),
Black-box Explanations through Transparent Approximations (BETA [67, 68]), PD [34], ICE [39] and Permutation
Importance (PI [18]), among many others. Most of these methods operate directly on raw data, with the exception of
LIME and anchors, which use interpretable representations to improve intelligibility of explanations composed for
complex data domains such as text and images. Another attractive avenue of explainability research, which partly
overlaps with post-hoc methods, is opening up (deep) neural networks by designing tools and techniques specific to
these approaches or, more broadly, compatible with differentiable predictors. These models are notoriously opaque,
however their superior predictive performance for a wide spectrum of applications increases their popularity and
accelerates their proliferation [71]. Relevant explainability techniques include global surrogates [26], saliency maps [146],
influential training instances [60], counterfactuals [136] (which are surprisingly similar to the problematic adversarial
examples [40]), and influential high-level, human-intelligible insights based on Testing with Concept Activation
Vectors (TCAV [56]). An alternative XAI and IML research agenda concentrates on inherently explainable predictive
models, and ante-hoc explainers designed for popular data-driven systems. Examples of the former are generalised
additive models [78] and falling rule list [140]; whereas the latter include global and local explanations of naïve Bayes
classifiers [64], and clustering insights based on prominent exemplars and dominating features [55].

20
6

Kacper Sokol and Peter Flach
TOWARDS UNDERSTANDING FACILITATED THROUGH INTELLIGIBLE AND ROBUST EXPLAINERS

In this paper we explored the relatively recent and still evolving domains of artificial intelligence explainability and
machine learning interpretability. We introduced the main topics and provided the philosophical, theoretical and
technical background needed to appreciate the depth and complexity of this research. In particular, we highlighted
two different mental models: functional – enough understanding to operationalise a concept; and structural – indepth, theoretical appreciation of underlying processes. We further argued that the former – a shallow form of
understanding – aligns with The Chinese Room Argument [103, 116] and the notion of simulatability [74]. We also
reviewed diverse notions of explainability, interpretability, transparency, intelligibility and many other terms that are
often used interchangeably in the literature, and argued in favour of explainability. We defined this concept as (logical)
reasoning applied to transparent XAI and IML insights interpreted under specific background knowledge within a given
context – a process that engenders understanding in explainees. We used these observations to challenge the popular
view that decision trees are explainable just because they are transparent. Deep or wide trees lack interpretability,
which can be restored by applying a suitable form of logical reasoning – a prerequisite of explainability – undertaken
by either an algorithm or a human investigator.
While the most visible aspect of XAI and IML research is the technology that enables it, explainees – the recipients
of such explanations who tend to be humans – are just as important (and ought to be treated as first-class citizens)
since their understanding of the underlying predictive system and its behaviour determines the ultimate success of an
explainer. We explored this topic by looking at human-centred explainability and various desiderata that this concept
entails, in particular focusing on explicitly acknowledging presence of humans and projecting the explanations directly
at them. To this end, we pursued important insights from the social sciences that prescribe how to adapt machine
explainability to fulfil expectations of the explainees, hence achieve seamless explanatory interaction. The two crucial
observations in this space are: (i) a preference for (meaningful) contrastive explanations, which form the cornerstone of
human-centred explainability; and (ii) facilitating an interactive, dialogue-like, bi-directional explanatory process – akin
to a conversation – as opposed to delivering a one-off “take it or leave it” explanation to ensure coherence with people’s
expectations regardless of their background knowledge and prior experience with this sort of technology. Notably, the
explanation type and delivery medium should also be adapted to the circumstances. This is particularly important
when the audience is diverse as one predefined type of an explanation may be insufficient since it is unlikely to address
all the possible concerns, questions and unique perspectives. An XAI or IML explainer that communicates through
contrastive explanations and provides the explainees with an opportunity to interactively customise and personalise
them [80] – offering a chance to contest and rebut them in case of a disagreement – should therefore be considered the
gold standard [88, 136].
In addition to enhancing explainee satisfaction, operating within this purview has other, far-reaching benefits
such as enabling algorithmic fairness evaluation, accountability assessment and debugging of predictive models. It is
also compatible with all the elements of the artificial intelligence or machine learning workflow – which consists of
data, models and predictions – as each of these components may be in need of interpretability. In view of a variety of
explainability approaches, each operating in a unique way, we also looked at the disputed trade-off between explainability
and predictive power, the existence of which has mostly been supported by anecdotal evidence thus far, albeit recent
studies show that this dependency may be more nuanced than previously expected. We then connected this debate to
the distinction between inherent (ante-hoc) and retrofitted (post-hoc) explainability: the former provides explanations
of superior quality but requires extensive engineering effort to be built, whereas the latter is flexible and universal at the

Explainability Is in the Mind of the Beholder

21

expense of fidelity. While the former may be shunned due to the required work, we argued that building trustworthy
post-hoc explainers may be just as complicated and demand just as much commitment since these seemingly easy
to use tools conceal a complex process governing their composition and influencing their quality behind the facade
of universality [124, 126, 130, 131]. This considerable effort required to set them up, therefore, illuminates a crucial
question: Is it better to spend time and effort on configuring post-hoc explainers or instead invest these resources into
building inherently explainable predictive models? Unsurprisingly, there is no definitive answer given the uniqueness
of each individual case, e.g., legacy systems and predictors built from scratch.
Regardless of the particular implementation and operationalisation details, explainers of automated decision-making
systems should adopt and embody as many of these findings as possible to engender trust in data-driven predictive
models. Since each explanation reveals just a fragment of the modelling process and only the right mixture of evidence
can paint the full picture, XAI and IML approaches need to be responsive and adapt seamlessly to the users’ requests
and expectations. Such an engaging algorithmic interlocutor should build logically consistent narratives and serve
more as a guide and a teacher than a facts reporter. To this end, we need to develop an explanatory process built on
top of a system that enables logical reasoning between intelligent agents: human–machine or machine–machine. An
appropriate foundation – managing the dialogue as well as tracking and storing the evolving knowledge base of the
involved parties – should benefit and encourage an interdisciplinary research agenda drawing from multiple areas of
computer and social sciences. In the end, nonetheless, the explainee needs to be a savvy interrogator, asking the right
questions and firmly navigating the entire process to understand the behaviour of such data-driven oracles. After all, in
Arthur C. Clarke’s words: “Any sufficiently advanced technology is indistinguishable from magic.” While this view may
partially reflect a broader perception of artificial intelligence and machine learning applications, the work presented
here reconciles XAI and IML research published to date to establish a solid foundation for addressing open questions
in an effort to demystify predictive algorithms and harness their full potential. The logical next step in this pursuit
is development of a comprehensive framework, flexible protocol and suite of (quantitative & qualitative) metrics to
meaningfully evaluate the quality and effectiveness of explainable AI and interpretable ML techniques, allowing us to
choose the best possible solution for each unique problem.
ACKNOWLEDGEMENTS
This research was partially supported by the TAILOR project, funded by EU Horizon 2020 research and innovation
programme under GA No 952215; and the ARC Centre of Excellence for Automated Decision-Making and Society,
funded by the Australian Government through the Australian Research Council (project number CE200100005).
AUTHOR CONTRIBUTIONS
Conceptualisation, K.S.; Methodology, K.S.; Investigation, K.S.; Writing – Original Draft, K.S.; Writing – Review &
Editing, K.S. and P.F.; Supervision, P.F.; Funding Acquisition, P.F.
DECLARATION OF INTERESTS
The authors declare no competing interests.
REFERENCES
[1] Peter Achinstein. 1983. The nature of explanation. Oxford University Press on Demand.
[2] Evan Ackerman. 2019. Three small stickers in intersection can cause Tesla autopilot to swerve into wrong lane. IEEE Spectrum, April 1 (2019).

22

Kacper Sokol and Peter Flach

[3] Amina Adadi and Mohammed Berrada. 2018. Peeking inside the black-box: A survey on explainable artificial intelligence (XAI). IEEE access 6
(2018), 52138–52160.
[4] Divyakant Agrawal, Philip Bernstein, Bertino Elisa, Davidson Susan, Dayal Umeshwar, Michael Franklin, and Y Papakonstantinou. 2012. Challenges
and Opportunities with Big Data: A white paper prepared for the Computing Community Consortium. Committee of the Computing Research
Association (2012).
[5] David Alvarez-Melis, Hal Daumé III, Jennifer Wortman Vaughan, and Hanna Wallach. 2019. Weight of evidence as a basis for human-oriented
explanations. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems
(NeurIPS 2019), Vancouver, Canada (2019). arXiv:1910.13503
[6] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias. ProPublica, May 23 (2016), 2016.
[7] Abdallah Arioua and Madalina Croitoru. 2015. Formalizing explanatory dialogues. In International Conference on Scalable Uncertainty Management.
Springer, 282–297.
[8] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio
Gil-López, Daniel Molina, Richard Benjamins, et al. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and
challenges toward responsible AI. Information fusion 58 (2020), 82–115.
[9] Roy F Baumeister and Leonard S Newman. 1994. Self-regulation of cognitive inference and decision processes. Personality and Social Psychology
Bulletin 20, 1 (1994), 3–19.
[10] Boris Beizer. 1995. Black-box testing: Techniques for functional testing of software and systems. John Wiley & Sons, Inc.
[11] Andrew Bell, Ian Solano-Kamaiko, Oded Nov, and Julia Stoyanovich. 2022. It’s Just Not That Simple: An Empirical Study of the AccuracyExplainability Trade-off in Machine Learning for Public Policy. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 248–266.
[12] Emily M Bender and Batya Friedman. 2018. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better
Science. Transactions of the Association for Computational Linguistics 6 (2018), 587–604. https://doi.org/10.1162/tacl_a_00041
[13] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José MF Moura, and Peter
Eckersley. 2020. Explainable machine learning in deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.
648–657.
[14] Adrien Bibal and Benoît Frénay. 2016. Interpretability of machine learning models and representations: An introduction.. In ESANN.
[15] Or Biran and Courtenay Cotton. 2017. Explanation and justification in machine learning: A survey. In Proceedings of the IJCAI Workshop on
eXplainable Artificial Intelligence (XAI 2017), Melbourne, Australia.
[16] Or Biran and Kathleen McKeown. 2014. Justification narratives for individual classifications. In Proceedings of the AutoML workshop at ICML,
Vol. 2014. 1–7.
[17] Dimitri Bohlender and Maximilian A Köhl. 2019. Towards a Characterization of Explainable Systems. GI-Dagstuhl Seminar 19023 Explainable
Software for Cyber-Phyisical Systems (ES4CPS) (2019). arXiv:1902.03096
[18] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32.
[19] Leo Breiman. 2001. Statistical modeling: The two cultures. Statist. Sci. 16, 3 (2001), 199–231.
[20] Oliver Buchholz. 2022. A Means-End Account of Explainable Artificial Intelligence. (2022). arXiv:2208.04638
[21] Mario Bunge. 1963. A general black box theory. Philosophy of Science 30, 4 (1963), 346–358.
[22] Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings
of the 1st Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research, Vol. 81), Sorelle A. Friedler and Christo
Wilson (Eds.). PMLR, New York, NY, USA, 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html
[23] Pete Chapman, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin Shearer, Rudiger Wirth, et al. 2000. CRISP-DM 1.0:
Step-by-step data mining guide. SPSS inc 9 (2000), 13.
[24] Chacha Chen, Shi Feng, Amit Sharma, and Chenhao Tan. 2022. Machine Explanations and Human Understanding. (2022). arXiv:2202.04092
[25] Chaofan Chen, Kangcheng Lin, Cynthia Rudin, Yaron Shaposhnik, Sijia Wang, and Tong Wang. 2018. An interpretable model with globally consistent
explanations for credit risk. 2018 Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability,
Accuracy, and Privacy at the 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada (2018). arXiv:1811.12615
[26] Mark Craven and Jude W Shavlik. 1996. Extracting tree-structured representations of trained networks. In Advances in neural information processing
systems. 24–30.
[27] Richard Dawkins. 2011. The tyranny of the discontinuous mind. New Statesman 19 (2011), 54–57.
[28] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. (2017). arXiv:1702.08608
[29] Phan Minh Dung, Robert A Kowalski, and Francesca Toni. 2009. Assumption-based argumentation. In Argumentation in artificial intelligence.
Springer, 199–218.
[30] Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth. 1996. From data mining to knowledge discovery in databases. AI magazine 17, 3
(1996), 37–37.
[31] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by
Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research 20, 177 (2019), 1–81.
[32] Ronald A Fisher. 1936. The use of multiple measurements in taxonomic problems. Annals of eugenics 7, 2 (1936), 179–188.
[33] Peter Flach. 2012. Machine Learning: The art and science of algorithms that make sense of data. Cambridge University Press.

Explainability Is in the Mind of the Beholder

23

[34] Jerome H Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of statistics (2001), 1189–1232.
[35] Jerome H Friedman, Bogdan E Popescu, et al. 2008. Predictive learning via rule ensembles. The Annals of Applied Statistics 2, 3 (2008), 916–954.
[36] Johannes Fürnkranz, Tomáš Kliegr, and Heiko Paulheim. 2020. On cognitive preferences and the plausibility of rule-based models. Machine
Learning 109, 4 (2020), 853–898.
[37] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2018.
Datasheets for Datasets. 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2018) at the 35th International
Conference on Machine Learning (ICML 2018), Stockholm, Sweden (2018). arXiv:1803.09010
[38] Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining Explanations: An Overview of
Interpretability of Machine Learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). IEEE, 80–89.
[39] Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. Peeking inside the black box: Visualizing statistical learning with plots of
individual conditional expectation. Journal of Computational and Graphical Statistics 24, 1 (2015), 44–65.
[40] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. Proceedings of the 3rd International
Conference on Learning Representations (ICLR), San Diego, California (2015). arXiv:1412.6572
[41] Shirley Gregor and Izak Benbasat. 1999. Explanations from intelligent systems: Theoretical foundations and implications for practice. MIS quarterly
(1999), 497–530.
[42] Adam Grzywaczewski. 2017. Training AI for self-driving vehicles: The challenge of scale. NVIDIA Developer Blog, October (2017).
[43] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining
black box models. ACM computing surveys (CSUR) 51, 5 (2018), 1–42.
[44] David Gunning. 2016. Broad Agency Announcement, Explainable Artificial Intelligence (XAI). Technical Report. Defense Advanced Research Projects
Agency (DARPA). https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf
[45] David Gunning. 2017. Explainable Artificial Intelligence (XAI). Defense Advanced Research Projects Agency (DARPA) 2 (2017), 2.
[46] Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana-Amate, Alistair Nottle, and Alun Preece. 2019. A Systematic
Method to Understand Requirements for Explainable AI (XAI) Systems. In Proceedings of the IJCAI Workshop on eXplainable Artificial Intelligence
(XAI 2019), Macau, China.
[47] Lukas-Valentin Herm, Jonas Wanner, Franz Seubert, and Christian Janiesch. 2021. I Don’t Get It, but It seems Valid! The Connection between
Explainability and Comprehensibility in (X)AI Research. In European Conference on Information Systems (ECIS), Virtual Conference, AIS.
[48] Bernease Herman. 2017. The promise and peril of human evaluation for model interpretability. Interpretable ML Symposium at the 31st Conference
on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA (2017). arXiv:1711.07414
[49] Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The Dataset Nutrition Label: A Framework To Drive
Higher Data Quality Standards. (2018). arXiv:1805.03677
[50] Google Inc. 2015. TensorBoard: TensorFlow’s visualization toolkit. https://www.tensorflow.org/tensorboard
[51] Hessie Jones. 2018. Geoff Hinton Dismissed The Need For Explainable AI: 8 Experts Explain Why He’s Wrong.
[52] Harmanpreet Kaur, Eytan Adar, Eric Gilbert, and Cliff Lampe. 2022. Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking
Theory. In 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT ’22). Association for Computing
Machinery, New York, NY, USA, 702–714. https://doi.org/10.1145/3531146.3533135
[53] Been Kim, Elena Glassman, Brittney Johnson, and Julie Shah. 2015. iBCM: Interactive Bayesian case model empowering humans via intuitive
interaction. MIT Libraries Technical Report: MIT-CSAIL-TR-2015-010 (2015).
[54] Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. 2016. Examples are not enough, learn to criticize! Criticism for interpretability. In Advances
in Neural Information Processing Systems. 2280–2288.
[55] Been Kim, Cynthia Rudin, and Julie A Shah. 2014. The Bayesian case model: A generative approach for case-based reasoning and prototype
classification. In Advances in Neural Information Processing Systems. 1952–1960.
[56] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres. 2018. Interpretability beyond feature
attribution: Quantitative testing with concept activation vectors (TCAV). In International conference on machine learning. 2668–2677.
[57] Mi-Young Kim, Shahin Atakishiyev, Housam Khalifa Bashier Babiker, Nawshad Farruque, Randy Goebel, Osmar R Zaïane, Mohammad-Hossein
Motallebi, Juliano Rabelo, Talat Syed, Hengshuai Yao, et al. 2021. A multi-component framework for the analysis and design of explainable artificial
intelligence. Machine Learning and Knowledge Extraction 3, 4 (2021), 900–921.
[58] Alexandra Kirsch. 2017. Explain to whom? Putting the User in the Center of Explainable AI. In Proceedings of the First International Workshop on
Comprehensibility and Explanation in AI and ML 2017 colocated with 16th International Conference of the Italian Association for Artificial Intelligence
(AI*IA 2017), Bari, Italy.
[59] Derek J Koehler. 1991. Explanation, imagination, and confidence in judgment. Psychological bulletin 110, 3 (1991), 499.
[60] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings of the 34th International
Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, International
Convention Centre, Sydney, Australia, 1885–1894. http://proceedings.mlr.press/v70/koh17a.html
[61] Antti Koura. 1988. An approach to why-questions. Synthese 74, 2 (1988), 191–206.
[62] Josua Krause, Adam Perer, and Enrico Bertini. 2016. Using visual analytics to interpret predictive machine learning models. Workshop on Human
Interpretability in Machine Learning (WHI 2016) at the 33rd International Conference on Machine Learning (ICML 2016), New York, New York (2016).

24

Kacper Sokol and Peter Flach

arXiv:1606.05685
[63] Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with predictions: Visual inspection of black-box machine learning models. In
Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 5686–5697.
[64] Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015. Principles of explanatory debugging to personalize interactive
machine learning. In Proceedings of the 20th International Conference on Intelligent User Interfaces. ACM, 126–137.
[65] Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng-Keen Wong. 2013. Too much, too little, or just right? Ways
explanations impact end users’ mental models. In Visual Languages and Human-Centric Computing (VL/HCC), 2013 IEEE Symposium on. IEEE, 3–10.
[66] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual fairness. In Advances in Neural Information Processing Systems.
4066–4076.
[67] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable decision sets: A joint framework for description and prediction. In
Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1675–1684.
[68] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2017. Interpretable & explorable approximations of black box models. 4th
Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2017) at the 23rd SIGKDD conference on Knowledge Discovery
and Data Mining (KDD 2017), Halifax, Nova Scotia, Canada (2017). arXiv:1707.01154
[69] Ellen J Langer, Arthur Blank, and Benzion Chanowitz. 1978. The mindlessness of ostensibly thoughtful action: The role of “placebic” information
in interpersonal interaction. Journal of personality and social psychology 36, 6 (1978), 635.
[70] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What do we
want from Explainable Artificial Intelligence (XAI)? – A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI
research. Artificial Intelligence 296 (2021), 103473.
[71] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436.
[72] Cornelius T Leondes. 2001. Expert systems: The technology of knowledge management and decision making for the 21st century. Elsevier.
[73] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. 2021. Explainable AI: A Review of Machine Learning Interpretability Methods.
Entropy 23, 1 (2021), 18.
[74] Zachary C. Lipton. 2018. The Mythos of Model Interpretability. Commun. ACM 16, 3, Article 30 (jun 2018), 27 pages. https://doi.org/10.1145/
3236386.3241340
[75] Tania Lombrozo. 2006. The structure and function of explanations. Trends in cognitive sciences 10, 10 (2006), 464–470.
[76] Tania Lombrozo and Nicholas Z Gwynne. 2014. Explanation and inference: Mechanistic and functional explanations guide property generalization.
Frontiers in Human Neuroscience 8 (2014), 700.
[77] Tania Lombrozo and Daniel Wilkenfeld. 2019. Mechanistic versus functional understanding. Varieties of understanding: New perspectives from
philosophy, psychology, and theology (2019), 209–229.
[78] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. Accurate intelligible models with pairwise interactions. In Proceedings of the 19th
ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 623–631.
[79] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems
30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 4765–4774.
[80] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. 2019. A Grounded Interaction Protocol for Explainable Artificial Intelligence. In
Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents
and Multiagent Systems, 1033–1041.
[81] Ričards Marcinkevičs and Julia E Vogt. 2020. Interpretability and explainability: A machine learning zoo mini-tour. (2020). arXiv:2012.01805
[82] Aniek F Markus, Jan A Kors, and Peter R Rijnbeek. 2021. The role of explainability in creating trustworthy artificial intelligence for health care: A
comprehensive survey of the terminology, design choices, and evaluation strategies. Journal of Biomedical Informatics 113 (2021), 103655.
[83] David Marr. 1982. Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. The MIT Press.
[84] Fernando Martínez-Plumed, Lidia Contreras-Ochando, Cèsar Ferri, José Hernández Orallo, Meelis Kull, Nicolas Lachiche, Maréa José Ramírez
Quintana, and Peter A Flach. 2019. CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories. IEEE Transactions on
Knowledge and Data Engineering (2019).
[85] Charles Marx, Flavio Calmon, and Berk Ustun. 2020. Predictive multiplicity in classification. In International Conference on Machine Learning.
PMLR, 6765–6774.
[86] R Mead. 2015. When a teacher’s job depends on a child’s test. The New Yorker (2015).
[87] Tim Miller. 2019. “But why?” Understanding explainable artificial intelligence. XRDS: Crossroads, The ACM Magazine for Students 25, 3 (2019),
20–25.
[88] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267 (2019), 1–38.
[89] Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and
love the social and behavioural sciences. In Proceedings of the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2017), Melbourne, Australia.
arXiv:1712.00547
[90] Brent Mittelstadt, Chris Russell, and Sandra Wachter. 2019. Explaining explanations in AI. In Proceedings of the conference on fairness, accountability,
and transparency. 279–288.

Explainability Is in the Mind of the Beholder

25

[91] Sina Mohseni, Niloofar Zarei, and Eric D Ragan. 2021. A multidisciplinary survey and framework for design and evaluation of explainable AI
systems. ACM Transactions on Interactive Intelligent Systems (TiiS) 11, 3-4 (2021), 1–45.
[92] Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. 2018. Methods for interpreting and understanding deep neural networks. Digital
signal processing 73 (2018), 1–15.
[93] W James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. 2019. Definitions, methods, and applications in interpretable
machine learning. Proceedings of the National Academy of Sciences 116, 44 (2019), 22071–22080.
[94] Peter Norvig. 2017. Google’s approach to artificial intelligence and machine learning.
[95] Fabian Offert. 2017. “I know it when I see it”. Visualization and Intuitive Interpretability. 2017 Symposium on Interpretable Machine Learning at the
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, California (2017). arXiv:1711.08042
[96] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social data: Biases, methodological pitfalls, and ethical boundaries.
Frontiers in Big Data 2 (2019), 13.
[97] Cathy O’neil. 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Crown.
[98] Andrés Páez. 2019. The pragmatic turn in explainable artificial intelligence (XAI). Minds and Machines 29, 3 (2019), 441–459.
[99] Sebastian Palacio, Adriano Lucieri, Mohsin Munir, Sheraz Ahmed, Jörn Hees, and Andreas Dengel. 2021. XAI handbook: Towards a unified
framework for explainable AI. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3766–3775.
[100] Andrea Papenmeier, Dagmar Kern, Gwenn Englebienne, and Christin Seifert. 2022. It’s Complicated: The Relationship between User Trust, Model
Accuracy and Explanations in AI. ACM Transactions on Computer-Human Interaction (TOCHI) 29, 4 (2022), 1–33.
[101] Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal inference in statistics: A primer. John Wiley & Sons.
[102] Judea Pearl and Dana Mackenzie. 2018. The book of why: The new science of cause and effect. Basic Books.
[103] Roger Penrose. 1989. The Emperor’s New Mind: Concerning Computers, Minds, and the Laws of Physics. Oxford University Press.
[104] François Portet, Ehud Reiter, Albert Gatt, Jim Hunter, Somayajulu Sripada, Yvonne Freer, and Cindy Sykes. 2009. Automatic generation of textual
summaries from neonatal intensive care data. Artificial Intelligence 173, 7 (2009), 789–816.
[105] Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. 2020. FACE: Feasible and actionable counterfactual explanations.
In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 344–350.
[106] Alun Preece, Dan Harborne, Dave Braines, Richard Tomsett, and Supriyo Chakraborty. 2018. Stakeholders in explainable AI. Proceedings of the
AAAI Fall Symposium on Artificial Intelligence in Government and Public Sector, Arlington, Virginia, USA (2018). arXiv:1810.00184
[107] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016.
1135–1144.
[108] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision model-agnostic explanations. In Thirty-Second AAAI
Conference on Artificial Intelligence.
[109] Ribana Roscher, Bastian Bohn, Marco F Duarte, and Jochen Garcke. 2020. Explainable machine learning for scientific insights and discoveries. IEEE
Access 8 (2020), 42200–42216.
[110] Avi Rosenfeld and Ariella Richardson. 2019. Explainability in human–agent systems. Autonomous Agents and Multi-Agent Systems 33, 6 (2019),
673–705.
[111] Leonid Rozenblit and Frank Keil. 2002. The misunderstood limits of folk science: An illusion of explanatory depth. Cognitive science 26, 5 (2002),
521–562.
[112] Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature
Machine Intelligence 1, 5 (2019), 206–215.
[113] John G Saxe. 2016. The blind men and the elephant. Enrich Spot Limited.
[114] Nripsuta Ani Saxena. 2019. Perceptions of Fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 537–538.
[115] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C Parkes, and Yang Liu. 2019. How do fairness definitions fare?
Examining public attitudes towards algorithmic definitions of fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society.
99–106.
[116] John R Searle. 1980. Minds, brains, and programs. Behavioral and brain sciences 3, 3 (1980), 417–424.
[117] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai,
Adrian Bolton, et al. 2017. Mastering the game of go without human knowledge. nature 550, 7676 (2017), 354–359.
[118] Tom Simonite. 2019. Google’s AI guru wants computers to think more like brains.
[119] Daniel Smilkov, Nikhil Thorat, Charles Nicholson, Emily Reif, Fernanda B Viégas, and Martin Wattenberg. 2016. Embedding projector: Interactive
visualization and interpretation of embeddings. Workshop on Interpretable Machine Learning in Complex Systems at the 30th Conference on Neural
Information Processing Systems (NIPS 2016), Barcelona, Spain (2016). arXiv:1611.05469
[120] Kacper Sokol. 2019. Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models. In Proceedings
of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 541–542.
[121] Kacper Sokol. 2021. Towards intelligible and robust surrogate explainers: A decision tree perspective. Ph. D. Dissertation. University of Bristol.
[122] Kacper Sokol and Peter Flach. 2019. Desiderata for Interpretability: Explaining Decision Tree Predictions with Counterfactuals. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 33. 10035–10036.

26

Kacper Sokol and Peter Flach

[123] Kacper Sokol and Peter Flach. 2020. Explainability fact sheets: A framework for systematic assessment of explainable approaches. In Proceedings of
the 2020 Conference on Fairness, Accountability, and Transparency. 56–67.
[124] Kacper Sokol and Peter Flach. 2020. LIMEtree: Interactively Customisable Explanations Based on Local Surrogate Multi-output Regression Trees.
(2020). arXiv:2005.01427
[125] Kacper Sokol and Peter Flach. 2020. One explanation does not fit all. KI-Künstliche Intelligenz (2020), 1–16.
[126] Kacper Sokol and Peter Flach. 2020. Towards Faithful and Meaningful Interpretable Representations. (2020). arXiv:2008.07007
[127] Kacper Sokol and Peter A Flach. 2017. The Role of Textualisation and Argumentation in Understanding the Machine Learning Process. In IJCAI.
5211–5212.
[128] Kacper Sokol and Peter A Flach. 2017. The Role of Textualisation and Argumentation in Understanding the Machine Learning Process: A position
paper. In Automated Reasoning Workshop. 11–12.
[129] Kacper Sokol and Peter A Flach. 2018. Glass-Box: Explaining AI Decisions With Counterfactual Statements Through Conversation With a
Voice-enabled Virtual Assistant. In IJCAI. 5868–5870.
[130] Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. 2019. bLIMEy: Surrogate Prediction Explanations Beyond LIME.
2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019),
Vancouver, Canada (2019). arXiv:1910.13016
[131] Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and Peter Flach. 2020. What and How of Machine Learning Transparency: Building
Bespoke Explainability Tools with Interoperable Algorithmic Components. Hands-on Tutorial at The European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), Ghent, Belgium (2020). https://events.fat-forensics.org/2020_ecml-pkdd
[132] Kacper Sokol, Meelis Kull, Jeffrey Chan, and Flora Dilys Salim. 2022. Ethical and Fairness Implications of Model Multiplicity. (2022). arXiv:2203.07139
[133] Ilia Stepin, Jose M Alonso, Alejandro Catala, and Martín Pereira-Fariña. 2021. A survey of contrastive and counterfactual explanation generation
methods for explainable artificial intelligence. IEEE Access 9 (2021), 11974–12001.
[134] Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu Brinkhuis, and Mark Neerincx. 2018. Contrastive explanations with local foil
trees. Workshop on Human Interpretability in Machine Learning (WHI 2018) at the 35th International Conference on Machine Learning (ICML 2018),
Stockholm, Sweden (2018). arXiv:1806.07470
[135] Giulia Vilone and Luca Longo. 2020. Explainable artificial intelligence: A systematic review. (2020). arXiv:2006.00093
[136] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual Explanations without Opening the Black Box: Automated Decisions
and the GPDR. Harv. JL & Tech. 31 (2017), 841.
[137] Douglas Walton. 2007. Dialogical Models of Explanation. ExaCt 2007 (2007), 1–9.
[138] Douglas Walton. 2011. A dialogue system specification for explanation. Synthese 182, 3 (2011), 349–374.
[139] Douglas Walton. 2016. A dialogue system for evaluating explanations. In Argument Evaluation and Evidence. Springer, 69–116.
[140] Fulton Wang and Cynthia Rudin. 2015. Falling rule lists. In Artificial Intelligence and Statistics. 1013–1022.
[141] Daniel S Weld and Gagan Bansal. 2019. The challenge of crafting intelligible intelligence. Commun. ACM 62, 6 (2019), 70–79.
[142] James Wexler. 2017. Facets: An open source visualization tool for machine learning training data. Google Open Source Blog (2017).
[143] James Woodward. 1979. Scientific Explanation. British Journal for the Philosophy of Science 30, 1 (1979), 41–67. https://doi.org/10.1093/bjps/30.1.41
[144] Yiheng Yao. 2021. Explanatory Pluralism in Explainable AI. In International Cross-Domain Conference for Machine Learning and Knowledge Extraction.
Springer, 275–292.
[145] Yujia Zhang, Kuangyan Song, Yiming Sun, Sarah Tan, and Madeleine Udell. 2019. “Why Should You Trust My Explanation?” Understanding
Uncertainty in LIME Explanations. AI for Social Good Workshop at the 36th International Conference on Machine Learning (ICML 2019), Long Beach,
California (2019). arXiv:1904.12991
[146] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. 2017. Visualizing deep neural network decisions: Prediction difference
analysis. Proceedings of the 5th International Conference on Learning Representations (ICLR), Toulon, France (2017). arXiv:1702.04595 https:
//openreview.net/forum?id=BJ5UeU9xx

