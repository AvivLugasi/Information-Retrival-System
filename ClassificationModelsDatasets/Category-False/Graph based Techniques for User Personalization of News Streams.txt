Graph based Techniques for User Personalization of News
Streams
Saurabh Kumar

Mayank Kulkarni

Dept. of Computer Engineering
College of Engineering, Pune
Pune, Maharashtra, India

Dept. of Computer Engineering
College of Engineering, Pune
Pune, Maharashtra, India

kumarsa10.comp@coep.ac.in
ABSTRACT
In this paper, we address the problem of user personalization
and recommendation of news streams. This involves ‘learning’ from past user behaviour, such as the articles she read or
did not read and accurately predicting new articles which she
would be most likely to read. Our contribution in this paper
is the development of a new algorithm for news personalization using an adaptation of the classical nearest neighbour
algorithm coupled with a knowledge graph which we create.
This algorithm provides a powerful tool for user behaviour
analysis as we demonstrate in subsequent sections. Using
implicit user data like the articles that were read as well as
the articles that weren’t along with their position and distance in the graph, we rank new articles on the basis of the
predicted interest of the user in the content of that article.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Search Process

General Terms
Algorithms

Keywords
User Personalization, User Recommendation

1.

INTRODUCTION

News streams and other content in general such as web
results, shopping results, etc. generally tended to use a ‘one
size fits all’ approach in which all users were shown the same
list of items based on a common query. However, with an
increase in the amount of data present on the web, the need
for personalization has grown. People’s interests are varied
and even hierarchical requiring personalization to be conducted on a finer scale. For example, We could consider the
category of ‘Sports’ of interest to the reader - now the reader

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
COMPUTE”13 Aug 22-24, Vellore, Tamil Nadu, India
Copyright 2013 ACM 978-1-4503-2545-5/13/08 ...$15.00.

kulkarnimr10.comp@coep.ac.in
will not read every article in this category. She may skip to
‘Basketball’ and even in that she may choose to read only
about her favourite team. The primary benefit of personalization is that content is ranked in the order of interest of the
user; people no longer have to search through floods of data
and items for the things they are most interested in. This
ordering is already done for them via the personalization
and recommendation engines.

2.

BACKGROUND AND RELATED WORK

Recommendation algorithms have long been an active area
of research. All of them require basic input from the user
regarding her likes and dislikes. Based on the method of
gathering, feedback is classified as explicit or implicit. Explicit feedback includes that data explicitly gathered from
the user such as asking a user to rate some content on the
basis of a scale, choosing one item over another, etc. A comparison of the various methods used for explicit feedback can
be found in [8]. Implicit feedback on the other hand relies
on data obtained without explicitly asking the user for it.
This includes gathering information about the items a user
views, clicks on, etc. Based on the characteristics of these
algorithms, they can broadly be divided into three classes Collaborative filtering, Content based filtering or a hybrid
approach involving a combination of the two. Collaborative
algorithms, recommend items to users based on the interests and likes of other ‘similar’ users [2, 7, 9]. Quantifying
user similarity and interests as this approach requires has
received much attention and led to the development of various algorithms based on explicit as well as implicit feedback. Hu et al. [5] develop a model for analyzing implicit
feedback datasets and making recommendations. The main
drawbacks of collaborative filtering are the requirement of a
large amount of feedback data and the sparsity of user feedback (only a small subset of the total items may have been
rated by the users). Content based filtering uses a user’s past
history to recommend items. These techniques recommend
items similar to items which the user may have liked (given
positive feedback about) in the past. Items are analyzed for
characteristic features and a set of liked and disliked characteristics are built for the user. Using this set, new items are
recommended. Content based techniques often involve some
form of machine learning in which a classifier is trained with
past user interest data to classify new items, for example,
Van Meteren and Van Someren developed the PRES system
for learning user profiles using the vector space model for
modeling documents and cosine similarity to identify similar
and relevant documents [10]. A hybrid approach combines

the previous two approaches. Billsus and Pazzani [1] developed a framework for news recommendations modeling both
short term and long term user interests. For short term interests, each article was converted into a TF-IDF vector and
cosine similarity was used to quantify similarity between articles. The nearest neighbor algorithm was used to compare
new articles with articles which the user had already given
feedback about. Long term interests were modeled using a
naive bayes classifier in which terms with the top TF-IDF
score are used to train it. Liu et al. [6] describe a Bayesian
framework to predict user interests and apply their results
to Google News.

3.

OUR ALGORITHM

Our algorithm assigns to each article a score based on how
likely the user is to read that article given her past history.
In order to do this, we apply the concept of keyword based
tagging. Based on the tags extracted from this techniques,
we map them to nodes in the concept graph and calculate
their distance to previously read articles via the graph in
order to obtain an interest score of that article.

3.1

Behaviour of Algorithm

1. List of articles is displayed to the user: This is the
personalization stage. Before displaying the list of articles to
the user, the algorithm parses the text (title and summary)
of each and identifies keyword tags. The algorithm scores
each article based on its score as defined above. The articles
are displayed to the user in descending order of scores. Only
the article titles are displayed.
2. User Interacts with List of Articles: This is the
learning stage. The user is shown a set of news titles. If she
clicks on one of them, a brief summary is displayed. If the
user clicks again, she is taken to the full article. When the
algorithm detects that the user has clicked on a title or goes
to the full article, it captures the text of the title and summary and analyzes its keyword tags. The keyword vector
generated from the article title and summary is stored along
with its corresponding ‘interest’. We define this interest in
the form of an interest function in a later section.

3.2

Keyword Based Tagging

Every news article has a title describing the news content
and a summary of the most important information contained
in the article. Inutitively, a user makes a decision about
whether or not to read an article by reading the title or
headline and perhaps a few sentences of the accompanying summary. Therefore, the keywords present in the title
and description become the basis on which a user reads or
does not read an article. To each article we assign keywords
and our algorithm assumes that the user makes the decision on whether or not to read that article by analyzing the
keywords that we have independently identified within the
text. We choose the keywords by applying natural language
processing (NLP) to the title and description and extracting
all nouns, proper nouns and verbs. These become the set of
keywords for that article.
For example, consider the following text, appearing as the
first paragraph in the wikipedia entry for Google:
Google Inc. is an American multinational corporation specializing in Internet-related services and products. These
include search, cloud computing, software and online advertising technologies. Most of its profits derive from AdWords.

Based on the above text, the keywords which can be extracted include: Google Inc, American multinational corporation, Internet-related services, products, search, cloud
computing, software, online advertising technologies, AdWords.

3.3
3.3.1

Wikipedia Concept Graph
Disadvantages of Keyword Matches

Before introducing our concept graph, we give a brief explanation of how keyword based recommendation systems
are used and highlight their drawbacks. Simple keyword
based recommendation works as follows: For each article
whose feedback (implicit or explicit, positive or negative) is
obtained, a keyword vector is created. Thus, a set of keyword vectors is formed. These keywords could be extracted
from the text using a number of techniques such as NLP or
TF-IDF. When a new article is to be judged, that article’s
keyword vector is created and, using the nearest neighbor
algorithm, matched with the ‘feedback’ set to identify the
vector or set of vectors most similar to it. This matching
is generally done using some similarity metric such as cosine distance or jaccard distance. Then, based on the outcome of the nearest neighbors (positive or negative), the
predicted outcome of the new article is determined. However, this technique can be used to rank an article only if the
new article and the ‘feedback’ set contain some keywords in
common. This is not always the case and placing the condition that the recommended article must contain at least one
keyword about which feedback has been given before is too
restrictive and could lead to less relevant results. To be able
to go beyond simple keyword matches, we use new graph
based distance metric for quantifying the similarity between
articles.

3.3.2

Overview of the Concept Graph

Due to social collaboration, Wikipedia has developed into
a massive source of semi-structured information. By analyzing links between pages and categories, we can extract and
quantify semantic relationships between topics. Extracting
semantic content from Wikipedia has been studied previously as well, such as the RCRank proposed by Bu et al. [3]
for quantifying concept-concept and concept-category relatedness and the work of Chernov et al. in identifying semantic
relationships between categories by analyzing the wikipedia
link structure [4]. We consider each Wikipedia keyword to
be a concept, and we use the terms concept graph and keyword graph interchangeably. We build a graph of semantically related keywords using Wikipedia in order to gather
more information about a certain topic or ‘node’. To ensure
only the most frequented or most important relationships
are generated we parse only the introductory section of each
page to using keyword based tagging techniques to identify
the subsets or ‘children’ of the node. The keyword nodes
generated above are added to a graph. If multiple nodes
have a common tag, these nodes will now be ‘related’ via the
path through that common tag node. We use these paths to
identify the relationships and their strengths between nodes.
Each edge is assigned a weight based on co-occurrence of the
two connected nodes or common backlinks.

3.3.3

Building the Concept Graph

In order to build a graph of wikipedia (english version

General theory of relativity
Quantum physics

Quantum theory

Photoelectric effect
German born
Albert Einstein

Modern physics

Theoretical physics
Mass-energy equivalence
formula

Nobel prize in physics

Most famous equation Theoretical physicist

Figure 1: Portion of Graph based on Albert Einstein
only), we used the wikipedia API1 . We used the requests
HTTP library and extracted the keywords from the introductory section of each wikipedia article page from our sample set of 130,000 nodes on Heroku. We classified wikipedia
pages into three types of nodes:
1. Article node: These correspond to standard wikipedia
pages or extracted keywords. An article node is connected to other nodes corresponding to articles to which
this article links to within its introductory section or
which also contains the extracted keyword in its introductory section.
2. Category node: These correspond to wikipedia categories. A category node is connected to all article
nodes which belong to that category.
3. Disambiguation node: These correspond to wikipedia
disambiguation pages. A disambiguation node links to
all article nodes corresponding to possible interpretations.
Each page (article or disambiguation page) title, the keywords in its introductory section and its categories formed
nodes. We defined three types of relationships - “node-node”
which related to nodes which linked to each other, “nodecategory” which linked a node to its particular category and
“node-disambiguate” which linked disambiguation nodes to
their corresponding possible interpretations. The graph was
stored in the Neo4j graph database. We used the REST API
via the Py2neo package in order to update it. Building the
graph was set up as a job on the Heroku platform2 . For our
experimental analysis, we allowed the job to create around
130,000 nodes.
As an example, consider the first few lines in the introductory section on the page titled ‘Albert Einstein’ (Note
that in our algorithm, we consider the entire introductory
section. Here, for brevity we are only including the first few
lines).
“Albert Einstein was a German-born theoretical physicist
who developed the general theory of relativity, one of the two
1
2

http://en.wikipedia.org/w/api.php
http://www.heroku.com

pillars of modern physics (alongside quantum mechanics).
While best known for his mass-energy equivalence formula
E = mc2 (which has been dubbed “the world’s most famous
equation”), he received the 1921 Nobel Prize in Physics “for
his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect”. The latter was
pivotal in establishing quantum theory.”

3.4

Ranking Articles

We use the k-nearest neighbors classifier to identify the
articles which the user will want to read and those which she
will not. The distance function used is the graph distance,
i.e the distance between two vectors in the graph.

3.4.1

Graph Distance between Vectors

Consider two vectors U = {ui } and V = {vi } with dimensions r1 and r2 respectively, each consisting of multiple keywords. Given a concept graph G, our aim is to quantify
the average distance between U and V using G. We denote
the least cost distance between two keywords nodes n1 and
n2 in the graph G by d(n1 , n2 ). This distance is obtained
with the edge cost being the weight of the edge. We force
that the maximum depth is restricted to a constant τ . If
the least cost path exceeds that constant, we assume that
the two values are not (or are very weakly) related and set
the least cost distance to a constant γ corresponding to a
maximum dissimilarity. Therefore, if d(n1 , n2 ) > τ , then
d(n1 , n2 ) = γ. For our implementation, we set γ = 10 and
τ = 4.
Consider a mapping f between the elements of the vectors
U and V of the form f : u → v, and a mapping g of the form
g : v → u where u ∈ U and v ∈ V . Note that we cannot
force these mappings to be bijective, because it is possible
that U and V have a different number of dimensions. Based
on these mappings f and g, the distance between U and V
is given by:
P
P
d(v, g(v))
u∈U d(u, f (u))
+ v∈V
(1)
Df (U, V ) =
|U |
|V |
We define the graph distance D between U and V to be
the minimum value of the Df function over all possible f .
Hence, we have:
D(U, V ) = argmin Df (U, V )

(2)

f

We note that D(U, U ) = 0 and D(U, V ) = D(V, U ).

3.4.2

Scoring of New Articles

We store the keyword vectors of all articles that the user
has given feedback for (we refer to these as feedbacked articles). When a new article is to be scored, we use the k
nearest neighbor algorithm with the metric defined above
to identify the top k closest neighbors to that article and
compute a weighted score. We represent each article by its
keyword vector created via keyword based tagging as discussed previously. Based on the behaviour of the user, we
define an interest function for each article A displayed to the
user as follows:


0 if user does not access summary
interest(A) = α if user accesses only summary
(3)

β if user accesses entire article

Here, α and β can be experimentally set with the condition:
α < β. In our implementation, we used the values α = 1
and β = 5.
Suppose A is the keyword vector corresponding to the new
article to be scored and the set of k articles (in the form of
keyword vectors) nearest to the new article A by KN N (A)
computed using the graph distance function D(A, U ) for all
previously accessed articles U stored for the user (assuming
non-zero graph distances), we obtain the score of A as:
P
interest(U )
S(A) =

U ∈KN N (A)

P

D(A,U )
1
U ∈KN N (A) D(A,U )

(4)

Based on this score, the set of new articles are ordered in
descending order and the top T articles displayed to the
user. Similar to Billsus et al. [1], if an article has a graph
distance below a certain threshold with any of the articles
already read (indicating very high similarity), the algorithm
assumes that the article has already been read by the user
and multiplies its score by a value k << 1 to reduce its
importance. If an article has a graph distance of zero with
any read article it is ignored completely and its score is not
calculated.
Once these scores are calculated, they can be used to personalize news results for the user or they can be used to recommend new articles. For personalizing news results, the
articles can be sorted in decreasing order of scores. For
recommendations, if the absolute score of an article crosses
some threshold, it can be recommended to the user.

4.

WORD SENSE DISAMBIGUATION

Programmatically analyzing the English language and mapping words to their corresponding nodes in the graph presents
two challenges - polysemy and synonymy.
Polysemy: Polysemy is the event that one term can relate to more than one concept i.e have different meanings in
different contexts. For example, the word Apple - it could
mean either the fruit or the company. Deciding which meaning to take depends wholly on the context. Therefore, when
the word Apple co-occurs in the same sentence with a word
like say Microsoft, it is much more likely that Apple disambiguates to the company Apple Inc.
Synonymy: Synonymy is the event that one concept is
represented by more than one name. For example, the word
car can also be referred to as an automobile. Sometimes,
synonymy is based on differences in variants of a language the word color in American English and the word colour in
British English both mean the same thing.
Synonymy is relatively easier to handle. Wikipedia, through
its system of redirects, maintains only one page for the concept that is referred to by car. All other synonyms redirect
to that page. Therefore, by storing all the redirects to a particular page, we can obtain the synonym set to some degree
of that particular concept. We use a key value store to map
synonyms to the wikipedia designated root. Only the root
corresponds to a node in the graph. Therefore, when mapping a concept to a node in the graph, we first check the
synonym datastore to map the word to its corresponding
root. Then, the node is identified.
Polysemy is handled using the disambiguation nodes in
the graph. Generally, most words which exhibit polysemy
have a corresponding disambiguation node in the graph.
This node links to all possible senses of that word. In order

to select the correct sense, we measure the graph distance
(which we have defined previously) of each sense with the
remaining words in the given context. Finally, we choose
the sense which exhibits the smallest distance.
We use these two concepts in the following way: given a
set of keywords, we partition them into two sets - vague and
precise. The precise keywords correspond to those that have
exact matches to article nodes within the concept graph (not
disambiguation nodes). These form the ‘context’ of keyword
set. The vague set corresponds to those keywords that do
not have exact matches to article nodes within the concept
graph, or have matches to disambiguation nodes. For example, consider the simple keyword set: {Apple, Microsoft}. In
this case, the keyword Microsoft has an exact match in the
concept graph and therefore forms a precise node. On the
other hand, the keyword Apple does not match to any article node directly, instead, it matches to a disambiguation
node of the form Apple (Disambiguation). This node, as described previously, links to all possible meanings of the word
Apple such as Apple (fruit), Apple II, Apple Inc. and so on.
In order to disambiguate the word Apple, we calculate the
least cost path from each possible sense to the precise word
Microsoft. This technique appears to work very well for
disambiguation. For example, the graph distance between
Apple (Fruit) and Microsoft is much greater than the graph
distance between Apple Inc. and Microsoft. Therefore, it
can be concluded that in the sentence - “Apple releases app
for Microsoft Platform”, the word Apple indeed refers to the
company and not the fruit.

5.

EXPERIMENTAL SETUP

In the previous sections, we explained our algorithm and
feedback model. We developed a client server application in
order to implement our algorithm.We built an Android application based client which connected to a central server. In
this section, we describe the overall architecture of our system including the actual method we used to obtain implicit
feedback from the user:

5.1

Client

Our client consisted of an Android application written using the Android API in Java. The application stores no
information about the user, that data is maintained on the
server for more flexible storage and to avoid increasing memory utilization on the mobile device. When first started,
the application identifies itself to the server via a secret key
which the server then verifies. On successful verification,
the server sends the personalized article list to the android
application for display. Whenever a user clicks on an article
title for the first time, the client sends a notification to the
server and displays the summary. If the user clicks a second time, the client notifies the server again and redirects
the user to the corresponding article webpage on the CNN
website.

5.2

Server

We wrote a custom server using the Flask microframework
in python. We used the py2neo module to interface with
the wikipedia concept graph stored on a Neo4j store via the
REST API. We implemented the nearest neighbour algorithm on the server along with the graph metric in python.
Our data source was the CNN newsfeed available via its RSS

ID
1

2

3
4
5
6

7

Table 1: Sample of News Articles and Extracted Keywords
Article Text
Keyword Vector
Mozilla Launches ScienceLab To Mozilla, ScienceLab, Researchers,
Help Researchers Use The Open Open Web, Science, Future
Web To Shape Science’s Future
PRISM Aftermath: Facebook, Mi- PRISM Aftermath, Facebook, Microsoft release NSA stats to reas- crosoft, NSA
sure users
Path said to be seeking valuation of Path, valuation
$1 billion
Google’s Android faces EU probe Google, Android, EU, licensing
over licensing practices
practices
Google+ for Android updates, no- Google+, Android, updates, notifitifications get synced
cations
Google combines Drive, Gmail and Google, Drive, GMail, Google+
Google+ Photo storage into a com- Photo Storage
mon 15GB pool
Twitter releases official Windows 8 Twitter, Windows 8
app

API3 which is free for non-commercial use. The CNN newsfeed provides multiple sub-feeds such as Science, Technology,
Top News, Business, etc. We used this granularity to further
refine our algorithm by applying the nearest neighbour algorithm on each set of articles within a sub-feed separately.
The CNN newsfeed only allows the article title and short
summary to be retrieved which suited our purposes. When
the user used the application, he would be shown a list of
titles, clicking on a title would show him the corresponding
summary. Clicking on the link again would take him to the
actual CNN article as a CNN webpage. In order to parse the
title and short summary of each article, we used the NLTK
library. We tagged the text with part of speech tags and
then chunked the tags together based on a set of regular
expressions (such as sequences of nouns and adjectives). In
order to store the feedback data of each user, we use a redis
datastore. Redis is a NoSQL key value datastore which is
extremely fast and easy to use. For each user, we store the
keyword vector of all the articles she has given feedback for
along with the corresponding interest score.
When the server receives a notification from the client,
the server adds the appropriate article to the datastore along
with the corresponding interest score. The server repeatedly
queries the CNN API after a fixed interval. When a new set
of articles is fetched from the CNN API, the server computes
the similarity score of each article based on the articles for
that user present in the redis store. If there are no articles
similar to the ones stored or there are no articles stored,
the articles are sent directly to the client, sorted only by
timestamp. Otherwise, the articles are sorted by score.

5.3

Multiple News Streams

In our experimental setup, we used only a single news
stream - CNN. However, this setup can easily be extended to
include multiple news streams. However, the only problem
in such a scenario is the possibility of multiple articles describing the same event being included in the recommended
article list.
In order to handle multiple streams, we modularized our
recommendation engine in order to allow addition of differ3

http://edition.cnn.com/services/rss/

Interest
0

5

0
5
1
1

5

ent news or article streams. This type of modularization is
necessary because each news stream has its own API and
stream specific data storage format. For example, CNN and
BBC have different API formats. Therefore, we built independent modules for each stream and converted all text into
a common format for further processing (keyword extraction, etc.).
In order to check whether two articles correspond to the
same news story, we perform the following steps:
1. Extract the keywords from each of the two articles and
map them to nodes in the concept graph.
2. Identify the timestamps of each article. If the two articles co-occur in a fixed size window (whose size can be
set experimentally), only then consider the two articles
for further processing. This is based on the expectation that two articles corresponding to the same news
story will be fetched at around the same time. In our
testing, we considered the window of one day.
3. Finally, calculate the cosine distance between the two
articles. We do not use graph distance here because if
two articles represent the same news story, it is likely
that they would have the same keywords, in which case
cosine distance suffices. If the cosine distance between
two articles crosses a threshold, only one of the two
articles is taken in the list. This threshold can be determined experimentally.
In our experiments, we set the threshold to be 0.7. Any
pair of articles with cosine similarity above this threshold
were taken to be the same and only one was selected at random (note: a cosine similarity score of 1 indicates maximum
similarity).

6.

USAGE EXAMPLE

We demonstrate the basic working of this algorithm as
follows: Consider a set of articles (title and/or summary) as
depicted in table 1. The second column describes the keywords which were extracted from the text and which form
the keyword vector for that article and the third column

Google+

Facebook

GMail

Twitter
Nexus

Drive

Apple

Google

Windows
Windows 8

Path

Android

Blink

Social
Network

Skype

IOS
Google
Search

Microsoft
Office

Chrome
Mozilla

Figure 3: Performance of the Graph Distance function

Bing
Firefox

Internet Explorer

XBox

Figure 2: Keyword Subgraph
Table 2: Graph Distances of Article
ID Graph Distance
1
14.25
2
8
3
20
4
8.5
5
10.5
6
7
7
7

contains the interest value as determined by the user’s interaction with that article. This set of articles forms the
user’s past history. Figure 2 depicts a portion of the concept graph generated from wikipedia relevant to this example. Using the history of the user and the concept graph as
shown, we can predict whether a user will read a new article
or not by assigning it a score based on graph distance. We
set constants to the following values: k = 4, α = 1, β = 2,
γ = 10, τ = 4. For this explanation, we assume that all
edges in the graph have the same weights.
Consider an article with the text “Why Microsoft Is Buying Skype for $8.5 Billion” (keywords: Microsoft, Skype).
The graph distances for all articles in table 1 with this article is shown in table 2. This score quantifies the similarity
of the target article with each article the user has previously
accessed. We set k to 3 and select the top 4 nearest neighbours to the article - articles with IDs 2, 4, 6 and 7. Based
on these articles, the weighted rank of this article is 3.9185
as per equation 4. For each article fetched, such a score is
calculated denoting the likely interest the user has in the
article and the final ranking is done.

7.

EVALUATION

7.1

Performance

In this section, we evaluate our algorithm in terms of performance. Based on the system we have implemented, the
maximum amount of time taken is (as expected) in the graph

distance step. More specifically, evaluating the least cost distance between each pair of nodes in the two vectors is very
time consuming. For shortest path calculations, Dijkstra’s
algorithm is popular. However, the runtime depends on the
number of edges and vertices in the graph. In order to prevent the graph database from calculating least cost distances
across the entire graph (which in itself would be meaningless because the final graph distance would be too large to be
significant), we allow only nodes uptil a maximum distance
of τ .
The graph distance algorithm is dependent on the length
(dimensionality) of the two vectors to be compared. We
evaluated the graph distance by varying the length of the
two vectors and analyzed the time taken. This represents an
average of a number of queries run on the system. The exact
time will vary depending on the actual query. Our results
are depicted in figure 3. We note that the time taken by
the function becomes prohibitively high when dealing with
vectors with more than 5 or 6 elements. However, for articles
the number of extracted keywords generally is not that high.
Based on a sample set of more than 100 news articles, the
average number of keywords extracted was approximately 4
thereby leading to an average of 1.1s for calculation.
We note that because the only operation on the graph
database is a read operation, the task of evaluating graph
distance between different keyword pairs can be parallelized
across multiple machines or cores. We are currently experimenting with these options to improve performance.

7.2

Recall

We also evaluated how well our algorithm recommended
new articles using the standard recall measure defined in this
context as:
recall =

no. of clicked articles recommended by algorithm
total no. of clicked articles

We stored the articles accessed by a user within a particular
stream (in this case technology). We analyzed the recall
obtained via our algorithm as the number of accesses to the
application increased. We evaluated the recall as a function
of the number of accesses to the application and plotted
a graph as shown in figure 4. This graph represents the
average recall over time for a set of 5 users. Note that the
recall values are high because the articles are all taken from
a single broad topic of technology and the average number
of articles read per access was around 4 to 5.
These represent preliminary results indicating that our algorithm does work in recommending articles to users. We

Figure 4: Recall vs Access
have yet to perform a more general and comparative evaluation of our algorithm in which articles could be taken from
multiple fields as well.

8.

CONCLUSION AND FUTURE WORK

In this paper, we have developed an algorithm for user
personalization and recommendation using a concept graph.
We would like to perform a more rigorous evaluation of our
algorithm and this will form the basis of our future work.
Apart from this, we see three directions in which our algorithm can be developed further:
1. Temporal Aspects: Interests are not permanent. They
may change with time or depend on some currently happening event. For example, a person may be interested in
the Olympics only when the Olympics are actually taking
place. This temporal aspect of interest leads us to a direction for future work: we plan to investigate the role of
the time factor in our algorithm, such as perhaps creating
another factor per article called ‘freshness’ which indicates
how new the article is. We have yet test this however.
2. Generalization to other Areas: As another direction,
we also believe that news represents only one aspect of user
content to which this algorithm can be applied and we believe that it can be applied with possibly some modification
to other situations as well such as personalization of social
media streams like Twitter and Facebook, music and other
multimedia, bookmarks, etc. We plan to investigate the usefulness of this algorithm in those scenarios.
3. Increasing Efficiency: In order to improve response
times, we wish to investigate the effects of caching and parallelizing some of the graph distance operations on a cluster
of computers. The algorithm presented has the potential for
parallelization due to repeating read only operations on the
graph. We are currently experimenting with MapReduce
for parallelizing our algorithm possibly allowing it to scale
to larger vector sizes. Similarly, caching the graph distances
of frequently co-occurring keywords could also speed up the
procedure.

9.

ACKNOWLEDGEMENTS

We wish to thank the anonymous reviewers for their invaluable advice and suggestions.

10.

REFERENCES

[1] D. Billsus and M. J. Pazzani. User modeling for
adaptive news access. User modeling and user-adapted
interaction, 10(2-3):147–180, 2000.

[2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical
analysis of predictive algorithms for collaborative
filtering. Technical report, Microsoft Research, 1998.
[3] F. Bu, Y. Hao, and X. Zhu. Semantic relationship
discovery with wikipedia structure. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages
1770–1775. AAAI Press, 2011.
[4] S. Chernov, T. Iofciu, W. Nejdl, and X. Zhou.
Extracting semantic relationships between wikipedia
categories. In Proc. of Workshop on Semantic Wikis
(SemWiki 2006). Citeseer, 2006.
[5] Y. Hu, Y. Koren, and C. Volinsky. Collaborative
filtering for implicit feedback datasets. In Data
Mining, 2008. ICDM’08. Eighth IEEE International
Conference on, pages 263–272. IEEE, 2008.
[6] J. Liu, P. Dolan, and E. R. Pedersen. Personalized
news recommendation based on click behavior. In
Proceedings of the 15th international conference on
Intelligent user interfaces, pages 31–40. ACM, 2010.
[7] B. Marlin. Collaborative filtering: A machine learning
perspective. Master’s thesis, 2004.
[8] E. R. Nunez-Valdez, J. M. Cueva-Lovelle, O. Sanjuan,
C. E. Montenegro-Marin, and G. I. Hernandez. Social
voting techniques: A comparison of the methods used
for explicit feedback in recommendation systems. 2011.
[9] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.
Item-based collaborative filtering recommendation
algorithms. In Proceedings of the 10th international
conference on World Wide Web, pages 285–295. ACM,
2001.
[10] R. Van Meteren and M. Van Someren. Using
content-based filtering for recommendation. In
Proceedings of the Machine Learning in the New
Information Age: MLnet/ECML2000 Workshop, 2000.

