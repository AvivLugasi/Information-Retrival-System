1611.07509v1 [cs.LG] 22 Nov 2016

arXiv

A Causal Framework for Discovering and Removing
Direct and Indirect Discrimination

Lu Zhang, Yongkai Wu, and Xintao Wu

University of Arkansas
{1z006,yw009,xintaowu} @uark.edu

Abstract

Anti-discrimination is an increasingly important task in data
science. In this paper, we investigate the problem of discover-
ing both direct and indirect discrimination from the historical
data, and removing the discriminatory effects before the data
is used for predictive analysis (e.g., building classifiers). We
make use of the causal network to capture the causal structure
of the data. Then we model direct and indirect discrimination
as the path-specific effects, which explicitly distinguish the
two types of discrimination as the causal effects transmitted
along different paths in the network. Based on that, we pro-
pose an effective algorithm for discovering direct and indirect
discrimination, as well as an algorithm for precisely removing
both types of discrimination while retaining good data utility.
Different from previous works, our approaches can ensure
that the predictive models built from the modified data will
not incur discrimination in decision making. Experiments us-
ing real datasets show the effectiveness of our approaches.

Introduction

Discrimination refers to unjustified distinctions in decisions
against individuals based on their membership in a certain
group. Federal Laws and regulations (e.g., the Equal Credit
Opportunity Act of 1974) have been established to prohibit
discrimination on several grounds, such as gender, age, sex-
ual orientation, race, religion or belief, and disability or ill-
ness, which are referred to as the protected attributes. Nowa-
days various predictive models have been built around the
collection and use of historical data to make important de-
cisions like employment, credit and insurance. If the histor-
ical data contains discrimination, the predictive models are
likely to learn the discriminatory relationship present in the
data and apply it when making new decisions. Therefore, it
is imperative to ensure that the data goes into the predictive
models and the decisions made with its assistance are not
subject to discrimination.

In the legal field, discrimination is usually divided into
direct and indirect discrimination. Direct discrimination oc-
curs when individuals receive less favorable treatment ex-
plicitly based on the protected attributes. An example would
be rejecting a qualified female applicant in applying a uni-
versity just because of her gender. Indirect discrimination

Copyright © 2017, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

refers to the situation where the treatment is based on appar-
ently neutral non-protected attributes but still results in un-
justified distinctions against individuals from the protected
group. A well-known example of indirect discrimination is
redlining, where the residential Zip code of the individual is
used for making decisions such as granting a loan. Although
Zip code is apparently a neutral attribute, it correlates with
race due to the racial makeups of certain areas. Thus, Zip
code can indirectly lead to racial discrimination if there is
no good reason to justify its use in making decisions.

Discrimination discovery and removal has received an in-
creasing attention over the past few years in data science
(Hajian and Domingo-Ferrer 2013; Kamiran and Calders
2012; Ruggieri, Pedreschi, and Turini 2010; Romei and
Ruggieri 2014; Feldman et al. 2015). Many approaches have
been proposed to deal with both direct and indirect discrim-
ination. However, significant issues exist in current tech-
niques. For discrimination discovery, the difference in de-
cisions across the protected and non-protected groups is a
combined (not necessarily linear) effect of direct discrimi-
nation, indirect discrimination, and other effects which are
objectively explainable and should not be considered as dis-
crimination. However, few works have explicitly identified
the three different effects when measuring discrimination.
For example, the classic metrics risk difference, risk ra-
tio, relative chance, odds ratio, etc. (Romei and Ruggieri
2014) treat all the difference in decisions as discrimination.
(Zliobaité, Kamiran, and Calders 2011) considered the ex-
plainable effect, but failed to distinguish the effects of di-
rect and indirect discrimination. For discrimination removal,
an algorithm must ensure that the predictive models built
from the historical data do not incur discrimination in deci-
sion making. However, as we will show in our experiments,
previous works in removing discrimination cannot guaran-
tee that the predictive models are not subject to discrim-
ination even though they attempt to modify the historical
data to contain no discrimination. In addition, it is a general
requirement is to preserve the data utility while achieving
non-discrimination. As will also shown in the experiments,
totally removing all connections between the protected at-
tribute and decision as proposed in (Feldman et al. 2015)
may suffer significant utility loss.

The causal modeling based discrimination detection has
been proposed most recently (Bonchi et al. 2015; Zhang,
Zip_code
~~,

Loan

a—

Race

 

Income

Figure 1: The toy model.

Wu, and Wu 2016b; 2016a) for improving the correlation
based approaches. In this paper, we develop a framework
for discovering and removing both direct and indirect dis-
crimination based on the causal network. A causal network
is a directed acyclic graph (DAG) widely used for causal
representation, reasoning and inference (Pearl 2009), where
causal effects are carried by the paths that trace arrows point-
ing from the cause to the effect which are referred to as the
causal paths. Using this model, direct and indirect discrimi-
nation can be captured by the causal effects of the protected
attribute on the decision transmitted along different paths.
Direct discrimination is modeled by the causal effect trans-
mitted along the direct path from the protected attribute to
the decision. Indirect discrimination, on the other hand, is
modeled by the causal effect transmitted along other causal
paths that contain any unjustified attribute. Consider a toy
model of a loan application system shown in Figure | for ex-
ample. Assume that we treat Race as the protected attribute,
Loan as the decision, and Zip_code as the unjustified at-
tribute that causes redlining. Direct discrimination is then
modeled by path Race —> Loan, and indirect discrimination
is modeled by path Race — Zip_code — Loan. Assume
that the use of Income can be objectively justified as it is
reasonable to deny a loan if the applicant has low income.
In this case, path Race — Income — Loan is explainable,
which means that the difference in loan issuance across dif-
ferent race groups can be explained by the fact that some
trace groups in the dataset tend to be under-paid.

To measure the causal effect transmitted along a certain
causal path, we employ the formulation of the path-specific
effect (Avin, Shpitser, and Pearl 2005; Shpitser 2013). We
define direct and indirect discrimination as different path-
specific effects and show how to accurately measure them
using the observational data. Based on that, we propose an
effective algorithm for discovering direct and indirect dis-
crimination, as well as an algorithm for precisely remov-
ing both types of discrimination while retaining good data
utility. Our approaches can ensure that the predictive mod-
els built from the modified data are not subject to any type
of discrimination. The experiments using two real datasets
show that our approaches are effective in discovering and
removing discrimination.

Preliminary Concepts

A causal network is a DAG G = (V,<A) where V is a set
of nodes and A is a set of arcs. Each node represents an at-
tribute. Each arc, denoted by an arrow — pointing from the
cause to the effect represents the direct causal relationship.
Throughout the paper, we denote an attribute by an upper-
case alphabet, e.g., X; denote a subset of attributes by a bold
uppercase alphabet, e.g., X. We denote a domain value of at-
tribute X by a lowercase alphabet, e.g., x; denote a value as-
signment of attributes X by a bold lowercase alphabet, e.g.,

x. For a node X, its parents are denoted by Pa(X), and its
children are denoted by Ch(X). Each node is associated with
a conditional probability table (CPT), i.e., P(x|Pa(X)). We
also use Pa(X) to represent a value assignment of X’s par-
ents if no unambiguity occurs in the context. The joint dis-
tribution over all attributes P(v) can be computed using the
factorization formula (Koller and Friedman 2009)

Pv) = [| P(v|PalV)), ()

VeV

where P(v|Pa(V)) is the observational distribution.

In the causal network, the measuring of causal effects is
facilitated with the do-calculus (Pearl 2009), which simu-
lates the physical interventions that force some attributes X
to take certain values x. The post-intervention distributions,
which represent the effect of the intervention, can be esti-
mated from the observational data. Formally, the interven-
tion that sets the value of X to x is denoted by do(X = x).
The post-intervention distribution of all other attributes Y =
V\X, ie., P(Y = yldo(X = x)) or simply P(y|do(x)), can be
expressed as a truncated factorization formula (Pearl 2009)

P(yldo(x)) = [| POy|Pa(Y))ox-x,
yey

where dx_, means assigning any attributes in X involved in
the term ahead with the corresponding values in x. Specifi-
cally, the post-intervention distribution of a single attribute
Y given an intervention on a single attribute X is given by

Poldox)y= >) [| POlPaV))oxx,

V\UGYL.Y=y VEV\{X}

where the summation is a marginalization that traverses all
value combinations of V\{X, Y}.

By using the do-calculus, the total causal effect of X on
Y is defined as follows (Pearl 2009). Note that in this defi-
nition, the effect of the intervention is transmitted along all
causal paths from the cause X to the effect Y.

Definition 1 (Total causal effect) The total causal effect of
the change of X from x, to x2. on Y = y is given by

TE(x2, X1) = P(yldo(x2)) — Pydo(x)).

The path-specific effect is an extension to the total causal
effect in the sense that the effect of the intervention is trans-
mitted only along a subset of causal paths from X to Y. De-
note a subset of causal paths by 2. The a-specific effect con-
siders a counterfactual situation where the effect of X on Y
with the intervention is transmitted along 7, while the effect
of X on Y without the intervention is transmitted along paths
not in z. We denote by P(y | do(x|,)) the distribution of Y
after an intervention of changing X from x; to x2 with the
effect transmitted along 7. Then, the z-specific effect of X
on ¥ is defined as follows (Avin, Shpitser, and Pearl 2005).

Definition 2 (Path-specific effect) Given a path set x, the
n-specific effect of the value change of X from x, to x2 on
Y = yis given by

S Ex(x2,%1) = Ply | do(xalz)) — Py | dol).
Ze
me
X—>Z,

 

Y

Figure 2: An example with the recanting witness criterion
satisfied.

The authors in (Avin, Shpitser, and Pearl 2005) have given
the condition under which the path-specific effect can be es-
timated from the observational data, known as the recanting
witness criterion.

Definition 3 (Recanting witness criterion) Given a path
set m, let Z be a node in G such that: 1) there exists a path
from X to Z which is a segment of a path in m; 2) there ex-
ists a path from Z to Y which is a segment of a path in n; 3)
there exists another path from Z to Y which is not a segment
of any path in n. Then, the recanting witness criterion for
the n-specific effect is satisfied with Z as a witness.

Theorem 1 (Identifiability) The 2-specific effect can be es-
timated from the observational data if and only if the recant-
ing witness criterion for the n-specific effect is not satisfied.

If the recanting witness criterion is not satisfied, the z-
specific effect S E(x, x,) can be computed as follows based
on (Shpitser 2013). First, express P(y|do(x,)) as the trun-
cated factorization formula according to Equation (2). Sec-
ond, to compute P(y | do(x2|7)), divide the children of X into
two sets S, and S,, i.e., Ch(X) = S, US,. Let S, contains
X’s each child § where arc X — S is a segment of a path
in z; let S, contains X’s each child S where either S is not
included in any path from C to E, or arc X — S is a seg-
ment of a path not in z. Finally, replace values x; with x2 for
the terms corresponding to nodes in S,, and keep values x;
unchanged for the terms corresponding to nodes in S,.

Note that the above computation requires §, 1S, = @.
Theorem 1 is reflected in that: S, 1S, # 0 if and only if the
recanting witness criterion for the z-specific effect is satis-
fied. Figure 2 shows an example with the recanting witness
criterion satisfied, where m = {X — Z| — Z, — Y}. Accord-
ing to the definitions, Z, is contained in both S, and S,.

Modeling Direct and Indirect Discrimination
as Path-Specific Effects

Consider a historical dataset D that contains a group of
tuples, each of which describes the profile of an individ-
ual. Each tuple is specified by a set of attributes V, in-
cluding the protected attributes, the decision, and the non-
protected attributes. Among the non-protected attributes, as-
sume there is a set of attributes that cannot be objectively
justified if used in the decision making process, which we
refer to as the redlining attributes denoted by R. For ease
of presentation, we assume that there is only one protected
attribute with binary values. We denote the protected at-
tribute by C associated with two domain values c” (e.g.,
female) and c* (e.g., male); denote the decision by FE asso-
ciated with two domain values e™ (i.e., negative decision)
and e* (i.e., positive decision). Our approach can extend
to handling multiple domain values of C and even mul-
tiple Cs. We assume that a causal graph G can be built

to correctly represent the causal structure of dataset D. In
the past decades, many algorithms have been proposed to
learn the causal network from data and they are proved to
be quite successful (Spirtes, Glymour, and Scheines 2000;
Neapolitan and others 2004; Colombo and Maathuis 2014;
Kalisch and Biihlmann 2007). We also make a reasonable
assumption that C has no parent in G, as the protected at-
tribute is always an inherent nature of an individual.

Discrimination can be captured by the causal effect of C
on £. In our context, the causal effect includes direct/indirect
discrimination and other explainable effects. We model di-
rect discrimination as the causal effect transmitted along the
direct path from C to E,ie., C > E. Define zy as the path
set that contains only C > E. The my-specific effect of the
change of C from c” to c* on E = e* is given by

SE, (c*,c°) = Ple* | do(c*|,,)) — P(e* | do(c)).

The physical meaning of SE,,(c*,c”) is the expected
change in decisions (in term of the probability of E = e*)
of individuals from protected group c’, if it is told that these
individuals were from the other group c* and everything else
remains unchanged. When applied to the example in Figure
1, it means the expected change in loan approval of appli-
cations actually from the disadvantage group (e.g., black),
when the bank is instructed to treat the applicants as from
the advantage group (e.g., white). Thus, the measurement of
the myg-specific effect exactly follows the definition of direct
discrimination and is appropriate for measuring the discrim-
inatory effect.

Similarly, we model indirect discrimination as the causal
effect transmitted along all the indirect paths from C to E
that contain the redlining attributes. Given the set of redlin-
ing attributes R, define z; as the path set that contains all the
causal paths from C to E which pass through R, Le., each
of the paths includes at least one node in R. Then, the ;-
specific effect, which is given by

SEx(c",€°) = Pte" | do(e"|,,))— P(e" | do(c’)),

represents the expected change in decisions of individuals
from protected group c’, if the profiles of these individuals
along path 7; were changed as if they were from the other
group c*. When applied to the example in Figure 1, it means
the expected change in loan approval of the disadvantage
group if they had the same racial makeups shown in the Zip
code as the advantage group. Thus, the z;-specific effect is
appropriate for measuring the discriminatory effect of indi-
rect discrimination.

Therefore, we propose the criterion for claiming direct
and indirect discrimination based on the path-specific effect.
We say that direct discrimination against protected group
c” is claimed if SE,,(c*,c") > tT, where t > 0 is a use-
defined threshold for discrimination depending on the law.
For instance, the 1975 British legislation for sex discrimina-
tion sets 7 = 0.05, namely a 5% difference. Similarly, given
the redlining attributes R, we say that indirect discrimination
against protected group c™ is claimed if SE,,(c*, c~) > 1. To
avoid reverse discrimination, we do not specify which group
is the protected group. Therefore, we give the following cri-
terion.
Theorem 2 Given the protected attribute C, decision E, and
redlining attributes R, direct discrimination is claimed if ei-
ther SE,,(c*,c”) > t or SE,,(c",c*) > t holds, and indi-
rect discrimination is claimed if either S E,,(c*,c~) > 7 or
SEx(c”,c*) > t holds.

It is worth noting that, if a path set 7 contains all causal
paths from C to £, it can be directly obtained from the defini-
tion that the 2-specific effect is equivalent to the total causal
effect, i.e.,

SE,(c*,c_) = TE(c*,c_) = P(e*|do(c*)) — P(e*|do(c_)).

It can be proved straightforwardly using Equation (2) that
the above equation equals to P(et|ct) — P(e*|c”), which
is known as risk difference (Romei and Ruggieri 2014)
widely used for discrimination measurement in the anti-
discrimination literature. Therefore, the path-specific effect
can be considered as a significant extension to risk dif-
ference for explicitly distinguishing the discriminatory ef-
fects of direct and indirect discrimination from the total
causal effect. On the other hand, we do not necessarily have
SE, (c*,0) + SEn(c,c7) = SExjurfc*,c”). This implies
that there might not be a linear connection between direct
and indirect discrimination.

According to Definition 3 and Theorem 1, it is guaranteed
that the recanting witness criterion for the z4-specific effect
is not satisfied since there is no intermediate node in the di-
rect path C > EF and 8,, contains £ only, and direct dis-
crimination can always be measured from the observational
data. Thus, S$ E,,, can be computed as follows.

SE,,(c7,¢) = » (Pee'te’, Pa(E\\{C})
VICE}

(3)
[| Potpavyie-c) = Pee'e>).
VeW\{C.E}

For indirect discrimination, we divide C’s children into
S,, and S,,- Different from above, the recanting witness cri-
terion for the 7;-specific effect might be satisfied or not.
When the recanting witness criterion is not satisfied, we have
S;, Sz, = @. Then, S E,,(c*,c~) can be computed as follows.

sEntc'.c)= >) ( [| Posie’, Pa@icy

VIC} GS,

[] Pale Paenicy P] Pepacoysc.)
HeS,, OEV\AC}UCK(C))
— Ple*|c’).

How to deal with the opposite situation will be discussed
later in the next section.

Discrimination Discovery and Removal
Discrimination Discovery

We propose a Path-Specific based Discrimination Discov-
ery (PSE-DD) algorithm based on Theorem 2. It first builds
the causal network from the historical dataset, and then com-
putes SE,, and S E,, according to Equations (3) and (4). The
procedure of the algorithm is shown in Algorithm 1.

 

Algorithm 1: PSE-DD

Input : Historical dataset D, protected attribute C, decision
attribute , user-defined parameter rt.

Output: Judgment of direct and indirect discrimination
judgeg, judge;.

G = buildCausalNetwork(D),

judge = judge; = false;

Compute S'E,,,(-) according to Equation (3);

if SE,,(c*,c°) >T||SE,,(c,c*) > 7 then

| judgea = true;

 

a bw Ne

a

Compute S'E,,,(-) according to Equation (4);
if SE,,(c*,c°) > 7 ||SE,(c°,c*) > 7 then
| judge; = true,

oon

°

return [judgeg, judge;],

 

The computational complexity of PSE-DD depends on
the complexities of building the causal network and comput-
ing the path-specific effect according to Equation (3) or (4).
Many researches have been devoted to improving the per-
formance of network construction (Kalisch and Biihlmann
2007; Tsamardinos et al. 2003; Aliferis et al. 2010) and
probabilistic inference in causal networks (Heckerman and
Breese 1994; 1996). These topics are beyond the scope of
this paper.

For indirect discrimination (line 7), the complexity fur-
ther depends on how to identify S,, and §,,. A straightfor-
ward method of finding all paths in z; may have an exponen-
tial complexity. On the other hand, it can be easily observed
that, anode S$ belongs to S,, if and only if there exists a path
from S$ to £ passing through R (a path from S to F passing
through R also includes the path where S itself belongs to
R). Similarly, § belongs to S,, if and only if there does not
exist a path from S to £ passing through R. It is relatively
easy to check the existence of a path between two nodes.
In our algorithm, we examine the existence of a path from
S to E£ passing through R by checking whether there exists
a node R € R so that R is S’s decedent and EF is R’s dece-
dent. The subroutine of finding S,, and S,, is presented in the
pseudo-code below, where De(-) denotes the decedents of a
node. Since the decedents of all the nodes involved in the
algorithm can be obtained by traversing the network starting
from C within the time of O(|A]), the computational com-
plexity of this procedure is given by O(V/* + |Al).

 

1S,,=6,8,, =9;
2 foreach S € Ch(C)\{E} do
foreach R ¢ Rdo
if R € De(S)U {S} && E € De(R) then
| Sx, = Sx, U IS);
else
LS, = 8, VIS):

a aAwm bw

 

Discrimination Removal

When direct or indirect discrimination is claimed for a
dataset, the discriminatory effects need to be removed before
the dataset is released for predictive analysis (e.g., building
a classifier). A naive approach would be simply not using
the protected attribute when building the predictive model,
which often incur significant utility loss. In addition, this ap-
proach can eliminate direct discrimination, but indirect dis-
crimination still presents.

We propose a Path-Specific Effect based Discrimination
Removal (PSE-DR) algorithm to remove both direct and
indirect discrimination. The general idea is to modify the
causal network and then use it to generate a new dataset.
Specifically, we modify the CPT of E, te., P(e|Pa(E)), to
obtain a new CPT P’(e|Pa(E)), so that the direct and in-
direct discriminatory effects are below the threshold 7. To
maximize the utility of the modified dataset, we minimize
the Euclidean distance between the joint distribution of the
original causal network (denoted by P(v)) and the joint dis-
tribution of the modified causal network (denoted by P’(v)).
As aresult, we obtain the following quadratic programming

problem.
2
minimize » (Pw) - P(v))
Vv
subject to SE, (c*,¢)<t, SE, c,c*) <1,

SE,(ct,¢) <7, SE, (c7,c*) <7,
VPa(E), P’(e"|Pa(E)) + P’(e*|Pa(E)) = 1,
VPa(E),e, Pr'(e|Pa(E)) = 0,

where P’(v) and P(v) are computed according to Equa-
tion (1) using P’(e|Pa(E)) and P(e|Pa(E)) respectively, and
SE,,(-) and S E,,(-) are computed according to Equations (3)
and (4) respectively using P’ (e|Pa(£)). The optimal solution
is obtained by solving the quadratic programming problem.
After that, the joint distribution of the modified causal net-
work is computed using Equation (1), and the new dataset is
generated based on the joint distribution. The procedure of
PSE-DR is shown in Algorithm 2.

 

Algorithm 2: PSE-DR

Input : Historical dataset D, protected attribute C, decision
attribute , use-defined redlining attributes R,
user-defined parameter T.

Output: Modified dataset *.

Obtain the modified CPT of E by solving the quadratic

programming problem;

Calculate P*(v) according to Equation (1) using the modified

CPTs;

Generate D* based on P*(v);

return 2";

 

e

N

hw

 

For discrimination removal, it is crucial to ensure that not
only the modified data does not contain discrimination, but
also the predictive models built on it will not incur biased
decision. The goal of a predictive model is to learn from data
the computational relationship between F and all the other
attributes, which is captured by the CPT of F in the causal
network. In our approach, we modify only the CPT of E
to remove discrimination. Therefore, we can ensure that the
predictive models can learn these modifications and will not
incur discrimination in decision making. We will evaluate
this result in the experiments.

The computational complexity of PSE-DR depends on the
complexity of solving the quadratic programming problem.
It can be easily shown that, the coefficients of the quadratic
terms in the objective function form a positive definite ma-
trix. According to (Kozlov, Tarasov, and Khachiyan 1980),
the quadratic programming can be solved in polynomial
time. Finally, it is also worth noting that our approach can
be easily extended to handle the situation where either di-
rect or indirect discrimination needs to be removed.

Dealing with Unidentifiable Situation

As stated in Theorem 1, when the recanting witness criterion
is satisfied, the ;-specific effect cannot be estimated from
the observational data. However, the structure of the recant-
ing witness criterion implies indirect discrimination as there
exist causal paths from C to E passing through the redlin-
ing attributes. From the data owners’ perspective, they may
want to ensure non-discrimination even though the discrimi-
natory effect cannot be accurately measured. In this case, we
remove discrimination by adapting Algorithm 2 as follows.
Recall that S;, 7 S,, # @ if and only if the recanting witness
criterion is satisfied. For each node S$ € S,, 9 Sis we cut off
all the causal paths from S to E that pass through R, so that
S would not belong to S,, any more. Then, we must have
Sn,9 S,, = Q after the modification. To cut off the paths, we
focus on the arc from F’s each parent Q,ie., Q > E. If
these exists a path from S to Q passing through R, then arc
Q — E is removed from the network. The pseudo-code of
this procedure is shown below, which can be added before
line 1 in Algorithm 2 to deal with this situation.

 

if S,, 0S, # 0 then
foreach S €S,,S,, do
foreach Q € Pa(E) do
foreach R ¢ R do
if Re De(S) && Q € De(R) then
l Remove arc Q > E from G;

Anam Bw ne

Break;

 

Experiments

In this section, we conduct experiments using two real
datasets: the Adult dataset (Lichman 2013) and the Dutch
Census of 2001 (Netherlands 2001). We compare our al-
gorithms with the local massaging (LMSG) and local pref-
erential sampling (LPS) algorithms proposed in (Zliobaité,
Kamiran, and Calders 2011) and disparate impact removal
algorithm (DJ) proposed in (Feldman et al. 2015; Adler et al.
2016). The causal networks are constructed and presented
by utilizing an open-source software TETRAD (Glymour
and others 2004). We employ the original PC algorithm
(Spirtes, Glymour, and Scheines 2000) and set the signifi-
cance threshold 0.01 for conditional independence testing in
causal network construction. The quadratic programming is
solved using CVXOPT (Dahl and Vandenberghe 2006).
 

Figure 3: Causal network for Adult dataset: the green path represents the direct path, and the blue paths represent the indirect

paths passing through marital_status.

Discrimination Discovery

The Adult dataset consists of 65123 tuples with 11 attributes
such as age, education, sex, occupation, income,
marital_status etc. Since the computational complex-
ity of the PC algorithm is an exponential function of the
number of attributes and their domain sizes, for computa-
tional feasibility we binarize each attribute’s domain val-
ues into two classes to reduce the domain sizes. We use
three tiers in the partial order for temporal priority: sex,
age, native_country, race are defined in the first tier,
edu_level and marital_status are defined in the sec-
ond tier, and all other attributes are defined in the third
tier. The causal network is shown in Figure 3. We treat
sex as the protected attribute, income as the decision,
and marital_status as the redlining attribute. The green
path represents the direct path from sex to income, and
the blue paths represent the indirect paths passing through
marital_status. We set the discrimination threshold 7 as
0.05. By computing the path-specific effects, we obtain that
SE,,(c*,c”) = 0.025 and S E,,(c*,c7) = 0.175, which indi-
cate no direct discrimination but significant indirect discrim-
ination against females according to our criterion.

The Dutch dataset consists of 60421 tuples with 12 at-
tributes. Similarly, we binarize the domain values of at-
tribute age due to its large domain size. Three tiers are
used in the partial order for temporal priority: sex, age,
country_birth are defined in the first tire, edu_level and
marital_status are defined in the second tire, and all other
attributes are defined in the third tire. The causal graph is
shown in Figure 4. Similarly we treat sex as the protected at-
tribute, occupation as the decision, and marital_status
as the redlining attribute. For this dataset, SE,,(c*,c") =
0.220 and SE,,(c*,c") = 0.001, indicating significant di-
rect discrimination but no indirect discrimination against fe-
males.

Discrimination Removal

We run the removal algorithm PSE-DR to remove discrim-
ination from the Adult and Dutch datasets. Then, we run
the discovery algorithm PSE-DD to further examine whether
discrimination is truly removed in the modified datasets. For
the modified Adult dataset we have SE,,(c*,c~) = 0.013
and SE,,(ct,c") = 0.049, and for the modified Dutch
dataset we have SE,,(c*,c") = 0.050 and SE,(c*,c7) =
0.001. The results show that the modified datasets contain
no direct and indirect discrimination.

Discrimination in predictive models. We aim to exam-
ine whether the predictive models built from the modified
dataset incur discrimination in decision making. We use
the Adult dataset where indirect discrimination is detected,
and divide the original dataset into the training and test-
ing datasets. First, we remove discrimination from the train-
ing dataset to obtain the modified training dataset. Then,
we build the predictive models from the modified training
dataset, and use them to make predictive decisions over the
testing data. Two classifiers, SVM and Decision Tree, are
used for prediction with five-fold cross-validation. Finally,
we run PSE-DD to examine whether the predictions for the
testing data contain discrimination. We also examine the
data utility (v7) and the prediction accuracy.

The results are shown in Table 1. As shown in the column
“PSE-DD”, both the modified training data and the predic-
tions for the testing data contain no direct and indirect dis-
crimination. In addition, PSE-DD produces relatively small
data utility loss in term of y* and good prediction accu-
racy. For comparison, we include algorithms from previous
works: LMSG, LPS and DI. For LMSG and LPS, discrimina-
tion is not removed even from the training data, and hence
also exists in the predictions. The D/ algorithm provides a
parameter 1 to indicate the amount of discrimination to be
removed, where 2 = 0 represents no modification and 2 = 1
 

Figure 4: Causal network for Dutch dataset: the green path represents the direct path, and the blue paths represent the indirect

paths passing through marital_status.

Table 1: Direct/indirect discriminatory effects in the modi-
fied training data and predictions for the testing data. Values
violating the discrimination criterion are marked in bold.

t
Train : :
(x10")}| 1.620 |7.031]| 1.924 | 1.292
Direct | 0.023 |0.005]-0.124]-0.051
SVM
ccu.

Predict
DTree

 

ccu.

represents full discrimination removal. However, 2 has no
direct connection with the threshold 7. In our experiments,
we execute D/ multiple times with different As and report
the one that is closest to achieve 7 = 0.05. As shown in
the column “D/’”, it indeed removes direct and indirect dis-
crimination from the training data. However, as indicated by
the bold values 0.167/0.168, significant amount of indirect
discrimination exists in the predictions of both classifiers.
In addition, its data utility is far more worse than PSE-DR,
implying that it removes many information unrelated to dis-
crimination.

Related Work

A number of techniques have been proposed to discover dis-
crimination in the literature. Classification rule-based meth-
ods such as elift (Pedreshi, Ruggieri, and Turini 2008) and
belift (Mancuhan and Clifton 2014) were proposed to repre-
sent certain discrimination patterns. (Luong, Ruggieri, and

Turini 2011; Zhang, Wu, and Wu 2016b) dealt with the in-
dividual discrimination by finding a group of similar indi-
viduals. (Zliobaité, Kamiran, and Calders 2011) proposed
conditional discrimination which considers some part of
the discrimination may be explainable by certain attributes.
None of these work explicitly identifies direct discrimi-
nation, indirect discrimination, and explainable effects. In
(Bonchi et al. 2015), the authors proposed a framework
based on the Suppes-Bayes causal network and developed
several random-walk-based methods to detect different types
of discrimination. However, the construction of the Suppes-
Bayes causal network is impractical with the large number
of attribute-value pairs. In addition, it is unclear how the
number of random walks is related to practical discrimina-
tion metrics, e.g., the difference in positive decision rates.

Proposed methods for discrimination removal are either
based on data preprocessing (Kamiran and Calders 2012;
Zliobaité, Kamiran, and Calders 201 1) or algorithm tweak-
ing (Kamiran, Calders, and Pechenizkiy 2010; Calders and
Verwer 2010; Kamishima, Akaho, and Sakuma 2011). In a
recent work (Feldman et al. 2015), the authors first ensure no
direct discrimination by completely removing the protected
attribute C from data, and then modify all the non-protected
attributes to ensure that C cannot be predicted from the non-
protected attributes. As a result, indirect discrimination is re-
moved since the decision £ has no connection with C via the
non-protected attributes. However, as shown in our experi-
ment results, this approach cannot ensure that predictions
made by the classifier built on the modified data do not con-
tain discrimination. In addition, it suffers significant utility
loss as it removes all the connections between C and E.
Conclusions

In this paper, we studied the problem of discovering both
direct and indirect discrimination from the historical data,
and removing the discriminatory effects before performing
predictive analysis. We made use of the causal network to
capture the causal structure of the data, and modeled direct
and indirect discrimination as different path-specific effects.
Based on that, we proposed the discovery algorithm PSE-
DD to discover both direct and indirect discrimination, and
the removal algorithm PSE-DR to remove them. The exper-
iments using real datasets show that, only our approach can
ensure that the predictive models built from the modified
data are not subject to any type of discrimination.

References
Adler, P.; Falk, C.; Friedler, S. A.; Rybeck, G.; Scheidegger,
C.; Smith, B.; and Venkatasubramanian, S. 2016. Auditing
Black-box Models by Obscuring Features.

Aliferis, C. F.; Statnikov, A.; Tsamardinos, I.; Mani, S.; and
Koutsoukos, X. D. 2010. Local causal and markov blanket
induction for causal discovery and feature selection for clas-
sification part i: Algorithms and empirical evaluation. Jour-
nal of Machine Learning Research \1(Jan):171-234.

Avin, C.; Shpitser, I.; and Pearl, J. 2005. Identifiability of
path-specific effects. In LJCAI’05, 357-363.

Bonchi, F.; Hajian, S.; Mishra, B.; and Ramazzotti, D. 2015.
Exposing the probabilistic causal structure of discrimina-
tion. CoRR abs/1510.00552.

Calders, T., and Verwer, S. 2010. Three naive bayes ap-
proaches for discrimination-free classification. Data Mining
and Knowledge Discovery 21(2):277-292.

Colombo, D., and Maathuis, M. H. 2014.  Order-
independent constraint-based causal structure learning.
JMLR 15(1):3741-3782.

Dahl, J., and Vandenberghe, L. 2006. Cvxopt: A python
package for convex optimization. In Proc. eur conf. op. res.

Feldman, M.; Friedler, S. A.; Moeller, J.; Scheidegger, C.;
and Venkatasubramanian, S. 2015. Certifying and removing
disparate impact. In KDD, 259-268. ACM.

Glymour, C., et al. 2004. The TETRAD project. http:
//wow.phil.cmu.edu/tetrad.

Hajian, S., and Domingo-Ferrer, J. 2013. A methodology for
direct and indirect discrimination prevention in data mining.
JKDE 25(7):1445-1459.

Heckerman, D., and Breese, J. S. 1994. A new look at
causal independence. In Proceedings of the Tenth interna-
tional conference on Uncertainty in artificial intelligence,
286-292. Morgan Kaufmann Publishers Inc.

Heckerman, D., and Breese, J. S. 1996. Causal inde-
pendence for probability assessment and inference using
bayesian networks. JEEE Transactions on Systems, Man,
and Cybernetics-Part A: Systems and Humans 26(6):826-
831.

Kalisch, M., and Btihlmann, P. 2007. Estimating high-
dimensional directed acyclic graphs with the pc-algorithm.
The Journal of Machine Learning Research 8:613-636.

Kamiran, F., and Calders, T. 2012. Data preprocessing
techniques for classification without discrimination. KAJS
33(1):1-33.
Kamiran, F.; Calders, T.; and Pechenizkiy, M. 2010. Dis-
crimination aware decision tree learning. In JCDM, 869-
874. IEEE.

Kamishima, T.; Akaho, S.; and Sakuma, J. 2011. Fairness-
aware learning through regularization approach. In JCDMW,
643-650. IEEE.

Koller, D., and Friedman, N. 2009. Probabilistic graphical
models: principles and techniques. MIT press.

Kozlov, M. K.; Tarasov, S. P.; and Khachiyan, L. G. 1980.
The polynomial solvability of convex quadratic program-
ming. USSR Computational Mathematics and Mathematical
Physics 20(5):223-228.

Lichman, M. 2013. UCI machine learning repository. http:
//archive.ics.uci.edu/ml.

Luong, B. T.; Ruggieri, S.; and Turini, F 2011. k-nn as an
implementation of situation testing for discrimination dis-
covery and prevention. In KDD, 502-510. ACM.

Mancuhan, K., and Clifton, C. 2014. Combating discrim-
ination using bayesian networks. Artificial intelligence and
law 22(2):211-238.

Neapolitan, R. E., et al. 2004. Learning bayesian networks,
volume 38. Prentice Hall Upper Saddle River.

Netherlands, S. 2001. Volkstelling. https://sites.
google.com/site/faisalkamiran/.

Pearl, J. 2009. Causality: models, reasoning and inference.
Cambridge university press.

Pedreshi, D.; Ruggieri, S.; and Turini, F 2008.
Discrimination-aware data mining. In KDD, 560-568.
ACM.

Romei, A., and Ruggieri, S. 2014. A multidisciplinary sur-
vey on discrimination analysis. The Knowledge Engineering
Review 29(05):582-638.

Ruggieri, S.; Pedreschi, D.; and Turini, F. 2010. Data mining
for discrimination discovery. TKDD 4(2):9.

Shpitser, I. 2013. Counterfactual graphical models for lon-
gitudinal mediation analysis with unobserved confounding.
Cognitive science 37(6):1011-1035.

Spirtes, P.; Glymour, C. N.; and Scheines, R. 2000. Causa-
tion, prediction, and search, volume 81. MIT press.
Tsamardinos, I.; Aliferis, C. F; Statnikov, A.; and Brown,
L. E. 2003. Scaling-up bayesian network learning to thou-
sands of variables using local learning techniques. Vander-
bilt University DSL TR-03-02.

Zliobaité, .; Kamiran, F.; and Calders, T. 2011. Handling
conditional discrimination. In /CDM, 992-1001. TEEE.
Zhang, L.; Wu, Y.; and Wu, X. 2016a. On discrimination
discovery using causal networks. In Proceedings of SBP-
BRiMS 2016.

Zhang, L.; Wu, Y.; and Wu, X. 2016b. Situation testing-
based discrimination discovery: a causal inference approach.
In Proceedings of IJCAI 2016.
